Student with Knowledge Distillation Approach
Step 0: loss = 51.20523
Training Data Eval:
  Num examples: 49920, Num correct: 5035, Precision @ 1: 0.1009
('Testing Data Eval: EPOCH->', 1)
  Num examples: 9984, Num correct: 1046, Precision @ 1: 0.1048
Step 5: loss = 32.52525
Step 10: loss = 29.42445
Step 15: loss = 19.67978
Step 20: loss = 18.36261
Step 25: loss = 13.69417
Step 30: loss = 9.52470
Step 35: loss = 9.33482
Step 40: loss = 7.58317
Step 45: loss = 5.48153
Step 50: loss = 5.66797
Step 55: loss = 4.95403
Step 60: loss = 6.06398
Step 65: loss = 5.16693
Step 70: loss = 5.27277
Step 75: loss = 6.36966
Step 80: loss = 4.45716
Step 85: loss = 5.01124
Step 90: loss = 5.62998
Step 95: loss = 6.21582
Step 100: loss = 5.71335
Step 105: loss = 7.60492
Step 110: loss = 3.97958
Step 115: loss = 4.12801
Step 120: loss = 6.18508
Step 125: loss = 2.82823
Step 130: loss = 2.81060
Step 135: loss = 4.59253
Step 140: loss = 3.38368
Step 145: loss = 3.90646
Step 150: loss = 3.62642
Step 155: loss = 2.66344
Step 160: loss = 2.78687
Step 165: loss = 3.98480
Step 170: loss = 6.35928
Step 175: loss = 2.53045
Step 180: loss = 4.95178
Step 185: loss = 3.11681
Step 190: loss = 2.50124
Step 195: loss = 3.87482
Step 200: loss = 2.95834
Step 205: loss = 5.35297
Step 210: loss = 5.52618
Step 215: loss = 2.56951
Step 220: loss = 4.17214
Step 225: loss = 2.68450
Step 230: loss = 4.02369
Step 235: loss = 3.03692
Step 240: loss = 4.10317
Step 245: loss = 3.85335
Step 250: loss = 2.74063
Step 255: loss = 4.33516
Step 260: loss = 2.52761
Step 265: loss = 4.45481
Step 270: loss = 3.84361
Step 275: loss = 3.44486
Step 280: loss = 2.56034
Step 285: loss = 2.46987
Step 290: loss = 3.55571
Step 295: loss = 3.55240
Step 300: loss = 3.05286
Step 305: loss = 4.11498
Step 310: loss = 3.84175
Step 315: loss = 2.39412
Step 320: loss = 2.38754
Step 325: loss = 4.07112
Step 330: loss = 2.76715
Step 335: loss = 2.88948
Step 340: loss = 3.85381
Step 345: loss = 3.84675
Step 350: loss = 3.70874
Step 355: loss = 2.35349
Step 360: loss = 3.70661
Step 365: loss = 2.35652
Step 370: loss = 2.88447
Step 375: loss = 3.65275
Step 380: loss = 2.74466
Step 385: loss = 2.43295
Step 390: loss = 4.28521
Training Data Eval:
  Num examples: 49920, Num correct: 9921, Precision @ 1: 0.1987
('Testing Data Eval: EPOCH->', 2)
  Num examples: 9984, Num correct: 1999, Precision @ 1: 0.2002
Step 395: loss = 2.77274
Step 400: loss = 4.71323
Step 405: loss = 2.93619
Step 410: loss = 3.60133
Step 415: loss = 2.92079
Step 420: loss = 2.79061
Step 425: loss = 2.85941
Step 430: loss = 2.61910
Step 435: loss = 3.34540
Step 440: loss = 2.09628
Step 445: loss = 2.00324
Step 450: loss = 4.18471
Step 455: loss = 3.64771
Step 460: loss = 3.30098
Step 465: loss = 2.10617
Step 470: loss = 2.25857
Step 475: loss = 3.50992
Step 480: loss = 2.61600
Step 485: loss = 2.74385
Step 490: loss = 2.43309
Step 495: loss = 2.11525
Step 500: loss = 2.45593
Step 505: loss = 2.04226
Step 510: loss = 1.88461
Step 515: loss = 1.95915
Step 520: loss = 2.35155
Step 525: loss = 1.87964
Step 530: loss = 1.88333
Step 535: loss = 1.79715
Step 540: loss = 2.91170
Step 545: loss = 3.79910
Step 550: loss = 2.36138
Step 555: loss = 2.58508
Step 560: loss = 3.27159
Step 565: loss = 2.68660
Step 570: loss = 2.74768
Step 575: loss = 1.84045
Step 580: loss = 1.97956
Step 585: loss = 4.30544
Step 590: loss = 2.31166
Step 595: loss = 2.64677
Step 600: loss = 2.02373
Step 605: loss = 2.11345
Step 610: loss = 2.00078
Step 615: loss = 1.92026
Step 620: loss = 1.91720
Step 625: loss = 1.92930
Step 630: loss = 3.52966
Step 635: loss = 3.00791
Step 640: loss = 3.71483
Step 645: loss = 2.28712
Step 650: loss = 1.88391
Step 655: loss = 2.94062
Step 660: loss = 2.07750
Step 665: loss = 1.84606
Step 670: loss = 1.88475
Step 675: loss = 3.27385
Step 680: loss = 2.02051
Step 685: loss = 1.92289
Step 690: loss = 1.97505
Step 695: loss = 3.02780
Step 700: loss = 1.84947
Step 705: loss = 2.07698
Step 710: loss = 1.90344
Step 715: loss = 2.33949
Step 720: loss = 1.82064
Step 725: loss = 1.94392
Step 730: loss = 2.02612
Step 735: loss = 2.37396
Step 740: loss = 2.85623
Step 745: loss = 2.05925
Step 750: loss = 2.52858
Step 755: loss = 2.65523
Step 760: loss = 2.09103
Step 765: loss = 2.25213
Step 770: loss = 1.61978
Step 775: loss = 1.81643
Step 780: loss = 2.56319
Training Data Eval:
  Num examples: 49920, Num correct: 15997, Precision @ 1: 0.3205
('Testing Data Eval: EPOCH->', 3)
  Num examples: 9984, Num correct: 3128, Precision @ 1: 0.3133
Step 785: loss = 1.68832
Step 790: loss = 2.28394
Step 795: loss = 2.01641
Step 800: loss = 1.91805
Step 805: loss = 1.69138
Step 810: loss = 2.20426
Step 815: loss = 1.82069
Step 820: loss = 1.71553
Step 825: loss = 1.89964
Step 830: loss = 1.91229
Step 835: loss = 2.19533
Step 840: loss = 1.93648
Step 845: loss = 1.94270
Step 850: loss = 2.36081
Step 855: loss = 2.10578
Step 860: loss = 1.84302
Step 865: loss = 1.67590
Step 870: loss = 3.99494
Step 875: loss = 2.44081
Step 880: loss = 1.84746
Step 885: loss = 1.92929
Step 890: loss = 1.62586
Step 895: loss = 2.44844
Step 900: loss = 1.73514
Step 905: loss = 1.75745
Step 910: loss = 1.71524
Step 915: loss = 1.84255
Step 920: loss = 1.73233
Step 925: loss = 1.81537
Step 930: loss = 1.80891
Step 935: loss = 1.58028
Step 940: loss = 1.75426
Step 945: loss = 1.76659
Step 950: loss = 1.95324
Step 955: loss = 1.97297
Step 960: loss = 1.91146
Step 965: loss = 1.70318
Step 970: loss = 1.69208
Step 975: loss = 2.03252
Step 980: loss = 1.77589
Step 985: loss = 2.28150
Step 990: loss = 1.89354
Step 995: loss = 1.63432
Step 1000: loss = 1.95227
Step 1005: loss = 1.95229
Step 1010: loss = 1.72384
Step 1015: loss = 1.52051
Step 1020: loss = 2.85194
Step 1025: loss = 1.72592
Step 1030: loss = 2.56175
Step 1035: loss = 1.65173
Step 1040: loss = 1.56094
Step 1045: loss = 1.76446
Step 1050: loss = 1.65241
Step 1055: loss = 2.14377
Step 1060: loss = 1.57229
Step 1065: loss = 1.65785
Step 1070: loss = 1.70973
Step 1075: loss = 2.31468
Step 1080: loss = 1.63704
Step 1085: loss = 1.52593
Step 1090: loss = 1.75169
Step 1095: loss = 1.70825
Step 1100: loss = 1.54165
Step 1105: loss = 1.72347
Step 1110: loss = 1.71026
Step 1115: loss = 1.66265
Step 1120: loss = 1.78670
Step 1125: loss = 3.17433
Step 1130: loss = 1.66583
Step 1135: loss = 1.61189
Step 1140: loss = 1.39340
Step 1145: loss = 1.44758
Step 1150: loss = 1.45985
Step 1155: loss = 1.66740
Step 1160: loss = 1.74426
Step 1165: loss = 1.81576
Step 1170: loss = 2.13982
Training Data Eval:
  Num examples: 49920, Num correct: 19597, Precision @ 1: 0.3926
('Testing Data Eval: EPOCH->', 4)
  Num examples: 9984, Num correct: 3595, Precision @ 1: 0.3601
Step 1175: loss = 2.00669
Step 1180: loss = 1.70036
Step 1185: loss = 1.77552
Step 1190: loss = 1.93731
Step 1195: loss = 1.72234
Step 1200: loss = 1.56657
Step 1205: loss = 1.54263
Step 1210: loss = 1.66946
Step 1215: loss = 2.24146
Step 1220: loss = 2.50987
Step 1225: loss = 1.96991
Step 1230: loss = 2.25754
Step 1235: loss = 2.01555
Step 1240: loss = 1.76010
Step 1245: loss = 1.49668
Step 1250: loss = 1.93663
Step 1255: loss = 1.52566
Step 1260: loss = 1.52394
Step 1265: loss = 1.44303
Step 1270: loss = 1.60996
Step 1275: loss = 1.50940
Step 1280: loss = 1.81512
Step 1285: loss = 1.72408
Step 1290: loss = 1.82517
Step 1295: loss = 1.55507
Step 1300: loss = 1.73958
Step 1305: loss = 2.77552
Step 1310: loss = 1.52121
Step 1315: loss = 1.83800
Step 1320: loss = 1.87931
Step 1325: loss = 1.46468
Step 1330: loss = 1.80316
Step 1335: loss = 1.72293
Step 1340: loss = 1.44079
Step 1345: loss = 2.37377
Step 1350: loss = 1.79665
Step 1355: loss = 1.46276
Step 1360: loss = 1.86939
Step 1365: loss = 1.94611
Step 1370: loss = 1.52122
Step 1375: loss = 1.60268
Step 1380: loss = 2.42167
Step 1385: loss = 1.66963
Step 1390: loss = 2.04633
Step 1395: loss = 1.88967
Step 1400: loss = 2.16224
Step 1405: loss = 1.72958
Step 1410: loss = 1.64860
Step 1415: loss = 1.60146
Step 1420: loss = 1.63229
Step 1425: loss = 1.57868
Step 1430: loss = 1.49968
Step 1435: loss = 1.49058
Step 1440: loss = 1.72765
Step 1445: loss = 1.62090
Step 1450: loss = 1.56475
Step 1455: loss = 1.45067
Step 1460: loss = 1.70903
Step 1465: loss = 1.70503
Step 1470: loss = 1.60897
Step 1475: loss = 1.69292
Step 1480: loss = 1.51816
Step 1485: loss = 1.80373
Step 1490: loss = 1.91643
Step 1495: loss = 1.51501
Step 1500: loss = 1.67518
Step 1505: loss = 1.53608
Step 1510: loss = 1.80521
Step 1515: loss = 1.53862
Step 1520: loss = 1.39894
Step 1525: loss = 1.50272
Step 1530: loss = 1.73974
Step 1535: loss = 1.89389
Step 1540: loss = 1.50753
Step 1545: loss = 1.46607
Step 1550: loss = 1.66076
Step 1555: loss = 1.48518
Step 1560: loss = 1.48645
Training Data Eval:
  Num examples: 49920, Num correct: 22600, Precision @ 1: 0.4527
('Testing Data Eval: EPOCH->', 5)
  Num examples: 9984, Num correct: 4276, Precision @ 1: 0.4283
Step 1565: loss = 1.28988
Step 1570: loss = 1.48708
Step 1575: loss = 1.51100
Step 1580: loss = 1.74944
Step 1585: loss = 1.42627
Step 1590: loss = 1.55950
Step 1595: loss = 1.68944
Step 1600: loss = 1.39057
Step 1605: loss = 1.41399
Step 1610: loss = 1.51376
Step 1615: loss = 1.71105
Step 1620: loss = 1.52872
Step 1625: loss = 1.60578
Step 1630: loss = 1.54601
Step 1635: loss = 1.56124
Step 1640: loss = 1.53454
Step 1645: loss = 1.43563
Step 1650: loss = 1.48181
Step 1655: loss = 1.38122
Step 1660: loss = 2.09880
Step 1665: loss = 1.72387
Step 1670: loss = 1.50159
Step 1675: loss = 1.49335
Step 1680: loss = 1.50613
Step 1685: loss = 1.56440
Step 1690: loss = 1.85088
Step 1695: loss = 1.90508
Step 1700: loss = 1.75558
Step 1705: loss = 1.45381
Step 1710: loss = 1.22073
Step 1715: loss = 1.31735
Step 1720: loss = 1.28251
Step 1725: loss = 1.29848
Step 1730: loss = 1.52558
Step 1735: loss = 1.43744
Step 1740: loss = 1.48480
Step 1745: loss = 1.36697
Step 1750: loss = 1.40606
Step 1755: loss = 1.53252
Step 1760: loss = 1.79652
Step 1765: loss = 1.34481
Step 1770: loss = 1.47291
Step 1775: loss = 1.34465
Step 1780: loss = 1.45099
Step 1785: loss = 1.56424
Step 1790: loss = 1.52961
Step 1795: loss = 1.57033
Step 1800: loss = 1.32612
Step 1805: loss = 1.36627
Step 1810: loss = 1.49727
Step 1815: loss = 1.26625
Step 1820: loss = 1.92480
Step 1825: loss = 1.63154
Step 1830: loss = 2.05149
Step 1835: loss = 1.65875
Step 1840: loss = 1.23576
Step 1845: loss = 1.46933
Step 1850: loss = 1.60372
Step 1855: loss = 1.44163
Step 1860: loss = 1.34833
Step 1865: loss = 1.99000
Step 1870: loss = 1.47100
Step 1875: loss = 1.22348
Step 1880: loss = 1.62346
Step 1885: loss = 1.45366
Step 1890: loss = 1.47015
Step 1895: loss = 1.34804
Step 1900: loss = 1.44840
Step 1905: loss = 1.59429
Step 1910: loss = 1.44276
Step 1915: loss = 1.45087
Step 1920: loss = 1.46847
Step 1925: loss = 1.34265
Step 1930: loss = 1.29060
Step 1935: loss = 1.45923
Step 1940: loss = 1.54649
Step 1945: loss = 1.46269
Step 1950: loss = 1.31767
Training Data Eval:
  Num examples: 49920, Num correct: 24527, Precision @ 1: 0.4913
('Testing Data Eval: EPOCH->', 6)
  Num examples: 9984, Num correct: 4507, Precision @ 1: 0.4514
Step 1955: loss = 1.62287
Step 1960: loss = 1.62904
Step 1965: loss = 1.44825
Step 1970: loss = 1.31920
Step 1975: loss = 1.31010
Step 1980: loss = 1.36072
Step 1985: loss = 1.41363
Step 1990: loss = 1.35242
Step 1995: loss = 1.29065
Step 2000: loss = 1.29133
Step 2005: loss = 1.60114
Step 2010: loss = 1.57953
Step 2015: loss = 1.23776
Step 2020: loss = 1.32752
Step 2025: loss = 1.51998
Step 2030: loss = 1.41373
Step 2035: loss = 1.17962
Step 2040: loss = 1.68030
Step 2045: loss = 1.51870
Step 2050: loss = 1.34688
Step 2055: loss = 1.34748
Step 2060: loss = 1.43254
Step 2065: loss = 1.28124
Step 2070: loss = 1.56071
Step 2075: loss = 1.32693
Step 2080: loss = 1.29040
Step 2085: loss = 1.52510
Step 2090: loss = 1.23064
Step 2095: loss = 1.37815
Step 2100: loss = 1.51349
Step 2105: loss = 1.57755
Step 2110: loss = 1.33653
Step 2115: loss = 1.45515
Step 2120: loss = 1.60891
Step 2125: loss = 1.37410
Step 2130: loss = 1.43869
Step 2135: loss = 1.44896
Step 2140: loss = 1.34813
Step 2145: loss = 1.41519
Step 2150: loss = 1.34972
Step 2155: loss = 1.46778
Step 2160: loss = 1.40266
Step 2165: loss = 1.36564
Step 2170: loss = 1.61176
Step 2175: loss = 1.58448
Step 2180: loss = 1.83882
Step 2185: loss = 1.15538
Step 2190: loss = 1.36687
Step 2195: loss = 1.48613
Step 2200: loss = 1.45784
Step 2205: loss = 1.56029
Step 2210: loss = 1.47511
Step 2215: loss = 1.46418
Step 2220: loss = 1.52067
Step 2225: loss = 1.34939
Step 2230: loss = 1.32206
Step 2235: loss = 1.47877
Step 2240: loss = 1.33962
Step 2245: loss = 1.81100
Step 2250: loss = 1.52031
Step 2255: loss = 1.37357
Step 2260: loss = 1.37559
Step 2265: loss = 1.52079
Step 2270: loss = 1.19487
Step 2275: loss = 1.26966
Step 2280: loss = 1.36126
Step 2285: loss = 1.41729
Step 2290: loss = 1.60489
Step 2295: loss = 1.24115
Step 2300: loss = 1.58204
Step 2305: loss = 1.74871
Step 2310: loss = 1.51865
Step 2315: loss = 1.32141
Step 2320: loss = 1.29371
Step 2325: loss = 1.48998
Step 2330: loss = 1.32669
Step 2335: loss = 1.44484
Step 2340: loss = 1.18421
Training Data Eval:
  Num examples: 49920, Num correct: 26219, Precision @ 1: 0.5252
('Testing Data Eval: EPOCH->', 7)
  Num examples: 9984, Num correct: 4840, Precision @ 1: 0.4848
Step 2345: loss = 1.41735
Step 2350: loss = 1.34316
Step 2355: loss = 1.37362
Step 2360: loss = 1.14770
Step 2365: loss = 1.35216
Step 2370: loss = 1.48610
Step 2375: loss = 1.51924
Step 2380: loss = 1.41013
Step 2385: loss = 1.23324
Step 2390: loss = 1.46749
Step 2395: loss = 1.31660
Step 2400: loss = 1.42541
Step 2405: loss = 1.33373
Step 2410: loss = 1.26120
Step 2415: loss = 1.98738
Step 2420: loss = 1.21706
Step 2425: loss = 1.31174
Step 2430: loss = 1.14490
Step 2435: loss = 1.52976
Step 2440: loss = 1.29020
Step 2445: loss = 1.35167
Step 2450: loss = 1.46182
Step 2455: loss = 1.31086
Step 2460: loss = 1.31562
Step 2465: loss = 1.33646
Step 2470: loss = 1.46831
Step 2475: loss = 1.48277
Step 2480: loss = 1.55675
Step 2485: loss = 1.36506
Step 2490: loss = 1.26557
Step 2495: loss = 1.19609
Step 2500: loss = 1.27368
Step 2505: loss = 1.37708
Step 2510: loss = 1.27419
Step 2515: loss = 1.28039
Step 2520: loss = 1.29950
Step 2525: loss = 1.32068
Step 2530: loss = 1.27107
Step 2535: loss = 1.30396
Step 2540: loss = 1.35055
Step 2545: loss = 1.17166
Step 2550: loss = 1.26222
Step 2555: loss = 1.38213
Step 2560: loss = 1.37334
Step 2565: loss = 1.18839
Step 2570: loss = 1.40668
Step 2575: loss = 1.92002
Step 2580: loss = 1.25195
Step 2585: loss = 1.12787
Step 2590: loss = 1.46837
Step 2595: loss = 1.19241
Step 2600: loss = 1.34123
Step 2605: loss = 1.35281
Step 2610: loss = 1.47630
Step 2615: loss = 1.32924
Step 2620: loss = 1.22755
Step 2625: loss = 1.50938
Step 2630: loss = 1.45214
Step 2635: loss = 1.22242
Step 2640: loss = 1.44584
Step 2645: loss = 1.44792
Step 2650: loss = 1.41630
Step 2655: loss = 1.34765
Step 2660: loss = 1.38321
Step 2665: loss = 1.49286
Step 2670: loss = 1.28780
Step 2675: loss = 1.26711
Step 2680: loss = 1.06024
Step 2685: loss = 1.34344
Step 2690: loss = 1.31981
Step 2695: loss = 1.13075
Step 2700: loss = 1.19046
Step 2705: loss = 1.34694
Step 2710: loss = 1.47537
Step 2715: loss = 1.21851
Step 2720: loss = 1.27994
Step 2725: loss = 1.44801
Step 2730: loss = 1.18383
Training Data Eval:
  Num examples: 49920, Num correct: 27511, Precision @ 1: 0.5511
('Testing Data Eval: EPOCH->', 8)
  Num examples: 9984, Num correct: 5104, Precision @ 1: 0.5112
Step 2735: loss = 1.17991
Step 2740: loss = 1.48351
Step 2745: loss = 1.39159
Step 2750: loss = 1.21824
Step 2755: loss = 1.34383
Step 2760: loss = 1.27235
Step 2765: loss = 1.54177
Step 2770: loss = 1.20910
Step 2775: loss = 1.35314
Step 2780: loss = 1.44076
Step 2785: loss = 1.41180
Step 2790: loss = 1.23481
Step 2795: loss = 1.17425
Step 2800: loss = 1.33166
Step 2805: loss = 1.39359
Step 2810: loss = 1.16226
Step 2815: loss = 1.26645
Step 2820: loss = 1.37352
Step 2825: loss = 1.23069
Step 2830: loss = 1.15852
Step 2835: loss = 1.37323
Step 2840: loss = 1.27505
Step 2845: loss = 1.27774
Step 2850: loss = 1.36195
Step 2855: loss = 1.29054
Step 2860: loss = 1.29281
Step 2865: loss = 1.25045
Step 2870: loss = 1.26732
Step 2875: loss = 1.31256
Step 2880: loss = 1.35663
Step 2885: loss = 1.34702
Step 2890: loss = 1.30808
Step 2895: loss = 1.05635
Step 2900: loss = 1.21703
Step 2905: loss = 1.21669
Step 2910: loss = 1.26140
Step 2915: loss = 1.00914
Step 2920: loss = 1.48272
Step 2925: loss = 1.41611
Step 2930: loss = 1.27573
Step 2935: loss = 1.14507
Step 2940: loss = 1.08455
Step 2945: loss = 1.09500
Step 2950: loss = 1.20050
Step 2955: loss = 1.11031
Step 2960: loss = 1.43857
Step 2965: loss = 1.11796
Step 2970: loss = 1.18319
Step 2975: loss = 1.23408
Step 2980: loss = 1.32016
Step 2985: loss = 1.30770
Step 2990: loss = 1.12238
Step 2995: loss = 1.26717
Step 3000: loss = 1.34520
Step 3005: loss = 1.17350
Step 3010: loss = 1.24436
Step 3015: loss = 1.16824
Step 3020: loss = 1.20574
Step 3025: loss = 1.16653
Step 3030: loss = 1.09157
Step 3035: loss = 1.14817
Step 3040: loss = 1.53162
Step 3045: loss = 1.28124
Step 3050: loss = 1.15707
Step 3055: loss = 1.28422
Step 3060: loss = 1.16321
Step 3065: loss = 1.28196
Step 3070: loss = 1.36298
Step 3075: loss = 1.09184
Step 3080: loss = 1.26471
Step 3085: loss = 1.10980
Step 3090: loss = 1.18531
Step 3095: loss = 1.19396
Step 3100: loss = 1.51931
Step 3105: loss = 1.28173
Step 3110: loss = 1.11449
Step 3115: loss = 1.24590
Step 3120: loss = 1.24726
Training Data Eval:
  Num examples: 49920, Num correct: 28897, Precision @ 1: 0.5789
('Testing Data Eval: EPOCH->', 9)
  Num examples: 9984, Num correct: 5248, Precision @ 1: 0.5256
Step 3125: loss = 1.26839
Step 3130: loss = 1.14423
Step 3135: loss = 1.12254
Step 3140: loss = 1.14416
Step 3145: loss = 1.18017
Step 3150: loss = 1.13894
Step 3155: loss = 1.33622
Step 3160: loss = 1.17589
Step 3165: loss = 1.22070
Step 3170: loss = 1.18590
Step 3175: loss = 1.26466
Step 3180: loss = 1.32688
Step 3185: loss = 1.20835
Step 3190: loss = 1.23710
Step 3195: loss = 1.14131
Step 3200: loss = 1.05311
Step 3205: loss = 1.22677
Step 3210: loss = 1.22816
Step 3215: loss = 1.16845
Step 3220: loss = 1.19029
Step 3225: loss = 1.28840
Step 3230: loss = 1.15756
Step 3235: loss = 1.41332
Step 3240: loss = 1.38405
Step 3245: loss = 1.12821
Step 3250: loss = 1.04866
Step 3255: loss = 1.11836
Step 3260: loss = 1.08793
Step 3265: loss = 1.00203
Step 3270: loss = 1.27452
Step 3275: loss = 1.17882
Step 3280: loss = 1.28501
Step 3285: loss = 1.11204
Step 3290: loss = 1.06196
Step 3295: loss = 1.19562
Step 3300: loss = 1.17733
Step 3305: loss = 1.06412
Step 3310: loss = 1.21280
Step 3315: loss = 1.20260
Step 3320: loss = 1.16609
Step 3325: loss = 1.15351
Step 3330: loss = 1.13798
Step 3335: loss = 1.04122
Step 3340: loss = 1.12917
Step 3345: loss = 1.18893
Step 3350: loss = 1.22446
Step 3355: loss = 1.17043
Step 3360: loss = 1.12684
Step 3365: loss = 1.30141
Step 3370: loss = 1.13783
Step 3375: loss = 1.15989
Step 3380: loss = 1.09663
Step 3385: loss = 1.32919
Step 3390: loss = 1.05911
Step 3395: loss = 1.19174
Step 3400: loss = 1.19147
Step 3405: loss = 1.15458
Step 3410: loss = 1.07119
Step 3415: loss = 0.94975
Step 3420: loss = 1.19999
Step 3425: loss = 0.94838
Step 3430: loss = 1.08797
Step 3435: loss = 1.14866
Step 3440: loss = 0.93987
Step 3445: loss = 1.22732
Step 3450: loss = 1.10416
Step 3455: loss = 1.17478
Step 3460: loss = 1.15806
Step 3465: loss = 1.14383
Step 3470: loss = 1.17697
Step 3475: loss = 1.29374
Step 3480: loss = 1.04826
Step 3485: loss = 1.26740
Step 3490: loss = 1.08600
Step 3495: loss = 1.19144
Step 3500: loss = 1.07883
Step 3505: loss = 1.01232
Step 3510: loss = 1.10370
Training Data Eval:
  Num examples: 49920, Num correct: 30170, Precision @ 1: 0.6044
('Testing Data Eval: EPOCH->', 10)
  Num examples: 9984, Num correct: 5475, Precision @ 1: 0.5484
Step 3515: loss = 1.12366
Step 3520: loss = 1.14152
Step 3525: loss = 1.10208
Step 3530: loss = 1.11737
Step 3535: loss = 1.12728
Step 3540: loss = 1.08399
Step 3545: loss = 1.22517
Step 3550: loss = 1.20297
Step 3555: loss = 1.17634
Step 3560: loss = 1.21779
Step 3565: loss = 1.28013
Step 3570: loss = 0.96186
Step 3575: loss = 1.04298
Step 3580: loss = 1.22616
Step 3585: loss = 1.12493
Step 3590: loss = 1.17064
Step 3595: loss = 1.40005
Step 3600: loss = 1.09610
Step 3605: loss = 1.21994
Step 3610: loss = 1.29277
Step 3615: loss = 1.13103
Step 3620: loss = 1.06034
Step 3625: loss = 1.00819
Step 3630: loss = 1.09091
Step 3635: loss = 1.08922
Step 3640: loss = 1.09526
Step 3645: loss = 1.00909
Step 3650: loss = 1.18627
Step 3655: loss = 1.25909
Step 3660: loss = 1.12651
Step 3665: loss = 1.16753
Step 3670: loss = 1.17758
Step 3675: loss = 1.14214
Step 3680: loss = 1.17288
Step 3685: loss = 1.01335
Step 3690: loss = 1.05384
Step 3695: loss = 1.10618
Step 3700: loss = 1.22099
Step 3705: loss = 1.12160
Step 3710: loss = 1.16456
Step 3715: loss = 1.19936
Step 3720: loss = 1.29125
Step 3725: loss = 1.22485
Step 3730: loss = 1.03419
Step 3735: loss = 1.12560
Step 3740: loss = 1.19321
Step 3745: loss = 1.23770
Step 3750: loss = 1.14620
Step 3755: loss = 1.05770
Step 3760: loss = 1.03965
Step 3765: loss = 1.19123
Step 3770: loss = 0.97135
Step 3775: loss = 1.05883
Step 3780: loss = 1.21296
Step 3785: loss = 1.06607
Step 3790: loss = 1.15734
Step 3795: loss = 1.10397
Step 3800: loss = 1.28131
Step 3805: loss = 1.19692
Step 3810: loss = 0.87637
Step 3815: loss = 1.21186
Step 3820: loss = 0.99407
Step 3825: loss = 1.09078
Step 3830: loss = 1.11171
Step 3835: loss = 1.14497
Step 3840: loss = 1.07339
Step 3845: loss = 1.05573
Step 3850: loss = 1.17630
Step 3855: loss = 1.27155
Step 3860: loss = 1.14680
Step 3865: loss = 1.05732
Step 3870: loss = 0.90872
Step 3875: loss = 1.17414
Step 3880: loss = 1.17928
Step 3885: loss = 1.15178
Step 3890: loss = 1.10458
Step 3895: loss = 1.07187
Step 3900: loss = 1.10059
Training Data Eval:
  Num examples: 49920, Num correct: 31364, Precision @ 1: 0.6283
('Testing Data Eval: EPOCH->', 11)
  Num examples: 9984, Num correct: 5648, Precision @ 1: 0.5657
Step 3905: loss = 1.01754
Step 3910: loss = 1.02093
Step 3915: loss = 0.87157
Step 3920: loss = 1.00196
Step 3925: loss = 1.05911
Step 3930: loss = 1.11537
Step 3935: loss = 1.10068
Step 3940: loss = 1.09316
Step 3945: loss = 1.06903
Step 3950: loss = 1.11563
Step 3955: loss = 1.04802
Step 3960: loss = 1.17314
Step 3965: loss = 1.11282
Step 3970: loss = 0.96066
Step 3975: loss = 0.99844
Step 3980: loss = 1.09560
Step 3985: loss = 1.19468
Step 3990: loss = 1.14420
Step 3995: loss = 1.08012
Step 4000: loss = 1.34053
Step 4005: loss = 1.03417
Step 4010: loss = 1.07078
Step 4015: loss = 1.08233
Step 4020: loss = 0.94534
Step 4025: loss = 0.99545
Step 4030: loss = 1.04524
Step 4035: loss = 1.00563
Step 4040: loss = 0.90716
Step 4045: loss = 1.14950
Step 4050: loss = 1.12798
Step 4055: loss = 1.08302
Step 4060: loss = 1.22520
Step 4065: loss = 1.00964
Step 4070: loss = 0.94414
Step 4075: loss = 0.99643
Step 4080: loss = 1.03928
Step 4085: loss = 1.07176
Step 4090: loss = 1.08167
Step 4095: loss = 0.92873
Step 4100: loss = 1.15475
Step 4105: loss = 1.13797
Step 4110: loss = 0.96174
Step 4115: loss = 0.90134
Step 4120: loss = 0.95642
Step 4125: loss = 1.15465
Step 4130: loss = 1.03195
Step 4135: loss = 0.89808
Step 4140: loss = 1.21216
Step 4145: loss = 1.17741
Step 4150: loss = 1.01874
Step 4155: loss = 0.99832
Step 4160: loss = 1.00591
Step 4165: loss = 1.23377
Step 4170: loss = 1.19202
Step 4175: loss = 1.11267
Step 4180: loss = 1.04844
Step 4185: loss = 1.08891
Step 4190: loss = 1.07561
Step 4195: loss = 1.13368
Step 4200: loss = 1.14203
Step 4205: loss = 0.99284
Step 4210: loss = 0.95392
Step 4215: loss = 1.04987
Step 4220: loss = 1.10491
Step 4225: loss = 1.06625
Step 4230: loss = 1.02657
Step 4235: loss = 1.08384
Step 4240: loss = 0.96943
Step 4245: loss = 1.09270
Step 4250: loss = 1.17225
Step 4255: loss = 1.14104
Step 4260: loss = 1.10484
Step 4265: loss = 0.97654
Step 4270: loss = 1.02460
Step 4275: loss = 1.09582
Step 4280: loss = 1.19114
Step 4285: loss = 1.00362
Step 4290: loss = 0.94141
Training Data Eval:
  Num examples: 49920, Num correct: 32023, Precision @ 1: 0.6415
('Testing Data Eval: EPOCH->', 12)
  Num examples: 9984, Num correct: 5542, Precision @ 1: 0.5551
Step 4295: loss = 1.14136
Step 4300: loss = 1.09987
Step 4305: loss = 0.99058
Step 4310: loss = 1.06580
Step 4315: loss = 1.04476
Step 4320: loss = 1.10302
Step 4325: loss = 0.76973
Step 4330: loss = 1.01832
Step 4335: loss = 0.94607
Step 4340: loss = 0.85154
Step 4345: loss = 0.94740
Step 4350: loss = 1.17669
Step 4355: loss = 1.05022
Step 4360: loss = 1.04516
Step 4365: loss = 1.06099
Step 4370: loss = 1.15945
Step 4375: loss = 1.07736
Step 4380: loss = 1.06028
Step 4385: loss = 1.20493
Step 4390: loss = 1.19042
Step 4395: loss = 1.03337
Step 4400: loss = 0.97210
Step 4405: loss = 1.07899
Step 4410: loss = 1.05504
Step 4415: loss = 1.13903
Step 4420: loss = 1.00681
Step 4425: loss = 0.97841
Step 4430: loss = 1.10058
Step 4435: loss = 0.98116
Step 4440: loss = 1.10305
Step 4445: loss = 1.03156
Step 4450: loss = 1.10592
Step 4455: loss = 1.10625
Step 4460: loss = 0.89402
Step 4465: loss = 1.07663
Step 4470: loss = 1.15868
Step 4475: loss = 0.99772
Step 4480: loss = 1.03616
Step 4485: loss = 0.93605
Step 4490: loss = 0.88177
Step 4495: loss = 1.21441
Step 4500: loss = 0.94615
Step 4505: loss = 0.96921
Step 4510: loss = 0.84555
Step 4515: loss = 1.00385
Step 4520: loss = 0.97771
Step 4525: loss = 1.06084
Step 4530: loss = 1.22026
Step 4535: loss = 0.82006
Step 4540: loss = 0.96055
Step 4545: loss = 0.95185
Step 4550: loss = 1.19849
Step 4555: loss = 0.97358
Step 4560: loss = 0.99099
Step 4565: loss = 1.11973
Step 4570: loss = 1.11061
Step 4575: loss = 0.92039
Step 4580: loss = 1.14749
Step 4585: loss = 1.01163
Step 4590: loss = 1.10404
Step 4595: loss = 0.90048
Step 4600: loss = 0.85166
Step 4605: loss = 0.89888
Step 4610: loss = 1.01377
Step 4615: loss = 0.95695
Step 4620: loss = 1.11945
Step 4625: loss = 1.06619
Step 4630: loss = 0.91530
Step 4635: loss = 0.97058
Step 4640: loss = 1.14522
Step 4645: loss = 0.93566
Step 4650: loss = 0.97743
Step 4655: loss = 1.10174
Step 4660: loss = 0.79606
Step 4665: loss = 0.93421
Step 4670: loss = 0.87230
Step 4675: loss = 1.01492
Step 4680: loss = 1.02662
Training Data Eval:
  Num examples: 49920, Num correct: 33020, Precision @ 1: 0.6615
('Testing Data Eval: EPOCH->', 13)
  Num examples: 9984, Num correct: 5809, Precision @ 1: 0.5818
Step 4685: loss = 0.74274
Step 4690: loss = 0.78472
Step 4695: loss = 0.98139
Step 4700: loss = 1.07460
Step 4705: loss = 0.83617
Step 4710: loss = 0.97814
Step 4715: loss = 0.91649
Step 4720: loss = 1.10453
Step 4725: loss = 1.06653
Step 4730: loss = 0.91315
Step 4735: loss = 1.05013
Step 4740: loss = 1.06220
Step 4745: loss = 1.06213
Step 4750: loss = 1.11255
Step 4755: loss = 1.00028
Step 4760: loss = 1.08222
Step 4765: loss = 1.13188
Step 4770: loss = 1.11745
Step 4775: loss = 1.12023
Step 4780: loss = 0.86534
Step 4785: loss = 0.93113
Step 4790: loss = 1.03864
Step 4795: loss = 1.10444
Step 4800: loss = 0.95627
Step 4805: loss = 1.03078
Step 4810: loss = 0.98568
Step 4815: loss = 0.83853
Step 4820: loss = 1.17271
Step 4825: loss = 1.14006
Step 4830: loss = 0.70781
Step 4835: loss = 0.98652
Step 4840: loss = 0.81160
Step 4845: loss = 0.83604
Step 4850: loss = 1.10832
Step 4855: loss = 0.89395
Step 4860: loss = 0.78250
Step 4865: loss = 1.07096
Step 4870: loss = 0.80830
Step 4875: loss = 0.99620
Step 4880: loss = 1.03012
Step 4885: loss = 1.00288
Step 4890: loss = 1.12976
Step 4895: loss = 0.95152
Step 4900: loss = 0.94879
Step 4905: loss = 1.04798
Step 4910: loss = 0.98180
Step 4915: loss = 1.00356
Step 4920: loss = 0.94895
Step 4925: loss = 0.94917
Step 4930: loss = 0.85359
Step 4935: loss = 0.92604
Step 4940: loss = 0.97912
Step 4945: loss = 0.92098
Step 4950: loss = 0.92799
Step 4955: loss = 0.89802
Step 4960: loss = 0.96734
Step 4965: loss = 1.02246
Step 4970: loss = 1.10276
Step 4975: loss = 0.83864
Step 4980: loss = 0.86650
Step 4985: loss = 0.87241
Step 4990: loss = 1.02889
Step 4995: loss = 0.87212
Step 5000: loss = 0.89628
Step 5005: loss = 0.94503
Step 5010: loss = 1.07679
Step 5015: loss = 1.32230
Step 5020: loss = 0.94466
Step 5025: loss = 1.11378
Step 5030: loss = 1.00445
Step 5035: loss = 0.87136
Step 5040: loss = 1.01096
Step 5045: loss = 0.93146
Step 5050: loss = 0.96740
Step 5055: loss = 0.92476
Step 5060: loss = 1.07554
Step 5065: loss = 1.07347
Step 5070: loss = 1.04631
Training Data Eval:
  Num examples: 49920, Num correct: 33411, Precision @ 1: 0.6693
('Testing Data Eval: EPOCH->', 14)
  Num examples: 9984, Num correct: 5909, Precision @ 1: 0.5918
Step 5075: loss = 0.87551
Step 5080: loss = 0.89535
Step 5085: loss = 1.05606
Step 5090: loss = 0.85135
Step 5095: loss = 0.87549
Step 5100: loss = 0.92295
Step 5105: loss = 0.96659
Step 5110: loss = 1.07877
Step 5115: loss = 0.85830
Step 5120: loss = 0.80443
Step 5125: loss = 0.75549
Step 5130: loss = 0.92314
Step 5135: loss = 0.80069
Step 5140: loss = 0.97256
Step 5145: loss = 1.01481
Step 5150: loss = 0.90213
Step 5155: loss = 0.91842
Step 5160: loss = 0.81830
Step 5165: loss = 0.82504
Step 5170: loss = 0.80167
Step 5175: loss = 0.96763
Step 5180: loss = 1.10177
Step 5185: loss = 0.81688
Step 5190: loss = 0.91027
Step 5195: loss = 1.03578
Step 5200: loss = 0.94795
Step 5205: loss = 0.74860
Step 5210: loss = 0.92260
Step 5215: loss = 0.91234
Step 5220: loss = 0.96047
Step 5225: loss = 1.00001
Step 5230: loss = 0.95420
Step 5235: loss = 0.86624
Step 5240: loss = 0.82240
Step 5245: loss = 0.94812
Step 5250: loss = 0.83067
Step 5255: loss = 0.82730
Step 5260: loss = 1.01561
Step 5265: loss = 0.88828
Step 5270: loss = 1.13661
Step 5275: loss = 0.81047
Step 5280: loss = 0.72503
Step 5285: loss = 0.88613
Step 5290: loss = 0.97887
Step 5295: loss = 0.97401
Step 5300: loss = 1.06121
Step 5305: loss = 0.95527
Step 5310: loss = 0.83915
Step 5315: loss = 0.82655
Step 5320: loss = 1.02852
Step 5325: loss = 0.77395
Step 5330: loss = 1.07910
Step 5335: loss = 0.95454
Step 5340: loss = 0.96133
Step 5345: loss = 0.95624
Step 5350: loss = 0.88588
Step 5355: loss = 1.08890
Step 5360: loss = 0.98543
Step 5365: loss = 0.98124
Step 5370: loss = 1.01419
Step 5375: loss = 0.85494
Step 5380: loss = 0.97327
Step 5385: loss = 1.09542
Step 5390: loss = 0.89221
Step 5395: loss = 1.24198
Step 5400: loss = 0.93855
Step 5405: loss = 0.98350
Step 5410: loss = 0.92031
Step 5415: loss = 0.89134
Step 5420: loss = 0.94141
Step 5425: loss = 0.89827
Step 5430: loss = 0.85418
Step 5435: loss = 0.94192
Step 5440: loss = 1.06035
Step 5445: loss = 0.89047
Step 5450: loss = 0.89746
Step 5455: loss = 0.91583
Step 5460: loss = 1.19598
Training Data Eval:
  Num examples: 49920, Num correct: 34933, Precision @ 1: 0.6998
('Testing Data Eval: EPOCH->', 15)
  Num examples: 9984, Num correct: 6163, Precision @ 1: 0.6173
Step 5465: loss = 0.81893
Step 5470: loss = 0.77356
Step 5475: loss = 0.86438
Step 5480: loss = 0.99544
Step 5485: loss = 0.76776
Step 5490: loss = 0.86212
Step 5495: loss = 0.97518
Step 5500: loss = 0.83259
Step 5505: loss = 0.78727
Step 5510: loss = 1.12131
Step 5515: loss = 0.84064
Step 5520: loss = 0.85469
Step 5525: loss = 0.69247
Step 5530: loss = 0.68789
Step 5535: loss = 0.70762
Step 5540: loss = 0.95992
Step 5545: loss = 0.89741
Step 5550: loss = 0.89829
Step 5555: loss = 1.13625
Step 5560: loss = 1.00538
Step 5565: loss = 0.88968
Step 5570: loss = 0.91387
Step 5575: loss = 0.81410
Step 5580: loss = 0.80869
Step 5585: loss = 0.99613
Step 5590: loss = 1.03427
Step 5595: loss = 0.84627
Step 5600: loss = 0.82547
Step 5605: loss = 0.97356
Step 5610: loss = 0.93171
Step 5615: loss = 0.80917
Step 5620: loss = 0.81061
Step 5625: loss = 0.85492
Step 5630: loss = 0.83091
Step 5635: loss = 0.79652
Step 5640: loss = 0.86762
Step 5645: loss = 0.82932
Step 5650: loss = 0.72351
Step 5655: loss = 0.90899
Step 5660: loss = 0.96095
Step 5665: loss = 0.89320
Step 5670: loss = 0.81536
Step 5675: loss = 0.99536
Step 5680: loss = 0.79643
Step 5685: loss = 0.76658
Step 5690: loss = 0.93747
Step 5695: loss = 0.83584
Step 5700: loss = 0.94227
Step 5705: loss = 0.86441
Step 5710: loss = 0.84425
Step 5715: loss = 0.93769
Step 5720: loss = 0.96414
Step 5725: loss = 0.75017
Step 5730: loss = 0.86255
Step 5735: loss = 0.92646
Step 5740: loss = 0.82248
Step 5745: loss = 1.12679
Step 5750: loss = 1.01976
Step 5755: loss = 1.10529
Step 5760: loss = 0.99589
Step 5765: loss = 0.81902
Step 5770: loss = 0.94042
Step 5775: loss = 0.97902
Step 5780: loss = 0.76839
Step 5785: loss = 1.13350
Step 5790: loss = 0.76357
Step 5795: loss = 1.02534
Step 5800: loss = 0.85259
Step 5805: loss = 1.02650
Step 5810: loss = 0.74359
Step 5815: loss = 0.72584
Step 5820: loss = 1.03313
Step 5825: loss = 0.86819
Step 5830: loss = 0.82978
Step 5835: loss = 0.92373
Step 5840: loss = 0.89769
Step 5845: loss = 0.84256
Step 5850: loss = 0.91673
Training Data Eval:
  Num examples: 49920, Num correct: 35453, Precision @ 1: 0.7102
('Testing Data Eval: EPOCH->', 16)
  Num examples: 9984, Num correct: 6331, Precision @ 1: 0.6341
Step 5855: loss = 0.89409
Step 5860: loss = 0.92338
Step 5865: loss = 0.65679
Step 5870: loss = 0.99683
Step 5875: loss = 0.89539
Step 5880: loss = 0.70101
Step 5885: loss = 0.79390
Step 5890: loss = 0.97460
Step 5895: loss = 0.82370
Step 5900: loss = 0.71091
Step 5905: loss = 0.85817
Step 5910: loss = 1.04548
Step 5915: loss = 0.67927
Step 5920: loss = 0.70436
Step 5925: loss = 0.73718
Step 5930: loss = 0.85716
Step 5935: loss = 0.82128
Step 5940: loss = 0.88789
Step 5945: loss = 0.81790
Step 5950: loss = 0.82870
Step 5955: loss = 0.82936
Step 5960: loss = 0.85145
Step 5965: loss = 1.01773
Step 5970: loss = 0.74621
Step 5975: loss = 0.86916
Step 5980: loss = 0.92931
Step 5985: loss = 0.71960
Step 5990: loss = 0.70591
Step 5995: loss = 0.68449
Step 6000: loss = 0.69414
Step 6005: loss = 0.85952
Step 6010: loss = 0.81237
Step 6015: loss = 0.91153
Step 6020: loss = 0.86869
Step 6025: loss = 1.01118
Step 6030: loss = 0.82777
Step 6035: loss = 0.96461
Step 6040: loss = 0.75834
Step 6045: loss = 0.78156
Step 6050: loss = 0.86074
Step 6055: loss = 0.98894
Step 6060: loss = 0.81935
Step 6065: loss = 0.84025
Step 6070: loss = 0.83795
Step 6075: loss = 0.96069
Step 6080: loss = 0.80245
Step 6085: loss = 0.68784
Step 6090: loss = 0.75306
Step 6095: loss = 0.78516
Step 6100: loss = 0.83470
Step 6105: loss = 0.87204
Step 6110: loss = 0.85105
Step 6115: loss = 0.93057
Step 6120: loss = 1.01869
Step 6125: loss = 0.88053
Step 6130: loss = 0.83615
Step 6135: loss = 0.78210
Step 6140: loss = 0.99314
Step 6145: loss = 1.12394
Step 6150: loss = 0.83292
Step 6155: loss = 0.82899
Step 6160: loss = 0.83805
Step 6165: loss = 0.91069
Step 6170: loss = 0.98372
Step 6175: loss = 0.72018
Step 6180: loss = 0.74459
Step 6185: loss = 0.75655
Step 6190: loss = 0.96335
Step 6195: loss = 0.90820
Step 6200: loss = 0.86957
Step 6205: loss = 0.94789
Step 6210: loss = 0.81524
Step 6215: loss = 0.87274
Step 6220: loss = 0.90224
Step 6225: loss = 0.79829
Step 6230: loss = 0.76163
Step 6235: loss = 0.84859
Step 6240: loss = 0.59786
Training Data Eval:
  Num examples: 49920, Num correct: 35973, Precision @ 1: 0.7206
('Testing Data Eval: EPOCH->', 17)
  Num examples: 9984, Num correct: 6314, Precision @ 1: 0.6324
Step 6245: loss = 0.71198
Step 6250: loss = 0.94564
Step 6255: loss = 0.76758
Step 6260: loss = 0.86735
Step 6265: loss = 0.77109
Step 6270: loss = 0.83167
Step 6275: loss = 0.82825
Step 6280: loss = 0.89103
Step 6285: loss = 0.83371
Step 6290: loss = 0.69120
Step 6295: loss = 0.71315
Step 6300: loss = 0.81767
Step 6305: loss = 0.92085
Step 6310: loss = 0.84447
Step 6315: loss = 0.86413
Step 6320: loss = 0.81306
Step 6325: loss = 0.75492
Step 6330: loss = 0.58830
Step 6335: loss = 0.81193
Step 6340: loss = 0.97270
Step 6345: loss = 0.83166
Step 6350: loss = 0.96640
Step 6355: loss = 0.89847
Step 6360: loss = 0.89863
Step 6365: loss = 0.71717
Step 6370: loss = 0.78809
Step 6375: loss = 0.82953
Step 6380: loss = 0.80241
Step 6385: loss = 0.66282
Step 6390: loss = 0.87002
Step 6395: loss = 0.89583
Step 6400: loss = 0.72205
Step 6405: loss = 0.94020
Step 6410: loss = 0.69582
Step 6415: loss = 0.78247
Step 6420: loss = 0.78688
Step 6425: loss = 0.61229
Step 6430: loss = 0.77041
Step 6435: loss = 0.64394
Step 6440: loss = 0.88645
Step 6445: loss = 0.68541
Step 6450: loss = 0.82637
Step 6455: loss = 0.74251
Step 6460: loss = 0.79601
Step 6465: loss = 0.74102
Step 6470: loss = 1.13272
Step 6475: loss = 0.88936
Step 6480: loss = 0.72812
Step 6485: loss = 0.97925
Step 6490: loss = 0.84508
Step 6495: loss = 0.66752
Step 6500: loss = 0.86994
Step 6505: loss = 0.69849
Step 6510: loss = 0.82756
Step 6515: loss = 1.00203
Step 6520: loss = 0.78709
Step 6525: loss = 1.01703
Step 6530: loss = 0.78284
Step 6535: loss = 0.83408
Step 6540: loss = 0.79770
Step 6545: loss = 0.70195
Step 6550: loss = 0.76699
Step 6555: loss = 0.72329
Step 6560: loss = 0.62663
Step 6565: loss = 0.92241
Step 6570: loss = 0.93672
Step 6575: loss = 0.84080
Step 6580: loss = 0.83694
Step 6585: loss = 0.75422
Step 6590: loss = 0.71527
Step 6595: loss = 0.57870
Step 6600: loss = 0.73007
Step 6605: loss = 0.73440
Step 6610: loss = 0.60710
Step 6615: loss = 0.69075
Step 6620: loss = 0.67658
Step 6625: loss = 0.69993
Step 6630: loss = 0.78473
Training Data Eval:
  Num examples: 49920, Num correct: 37380, Precision @ 1: 0.7488
('Testing Data Eval: EPOCH->', 18)
  Num examples: 9984, Num correct: 6625, Precision @ 1: 0.6636
Step 6635: loss = 0.62342
Step 6640: loss = 0.70832
Step 6645: loss = 0.56833
Step 6650: loss = 0.78782
Step 6655: loss = 0.68767
Step 6660: loss = 0.81667
Step 6665: loss = 0.84506
Step 6670: loss = 0.83789
Step 6675: loss = 0.67937
Step 6680: loss = 0.79279
Step 6685: loss = 0.64917
Step 6690: loss = 0.84786
Step 6695: loss = 0.81100
Step 6700: loss = 0.84738
Step 6705: loss = 0.70110
Step 6710: loss = 0.82405
Step 6715: loss = 0.67839
Step 6720: loss = 0.76147
Step 6725: loss = 0.86118
Step 6730: loss = 0.73957
Step 6735: loss = 0.62888
Step 6740: loss = 0.82277
Step 6745: loss = 0.75926
Step 6750: loss = 0.61495
Step 6755: loss = 0.70501
Step 6760: loss = 0.92006
Step 6765: loss = 0.67749
Step 6770: loss = 0.82387
Step 6775: loss = 0.86280
Step 6780: loss = 0.77318
Step 6785: loss = 0.63398
Step 6790: loss = 0.57178
Step 6795: loss = 0.94026
Step 6800: loss = 0.79539
Step 6805: loss = 0.59134
Step 6810: loss = 0.63235
Step 6815: loss = 0.76167
Step 6820: loss = 0.65366
Step 6825: loss = 0.74565
Step 6830: loss = 0.84096
Step 6835: loss = 0.75797
Step 6840: loss = 0.85112
Step 6845: loss = 0.70837
Step 6850: loss = 0.66106
Step 6855: loss = 0.70514
Step 6860: loss = 0.71217
Step 6865: loss = 0.72375
Step 6870: loss = 0.72404
Step 6875: loss = 0.73610
Step 6880: loss = 0.63180
Step 6885: loss = 0.74352
Step 6890: loss = 0.80266
Step 6895: loss = 0.71706
Step 6900: loss = 0.77658
Step 6905: loss = 0.71399
Step 6910: loss = 0.82780
Step 6915: loss = 0.96278
Step 6920: loss = 0.76162
Step 6925: loss = 0.66676
Step 6930: loss = 0.88783
Step 6935: loss = 0.69075
Step 6940: loss = 0.73831
Step 6945: loss = 0.73119
Step 6950: loss = 0.68417
Step 6955: loss = 0.80702
Step 6960: loss = 0.73374
Step 6965: loss = 0.77722
Step 6970: loss = 0.84405
Step 6975: loss = 0.67710
Step 6980: loss = 0.92641
Step 6985: loss = 0.77795
Step 6990: loss = 0.74501
Step 6995: loss = 0.79422
Step 7000: loss = 0.89708
Step 7005: loss = 0.70532
Step 7010: loss = 0.68880
Step 7015: loss = 0.97778
Step 7020: loss = 0.63591
Training Data Eval:
  Num examples: 49920, Num correct: 37982, Precision @ 1: 0.7609
('Testing Data Eval: EPOCH->', 19)
  Num examples: 9984, Num correct: 6673, Precision @ 1: 0.6684
Step 7025: loss = 0.76071
Step 7030: loss = 0.71576
Step 7035: loss = 0.73941
Step 7040: loss = 0.72594
Step 7045: loss = 0.86740
Step 7050: loss = 0.83139
Step 7055: loss = 0.76119
Step 7060: loss = 0.78478
Step 7065: loss = 0.70740
Step 7070: loss = 0.78164
Step 7075: loss = 0.63840
Step 7080: loss = 0.77943
Step 7085: loss = 0.80742
Step 7090: loss = 0.70337
Step 7095: loss = 0.69455
Step 7100: loss = 0.72374
Step 7105: loss = 0.73627
Step 7110: loss = 0.74134
Step 7115: loss = 0.66350
Step 7120: loss = 0.75059
Step 7125: loss = 0.79367
Step 7130: loss = 0.70090
Step 7135: loss = 0.61519
Step 7140: loss = 0.67312
Step 7145: loss = 0.67011
Step 7150: loss = 0.78501
Step 7155: loss = 0.65341
Step 7160: loss = 0.67842
Step 7165: loss = 0.69155
Step 7170: loss = 0.85333
Step 7175: loss = 0.66437
Step 7180: loss = 0.77105
Step 7185: loss = 0.77825
Step 7190: loss = 0.65645
Step 7195: loss = 0.76898
Step 7200: loss = 0.72725
Step 7205: loss = 0.83890
Step 7210: loss = 0.64768
Step 7215: loss = 0.80856
Step 7220: loss = 0.69838
Step 7225: loss = 0.67962
Step 7230: loss = 0.61550
Step 7235: loss = 0.63475
Step 7240: loss = 0.67775
Step 7245: loss = 0.80333
Step 7250: loss = 0.75142
Step 7255: loss = 0.59950
Step 7260: loss = 0.62124
Step 7265: loss = 0.65599
Step 7270: loss = 0.70147
Step 7275: loss = 0.82585
Step 7280: loss = 0.72240
Step 7285: loss = 0.68704
Step 7290: loss = 0.70970
Step 7295: loss = 0.60277
Step 7300: loss = 0.73644
Step 7305: loss = 0.86710
Step 7310: loss = 0.81605
Step 7315: loss = 0.69001
Step 7320: loss = 0.70513
Step 7325: loss = 0.71328
Step 7330: loss = 0.72504
Step 7335: loss = 0.69943
Step 7340: loss = 0.84241
Step 7345: loss = 0.58830
Step 7350: loss = 0.59167
Step 7355: loss = 0.82510
Step 7360: loss = 0.63955
Step 7365: loss = 0.68385
Step 7370: loss = 0.75509
Step 7375: loss = 0.71383
Step 7380: loss = 1.03104
Step 7385: loss = 0.61082
Step 7390: loss = 0.66832
Step 7395: loss = 0.93153
Step 7400: loss = 0.71931
Step 7405: loss = 0.77682
Step 7410: loss = 0.82194
Training Data Eval:
  Num examples: 49920, Num correct: 38870, Precision @ 1: 0.7786
('Testing Data Eval: EPOCH->', 20)
  Num examples: 9984, Num correct: 6789, Precision @ 1: 0.6800
Step 7415: loss = 0.70597
Step 7420: loss = 0.67871
Step 7425: loss = 0.66339
Step 7430: loss = 0.61176
Step 7435: loss = 0.66504
Step 7440: loss = 0.51880
Step 7445: loss = 0.59990
Step 7450: loss = 0.70690
Step 7455: loss = 0.67929
Step 7460: loss = 0.52924
Step 7465: loss = 0.69337
Step 7470: loss = 0.67498
Step 7475: loss = 0.64662
Step 7480: loss = 0.67039
Step 7485: loss = 0.49022
Step 7490: loss = 0.68388
Step 7495: loss = 0.58811
Step 7500: loss = 0.66813
Step 7505: loss = 0.79301
Step 7510: loss = 0.61373
Step 7515: loss = 0.65540
Step 7520: loss = 0.59805
Step 7525: loss = 0.82424
Step 7530: loss = 0.64434
Step 7535: loss = 0.64335
Step 7540: loss = 0.81904
Step 7545: loss = 0.57145
Step 7550: loss = 0.65778
Step 7555: loss = 0.77174
Step 7560: loss = 0.65931
Step 7565: loss = 0.67538
Step 7570: loss = 0.59143
Step 7575: loss = 0.76229
Step 7580: loss = 0.51550
Step 7585: loss = 0.65730
Step 7590: loss = 0.47584
Step 7595: loss = 0.74997
Step 7600: loss = 0.72938
Step 7605: loss = 0.64341
Step 7610: loss = 0.62257
Step 7615: loss = 0.57286
Step 7620: loss = 0.67118
Step 7625: loss = 0.70290
Step 7630: loss = 0.71302
Step 7635: loss = 0.66434
Step 7640: loss = 0.70629
Step 7645: loss = 0.50708
Step 7650: loss = 0.68144
Step 7655: loss = 0.78296
Step 7660: loss = 0.90032
Step 7665: loss = 0.60256
Step 7670: loss = 0.77429
Step 7675: loss = 0.91090
Step 7680: loss = 0.74285
Step 7685: loss = 0.67586
Step 7690: loss = 0.63243
Step 7695: loss = 0.72994
Step 7700: loss = 0.69336
Step 7705: loss = 0.54866
Step 7710: loss = 0.74654
Step 7715: loss = 0.61655
Step 7720: loss = 0.61249
Step 7725: loss = 0.68492
Step 7730: loss = 0.64688
Step 7735: loss = 0.80299
Step 7740: loss = 0.69628
Step 7745: loss = 0.58242
Step 7750: loss = 0.72759
Step 7755: loss = 0.58752
Step 7760: loss = 0.72428
Step 7765: loss = 0.62469
Step 7770: loss = 0.71232
Step 7775: loss = 0.75698
Step 7780: loss = 0.56381
Step 7785: loss = 0.65159
Step 7790: loss = 0.58483
Step 7795: loss = 0.64387
Step 7800: loss = 0.71755
Training Data Eval:
  Num examples: 49920, Num correct: 39236, Precision @ 1: 0.7860
('Testing Data Eval: EPOCH->', 21)
  Num examples: 9984, Num correct: 6774, Precision @ 1: 0.6785
Step 7805: loss = 0.56728
Step 7810: loss = 0.59862
Step 7815: loss = 0.44715
Step 7820: loss = 0.70779
Step 7825: loss = 0.51147
Step 7830: loss = 0.47734
Step 7835: loss = 0.71945
Step 7840: loss = 0.60562
Step 7845: loss = 0.51944
Step 7850: loss = 0.68086
Step 7855: loss = 0.63462
Step 7860: loss = 0.62259
Step 7865: loss = 0.49452
Step 7870: loss = 0.54065
Step 7875: loss = 0.74322
Step 7880: loss = 0.67185
Step 7885: loss = 0.48086
Step 7890: loss = 0.78884
Step 7895: loss = 0.60699
Step 7900: loss = 0.61203
Step 7905: loss = 0.76428
Step 7910: loss = 0.64309
Step 7915: loss = 0.86190
Step 7920: loss = 0.58667
Step 7925: loss = 0.78039
Step 7930: loss = 0.72182
Step 7935: loss = 0.66477
Step 7940: loss = 0.65424
Step 7945: loss = 0.54152
Step 7950: loss = 0.57978
Step 7955: loss = 0.70273
Step 7960: loss = 0.68129
Step 7965: loss = 0.81183
Step 7970: loss = 0.58462
Step 7975: loss = 0.70778
Step 7980: loss = 0.74958
Step 7985: loss = 0.59089
Step 7990: loss = 0.60076
Step 7995: loss = 0.76225
Step 8000: loss = 0.62585
Step 8005: loss = 0.68683
Step 8010: loss = 0.47420
Step 8015: loss = 0.53571
Step 8020: loss = 0.58256
Step 8025: loss = 0.82488
Step 8030: loss = 0.62616
Step 8035: loss = 0.48238
Step 8040: loss = 0.64938
Step 8045: loss = 0.62795
Step 8050: loss = 0.55330
Step 8055: loss = 0.62370
Step 8060: loss = 0.71656
Step 8065: loss = 0.61666
Step 8070: loss = 0.73106
Step 8075: loss = 0.64844
Step 8080: loss = 0.59945
Step 8085: loss = 0.76897
Step 8090: loss = 0.67652
Step 8095: loss = 0.73527
Step 8100: loss = 0.57517
Step 8105: loss = 0.71229
Step 8110: loss = 0.69287
Step 8115: loss = 0.60642
Step 8120: loss = 0.74497
Step 8125: loss = 0.77654
Step 8130: loss = 0.58083
Step 8135: loss = 0.67646
Step 8140: loss = 0.70716
Step 8145: loss = 0.71627
Step 8150: loss = 0.62640
Step 8155: loss = 0.55882
Step 8160: loss = 0.67409
Step 8165: loss = 0.67781
Step 8170: loss = 0.55754
Step 8175: loss = 0.62651
Step 8180: loss = 0.53768
Step 8185: loss = 0.49554
Step 8190: loss = 0.58516
Training Data Eval:
  Num examples: 49920, Num correct: 39760, Precision @ 1: 0.7965
('Testing Data Eval: EPOCH->', 22)
  Num examples: 9984, Num correct: 6870, Precision @ 1: 0.6881
Step 8195: loss = 0.57872
Step 8200: loss = 0.63694
Step 8205: loss = 0.59967
Step 8210: loss = 0.59409
Step 8215: loss = 0.56577
Step 8220: loss = 0.54692
Step 8225: loss = 0.58479
Step 8230: loss = 0.64901
Step 8235: loss = 0.62435
Step 8240: loss = 0.58714
Step 8245: loss = 0.69084
Step 8250: loss = 0.59745
Step 8255: loss = 0.57685
Step 8260: loss = 0.86787
Step 8265: loss = 0.61476
Step 8270: loss = 0.54296
Step 8275: loss = 0.65624
Step 8280: loss = 0.77972
Step 8285: loss = 0.59893
Step 8290: loss = 0.47648
Step 8295: loss = 0.68039
Step 8300: loss = 0.58571
Step 8305: loss = 0.55984
Step 8310: loss = 0.60246
Step 8315: loss = 0.50685
Step 8320: loss = 0.75389
Step 8325: loss = 0.59802
Step 8330: loss = 0.60157
Step 8335: loss = 0.63742
Step 8340: loss = 0.62506
Step 8345: loss = 0.65643
Step 8350: loss = 0.62105
Step 8355: loss = 0.67839
Step 8360: loss = 0.47463
Step 8365: loss = 0.53998
Step 8370: loss = 0.68685
Step 8375: loss = 0.64859
Step 8380: loss = 0.59609
Step 8385: loss = 0.61089
Step 8390: loss = 0.56208
Step 8395: loss = 0.58910
Step 8400: loss = 0.45985
Step 8405: loss = 0.68116
Step 8410: loss = 0.63266
Step 8415: loss = 0.43915
Step 8420: loss = 0.58562
Step 8425: loss = 0.57005
Step 8430: loss = 0.73966
Step 8435: loss = 0.73480
Step 8440: loss = 0.58206
Step 8445: loss = 0.48980
Step 8450: loss = 0.39854
Step 8455: loss = 0.52949
Step 8460: loss = 0.81673
Step 8465: loss = 0.73126
Step 8470: loss = 0.73254
Step 8475: loss = 0.68293
Step 8480: loss = 0.64885
Step 8485: loss = 0.55842
Step 8490: loss = 0.57772
Step 8495: loss = 0.56316
Step 8500: loss = 0.63543
Step 8505: loss = 0.58566
Step 8510: loss = 0.78042
Step 8515: loss = 0.82921
Step 8520: loss = 0.58925
Step 8525: loss = 0.58679
Step 8530: loss = 0.59655
Step 8535: loss = 0.66827
Step 8540: loss = 0.45424
Step 8545: loss = 0.55550
Step 8550: loss = 0.69096
Step 8555: loss = 0.58563
Step 8560: loss = 0.57511
Step 8565: loss = 0.70155
Step 8570: loss = 0.48884
Step 8575: loss = 0.65958
Step 8580: loss = 0.56604
Training Data Eval:
  Num examples: 49920, Num correct: 40030, Precision @ 1: 0.8019
('Testing Data Eval: EPOCH->', 23)
  Num examples: 9984, Num correct: 6833, Precision @ 1: 0.6844
Step 8585: loss = 0.73048
Step 8590: loss = 0.42231
Step 8595: loss = 0.59889
Step 8600: loss = 0.50266
Step 8605: loss = 0.51798
Step 8610: loss = 0.54736
Step 8615: loss = 0.54455
Step 8620: loss = 0.45038
Step 8625: loss = 0.54832
Step 8630: loss = 0.57146
Step 8635: loss = 0.60041
Step 8640: loss = 0.48803
Step 8645: loss = 0.63733
Step 8650: loss = 0.46508
Step 8655: loss = 0.48965
Step 8660: loss = 0.51903
Step 8665: loss = 0.68518
Step 8670: loss = 0.48576
Step 8675: loss = 0.54555
Step 8680: loss = 0.48893
Step 8685: loss = 0.65954
Step 8690: loss = 0.71507
Step 8695: loss = 0.60634
Step 8700: loss = 0.60547
Step 8705: loss = 0.54302
Step 8710: loss = 0.48549
Step 8715: loss = 0.61956
Step 8720: loss = 0.61864
Step 8725: loss = 0.56468
Step 8730: loss = 0.64742
Step 8735: loss = 0.54199
Step 8740: loss = 0.73157
Step 8745: loss = 0.55230
Step 8750: loss = 0.58649
Step 8755: loss = 0.52203
Step 8760: loss = 0.53227
Step 8765: loss = 0.53968
Step 8770: loss = 0.62127
Step 8775: loss = 0.52033
Step 8780: loss = 0.61543
Step 8785: loss = 0.52666
Step 8790: loss = 0.61703
Step 8795: loss = 0.49590
Step 8800: loss = 0.57276
Step 8805: loss = 0.60407
Step 8810: loss = 0.56579
Step 8815: loss = 0.64315
Step 8820: loss = 0.50951
Step 8825: loss = 0.68918
Step 8830: loss = 0.92722
Step 8835: loss = 0.90568
Step 8840: loss = 0.70073
Step 8845: loss = 0.58329
Step 8850: loss = 0.42363
Step 8855: loss = 0.52915
Step 8860: loss = 0.70433
Step 8865: loss = 0.76214
Step 8870: loss = 0.45373
Step 8875: loss = 0.48461
Step 8880: loss = 0.58304
Step 8885: loss = 0.63613
Step 8890: loss = 0.57804
Step 8895: loss = 0.55962
Step 8900: loss = 0.63056
Step 8905: loss = 0.44990
Step 8910: loss = 0.53021
Step 8915: loss = 0.46996
Step 8920: loss = 0.63215
Step 8925: loss = 0.54561
Step 8930: loss = 0.75637
Step 8935: loss = 0.56640
Step 8940: loss = 0.67288
Step 8945: loss = 0.47256
Step 8950: loss = 0.58614
Step 8955: loss = 0.46366
Step 8960: loss = 0.62701
Step 8965: loss = 0.73848
Step 8970: loss = 0.56356
Training Data Eval:
  Num examples: 49920, Num correct: 40440, Precision @ 1: 0.8101
('Testing Data Eval: EPOCH->', 24)
  Num examples: 9984, Num correct: 6852, Precision @ 1: 0.6863
Step 8975: loss = 0.56456
Step 8980: loss = 0.55995
Step 8985: loss = 0.52605
Step 8990: loss = 0.45888
Step 8995: loss = 0.47367
Step 9000: loss = 0.51038
Step 9005: loss = 0.47978
Step 9010: loss = 0.58846
Step 9015: loss = 0.51710
Step 9020: loss = 0.67997
Step 9025: loss = 0.62328
Step 9030: loss = 0.62009
Step 9035: loss = 0.60332
Step 9040: loss = 0.51213
Step 9045: loss = 0.68644
Step 9050: loss = 0.58170
Step 9055: loss = 0.49357
Step 9060: loss = 0.49423
Step 9065: loss = 0.47918
Step 9070: loss = 0.68809
Step 9075: loss = 0.60534
Step 9080: loss = 0.53845
Step 9085: loss = 0.55158
Step 9090: loss = 0.55135
Step 9095: loss = 0.57037
Step 9100: loss = 0.42081
Step 9105: loss = 0.60414
Step 9110: loss = 0.49510
Step 9115: loss = 0.68566
Step 9120: loss = 0.67142
Step 9125: loss = 0.63273
Step 9130: loss = 0.47504
Step 9135: loss = 0.43862
Step 9140: loss = 0.49612
Step 9145: loss = 0.48104
Step 9150: loss = 0.47197
Step 9155: loss = 0.57622
Step 9160: loss = 0.50755
Step 9165: loss = 0.55059
Step 9170: loss = 0.54673
Step 9175: loss = 0.53198
Step 9180: loss = 0.58346
Step 9185: loss = 0.57627
Step 9190: loss = 0.60694
Step 9195: loss = 0.58798
Step 9200: loss = 0.64673
Step 9205: loss = 0.59611
Step 9210: loss = 0.65933
Step 9215: loss = 0.62803
Step 9220: loss = 0.56261
Step 9225: loss = 0.63378
Step 9230: loss = 0.43157
Step 9235: loss = 0.52585
Step 9240: loss = 0.54758
Step 9245: loss = 0.55399
Step 9250: loss = 0.55631
Step 9255: loss = 0.49246
Step 9260: loss = 0.43217
Step 9265: loss = 0.46100
Step 9270: loss = 0.74507
Step 9275: loss = 0.67833
Step 9280: loss = 0.69929
Step 9285: loss = 0.56827
Step 9290: loss = 0.54076
Step 9295: loss = 0.53908
Step 9300: loss = 0.45882
Step 9305: loss = 0.52571
Step 9310: loss = 0.51937
Step 9315: loss = 0.66118
Step 9320: loss = 0.66546
Step 9325: loss = 0.60359
Step 9330: loss = 0.56767
Step 9335: loss = 0.49164
Step 9340: loss = 0.64506
Step 9345: loss = 0.67171
Step 9350: loss = 0.75347
Step 9355: loss = 0.64270
Step 9360: loss = 0.56210
Training Data Eval:
  Num examples: 49920, Num correct: 40938, Precision @ 1: 0.8201
('Testing Data Eval: EPOCH->', 25)
  Num examples: 9984, Num correct: 7025, Precision @ 1: 0.7036
Step 9365: loss = 0.55751
Step 9370: loss = 0.52191
Step 9375: loss = 0.65582
Step 9380: loss = 0.64300
Step 9385: loss = 0.67617
Step 9390: loss = 0.45658
Step 9395: loss = 0.57130
Step 9400: loss = 0.51935
Step 9405: loss = 0.61357
Step 9410: loss = 0.61204
Step 9415: loss = 0.61822
Step 9420: loss = 0.65654
Step 9425: loss = 0.57154
Step 9430: loss = 0.52264
Step 9435: loss = 0.46309
Step 9440: loss = 0.46395
Step 9445: loss = 0.50770
Step 9450: loss = 0.64697
Step 9455: loss = 0.49034
Step 9460: loss = 0.62408
Step 9465: loss = 0.67186
Step 9470: loss = 0.56518
Step 9475: loss = 0.46849
Step 9480: loss = 0.58688
Step 9485: loss = 0.54630
Step 9490: loss = 0.44505
Step 9495: loss = 0.57078
Step 9500: loss = 0.55760
Step 9505: loss = 0.41157
Step 9510: loss = 0.58455
Step 9515: loss = 0.56311
Step 9520: loss = 0.51873
Step 9525: loss = 0.53547
Step 9530: loss = 0.47100
Step 9535: loss = 0.52469
Step 9540: loss = 0.68485
Step 9545: loss = 0.56037
Step 9550: loss = 0.60211
Step 9555: loss = 0.49931
Step 9560: loss = 0.53050
Step 9565: loss = 0.63970
Step 9570: loss = 0.55481
Step 9575: loss = 0.68461
Step 9580: loss = 0.53837
Step 9585: loss = 0.53418
Step 9590: loss = 0.58533
Step 9595: loss = 0.60010
Step 9600: loss = 0.51642
Step 9605: loss = 0.54657
Step 9610: loss = 0.44250
Step 9615: loss = 0.63576
Step 9620: loss = 0.45182
Step 9625: loss = 0.51670
Step 9630: loss = 0.48990
Step 9635: loss = 0.53151
Step 9640: loss = 0.41344
Step 9645: loss = 0.47319
Step 9650: loss = 0.54670
Step 9655: loss = 0.38643
Step 9660: loss = 0.55369
Step 9665: loss = 0.42156
Step 9670: loss = 0.56600
Step 9675: loss = 0.62494
Step 9680: loss = 0.61094
Step 9685: loss = 0.64433
Step 9690: loss = 0.56633
Step 9695: loss = 0.54922
Step 9700: loss = 0.61725
Step 9705: loss = 0.43921
Step 9710: loss = 0.68094
Step 9715: loss = 0.70366
Step 9720: loss = 0.54058
Step 9725: loss = 0.63038
Step 9730: loss = 0.58701
Step 9735: loss = 0.49760
Step 9740: loss = 0.62722
Step 9745: loss = 0.65516
Step 9750: loss = 0.63088
Training Data Eval:
  Num examples: 49920, Num correct: 41454, Precision @ 1: 0.8304
('Testing Data Eval: EPOCH->', 26)
  Num examples: 9984, Num correct: 7022, Precision @ 1: 0.7033
Step 9755: loss = 0.41309
Step 9760: loss = 0.47047
Step 9765: loss = 0.41817
Step 9770: loss = 0.62081
Step 9775: loss = 0.50615
Step 9780: loss = 0.41419
Step 9785: loss = 0.46801
Step 9790: loss = 0.59514
Step 9795: loss = 0.49653
Step 9800: loss = 0.44231
Step 9805: loss = 0.47837
Step 9810: loss = 0.45162
Step 9815: loss = 0.43167
Step 9820: loss = 0.43942
Step 9825: loss = 0.50436
Step 9830: loss = 0.43627
Step 9835: loss = 0.38640
Step 9840: loss = 0.58327
Step 9845: loss = 0.62484
Step 9850: loss = 0.43933
Step 9855: loss = 0.56895
Step 9860: loss = 0.63047
Step 9865: loss = 0.52584
Step 9870: loss = 0.57566
Step 9875: loss = 0.68287
Step 9880: loss = 0.40551
Step 9885: loss = 0.56015
Step 9890: loss = 0.44445
Step 9895: loss = 0.52502
Step 9900: loss = 0.39011
Step 9905: loss = 0.45520
Step 9910: loss = 0.46030
Step 9915: loss = 0.46853
Step 9920: loss = 0.49733
Step 9925: loss = 0.43226
Step 9930: loss = 0.47318
Step 9935: loss = 0.63389
Step 9940: loss = 0.52175
Step 9945: loss = 0.61425
Step 9950: loss = 0.40918
Step 9955: loss = 0.58175
Step 9960: loss = 0.56961
Step 9965: loss = 0.56466
Step 9970: loss = 0.63674
Step 9975: loss = 0.50222
Step 9980: loss = 0.52850
Step 9985: loss = 0.43455
Step 9990: loss = 0.50745
Step 9995: loss = 0.45405
Step 10000: loss = 0.53687
Step 10005: loss = 0.48241
Step 10010: loss = 0.46822
Step 10015: loss = 0.49571
Step 10020: loss = 0.57739
Step 10025: loss = 0.57142
Step 10030: loss = 0.52111
Step 10035: loss = 0.52487
Step 10040: loss = 0.53723
Step 10045: loss = 0.71832
Step 10050: loss = 0.48712
Step 10055: loss = 0.49853
Step 10060: loss = 0.58721
Step 10065: loss = 0.52714
Step 10070: loss = 0.42642
Step 10075: loss = 0.46798
Step 10080: loss = 0.42654
Step 10085: loss = 0.56664
Step 10090: loss = 0.48422
Step 10095: loss = 0.60533
Step 10100: loss = 0.53928
Step 10105: loss = 0.59376
Step 10110: loss = 0.39872
Step 10115: loss = 0.38108
Step 10120: loss = 0.57778
Step 10125: loss = 0.44333
Step 10130: loss = 0.40507
Step 10135: loss = 0.65218
Step 10140: loss = 0.50797
Training Data Eval:
  Num examples: 49920, Num correct: 42320, Precision @ 1: 0.8478
('Testing Data Eval: EPOCH->', 27)
  Num examples: 9984, Num correct: 7132, Precision @ 1: 0.7143
Step 10145: loss = 0.38185
Step 10150: loss = 0.46714
Step 10155: loss = 0.57230
Step 10160: loss = 0.45608
Step 10165: loss = 0.47638
Step 10170: loss = 0.57346
Step 10175: loss = 0.54194
Step 10180: loss = 0.39979
Step 10185: loss = 0.39082
Step 10190: loss = 0.33396
Step 10195: loss = 0.42580
Step 10200: loss = 0.43093
Step 10205: loss = 0.53371
Step 10210: loss = 0.54418
Step 10215: loss = 0.51026
Step 10220: loss = 0.34464
Step 10225: loss = 0.61044
Step 10230: loss = 0.36950
Step 10235: loss = 0.44589
Step 10240: loss = 0.61844
Step 10245: loss = 0.52882
Step 10250: loss = 0.51652
Step 10255: loss = 0.57828
Step 10260: loss = 0.49346
Step 10265: loss = 0.55124
Step 10270: loss = 0.55642
Step 10275: loss = 0.62487
Step 10280: loss = 0.41278
Step 10285: loss = 0.50066
Step 10290: loss = 0.36232
Step 10295: loss = 0.53348
Step 10300: loss = 0.51132
Step 10305: loss = 0.56072
Step 10310: loss = 0.47206
Step 10315: loss = 0.49064
Step 10320: loss = 0.58145
Step 10325: loss = 0.45886
Step 10330: loss = 0.41353
Step 10335: loss = 0.47541
Step 10340: loss = 0.50620
Step 10345: loss = 0.37768
Step 10350: loss = 0.41217
Step 10355: loss = 0.51084
Step 10360: loss = 0.53560
Step 10365: loss = 0.31034
Step 10370: loss = 0.52952
Step 10375: loss = 0.67238
Step 10380: loss = 0.49878
Step 10385: loss = 0.52070
Step 10390: loss = 0.30434
Step 10395: loss = 0.36189
Step 10400: loss = 0.43106
Step 10405: loss = 0.51462
Step 10410: loss = 0.51706
Step 10415: loss = 0.58439
Step 10420: loss = 0.41465
Step 10425: loss = 0.50583
Step 10430: loss = 0.53425
Step 10435: loss = 0.50041
Step 10440: loss = 0.53137
Step 10445: loss = 0.48706
Step 10450: loss = 0.43475
Step 10455: loss = 0.51069
Step 10460: loss = 0.56389
Step 10465: loss = 0.48579
Step 10470: loss = 0.50208
Step 10475: loss = 0.50233
Step 10480: loss = 0.40060
Step 10485: loss = 0.56400
Step 10490: loss = 0.53510
Step 10495: loss = 0.44050
Step 10500: loss = 0.61488
Step 10505: loss = 0.62790
Step 10510: loss = 0.51440
Step 10515: loss = 0.35763
Step 10520: loss = 0.48035
Step 10525: loss = 0.43653
Step 10530: loss = 0.54421
Training Data Eval:
  Num examples: 49920, Num correct: 42750, Precision @ 1: 0.8564
('Testing Data Eval: EPOCH->', 28)
  Num examples: 9984, Num correct: 7102, Precision @ 1: 0.7113
Step 10535: loss = 0.40095
Step 10540: loss = 0.45354
Step 10545: loss = 0.27962
Step 10550: loss = 0.46357
Step 10555: loss = 0.49799
Step 10560: loss = 0.40519
Step 10565: loss = 0.33305
Step 10570: loss = 0.56603
Step 10575: loss = 0.52682
Step 10580: loss = 0.31650
Step 10585: loss = 0.45959
Step 10590: loss = 0.40306
Step 10595: loss = 0.47532
Step 10600: loss = 0.42124
Step 10605: loss = 0.36073
Step 10610: loss = 0.40194
Step 10615: loss = 0.37148
Step 10620: loss = 0.53249
Step 10625: loss = 0.42077
Step 10630: loss = 0.41164
Step 10635: loss = 0.58198
Step 10640: loss = 0.40425
Step 10645: loss = 0.44257
Step 10650: loss = 0.45673
Step 10655: loss = 0.47162
Step 10660: loss = 0.51028
Step 10665: loss = 0.42085
Step 10670: loss = 0.57599
Step 10675: loss = 0.45743
Step 10680: loss = 0.38439
Step 10685: loss = 0.50602
Step 10690: loss = 0.43202
Step 10695: loss = 0.44610
Step 10700: loss = 0.67926
Step 10705: loss = 0.38143
Step 10710: loss = 0.40349
Step 10715: loss = 0.51306
Step 10720: loss = 0.47943
Step 10725: loss = 0.45809
Step 10730: loss = 0.56295
Step 10735: loss = 0.42912
Step 10740: loss = 0.42605
Step 10745: loss = 0.55041
Step 10750: loss = 0.38705
Step 10755: loss = 0.39862
Step 10760: loss = 0.40872
Step 10765: loss = 0.42863
Step 10770: loss = 0.57275
Step 10775: loss = 0.61455
Step 10780: loss = 0.39530
Step 10785: loss = 0.42895
Step 10790: loss = 0.37339
Step 10795: loss = 0.32426
Step 10800: loss = 0.46648
Step 10805: loss = 0.45190
Step 10810: loss = 0.56294
Step 10815: loss = 0.43740
Step 10820: loss = 0.40841
Step 10825: loss = 0.57473
Step 10830: loss = 0.54394
Step 10835: loss = 0.54886
Step 10840: loss = 0.46390
Step 10845: loss = 0.55318
Step 10850: loss = 0.50059
Step 10855: loss = 0.47325
Step 10860: loss = 0.31675
Step 10865: loss = 0.46714
Step 10870: loss = 0.40682
Step 10875: loss = 0.55984
Step 10880: loss = 0.46939
Step 10885: loss = 0.50254
Step 10890: loss = 0.43837
Step 10895: loss = 0.53268
Step 10900: loss = 0.46156
Step 10905: loss = 0.50577
Step 10910: loss = 0.60570
Step 10915: loss = 0.44983
Step 10920: loss = 0.55611
Training Data Eval:
  Num examples: 49920, Num correct: 43039, Precision @ 1: 0.8622
('Testing Data Eval: EPOCH->', 29)
  Num examples: 9984, Num correct: 7193, Precision @ 1: 0.7205
Step 10925: loss = 0.44656
Step 10930: loss = 0.33375
Step 10935: loss = 0.25262
Step 10940: loss = 0.28532
Step 10945: loss = 0.49448
Step 10950: loss = 0.41373
Step 10955: loss = 0.32173
Step 10960: loss = 0.50328
Step 10965: loss = 0.47451
Step 10970: loss = 0.37666
Step 10975: loss = 0.55687
Step 10980: loss = 0.44292
Step 10985: loss = 0.38030
Step 10990: loss = 0.63789
Step 10995: loss = 0.41431
Step 11000: loss = 0.35656
Step 11005: loss = 0.42218
Step 11010: loss = 0.38539
Step 11015: loss = 0.50846
Step 11020: loss = 0.49194
Step 11025: loss = 0.46827
Step 11030: loss = 0.30696
Step 11035: loss = 0.45866
Step 11040: loss = 0.49744
Step 11045: loss = 0.39247
Step 11050: loss = 0.58235
Step 11055: loss = 0.36517
Step 11060: loss = 0.41662
Step 11065: loss = 0.52580
Step 11070: loss = 0.51261
Step 11075: loss = 0.59044
Step 11080: loss = 0.54488
Step 11085: loss = 0.44097
Step 11090: loss = 0.38850
Step 11095: loss = 0.47045
Step 11100: loss = 0.41869
Step 11105: loss = 0.39808
Step 11110: loss = 0.55390
Step 11115: loss = 0.50099
Step 11120: loss = 0.38505
Step 11125: loss = 0.39573
Step 11130: loss = 0.52438
Step 11135: loss = 0.53960
Step 11140: loss = 0.39606
Step 11145: loss = 0.44026
Step 11150: loss = 0.36400
Step 11155: loss = 0.58427
Step 11160: loss = 0.42735
Step 11165: loss = 0.52409
Step 11170: loss = 0.51775
Step 11175: loss = 0.49996
Step 11180: loss = 0.40696
Step 11185: loss = 0.36276
Step 11190: loss = 0.42167
Step 11195: loss = 0.32528
Step 11200: loss = 0.56050
Step 11205: loss = 0.40439
Step 11210: loss = 0.41210
Step 11215: loss = 0.37278
Step 11220: loss = 0.45719
Step 11225: loss = 0.40743
Step 11230: loss = 0.44550
Step 11235: loss = 0.36716
Step 11240: loss = 0.48063
Step 11245: loss = 0.35556
Step 11250: loss = 0.56503
Step 11255: loss = 0.60154
Step 11260: loss = 0.31130
Step 11265: loss = 0.42097
Step 11270: loss = 0.46379
Step 11275: loss = 0.33018
Step 11280: loss = 0.52818
Step 11285: loss = 0.46637
Step 11290: loss = 0.54570
Step 11295: loss = 0.35809
Step 11300: loss = 0.34477
Step 11305: loss = 0.48475
Step 11310: loss = 0.51904
Training Data Eval:
  Num examples: 49920, Num correct: 43524, Precision @ 1: 0.8719
('Testing Data Eval: EPOCH->', 30)
  Num examples: 9984, Num correct: 7167, Precision @ 1: 0.7178
Step 11315: loss = 0.37163
Step 11320: loss = 0.39881
Step 11325: loss = 0.34635
Step 11330: loss = 0.40042
Step 11335: loss = 0.57092
Step 11340: loss = 0.41610
Step 11345: loss = 0.46151
Step 11350: loss = 0.41785
Step 11355: loss = 0.36141
Step 11360: loss = 0.40528
Step 11365: loss = 0.50519
Step 11370: loss = 0.36979
Step 11375: loss = 0.42006
Step 11380: loss = 0.37167
Step 11385: loss = 0.33086
Step 11390: loss = 0.49569
Step 11395: loss = 0.38432
Step 11400: loss = 0.44096
Step 11405: loss = 0.45213
Step 11410: loss = 0.41922
Step 11415: loss = 0.40347
Step 11420: loss = 0.36105
Step 11425: loss = 0.55069
Step 11430: loss = 0.36070
Step 11435: loss = 0.45940
Step 11440: loss = 0.41473
Step 11445: loss = 0.47930
Step 11450: loss = 0.38928
Step 11455: loss = 0.35203
Step 11460: loss = 0.46682
Step 11465: loss = 0.40377
Step 11470: loss = 0.28902
Step 11475: loss = 0.41822
Step 11480: loss = 0.48215
Step 11485: loss = 0.52301
Step 11490: loss = 0.40273
Step 11495: loss = 0.30300
Step 11500: loss = 0.36401
Step 11505: loss = 0.40711
Step 11510: loss = 0.44171
Step 11515: loss = 0.42014
Step 11520: loss = 0.54273
Step 11525: loss = 0.36657
Step 11530: loss = 0.41700
Step 11535: loss = 0.35932
Step 11540: loss = 0.34799
Step 11545: loss = 0.44038
Step 11550: loss = 0.43524
Step 11555: loss = 0.48279
Step 11560: loss = 0.41822
Step 11565: loss = 0.46586
Step 11570: loss = 0.54533
Step 11575: loss = 0.42936
Step 11580: loss = 0.55728
Step 11585: loss = 0.40646
Step 11590: loss = 0.48345
Step 11595: loss = 0.41694
Step 11600: loss = 0.35399
Step 11605: loss = 0.41624
Step 11610: loss = 0.48681
Step 11615: loss = 0.30305
Step 11620: loss = 0.41868
Step 11625: loss = 0.35327
Step 11630: loss = 0.30093
Step 11635: loss = 0.41346
Step 11640: loss = 0.40754
Step 11645: loss = 0.24908
Step 11650: loss = 0.41170
Step 11655: loss = 0.31951
Step 11660: loss = 0.39926
Step 11665: loss = 0.42856
Step 11670: loss = 0.43360
Step 11675: loss = 0.64802
Step 11680: loss = 0.44173
Step 11685: loss = 0.49243
Step 11690: loss = 0.40125
Step 11695: loss = 0.49176
Step 11700: loss = 0.38995
Training Data Eval:
  Num examples: 49920, Num correct: 43528, Precision @ 1: 0.8720
('Testing Data Eval: EPOCH->', 31)
  Num examples: 9984, Num correct: 7152, Precision @ 1: 0.7163
Step 11705: loss = 0.28396
Step 11710: loss = 0.49538
Step 11715: loss = 0.49546
Step 11720: loss = 0.42186
Step 11725: loss = 0.43045
Step 11730: loss = 0.25377
Step 11735: loss = 0.34193
Step 11740: loss = 0.40546
Step 11745: loss = 0.42345
Step 11750: loss = 0.33783
Step 11755: loss = 0.36690
Step 11760: loss = 0.45375
Step 11765: loss = 0.46113
Step 11770: loss = 0.55394
Step 11775: loss = 0.42326
Step 11780: loss = 0.42145
Step 11785: loss = 0.39788
Step 11790: loss = 0.38576
Step 11795: loss = 0.36559
Step 11800: loss = 0.45956
Step 11805: loss = 0.35349
Step 11810: loss = 0.43553
Step 11815: loss = 0.39702
Step 11820: loss = 0.51208
Step 11825: loss = 0.37847
Step 11830: loss = 0.59212
Step 11835: loss = 0.33883
Step 11840: loss = 0.28927
Step 11845: loss = 0.38327
Step 11850: loss = 0.38969
Step 11855: loss = 0.37447
Step 11860: loss = 0.48684
Step 11865: loss = 0.41198
Step 11870: loss = 0.42202
Step 11875: loss = 0.37754
Step 11880: loss = 0.44805
Step 11885: loss = 0.38164
Step 11890: loss = 0.44451
Step 11895: loss = 0.32783
Step 11900: loss = 0.47236
Step 11905: loss = 0.28638
Step 11910: loss = 0.40810
Step 11915: loss = 0.38675
Step 11920: loss = 0.44821
Step 11925: loss = 0.31509
Step 11930: loss = 0.35060
Step 11935: loss = 0.31793
Step 11940: loss = 0.34997
Step 11945: loss = 0.32618
Step 11950: loss = 0.35618
Step 11955: loss = 0.45760
Step 11960: loss = 0.38690
Step 11965: loss = 0.30280
Step 11970: loss = 0.33824
Step 11975: loss = 0.44041
Step 11980: loss = 0.34399
Step 11985: loss = 0.42079
Step 11990: loss = 0.45663
Step 11995: loss = 0.49589
Step 12000: loss = 0.37387
Step 12005: loss = 0.29499
Step 12010: loss = 0.28669
Step 12015: loss = 0.42227
Step 12020: loss = 0.34655
Step 12025: loss = 0.31620
Step 12030: loss = 0.45211
Step 12035: loss = 0.37434
Step 12040: loss = 0.35523
Step 12045: loss = 0.34529
Step 12050: loss = 0.42905
Step 12055: loss = 0.56980
Step 12060: loss = 0.35827
Step 12065: loss = 0.42622
Step 12070: loss = 0.44252
Step 12075: loss = 0.39005
Step 12080: loss = 0.35342
Step 12085: loss = 0.42947
Step 12090: loss = 0.33072
Training Data Eval:
  Num examples: 49920, Num correct: 43980, Precision @ 1: 0.8810
('Testing Data Eval: EPOCH->', 32)
  Num examples: 9984, Num correct: 7187, Precision @ 1: 0.7199
Step 12095: loss = 0.41187
Step 12100: loss = 0.35166
Step 12105: loss = 0.34195
Step 12110: loss = 0.32138
Step 12115: loss = 0.23699
Step 12120: loss = 0.38592
Step 12125: loss = 0.31219
Step 12130: loss = 0.31802
Step 12135: loss = 0.41868
Step 12140: loss = 0.32750
Step 12145: loss = 0.37895
Step 12150: loss = 0.36149
Step 12155: loss = 0.39426
Step 12160: loss = 0.32931
Step 12165: loss = 0.40583
Step 12170: loss = 0.36706
Step 12175: loss = 0.45742
Step 12180: loss = 0.47088
Step 12185: loss = 0.37427
Step 12190: loss = 0.38497
Step 12195: loss = 0.33707
Step 12200: loss = 0.36743
Step 12205: loss = 0.32406
Step 12210: loss = 0.28243
Step 12215: loss = 0.44933
Step 12220: loss = 0.46341
Step 12225: loss = 0.25794
Step 12230: loss = 0.33037
Step 12235: loss = 0.36569
Step 12240: loss = 0.30028
Step 12245: loss = 0.33324
Step 12250: loss = 0.31612
Step 12255: loss = 0.48067
Step 12260: loss = 0.39929
Step 12265: loss = 0.38089
Step 12270: loss = 0.32022
Step 12275: loss = 0.45135
Step 12280: loss = 0.33123
Step 12285: loss = 0.47203
Step 12290: loss = 0.34552
Step 12295: loss = 0.44198
Step 12300: loss = 0.40760
Step 12305: loss = 0.30564
Step 12310: loss = 0.34980
Step 12315: loss = 0.34313
Step 12320: loss = 0.39256
Step 12325: loss = 0.35171
Step 12330: loss = 0.34655
Step 12335: loss = 0.34153
Step 12340: loss = 0.38503
Step 12345: loss = 0.40635
Step 12350: loss = 0.30155
Step 12355: loss = 0.39300
Step 12360: loss = 0.43129
Step 12365: loss = 0.45386
Step 12370: loss = 0.53006
Step 12375: loss = 0.35819
Step 12380: loss = 0.28325
Step 12385: loss = 0.37079
Step 12390: loss = 0.43041
Step 12395: loss = 0.36894
Step 12400: loss = 0.45356
Step 12405: loss = 0.44842
Step 12410: loss = 0.36392
Step 12415: loss = 0.41529
Step 12420: loss = 0.40736
Step 12425: loss = 0.34814
Step 12430: loss = 0.32808
Step 12435: loss = 0.37204
Step 12440: loss = 0.36404
Step 12445: loss = 0.37775
Step 12450: loss = 0.42478
Step 12455: loss = 0.37228
Step 12460: loss = 0.49356
Step 12465: loss = 0.44688
Step 12470: loss = 0.36773
Step 12475: loss = 0.42993
Step 12480: loss = 0.41323
Training Data Eval:
  Num examples: 49920, Num correct: 43780, Precision @ 1: 0.8770
('Testing Data Eval: EPOCH->', 33)
  Num examples: 9984, Num correct: 7107, Precision @ 1: 0.7118
Step 12485: loss = 0.33547
Step 12490: loss = 0.32073
Step 12495: loss = 0.40776
Step 12500: loss = 0.31796
Step 12505: loss = 0.42703
Step 12510: loss = 0.37324
Step 12515: loss = 0.27516
Step 12520: loss = 0.28161
Step 12525: loss = 0.44006
Step 12530: loss = 0.28140
Step 12535: loss = 0.45441
Step 12540: loss = 0.25075
Step 12545: loss = 0.42012
Step 12550: loss = 0.42155
Step 12555: loss = 0.36486
Step 12560: loss = 0.40176
Step 12565: loss = 0.32910
Step 12570: loss = 0.35950
Step 12575: loss = 0.37581
Step 12580: loss = 0.36701
Step 12585: loss = 0.39251
Step 12590: loss = 0.51805
Step 12595: loss = 0.42378
Step 12600: loss = 0.53585
Step 12605: loss = 0.36128
Step 12610: loss = 0.42570
Step 12615: loss = 0.31553
Step 12620: loss = 0.38762
Step 12625: loss = 0.41063
Step 12630: loss = 0.35665
Step 12635: loss = 0.44809
Step 12640: loss = 0.29668
Step 12645: loss = 0.34244
Step 12650: loss = 0.44958
Step 12655: loss = 0.31157
Step 12660: loss = 0.34563
Step 12665: loss = 0.40882
Step 12670: loss = 0.43874
Step 12675: loss = 0.36078
Step 12680: loss = 0.41120
Step 12685: loss = 0.38352
Step 12690: loss = 0.32361
Step 12695: loss = 0.40428
Step 12700: loss = 0.42011
Step 12705: loss = 0.24417
Step 12710: loss = 0.50365
Step 12715: loss = 0.40090
Step 12720: loss = 0.38196
Step 12725: loss = 0.40002
Step 12730: loss = 0.33669
Step 12735: loss = 0.45132
Step 12740: loss = 0.32982
Step 12745: loss = 0.33608
Step 12750: loss = 0.29998
Step 12755: loss = 0.42630
Step 12760: loss = 0.26950
Step 12765: loss = 0.52013
Step 12770: loss = 0.26354
Step 12775: loss = 0.34856
Step 12780: loss = 0.42494
Step 12785: loss = 0.54938
Step 12790: loss = 0.30250
Step 12795: loss = 0.23063
Step 12800: loss = 0.46638
Step 12805: loss = 0.36039
Step 12810: loss = 0.34971
Step 12815: loss = 0.38573
Step 12820: loss = 0.43285
Step 12825: loss = 0.31540
Step 12830: loss = 0.33428
Step 12835: loss = 0.37647
Step 12840: loss = 0.42018
Step 12845: loss = 0.29212
Step 12850: loss = 0.43877
Step 12855: loss = 0.42486
Step 12860: loss = 0.41084
Step 12865: loss = 0.34698
Step 12870: loss = 0.36060
Training Data Eval:
  Num examples: 49920, Num correct: 44253, Precision @ 1: 0.8865
('Testing Data Eval: EPOCH->', 34)
  Num examples: 9984, Num correct: 7112, Precision @ 1: 0.7123
Step 12875: loss = 0.31101
Step 12880: loss = 0.42330
Step 12885: loss = 0.31228
Step 12890: loss = 0.38420
Step 12895: loss = 0.27646
Step 12900: loss = 0.25248
Step 12905: loss = 0.29743
Step 12910: loss = 0.26906
Step 12915: loss = 0.44870
Step 12920: loss = 0.31452
Step 12925: loss = 0.35533
Step 12930: loss = 0.62863
Step 12935: loss = 0.30865
Step 12940: loss = 0.45135
Step 12945: loss = 0.28551
Step 12950: loss = 0.33168
Step 12955: loss = 0.35212
Step 12960: loss = 0.36298
Step 12965: loss = 0.32070
Step 12970: loss = 0.29368
Step 12975: loss = 0.33129
Step 12980: loss = 0.44608
Step 12985: loss = 0.35064
Step 12990: loss = 0.28985
Step 12995: loss = 0.41715
Step 13000: loss = 0.29009
Step 13005: loss = 0.22045
Step 13010: loss = 0.37326
Step 13015: loss = 0.39538
Step 13020: loss = 0.42151
Step 13025: loss = 0.33791
Step 13030: loss = 0.27375
Step 13035: loss = 0.49799
Step 13040: loss = 0.38282
Step 13045: loss = 0.24271
Step 13050: loss = 0.36077
Step 13055: loss = 0.30044
Step 13060: loss = 0.28446
Step 13065: loss = 0.31983
Step 13070: loss = 0.33731
Step 13075: loss = 0.29725
Step 13080: loss = 0.23466
Step 13085: loss = 0.32820
Step 13090: loss = 0.43660
Step 13095: loss = 0.31821
Step 13100: loss = 0.28877
Step 13105: loss = 0.49586
Step 13110: loss = 0.41566
Step 13115: loss = 0.32728
Step 13120: loss = 0.27426
Step 13125: loss = 0.33063
Step 13130: loss = 0.25422
Step 13135: loss = 0.42226
Step 13140: loss = 0.20974
Step 13145: loss = 0.36045
Step 13150: loss = 0.42801
Step 13155: loss = 0.30310
Step 13160: loss = 0.34232
Step 13165: loss = 0.32399
Step 13170: loss = 0.30323
Step 13175: loss = 0.32285
Step 13180: loss = 0.44245
Step 13185: loss = 0.33475
Step 13190: loss = 0.31263
Step 13195: loss = 0.32157
Step 13200: loss = 0.30175
Step 13205: loss = 0.44353
Step 13210: loss = 0.37357
Step 13215: loss = 0.33107
Step 13220: loss = 0.32330
Step 13225: loss = 0.23482
Step 13230: loss = 0.34017
Step 13235: loss = 0.47167
Step 13240: loss = 0.37699
Step 13245: loss = 0.35084
Step 13250: loss = 0.28517
Step 13255: loss = 0.32654
Step 13260: loss = 0.23900
Training Data Eval:
  Num examples: 49920, Num correct: 45185, Precision @ 1: 0.9051
('Testing Data Eval: EPOCH->', 35)
  Num examples: 9984, Num correct: 7209, Precision @ 1: 0.7221
Step 13265: loss = 0.18336
Step 13270: loss = 0.25461
Step 13275: loss = 0.33692
Step 13280: loss = 0.21179
Step 13285: loss = 0.31889
Step 13290: loss = 0.27967
Step 13295: loss = 0.22462
Step 13300: loss = 0.29382
Step 13305: loss = 0.19473
Step 13310: loss = 0.28056
Step 13315: loss = 0.24003
Step 13320: loss = 0.30497
Step 13325: loss = 0.28748
Step 13330: loss = 0.25435
Step 13335: loss = 0.35935
Step 13340: loss = 0.34238
Step 13345: loss = 0.29177
Step 13350: loss = 0.36424
Step 13355: loss = 0.33030
Step 13360: loss = 0.33343
Step 13365: loss = 0.27353
Step 13370: loss = 0.28906
Step 13375: loss = 0.29279
Step 13380: loss = 0.38775
Step 13385: loss = 0.44590
Step 13390: loss = 0.35070
Step 13395: loss = 0.39419
Step 13400: loss = 0.22729
Step 13405: loss = 0.33084
Step 13410: loss = 0.29660
Step 13415: loss = 0.32156
Step 13420: loss = 0.32403
Step 13425: loss = 0.31409
Step 13430: loss = 0.36912
Step 13435: loss = 0.25248
Step 13440: loss = 0.21310
Step 13445: loss = 0.30184
Step 13450: loss = 0.36034
Step 13455: loss = 0.33840
Step 13460: loss = 0.49401
Step 13465: loss = 0.36965
Step 13470: loss = 0.34628
Step 13475: loss = 0.37297
Step 13480: loss = 0.42298
Step 13485: loss = 0.32634
Step 13490: loss = 0.31227
Step 13495: loss = 0.25264
Step 13500: loss = 0.32967
Step 13505: loss = 0.37108
Step 13510: loss = 0.52228
Step 13515: loss = 0.29897
Step 13520: loss = 0.36042
Step 13525: loss = 0.36142
Step 13530: loss = 0.27115
Step 13535: loss = 0.34104
Step 13540: loss = 0.27629
Step 13545: loss = 0.38364
Step 13550: loss = 0.29428
Step 13555: loss = 0.30591
Step 13560: loss = 0.27751
Step 13565: loss = 0.35904
Step 13570: loss = 0.48567
Step 13575: loss = 0.25802
Step 13580: loss = 0.38459
Step 13585: loss = 0.26494
Step 13590: loss = 0.30142
Step 13595: loss = 0.28721
Step 13600: loss = 0.29333
Step 13605: loss = 0.22449
Step 13610: loss = 0.32306
Step 13615: loss = 0.32043
Step 13620: loss = 0.31492
Step 13625: loss = 0.25402
Step 13630: loss = 0.37101
Step 13635: loss = 0.33922
Step 13640: loss = 0.35115
Step 13645: loss = 0.26608
Step 13650: loss = 0.36289
Training Data Eval:
  Num examples: 49920, Num correct: 45161, Precision @ 1: 0.9047
('Testing Data Eval: EPOCH->', 36)
  Num examples: 9984, Num correct: 7220, Precision @ 1: 0.7232
Step 13655: loss = 0.38690
Step 13660: loss = 0.23780
Step 13665: loss = 0.29437
Step 13670: loss = 0.37875
Step 13675: loss = 0.22925
Step 13680: loss = 0.31091
Step 13685: loss = 0.33795
Step 13690: loss = 0.35863
Step 13695: loss = 0.42487
Step 13700: loss = 0.28867
Step 13705: loss = 0.16204
Step 13710: loss = 0.36483
Step 13715: loss = 0.31387
Step 13720: loss = 0.21388
Step 13725: loss = 0.28516
Step 13730: loss = 0.32189
Step 13735: loss = 0.35951
Step 13740: loss = 0.18875
Step 13745: loss = 0.27559
Step 13750: loss = 0.30059
Step 13755: loss = 0.29479
Step 13760: loss = 0.28516
Step 13765: loss = 0.26539
Step 13770: loss = 0.31287
Step 13775: loss = 0.18520
Step 13780: loss = 0.31882
Step 13785: loss = 0.38081
Step 13790: loss = 0.20049
Step 13795: loss = 0.31019
Step 13800: loss = 0.27582
Step 13805: loss = 0.29593
Step 13810: loss = 0.28140
Step 13815: loss = 0.28366
Step 13820: loss = 0.41816
Step 13825: loss = 0.33764
Step 13830: loss = 0.46190
Step 13835: loss = 0.33588
Step 13840: loss = 0.30158
Step 13845: loss = 0.37199
Step 13850: loss = 0.33955
Step 13855: loss = 0.31134
Step 13860: loss = 0.35647
Step 13865: loss = 0.31421
Step 13870: loss = 0.32479
Step 13875: loss = 0.25981
Step 13880: loss = 0.28121
Step 13885: loss = 0.33611
Step 13890: loss = 0.37719
Step 13895: loss = 0.35484
Step 13900: loss = 0.27618
Step 13905: loss = 0.30008
Step 13910: loss = 0.22267
Step 13915: loss = 0.27998
Step 13920: loss = 0.30009
Step 13925: loss = 0.26556
Step 13930: loss = 0.27356
Step 13935: loss = 0.28257
Step 13940: loss = 0.26202
Step 13945: loss = 0.23416
Step 13950: loss = 0.22193
Step 13955: loss = 0.25829
Step 13960: loss = 0.20809
Step 13965: loss = 0.29537
Step 13970: loss = 0.25888
Step 13975: loss = 0.20105
Step 13980: loss = 0.45970
Step 13985: loss = 0.23078
Step 13990: loss = 0.29196
Step 13995: loss = 0.35188
Step 14000: loss = 0.32811
Step 14005: loss = 0.27286
Step 14010: loss = 0.39375
Step 14015: loss = 0.37432
Step 14020: loss = 0.30441
Step 14025: loss = 0.30292
Step 14030: loss = 0.30342
Step 14035: loss = 0.31366
Step 14040: loss = 0.25043
Training Data Eval:
  Num examples: 49920, Num correct: 45792, Precision @ 1: 0.9173
('Testing Data Eval: EPOCH->', 37)
  Num examples: 9984, Num correct: 7248, Precision @ 1: 0.7260
Step 14045: loss = 0.26453
Step 14050: loss = 0.22587
Step 14055: loss = 0.37955
Step 14060: loss = 0.34093
Step 14065: loss = 0.21456
Step 14070: loss = 0.24545
Step 14075: loss = 0.39608
Step 14080: loss = 0.27194
Step 14085: loss = 0.39170
Step 14090: loss = 0.19771
Step 14095: loss = 0.29944
Step 14100: loss = 0.22921
Step 14105: loss = 0.22031
Step 14110: loss = 0.40643
Step 14115: loss = 0.30150
Step 14120: loss = 0.36683
Step 14125: loss = 0.20293
Step 14130: loss = 0.20416
Step 14135: loss = 0.24840
Step 14140: loss = 0.25705
Step 14145: loss = 0.41039
Step 14150: loss = 0.31835
Step 14155: loss = 0.26276
Step 14160: loss = 0.31346
Step 14165: loss = 0.21292
Step 14170: loss = 0.36465
Step 14175: loss = 0.36903
Step 14180: loss = 0.40979
Step 14185: loss = 0.26819
Step 14190: loss = 0.35545
Step 14195: loss = 0.32644
Step 14200: loss = 0.31340
Step 14205: loss = 0.29920
Step 14210: loss = 0.29615
Step 14215: loss = 0.27309
Step 14220: loss = 0.45241
Step 14225: loss = 0.29667
Step 14230: loss = 0.31919
Step 14235: loss = 0.23516
Step 14240: loss = 0.19899
Step 14245: loss = 0.38127
Step 14250: loss = 0.29413
Step 14255: loss = 0.36276
Step 14260: loss = 0.26571
Step 14265: loss = 0.28495
Step 14270: loss = 0.36775
Step 14275: loss = 0.38317
Step 14280: loss = 0.39206
Step 14285: loss = 0.21027
Step 14290: loss = 0.20772
Step 14295: loss = 0.26179
Step 14300: loss = 0.40571
Step 14305: loss = 0.32110
Step 14310: loss = 0.29661
Step 14315: loss = 0.30987
Step 14320: loss = 0.24757
Step 14325: loss = 0.47529
Step 14330: loss = 0.35797
Step 14335: loss = 0.30760
Step 14340: loss = 0.30987
Step 14345: loss = 0.22629
Step 14350: loss = 0.28405
Step 14355: loss = 0.25249
Step 14360: loss = 0.28864
Step 14365: loss = 0.40648
Step 14370: loss = 0.34050
Step 14375: loss = 0.37131
Step 14380: loss = 0.35320
Step 14385: loss = 0.33149
Step 14390: loss = 0.34153
Step 14395: loss = 0.29296
Step 14400: loss = 0.31665
Step 14405: loss = 0.42373
Step 14410: loss = 0.34289
Step 14415: loss = 0.18172
Step 14420: loss = 0.35729
Step 14425: loss = 0.36787
Step 14430: loss = 0.27773
Training Data Eval:
  Num examples: 49920, Num correct: 45710, Precision @ 1: 0.9157
('Testing Data Eval: EPOCH->', 38)
  Num examples: 9984, Num correct: 7247, Precision @ 1: 0.7259
Step 14435: loss = 0.24710
Step 14440: loss = 0.23780
Step 14445: loss = 0.23432
Step 14450: loss = 0.27800
Step 14455: loss = 0.23991
Step 14460: loss = 0.26423
Step 14465: loss = 0.21933
Step 14470: loss = 0.20903
Step 14475: loss = 0.30587
Step 14480: loss = 0.29035
Step 14485: loss = 0.22708
Step 14490: loss = 0.21657
Step 14495: loss = 0.25977
Step 14500: loss = 0.22512
Step 14505: loss = 0.27543
Step 14510: loss = 0.25579
Step 14515: loss = 0.30318
Step 14520: loss = 0.20685
Step 14525: loss = 0.28090
Step 14530: loss = 0.27920
Step 14535: loss = 0.29530
Step 14540: loss = 0.27121
Step 14545: loss = 0.30028
Step 14550: loss = 0.22135
Step 14555: loss = 0.23351
Step 14560: loss = 0.25409
Step 14565: loss = 0.24394
Step 14570: loss = 0.22274
Step 14575: loss = 0.23447
Step 14580: loss = 0.26890
Step 14585: loss = 0.32538
Step 14590: loss = 0.29597
Step 14595: loss = 0.22706
Step 14600: loss = 0.37896
Step 14605: loss = 0.22483
Step 14610: loss = 0.35604
Step 14615: loss = 0.25024
Step 14620: loss = 0.18896
Step 14625: loss = 0.24228
Step 14630: loss = 0.19860
Step 14635: loss = 0.33386
Step 14640: loss = 0.34553
Step 14645: loss = 0.30554
Step 14650: loss = 0.34898
Step 14655: loss = 0.31717
Step 14660: loss = 0.31856
Step 14665: loss = 0.26485
Step 14670: loss = 0.26960
Step 14675: loss = 0.26826
Step 14680: loss = 0.28091
Step 14685: loss = 0.32882
Step 14690: loss = 0.18659
Step 14695: loss = 0.27122
Step 14700: loss = 0.30066
Step 14705: loss = 0.35979
Step 14710: loss = 0.39287
Step 14715: loss = 0.31941
Step 14720: loss = 0.24570
Step 14725: loss = 0.27743
Step 14730: loss = 0.27447
Step 14735: loss = 0.24105
Step 14740: loss = 0.23135
Step 14745: loss = 0.37856
Step 14750: loss = 0.31669
Step 14755: loss = 0.33557
Step 14760: loss = 0.22659
Step 14765: loss = 0.24914
Step 14770: loss = 0.27167
Step 14775: loss = 0.43165
Step 14780: loss = 0.38415
Step 14785: loss = 0.23776
Step 14790: loss = 0.25777
Step 14795: loss = 0.22828
Step 14800: loss = 0.19549
Step 14805: loss = 0.29854
Step 14810: loss = 0.31703
Step 14815: loss = 0.29112
Step 14820: loss = 0.30427
Training Data Eval:
  Num examples: 49920, Num correct: 45795, Precision @ 1: 0.9174
('Testing Data Eval: EPOCH->', 39)
  Num examples: 9984, Num correct: 7252, Precision @ 1: 0.7264
Step 14825: loss = 0.17973
Step 14830: loss = 0.19817
Step 14835: loss = 0.21922
Step 14840: loss = 0.16911
Step 14845: loss = 0.25360
Step 14850: loss = 0.23961
Step 14855: loss = 0.29383
Step 14860: loss = 0.23601
Step 14865: loss = 0.26091
Step 14870: loss = 0.36655
Step 14875: loss = 0.25138
Step 14880: loss = 0.25847
Step 14885: loss = 0.27668
Step 14890: loss = 0.34756
Step 14895: loss = 0.33962
Step 14900: loss = 0.27544
Step 14905: loss = 0.21501
Step 14910: loss = 0.34059
Step 14915: loss = 0.21485
Step 14920: loss = 0.21260
Step 14925: loss = 0.25547
Step 14930: loss = 0.31440
Step 14935: loss = 0.28215
Step 14940: loss = 0.23081
Step 14945: loss = 0.25723
Step 14950: loss = 0.27425
Step 14955: loss = 0.25529
Step 14960: loss = 0.28675
Step 14965: loss = 0.30168
Step 14970: loss = 0.21371
Step 14975: loss = 0.24400
Step 14980: loss = 0.30095
Step 14985: loss = 0.39010
Step 14990: loss = 0.29629
Step 14995: loss = 0.48735
Step 15000: loss = 0.25288
Step 15005: loss = 0.26246
Step 15010: loss = 0.31540
Step 15015: loss = 0.34549
Step 15020: loss = 0.26285
Step 15025: loss = 0.26784
Step 15030: loss = 0.15181
Step 15035: loss = 0.19708
Step 15040: loss = 0.26903
Step 15045: loss = 0.32047
Step 15050: loss = 0.28650
Step 15055: loss = 0.24542
Step 15060: loss = 0.29506
Step 15065: loss = 0.19799
Step 15070: loss = 0.19592
Step 15075: loss = 0.30816
Step 15080: loss = 0.24755
Step 15085: loss = 0.20618
Step 15090: loss = 0.21345
Step 15095: loss = 0.21350
Step 15100: loss = 0.27988
Step 15105: loss = 0.22313
Step 15110: loss = 0.32502
Step 15115: loss = 0.27680
Step 15120: loss = 0.27164
Step 15125: loss = 0.29883
Step 15130: loss = 0.18348
Step 15135: loss = 0.26801
Step 15140: loss = 0.35835
Step 15145: loss = 0.25199
Step 15150: loss = 0.23307
Step 15155: loss = 0.24515
Step 15160: loss = 0.32234
Step 15165: loss = 0.57164
Step 15170: loss = 0.28463
Step 15175: loss = 0.22246
Step 15180: loss = 0.28050
Step 15185: loss = 0.23170
Step 15190: loss = 0.23406
Step 15195: loss = 0.29830
Step 15200: loss = 0.19412
Step 15205: loss = 0.27096
Step 15210: loss = 0.17819
Training Data Eval:
  Num examples: 49920, Num correct: 46665, Precision @ 1: 0.9348
('Testing Data Eval: EPOCH->', 40)
  Num examples: 9984, Num correct: 7309, Precision @ 1: 0.7321
Step 15215: loss = 0.22226
Step 15220: loss = 0.29326
Step 15225: loss = 0.23764
Step 15230: loss = 0.16910
Step 15235: loss = 0.15140
Step 15240: loss = 0.23363
Step 15245: loss = 0.24557
Step 15250: loss = 0.23681
Step 15255: loss = 0.16615
Step 15260: loss = 0.25399
Step 15265: loss = 0.22638
Step 15270: loss = 0.25937
Step 15275: loss = 0.28155
Step 15280: loss = 0.20836
Step 15285: loss = 0.12833
Step 15290: loss = 0.21244
Step 15295: loss = 0.28190
Step 15300: loss = 0.17295
Step 15305: loss = 0.24493
Step 15310: loss = 0.20338
Step 15315: loss = 0.27969
Step 15320: loss = 0.26932
Step 15325: loss = 0.17704
Step 15330: loss = 0.24038
Step 15335: loss = 0.21329
Step 15340: loss = 0.24781
Step 15345: loss = 0.31920
Step 15350: loss = 0.23601
Step 15355: loss = 0.18604
Step 15360: loss = 0.31656
Step 15365: loss = 0.19792
Step 15370: loss = 0.22259
Step 15375: loss = 0.30862
Step 15380: loss = 0.36131
Step 15385: loss = 0.13428
Step 15390: loss = 0.21023
Step 15395: loss = 0.29832
Step 15400: loss = 0.23085
Step 15405: loss = 0.19717
Step 15410: loss = 0.23696
Step 15415: loss = 0.25925
Step 15420: loss = 0.32255
Step 15425: loss = 0.27817
Step 15430: loss = 0.36208
Step 15435: loss = 0.27316
Step 15440: loss = 0.31730
Step 15445: loss = 0.27577
Step 15450: loss = 0.25191
Step 15455: loss = 0.34152
Step 15460: loss = 0.20889
Step 15465: loss = 0.25019
Step 15470: loss = 0.26093
Step 15475: loss = 0.32235
Step 15480: loss = 0.35195
Step 15485: loss = 0.21606
Step 15490: loss = 0.20341
Step 15495: loss = 0.16853
Step 15500: loss = 0.23634
Step 15505: loss = 0.24708
Step 15510: loss = 0.30904
Step 15515: loss = 0.25356
Step 15520: loss = 0.40167
Step 15525: loss = 0.28771
Step 15530: loss = 0.28161
Step 15535: loss = 0.24983
Step 15540: loss = 0.13353
Step 15545: loss = 0.25641
Step 15550: loss = 0.21037
Step 15555: loss = 0.39840
Step 15560: loss = 0.30141
Step 15565: loss = 0.27347
Step 15570: loss = 0.26648
Step 15575: loss = 0.25834
Step 15580: loss = 0.30424
Step 15585: loss = 0.25146
Step 15590: loss = 0.23621
Step 15595: loss = 0.21797
Step 15600: loss = 0.28332
Training Data Eval:
  Num examples: 49920, Num correct: 46423, Precision @ 1: 0.9299
('Testing Data Eval: EPOCH->', 41)
  Num examples: 9984, Num correct: 7315, Precision @ 1: 0.7327
Step 15605: loss = 0.25137
Step 15610: loss = 0.15654
Step 15615: loss = 0.21021
Step 15620: loss = 0.18525
Step 15625: loss = 0.27166
Step 15630: loss = 0.14120
Step 15635: loss = 0.17819
Step 15640: loss = 0.16796
Step 15645: loss = 0.41507
Step 15650: loss = 0.21958
Step 15655: loss = 0.23331
Step 15660: loss = 0.25984
Step 15665: loss = 0.32441
Step 15670: loss = 0.34484
Step 15675: loss = 0.21609
Step 15680: loss = 0.31784
Step 15685: loss = 0.23108
Step 15690: loss = 0.31719
Step 15695: loss = 0.22924
Step 15700: loss = 0.17097
Step 15705: loss = 0.34230
Step 15710: loss = 0.14650
Step 15715: loss = 0.24936
Step 15720: loss = 0.19562
Step 15725: loss = 0.13296
Step 15730: loss = 0.23535
Step 15735: loss = 0.24246
Step 15740: loss = 0.23793
Step 15745: loss = 0.30063
Step 15750: loss = 0.18627
Step 15755: loss = 0.26193
Step 15760: loss = 0.21245
Step 15765: loss = 0.16589
Step 15770: loss = 0.22199
Step 15775: loss = 0.23029
Step 15780: loss = 0.23536
Step 15785: loss = 0.27695
Step 15790: loss = 0.21661
Step 15795: loss = 0.33944
Step 15800: loss = 0.23064
Step 15805: loss = 0.20241
Step 15810: loss = 0.21728
Step 15815: loss = 0.22369
Step 15820: loss = 0.25873
Step 15825: loss = 0.23668
Step 15830: loss = 0.39391
Step 15835: loss = 0.26484
Step 15840: loss = 0.30824
Step 15845: loss = 0.31241
Step 15850: loss = 0.34364
Step 15855: loss = 0.29047
Step 15860: loss = 0.26800
Step 15865: loss = 0.25995
Step 15870: loss = 0.26809
Step 15875: loss = 0.25801
Step 15880: loss = 0.37650
Step 15885: loss = 0.26971
Step 15890: loss = 0.39971
Step 15895: loss = 0.29850
Step 15900: loss = 0.26491
Step 15905: loss = 0.27644
Step 15910: loss = 0.23681
Step 15915: loss = 0.23308
Step 15920: loss = 0.17577
Step 15925: loss = 0.23838
Step 15930: loss = 0.23857
Step 15935: loss = 0.27481
Step 15940: loss = 0.31775
Step 15945: loss = 0.28689
Step 15950: loss = 0.25116
Step 15955: loss = 0.26736
Step 15960: loss = 0.20812
Step 15965: loss = 0.30511
Step 15970: loss = 0.28154
Step 15975: loss = 0.21213
Step 15980: loss = 0.21968
Step 15985: loss = 0.26710
Step 15990: loss = 0.29711
Training Data Eval:
  Num examples: 49920, Num correct: 46675, Precision @ 1: 0.9350
('Testing Data Eval: EPOCH->', 42)
  Num examples: 9984, Num correct: 7291, Precision @ 1: 0.7303
Step 15995: loss = 0.18918
Step 16000: loss = 0.28265
Step 16005: loss = 0.22635
Step 16010: loss = 0.19384
Step 16015: loss = 0.21927
Step 16020: loss = 0.22930
Step 16025: loss = 0.25966
Step 16030: loss = 0.12597
Step 16035: loss = 0.33437
Step 16040: loss = 0.19474
Step 16045: loss = 0.21614
Step 16050: loss = 0.17690
Step 16055: loss = 0.16670
Step 16060: loss = 0.28902
Step 16065: loss = 0.27249
Step 16070: loss = 0.26838
Step 16075: loss = 0.22951
Step 16080: loss = 0.18514
Step 16085: loss = 0.34241
Step 16090: loss = 0.19976
Step 16095: loss = 0.19452
Step 16100: loss = 0.17741
Step 16105: loss = 0.19534
Step 16110: loss = 0.21079
Step 16115: loss = 0.24743
Step 16120: loss = 0.18789
Step 16125: loss = 0.28305
Step 16130: loss = 0.17105
Step 16135: loss = 0.27220
Step 16140: loss = 0.27657
Step 16145: loss = 0.32095
Step 16150: loss = 0.23324
Step 16155: loss = 0.21607
Step 16160: loss = 0.25916
Step 16165: loss = 0.25172
Step 16170: loss = 0.17637
Step 16175: loss = 0.22495
Step 16180: loss = 0.29716
Step 16185: loss = 0.24143
Step 16190: loss = 0.21647
Step 16195: loss = 0.22447
Step 16200: loss = 0.29459
Step 16205: loss = 0.28145
Step 16210: loss = 0.22515
Step 16215: loss = 0.29670
Step 16220: loss = 0.33556
Step 16225: loss = 0.30350
Step 16230: loss = 0.34029
Step 16235: loss = 0.27773
Step 16240: loss = 0.23539
Step 16245: loss = 0.13268
Step 16250: loss = 0.18560
Step 16255: loss = 0.27615
Step 16260: loss = 0.25317
Step 16265: loss = 0.24330
Step 16270: loss = 0.29281
Step 16275: loss = 0.18474
Step 16280: loss = 0.24313
Step 16285: loss = 0.23572
Step 16290: loss = 0.24888
Step 16295: loss = 0.34162
Step 16300: loss = 0.17707
Step 16305: loss = 0.33252
Step 16310: loss = 0.20298
Step 16315: loss = 0.22684
Step 16320: loss = 0.26485
Step 16325: loss = 0.17250
Step 16330: loss = 0.16417
Step 16335: loss = 0.20177
Step 16340: loss = 0.25985
Step 16345: loss = 0.23520
Step 16350: loss = 0.21967
Step 16355: loss = 0.21920
Step 16360: loss = 0.16597
Step 16365: loss = 0.34148
Step 16370: loss = 0.25500
Step 16375: loss = 0.40104
Step 16380: loss = 0.15202
Training Data Eval:
  Num examples: 49920, Num correct: 46706, Precision @ 1: 0.9356
('Testing Data Eval: EPOCH->', 43)
  Num examples: 9984, Num correct: 7158, Precision @ 1: 0.7169
Step 16385: loss = 0.26976
Step 16390: loss = 0.16817
Step 16395: loss = 0.18992
Step 16400: loss = 0.24323
Step 16405: loss = 0.20484
Step 16410: loss = 0.20083
Step 16415: loss = 0.20340
Step 16420: loss = 0.20576
Step 16425: loss = 0.18255
Step 16430: loss = 0.27307
Step 16435: loss = 0.14102
Step 16440: loss = 0.25430
Step 16445: loss = 0.12906
Step 16450: loss = 0.18029
Step 16455: loss = 0.19032
Step 16460: loss = 0.27647
Step 16465: loss = 0.18932
Step 16470: loss = 0.20451
Step 16475: loss = 0.14420
Step 16480: loss = 0.25323
Step 16485: loss = 0.20094
Step 16490: loss = 0.25031
Step 16495: loss = 0.18104
Step 16500: loss = 0.17831
Step 16505: loss = 0.20213
Step 16510: loss = 0.21670
Step 16515: loss = 0.27537
Step 16520: loss = 0.25922
Step 16525: loss = 0.20458
Step 16530: loss = 0.15635
Step 16535: loss = 0.22467
Step 16540: loss = 0.18577
Step 16545: loss = 0.22297
Step 16550: loss = 0.24714
Step 16555: loss = 0.20668
Step 16560: loss = 0.30503
Step 16565: loss = 0.15105
Step 16570: loss = 0.16918
Step 16575: loss = 0.16878
Step 16580: loss = 0.25089
Step 16585: loss = 0.22198
Step 16590: loss = 0.24066
Step 16595: loss = 0.19879
Step 16600: loss = 0.24672
Step 16605: loss = 0.25436
Step 16610: loss = 0.21849
Step 16615: loss = 0.25294
Step 16620: loss = 0.28294
Step 16625: loss = 0.18662
Step 16630: loss = 0.20878
Step 16635: loss = 0.21885
Step 16640: loss = 0.14678
Step 16645: loss = 0.21958
Step 16650: loss = 0.29861
Step 16655: loss = 0.23450
Step 16660: loss = 0.30122
Step 16665: loss = 0.18811
Step 16670: loss = 0.18474
Step 16675: loss = 0.31309
Step 16680: loss = 0.23036
Step 16685: loss = 0.22762
Step 16690: loss = 0.19070
Step 16695: loss = 0.29761
Step 16700: loss = 0.18636
Step 16705: loss = 0.30096
Step 16710: loss = 0.18709
Step 16715: loss = 0.18803
Step 16720: loss = 0.29210
Step 16725: loss = 0.23643
Step 16730: loss = 0.25857
Step 16735: loss = 0.23632
Step 16740: loss = 0.22827
Step 16745: loss = 0.34109
Step 16750: loss = 0.34392
Step 16755: loss = 0.25961
Step 16760: loss = 0.29482
Step 16765: loss = 0.42289
Step 16770: loss = 0.48773
Training Data Eval:
  Num examples: 49920, Num correct: 46396, Precision @ 1: 0.9294
('Testing Data Eval: EPOCH->', 44)
  Num examples: 9984, Num correct: 7174, Precision @ 1: 0.7185
Step 16775: loss = 0.27649
Step 16780: loss = 0.21456
Step 16785: loss = 0.19934
Step 16790: loss = 0.26281
Step 16795: loss = 0.22739
Step 16800: loss = 0.20427
Step 16805: loss = 0.19011
Step 16810: loss = 0.15052
Step 16815: loss = 0.12643
Step 16820: loss = 0.17076
Step 16825: loss = 0.17934
Step 16830: loss = 0.15148
Step 16835: loss = 0.26902
Step 16840: loss = 0.24117
Step 16845: loss = 0.27164
Step 16850: loss = 0.15259
Step 16855: loss = 0.18295
Step 16860: loss = 0.24462
Step 16865: loss = 0.20691
Step 16870: loss = 0.18596
Step 16875: loss = 0.31871
Step 16880: loss = 0.25153
Step 16885: loss = 0.22888
Step 16890: loss = 0.24095
Step 16895: loss = 0.24361
Step 16900: loss = 0.23201
Step 16905: loss = 0.11962
Step 16910: loss = 0.22856
Step 16915: loss = 0.14532
Step 16920: loss = 0.18965
Step 16925: loss = 0.26278
Step 16930: loss = 0.23934
Step 16935: loss = 0.17149
Step 16940: loss = 0.21045
Step 16945: loss = 0.35333
Step 16950: loss = 0.17550
Step 16955: loss = 0.24992
Step 16960: loss = 0.27289
Step 16965: loss = 0.20231
Step 16970: loss = 0.21480
Step 16975: loss = 0.24861
Step 16980: loss = 0.26853
Step 16985: loss = 0.29820
Step 16990: loss = 0.16668
Step 16995: loss = 0.21162
Step 17000: loss = 0.26800
Step 17005: loss = 0.20393
Step 17010: loss = 0.19743
Step 17015: loss = 0.16871
Step 17020: loss = 0.24605
Step 17025: loss = 0.17708
Step 17030: loss = 0.26674
Step 17035: loss = 0.29797
Step 17040: loss = 0.18475
Step 17045: loss = 0.21358
Step 17050: loss = 0.25094
Step 17055: loss = 0.23324
Step 17060: loss = 0.29492
Step 17065: loss = 0.23307
Step 17070: loss = 0.21474
Step 17075: loss = 0.25770
Step 17080: loss = 0.21969
Step 17085: loss = 0.22942
Step 17090: loss = 0.24112
Step 17095: loss = 0.17351
Step 17100: loss = 0.20669
Step 17105: loss = 0.15925
Step 17110: loss = 0.17259
Step 17115: loss = 0.19444
Step 17120: loss = 0.26100
Step 17125: loss = 0.19563
Step 17130: loss = 0.19729
Step 17135: loss = 0.20322
Step 17140: loss = 0.29490
Step 17145: loss = 0.15289
Step 17150: loss = 0.22197
Step 17155: loss = 0.24227
Step 17160: loss = 0.19592
Training Data Eval:
  Num examples: 49920, Num correct: 46649, Precision @ 1: 0.9345
('Testing Data Eval: EPOCH->', 45)
  Num examples: 9984, Num correct: 7163, Precision @ 1: 0.7174
Step 17165: loss = 0.22144
Step 17170: loss = 0.26481
Step 17175: loss = 0.16413
Step 17180: loss = 0.21983
Step 17185: loss = 0.22952
Step 17190: loss = 0.15070
Step 17195: loss = 0.22863
Step 17200: loss = 0.17477
Step 17205: loss = 0.22064
Step 17210: loss = 0.20099
Step 17215: loss = 0.20057
Step 17220: loss = 0.16873
Step 17225: loss = 0.17264
Step 17230: loss = 0.28505
Step 17235: loss = 0.14389
Step 17240: loss = 0.19402
Step 17245: loss = 0.21137
Step 17250: loss = 0.15486
Step 17255: loss = 0.21237
Step 17260: loss = 0.18753
Step 17265: loss = 0.22616
Step 17270: loss = 0.16950
Step 17275: loss = 0.32393
Step 17280: loss = 0.27458
Step 17285: loss = 0.20052
Step 17290: loss = 0.24099
Step 17295: loss = 0.19372
Step 17300: loss = 0.27734
Step 17305: loss = 0.26352
Step 17310: loss = 0.19202
Step 17315: loss = 0.18283
Step 17320: loss = 0.20067
Step 17325: loss = 0.29282
Step 17330: loss = 0.15781
Step 17335: loss = 0.17197
Step 17340: loss = 0.16167
Step 17345: loss = 0.14740
Step 17350: loss = 0.12848
Step 17355: loss = 0.21338
Step 17360: loss = 0.11761
Step 17365: loss = 0.17964
Step 17370: loss = 0.18035
Step 17375: loss = 0.24984
Step 17380: loss = 0.19928
Step 17385: loss = 0.20326
Step 17390: loss = 0.15884
Step 17395: loss = 0.22690
Step 17400: loss = 0.20525
Step 17405: loss = 0.14412
Step 17410: loss = 0.21859
Step 17415: loss = 0.27539
Step 17420: loss = 0.22291
Step 17425: loss = 0.09760
Step 17430: loss = 0.22125
Step 17435: loss = 0.23734
Step 17440: loss = 0.15305
Step 17445: loss = 0.13818
Step 17450: loss = 0.22088
Step 17455: loss = 0.24887
Step 17460: loss = 0.14320
Step 17465: loss = 0.29035
Step 17470: loss = 0.23379
Step 17475: loss = 0.19862
Step 17480: loss = 0.27014
Step 17485: loss = 0.20419
Step 17490: loss = 0.17570
Step 17495: loss = 0.18543
Step 17500: loss = 0.15342
Step 17505: loss = 0.17928
Step 17510: loss = 0.23020
Step 17515: loss = 0.24697
Step 17520: loss = 0.18183
Step 17525: loss = 0.16890
Step 17530: loss = 0.24006
Step 17535: loss = 0.24458
Step 17540: loss = 0.16645
Step 17545: loss = 0.27200
Step 17550: loss = 0.10948
Training Data Eval:
  Num examples: 49920, Num correct: 47264, Precision @ 1: 0.9468
('Testing Data Eval: EPOCH->', 46)
  Num examples: 9984, Num correct: 7274, Precision @ 1: 0.7286
Step 17555: loss = 0.14917
Step 17560: loss = 0.12583
Step 17565: loss = 0.16413
Step 17570: loss = 0.19060
Step 17575: loss = 0.19276
Step 17580: loss = 0.23494
Step 17585: loss = 0.15484
Step 17590: loss = 0.21512
Step 17595: loss = 0.23777
Step 17600: loss = 0.15309
Step 17605: loss = 0.17653
Step 17610: loss = 0.30318
Step 17615: loss = 0.20989
Step 17620: loss = 0.12009
Step 17625: loss = 0.17553
Step 17630: loss = 0.26516
Step 17635: loss = 0.24631
Step 17640: loss = 0.13042
Step 17645: loss = 0.24538
Step 17650: loss = 0.18852
Step 17655: loss = 0.11428
Step 17660: loss = 0.19388
Step 17665: loss = 0.18364
Step 17670: loss = 0.18025
Step 17675: loss = 0.13696
Step 17680: loss = 0.14072
Step 17685: loss = 0.21640
Step 17690: loss = 0.17013
Step 17695: loss = 0.28132
Step 17700: loss = 0.23827
Step 17705: loss = 0.26228
Step 17710: loss = 0.23447
Step 17715: loss = 0.30391
Step 17720: loss = 0.13724
Step 17725: loss = 0.18331
Step 17730: loss = 0.18130
Step 17735: loss = 0.24776
Step 17740: loss = 0.17872
Step 17745: loss = 0.16474
Step 17750: loss = 0.18639
Step 17755: loss = 0.21699
Step 17760: loss = 0.21628
Step 17765: loss = 0.16728
Step 17770: loss = 0.21448
Step 17775: loss = 0.19790
Step 17780: loss = 0.18324
Step 17785: loss = 0.37126
Step 17790: loss = 0.22499
Step 17795: loss = 0.19083
Step 17800: loss = 0.26242
Step 17805: loss = 0.16772
Step 17810: loss = 0.11752
Step 17815: loss = 0.17852
Step 17820: loss = 0.25279
Step 17825: loss = 0.24973
Step 17830: loss = 0.25836
Step 17835: loss = 0.35924
Step 17840: loss = 0.29782
Step 17845: loss = 0.28154
Step 17850: loss = 0.22315
Step 17855: loss = 0.19737
Step 17860: loss = 0.16074
Step 17865: loss = 0.26336
Step 17870: loss = 0.18994
Step 17875: loss = 0.21674
Step 17880: loss = 0.15280
Step 17885: loss = 0.16334
Step 17890: loss = 0.20911
Step 17895: loss = 0.15825
Step 17900: loss = 0.18288
Step 17905: loss = 0.14421
Step 17910: loss = 0.14909
Step 17915: loss = 0.21109
Step 17920: loss = 0.26176
Step 17925: loss = 0.19719
Step 17930: loss = 0.15587
Step 17935: loss = 0.23000
Step 17940: loss = 0.22618
Training Data Eval:
  Num examples: 49920, Num correct: 47563, Precision @ 1: 0.9528
('Testing Data Eval: EPOCH->', 47)
  Num examples: 9984, Num correct: 7316, Precision @ 1: 0.7328
Step 17945: loss = 0.13518
Step 17950: loss = 0.17047
Step 17955: loss = 0.21497
Step 17960: loss = 0.14408
Step 17965: loss = 0.22805
Step 17970: loss = 0.20158
Step 17975: loss = 0.17179
Step 17980: loss = 0.20743
Step 17985: loss = 0.22666
Step 17990: loss = 0.24126
Step 17995: loss = 0.14643
Step 18000: loss = 0.15047
Step 18005: loss = 0.12417
Step 18010: loss = 0.24145
Step 18015: loss = 0.24256
Step 18020: loss = 0.23103
Step 18025: loss = 0.12283
Step 18030: loss = 0.17738
Step 18035: loss = 0.12903
Step 18040: loss = 0.16640
Step 18045: loss = 0.19742
Step 18050: loss = 0.18694
Step 18055: loss = 0.15168
Step 18060: loss = 0.25230
Step 18065: loss = 0.15542
Step 18070: loss = 0.21962
Step 18075: loss = 0.18985
Step 18080: loss = 0.11732
Step 18085: loss = 0.15143
Step 18090: loss = 0.11138
Step 18095: loss = 0.24075
Step 18100: loss = 0.10177
Step 18105: loss = 0.20563
Step 18110: loss = 0.17101
Step 18115: loss = 0.10446
Step 18120: loss = 0.13490
Step 18125: loss = 0.10143
Step 18130: loss = 0.16537
Step 18135: loss = 0.18302
Step 18140: loss = 0.22281
Step 18145: loss = 0.18079
Step 18150: loss = 0.11072
Step 18155: loss = 0.22771
Step 18160: loss = 0.14387
Step 18165: loss = 0.21001
Step 18170: loss = 0.17059
Step 18175: loss = 0.19576
Step 18180: loss = 0.17019
Step 18185: loss = 0.22112
Step 18190: loss = 0.13932
Step 18195: loss = 0.18579
Step 18200: loss = 0.18656
Step 18205: loss = 0.25250
Step 18210: loss = 0.16794
Step 18215: loss = 0.20773
Step 18220: loss = 0.13866
Step 18225: loss = 0.23663
Step 18230: loss = 0.18489
Step 18235: loss = 0.30417
Step 18240: loss = 0.24243
Step 18245: loss = 0.28974
Step 18250: loss = 0.23654
Step 18255: loss = 0.16181
Step 18260: loss = 0.20975
Step 18265: loss = 0.18530
Step 18270: loss = 0.29589
Step 18275: loss = 0.21369
Step 18280: loss = 0.14514
Step 18285: loss = 0.30748
Step 18290: loss = 0.22399
Step 18295: loss = 0.16215
Step 18300: loss = 0.18192
Step 18305: loss = 0.26759
Step 18310: loss = 0.34137
Step 18315: loss = 0.28979
Step 18320: loss = 0.16926
Step 18325: loss = 0.22755
Step 18330: loss = 0.28853
Training Data Eval:
  Num examples: 49920, Num correct: 47250, Precision @ 1: 0.9465
('Testing Data Eval: EPOCH->', 48)
  Num examples: 9984, Num correct: 7303, Precision @ 1: 0.7315
Step 18335: loss = 0.13474
Step 18340: loss = 0.22659
Step 18345: loss = 0.12736
Step 18350: loss = 0.15870
Step 18355: loss = 0.17162
Step 18360: loss = 0.17205
Step 18365: loss = 0.23576
Step 18370: loss = 0.10215
Step 18375: loss = 0.15731
Step 18380: loss = 0.19072
Step 18385: loss = 0.22409
Step 18390: loss = 0.22942
Step 18395: loss = 0.23015
Step 18400: loss = 0.20531
Step 18405: loss = 0.32862
Step 18410: loss = 0.13538
Step 18415: loss = 0.20497
Step 18420: loss = 0.17630
Step 18425: loss = 0.14469
Step 18430: loss = 0.22153
Step 18435: loss = 0.22404
Step 18440: loss = 0.15866
Step 18445: loss = 0.22141
Step 18450: loss = 0.18935
Step 18455: loss = 0.27334
Step 18460: loss = 0.15924
Step 18465: loss = 0.24399
Step 18470: loss = 0.17375
Step 18475: loss = 0.19899
Step 18480: loss = 0.21456
Step 18485: loss = 0.21089
Step 18490: loss = 0.19999
Step 18495: loss = 0.21200
Step 18500: loss = 0.22792
Step 18505: loss = 0.16194
Step 18510: loss = 0.30554
Step 18515: loss = 0.17524
Step 18520: loss = 0.15316
Step 18525: loss = 0.21398
Step 18530: loss = 0.17752
Step 18535: loss = 0.15853
Step 18540: loss = 0.27494
Step 18545: loss = 0.18230
Step 18550: loss = 0.17777
Step 18555: loss = 0.16634
Step 18560: loss = 0.19736
Step 18565: loss = 0.28439
Step 18570: loss = 0.15488
Step 18575: loss = 0.15293
Step 18580: loss = 0.19305
Step 18585: loss = 0.13379
Step 18590: loss = 0.14684
Step 18595: loss = 0.17941
Step 18600: loss = 0.14227
Step 18605: loss = 0.16185
Step 18610: loss = 0.24326
Step 18615: loss = 0.11465
Step 18620: loss = 0.18029
Step 18625: loss = 0.15296
Step 18630: loss = 0.16070
Step 18635: loss = 0.14778
Step 18640: loss = 0.15389
Step 18645: loss = 0.17217
Step 18650: loss = 0.15570
Step 18655: loss = 0.16385
Step 18660: loss = 0.21047
Step 18665: loss = 0.14138
Step 18670: loss = 0.12341
Step 18675: loss = 0.19313
Step 18680: loss = 0.16352
Step 18685: loss = 0.17800
Step 18690: loss = 0.26393
Step 18695: loss = 0.15801
Step 18700: loss = 0.16796
Step 18705: loss = 0.19888
Step 18710: loss = 0.20820
Step 18715: loss = 0.13969
Step 18720: loss = 0.21767
Training Data Eval:
  Num examples: 49920, Num correct: 47581, Precision @ 1: 0.9531
('Testing Data Eval: EPOCH->', 49)
  Num examples: 9984, Num correct: 7352, Precision @ 1: 0.7364
Step 18725: loss = 0.13039
Step 18730: loss = 0.16778
Step 18735: loss = 0.16028
Step 18740: loss = 0.14206
Step 18745: loss = 0.15238
Step 18750: loss = 0.12657
Step 18755: loss = 0.21164
Step 18760: loss = 0.13849
Step 18765: loss = 0.14281
Step 18770: loss = 0.19174
Step 18775: loss = 0.17920
Step 18780: loss = 0.13767
Step 18785: loss = 0.15548
Step 18790: loss = 0.18638
Step 18795: loss = 0.23136
Step 18800: loss = 0.15215
Step 18805: loss = 0.10968
Step 18810: loss = 0.24240
Step 18815: loss = 0.17194
Step 18820: loss = 0.15186
Step 18825: loss = 0.20852
Step 18830: loss = 0.13619
Step 18835: loss = 0.18137
Step 18840: loss = 0.20204
Step 18845: loss = 0.13485
Step 18850: loss = 0.17144
Step 18855: loss = 0.24908
Step 18860: loss = 0.13651
Step 18865: loss = 0.23611
Step 18870: loss = 0.18329
Step 18875: loss = 0.24074
Step 18880: loss = 0.14530
Step 18885: loss = 0.11527
Step 18890: loss = 0.19416
Step 18895: loss = 0.21172
Step 18900: loss = 0.18297
Step 18905: loss = 0.18947
Step 18910: loss = 0.18004
Step 18915: loss = 0.17250
Step 18920: loss = 0.11945
Step 18925: loss = 0.16553
Step 18930: loss = 0.12493
Step 18935: loss = 0.10621
Step 18940: loss = 0.19179
Step 18945: loss = 0.18887
Step 18950: loss = 0.18452
Step 18955: loss = 0.12869
Step 18960: loss = 0.15022
Step 18965: loss = 0.18254
Step 18970: loss = 0.26084
Step 18975: loss = 0.17090
Step 18980: loss = 0.16922
Step 18985: loss = 0.21476
Step 18990: loss = 0.16103
Step 18995: loss = 0.19323
Step 19000: loss = 0.19535
Step 19005: loss = 0.15529
Step 19010: loss = 0.14700
Step 19015: loss = 0.21253
Step 19020: loss = 0.17299
Step 19025: loss = 0.18056
Step 19030: loss = 0.14304
Step 19035: loss = 0.19865
Step 19040: loss = 0.19121
Step 19045: loss = 0.13699
Step 19050: loss = 0.21803
Step 19055: loss = 0.21827
Step 19060: loss = 0.23927
Step 19065: loss = 0.11673
Step 19070: loss = 0.18152
Step 19075: loss = 0.25306
Step 19080: loss = 0.18386
Step 19085: loss = 0.19010
Step 19090: loss = 0.17587
Step 19095: loss = 0.19356
Step 19100: loss = 0.28769
Step 19105: loss = 0.18781
Step 19110: loss = 0.21766
Training Data Eval:
  Num examples: 49920, Num correct: 47532, Precision @ 1: 0.9522
('Testing Data Eval: EPOCH->', 50)
  Num examples: 9984, Num correct: 7289, Precision @ 1: 0.7301
Step 19115: loss = 0.15095
Step 19120: loss = 0.20751
Step 19125: loss = 0.21558
Step 19130: loss = 0.14658
Step 19135: loss = 0.20382
Step 19140: loss = 0.16213
Step 19145: loss = 0.23671
Step 19150: loss = 0.19292
Step 19155: loss = 0.24564
Step 19160: loss = 0.15478
Step 19165: loss = 0.23413
Step 19170: loss = 0.13218
Step 19175: loss = 0.20931
Step 19180: loss = 0.14827
Step 19185: loss = 0.19166
Step 19190: loss = 0.24726
Step 19195: loss = 0.17768
Step 19200: loss = 0.11746
Step 19205: loss = 0.17806
Step 19210: loss = 0.11621
Step 19215: loss = 0.12603
Step 19220: loss = 0.19811
Step 19225: loss = 0.20190
Step 19230: loss = 0.18347
Step 19235: loss = 0.29510
Step 19240: loss = 0.18671
Step 19245: loss = 0.11555
Step 19250: loss = 0.20328
Step 19255: loss = 0.23301
Step 19260: loss = 0.19330
Step 19265: loss = 0.18073
Step 19270: loss = 0.18944
Step 19275: loss = 0.19431
Step 19280: loss = 0.25638
Step 19285: loss = 0.14038
Step 19290: loss = 0.18825
Step 19295: loss = 0.22039
Step 19300: loss = 0.17469
Step 19305: loss = 0.21014
Step 19310: loss = 0.18960
Step 19315: loss = 0.24024
Step 19320: loss = 0.14748
Step 19325: loss = 0.24484
Step 19330: loss = 0.10071
Step 19335: loss = 0.17813
Step 19340: loss = 0.14009
Step 19345: loss = 0.18491
Step 19350: loss = 0.28948
Step 19355: loss = 0.17259
Step 19360: loss = 0.15208
Step 19365: loss = 0.20749
Step 19370: loss = 0.20857
Step 19375: loss = 0.23345
Step 19380: loss = 0.22838
Step 19385: loss = 0.16192
Step 19390: loss = 0.13393
Step 19395: loss = 0.14892
Step 19400: loss = 0.22153
Step 19405: loss = 0.22091
Step 19410: loss = 0.12955
Step 19415: loss = 0.13918
Step 19420: loss = 0.31734
Step 19425: loss = 0.18201
Step 19430: loss = 0.17720
Step 19435: loss = 0.17563
Step 19440: loss = 0.26023
Step 19445: loss = 0.20783
Step 19450: loss = 0.18615
Step 19455: loss = 0.18286
Step 19460: loss = 0.16027
Step 19465: loss = 0.14888
Step 19470: loss = 0.27091
Step 19475: loss = 0.15477
Step 19480: loss = 0.19741
Step 19485: loss = 0.18794
Step 19490: loss = 0.18311
Step 19495: loss = 0.11558
Step 19500: loss = 0.13311
Training Data Eval:
  Num examples: 49920, Num correct: 47702, Precision @ 1: 0.9556
('Testing Data Eval: EPOCH->', 51)
  Num examples: 9984, Num correct: 7198, Precision @ 1: 0.7210
Step 19505: loss = 0.09391
Step 19510: loss = 0.17746
Step 19515: loss = 0.17251
Step 19520: loss = 0.20502
Step 19525: loss = 0.13027
Step 19530: loss = 0.15719
Step 19535: loss = 0.15921
Step 19540: loss = 0.15815
Step 19545: loss = 0.16559
Step 19550: loss = 0.14049
Step 19555: loss = 0.24661
Step 19560: loss = 0.14297
Step 19565: loss = 0.10447
Step 19570: loss = 0.10798
Step 19575: loss = 0.17529
Step 19580: loss = 0.22780
Step 19585: loss = 0.15802
Step 19590: loss = 0.14950
Step 19595: loss = 0.13889
Step 19600: loss = 0.14328
Step 19605: loss = 0.23472
Step 19610: loss = 0.14816
Step 19615: loss = 0.12816
Step 19620: loss = 0.13103
Step 19625: loss = 0.16420
Step 19630: loss = 0.16578
Step 19635: loss = 0.22687
Step 19640: loss = 0.26866
Step 19645: loss = 0.16965
Step 19650: loss = 0.18971
Step 19655: loss = 0.12323
Step 19660: loss = 0.16497
Step 19665: loss = 0.26795
Step 19670: loss = 0.18317
Step 19675: loss = 0.20576
Step 19680: loss = 0.22929
Step 19685: loss = 0.17904
Step 19690: loss = 0.21924
Step 19695: loss = 0.20080
Step 19700: loss = 0.16072
Step 19705: loss = 0.16301
Step 19710: loss = 0.15047
Step 19715: loss = 0.19057
Step 19720: loss = 0.13438
Step 19725: loss = 0.14645
Step 19730: loss = 0.16982
Step 19735: loss = 0.14461
Step 19740: loss = 0.13300
Step 19745: loss = 0.18845
Step 19750: loss = 0.44262
Step 19755: loss = 0.15378
Step 19760: loss = 0.10322
Step 19765: loss = 0.23598
Step 19770: loss = 0.21851
Step 19775: loss = 0.18570
Step 19780: loss = 0.14738
Step 19785: loss = 0.22957
Step 19790: loss = 0.10518
Step 19795: loss = 0.20613
Step 19800: loss = 0.14179
Step 19805: loss = 0.16222
Step 19810: loss = 0.16632
Step 19815: loss = 0.20217
Step 19820: loss = 0.15905
Step 19825: loss = 0.13995
Step 19830: loss = 0.23266
Step 19835: loss = 0.10552
Step 19840: loss = 0.18928
Step 19845: loss = 0.26507
Step 19850: loss = 0.11486
Step 19855: loss = 0.17984
Step 19860: loss = 0.12047
Step 19865: loss = 0.21180
Step 19870: loss = 0.15903
Step 19875: loss = 0.19565
Step 19880: loss = 0.17960
Step 19885: loss = 0.19724
Step 19890: loss = 0.17266
Training Data Eval:
  Num examples: 49920, Num correct: 48033, Precision @ 1: 0.9622
('Testing Data Eval: EPOCH->', 52)
  Num examples: 9984, Num correct: 7252, Precision @ 1: 0.7264
Step 19895: loss = 0.10632
Step 19900: loss = 0.16795
Step 19905: loss = 0.09731
Step 19910: loss = 0.21253
Step 19915: loss = 0.15389
Step 19920: loss = 0.15798
Step 19925: loss = 0.21820
Step 19930: loss = 0.10242
Step 19935: loss = 0.16534
Step 19940: loss = 0.21238
Step 19945: loss = 0.19773
Step 19950: loss = 0.15426
Step 19955: loss = 0.28130
Step 19960: loss = 0.11421
Step 19965: loss = 0.11411
Step 19970: loss = 0.14134
Step 19975: loss = 0.21559
Step 19980: loss = 0.21196
Step 19985: loss = 0.12016
Step 19990: loss = 0.15332
Step 19995: loss = 0.15627
Step 20000: loss = 0.12479
Step 20005: loss = 0.16032
Step 20010: loss = 0.18556
Step 20015: loss = 0.19354
Step 20020: loss = 0.14919
Step 20025: loss = 0.14553
Step 20030: loss = 0.11185
Step 20035: loss = 0.12648
Step 20040: loss = 0.32350
Step 20045: loss = 0.14852
Step 20050: loss = 0.16187
Step 20055: loss = 0.11768
Step 20060: loss = 0.16021
Step 20065: loss = 0.09886
Step 20070: loss = 0.09858
Step 20075: loss = 0.17388
Step 20080: loss = 0.17659
Step 20085: loss = 0.12980
Step 20090: loss = 0.14881
Step 20095: loss = 0.15011
Step 20100: loss = 0.16198
Step 20105: loss = 0.12798
Step 20110: loss = 0.17289
Step 20115: loss = 0.09984
Step 20120: loss = 0.19173
Step 20125: loss = 0.10907
Step 20130: loss = 0.15936
Step 20135: loss = 0.08011
Step 20140: loss = 0.17653
Step 20145: loss = 0.12405
Step 20150: loss = 0.12315
Step 20155: loss = 0.09926
Step 20160: loss = 0.13977
Step 20165: loss = 0.16115
Step 20170: loss = 0.15810
Step 20175: loss = 0.07408
Step 20180: loss = 0.18189
Step 20185: loss = 0.16084
Step 20190: loss = 0.13220
Step 20195: loss = 0.15817
Step 20200: loss = 0.21598
Step 20205: loss = 0.17252
Step 20210: loss = 0.13266
Step 20215: loss = 0.17148
Step 20220: loss = 0.18009
Step 20225: loss = 0.21595
Step 20230: loss = 0.11566
Step 20235: loss = 0.13635
Step 20240: loss = 0.12969
Step 20245: loss = 0.17974
Step 20250: loss = 0.09693
Step 20255: loss = 0.15356
Step 20260: loss = 0.17302
Step 20265: loss = 0.14359
Step 20270: loss = 0.18261
Step 20275: loss = 0.09334
Step 20280: loss = 0.16690
Training Data Eval:
  Num examples: 49920, Num correct: 48083, Precision @ 1: 0.9632
('Testing Data Eval: EPOCH->', 53)
  Num examples: 9984, Num correct: 7331, Precision @ 1: 0.7343
Step 20285: loss = 0.15071
Step 20290: loss = 0.11208
Step 20295: loss = 0.13177
Step 20300: loss = 0.14555
Step 20305: loss = 0.17028
Step 20310: loss = 0.15891
Step 20315: loss = 0.08910
Step 20320: loss = 0.13378
Step 20325: loss = 0.17771
Step 20330: loss = 0.16005
Step 20335: loss = 0.12746
Step 20340: loss = 0.18229
Step 20345: loss = 0.17310
Step 20350: loss = 0.25380
Step 20355: loss = 0.20320
Step 20360: loss = 0.16403
Step 20365: loss = 0.28142
Step 20370: loss = 0.19226
Step 20375: loss = 0.17623
Step 20380: loss = 0.19095
Step 20385: loss = 0.13858
Step 20390: loss = 0.17734
Step 20395: loss = 0.14328
Step 20400: loss = 0.16978
Step 20405: loss = 0.13860
Step 20410: loss = 0.11698
Step 20415: loss = 0.15867
Step 20420: loss = 0.23084
Step 20425: loss = 0.14792
Step 20430: loss = 0.15167
Step 20435: loss = 0.18161
Step 20440: loss = 0.27288
Step 20445: loss = 0.14388
Step 20450: loss = 0.18057
Step 20455: loss = 0.16413
Step 20460: loss = 0.14144
Step 20465: loss = 0.16335
Step 20470: loss = 0.15011
Step 20475: loss = 0.10629
Step 20480: loss = 0.12383
Step 20485: loss = 0.10285
Step 20490: loss = 0.18392
Step 20495: loss = 0.11199
Step 20500: loss = 0.14140
Step 20505: loss = 0.11273
Step 20510: loss = 0.11919
Step 20515: loss = 0.12265
Step 20520: loss = 0.24868
Step 20525: loss = 0.18781
Step 20530: loss = 0.14319
Step 20535: loss = 0.13655
Step 20540: loss = 0.10759
Step 20545: loss = 0.15691
Step 20550: loss = 0.11385
Step 20555: loss = 0.25384
Step 20560: loss = 0.22337
Step 20565: loss = 0.14816
Step 20570: loss = 0.17811
Step 20575: loss = 0.09696
Step 20580: loss = 0.12947
Step 20585: loss = 0.13994
Step 20590: loss = 0.11977
Step 20595: loss = 0.13027
Step 20600: loss = 0.11917
Step 20605: loss = 0.17957
Step 20610: loss = 0.16422
Step 20615: loss = 0.15146
Step 20620: loss = 0.12709
Step 20625: loss = 0.12410
Step 20630: loss = 0.09159
Step 20635: loss = 0.11500
Step 20640: loss = 0.14197
Step 20645: loss = 0.17613
Step 20650: loss = 0.18969
Step 20655: loss = 0.18763
Step 20660: loss = 0.11937
Step 20665: loss = 0.13245
Step 20670: loss = 0.15158
Training Data Eval:
  Num examples: 49920, Num correct: 48198, Precision @ 1: 0.9655
('Testing Data Eval: EPOCH->', 54)
  Num examples: 9984, Num correct: 7328, Precision @ 1: 0.7340
Step 20675: loss = 0.08957
Step 20680: loss = 0.17209
Step 20685: loss = 0.17388
Step 20690: loss = 0.13052
Step 20695: loss = 0.11825
Step 20700: loss = 0.11138
Step 20705: loss = 0.14034
Step 20710: loss = 0.13096
Step 20715: loss = 0.12312
Step 20720: loss = 0.16348
Step 20725: loss = 0.11425
Step 20730: loss = 0.23271
Step 20735: loss = 0.15170
Step 20740: loss = 0.11995
Step 20745: loss = 0.12881
Step 20750: loss = 0.18694
Step 20755: loss = 0.13536
Step 20760: loss = 0.17095
Step 20765: loss = 0.12279
Step 20770: loss = 0.10500
Step 20775: loss = 0.13043
Step 20780: loss = 0.11047
Step 20785: loss = 0.12710
Step 20790: loss = 0.13918
Step 20795: loss = 0.14837
Step 20800: loss = 0.15833
Step 20805: loss = 0.14337
Step 20810: loss = 0.17771
Step 20815: loss = 0.20429
Step 20820: loss = 0.16548
Step 20825: loss = 0.15059
Step 20830: loss = 0.14139
Step 20835: loss = 0.12483
Step 20840: loss = 0.14120
Step 20845: loss = 0.10719
Step 20850: loss = 0.13508
Step 20855: loss = 0.20508
Step 20860: loss = 0.14273
Step 20865: loss = 0.23265
Step 20870: loss = 0.18041
Step 20875: loss = 0.11973
Step 20880: loss = 0.14774
Step 20885: loss = 0.07897
Step 20890: loss = 0.22795
Step 20895: loss = 0.15564
Step 20900: loss = 0.10264
Step 20905: loss = 0.09977
Step 20910: loss = 0.20471
Step 20915: loss = 0.13014
Step 20920: loss = 0.11574
Step 20925: loss = 0.15395
Step 20930: loss = 0.16012
Step 20935: loss = 0.15194
Step 20940: loss = 0.07441
Step 20945: loss = 0.13028
Step 20950: loss = 0.09323
Step 20955: loss = 0.16474
Step 20960: loss = 0.19655
Step 20965: loss = 0.12709
Step 20970: loss = 0.10944
Step 20975: loss = 0.15144
Step 20980: loss = 0.09710
Step 20985: loss = 0.12882
Step 20990: loss = 0.13167
Step 20995: loss = 0.14519
Step 21000: loss = 0.15690
Step 21005: loss = 0.20636
Step 21010: loss = 0.16902
Step 21015: loss = 0.17778
Step 21020: loss = 0.14509
Step 21025: loss = 0.10627
Step 21030: loss = 0.14072
Step 21035: loss = 0.12742
Step 21040: loss = 0.14620
Step 21045: loss = 0.14166
Step 21050: loss = 0.11142
Step 21055: loss = 0.13968
Step 21060: loss = 0.14795
Training Data Eval:
  Num examples: 49920, Num correct: 48138, Precision @ 1: 0.9643
('Testing Data Eval: EPOCH->', 55)
  Num examples: 9984, Num correct: 7306, Precision @ 1: 0.7318
Step 21065: loss = 0.19707
Step 21070: loss = 0.10930
Step 21075: loss = 0.14787
Step 21080: loss = 0.11588
Step 21085: loss = 0.14719
Step 21090: loss = 0.11918
Step 21095: loss = 0.09899
Step 21100: loss = 0.20024
Step 21105: loss = 0.18528
Step 21110: loss = 0.17052
Step 21115: loss = 0.09416
Step 21120: loss = 0.11556
Step 21125: loss = 0.12122
Step 21130: loss = 0.10271
Step 21135: loss = 0.09192
Step 21140: loss = 0.09791
Step 21145: loss = 0.16087
Step 21150: loss = 0.19051
Step 21155: loss = 0.08091
Step 21160: loss = 0.15794
Step 21165: loss = 0.16911
Step 21170: loss = 0.09333
Step 21175: loss = 0.10419
Step 21180: loss = 0.10013
Step 21185: loss = 0.16292
Step 21190: loss = 0.16463
Step 21195: loss = 0.11165
Step 21200: loss = 0.14710
Step 21205: loss = 0.15198
Step 21210: loss = 0.13070
Step 21215: loss = 0.08681
Step 21220: loss = 0.10235
Step 21225: loss = 0.10822
Step 21230: loss = 0.12858
Step 21235: loss = 0.18913
Step 21240: loss = 0.15125
Step 21245: loss = 0.17127
Step 21250: loss = 0.14993
Step 21255: loss = 0.13271
Step 21260: loss = 0.24346
Step 21265: loss = 0.20573
Step 21270: loss = 0.14016
Step 21275: loss = 0.16480
Step 21280: loss = 0.19813
Step 21285: loss = 0.12489
Step 21290: loss = 0.12657
Step 21295: loss = 0.08210
Step 21300: loss = 0.16480
Step 21305: loss = 0.16704
Step 21310: loss = 0.08558
Step 21315: loss = 0.17258
Step 21320: loss = 0.23261
Step 21325: loss = 0.15340
Step 21330: loss = 0.18010
Step 21335: loss = 0.13728
Step 21340: loss = 0.27160
Step 21345: loss = 0.11424
Step 21350: loss = 0.17876
Step 21355: loss = 0.12361
Step 21360: loss = 0.09602
Step 21365: loss = 0.13541
Step 21370: loss = 0.11861
Step 21375: loss = 0.11301
Step 21380: loss = 0.19545
Step 21385: loss = 0.12232
Step 21390: loss = 0.17339
Step 21395: loss = 0.18149
Step 21400: loss = 0.23265
Step 21405: loss = 0.13778
Step 21410: loss = 0.20391
Step 21415: loss = 0.13188
Step 21420: loss = 0.18851
Step 21425: loss = 0.18133
Step 21430: loss = 0.11504
Step 21435: loss = 0.17344
Step 21440: loss = 0.20571
Step 21445: loss = 0.19622
Step 21450: loss = 0.16948
Training Data Eval:
  Num examples: 49920, Num correct: 48192, Precision @ 1: 0.9654
('Testing Data Eval: EPOCH->', 56)
  Num examples: 9984, Num correct: 7308, Precision @ 1: 0.7320
Step 21455: loss = 0.09753
Step 21460: loss = 0.16824
Step 21465: loss = 0.09939
Step 21470: loss = 0.17897
Step 21475: loss = 0.13238
Step 21480: loss = 0.15809
Step 21485: loss = 0.09493
Step 21490: loss = 0.11456
Step 21495: loss = 0.06797
Step 21500: loss = 0.16992
Step 21505: loss = 0.05411
Step 21510: loss = 0.10325
Step 21515: loss = 0.13996
Step 21520: loss = 0.14625
Step 21525: loss = 0.07489
Step 21530: loss = 0.12419
Step 21535: loss = 0.21447
Step 21540: loss = 0.13527
Step 21545: loss = 0.12733
Step 21550: loss = 0.13185
Step 21555: loss = 0.11785
Step 21560: loss = 0.14331
Step 21565: loss = 0.12276
Step 21570: loss = 0.25858
Step 21575: loss = 0.17009
Step 21580: loss = 0.08113
Step 21585: loss = 0.18104
Step 21590: loss = 0.19518
Step 21595: loss = 0.09867
Step 21600: loss = 0.10218
Step 21605: loss = 0.09966
Step 21610: loss = 0.15328
Step 21615: loss = 0.11412
Step 21620: loss = 0.14843
Step 21625: loss = 0.09985
Step 21630: loss = 0.11182
Step 21635: loss = 0.19445
Step 21640: loss = 0.15814
Step 21645: loss = 0.13907
Step 21650: loss = 0.10796
Step 21655: loss = 0.18484
Step 21660: loss = 0.15054
Step 21665: loss = 0.17790
Step 21670: loss = 0.20075
Step 21675: loss = 0.18743
Step 21680: loss = 0.13532
Step 21685: loss = 0.16364
Step 21690: loss = 0.12396
Step 21695: loss = 0.18251
Step 21700: loss = 0.11952
Step 21705: loss = 0.10784
Step 21710: loss = 0.06709
Step 21715: loss = 0.16077
Step 21720: loss = 0.10209
Step 21725: loss = 0.12185
Step 21730: loss = 0.11440
Step 21735: loss = 0.12357
Step 21740: loss = 0.12856
Step 21745: loss = 0.12432
Step 21750: loss = 0.10509
Step 21755: loss = 0.14092
Step 21760: loss = 0.10364
Step 21765: loss = 0.11076
Step 21770: loss = 0.09789
Step 21775: loss = 0.11081
Step 21780: loss = 0.11601
Step 21785: loss = 0.13688
Step 21790: loss = 0.18004
Step 21795: loss = 0.14892
Step 21800: loss = 0.13618
Step 21805: loss = 0.13208
Step 21810: loss = 0.13531
Step 21815: loss = 0.13517
Step 21820: loss = 0.20563
Step 21825: loss = 0.13125
Step 21830: loss = 0.17128
Step 21835: loss = 0.21954
Step 21840: loss = 0.09828
Training Data Eval:
  Num examples: 49920, Num correct: 48510, Precision @ 1: 0.9718
('Testing Data Eval: EPOCH->', 57)
  Num examples: 9984, Num correct: 7453, Precision @ 1: 0.7465
Step 21845: loss = 0.17990
Step 21850: loss = 0.13819
Step 21855: loss = 0.12979
Step 21860: loss = 0.17279
Step 21865: loss = 0.20982
Step 21870: loss = 0.16528
Step 21875: loss = 0.13845
Step 21880: loss = 0.14525
Step 21885: loss = 0.09960
Step 21890: loss = 0.12789
Step 21895: loss = 0.10394
Step 21900: loss = 0.14435
Step 21905: loss = 0.22720
Step 21910: loss = 0.13197
Step 21915: loss = 0.17346
Step 21920: loss = 0.08681
Step 21925: loss = 0.13182
Step 21930: loss = 0.15701
Step 21935: loss = 0.17723
Step 21940: loss = 0.10920
Step 21945: loss = 0.16905
Step 21950: loss = 0.07746
Step 21955: loss = 0.14304
Step 21960: loss = 0.14647
Step 21965: loss = 0.19545
Step 21970: loss = 0.12916
Step 21975: loss = 0.17250
Step 21980: loss = 0.11996
Step 21985: loss = 0.19062
Step 21990: loss = 0.22544
Step 21995: loss = 0.15266
Step 22000: loss = 0.16646
Step 22005: loss = 0.13637
Step 22010: loss = 0.15786
Step 22015: loss = 0.07410
Step 22020: loss = 0.11752
Step 22025: loss = 0.09327
Step 22030: loss = 0.09522
Step 22035: loss = 0.12533
Step 22040: loss = 0.20151
Step 22045: loss = 0.13362
Step 22050: loss = 0.11333
Step 22055: loss = 0.05288
Step 22060: loss = 0.11691
Step 22065: loss = 0.10623
Step 22070: loss = 0.17196
Step 22075: loss = 0.12012
Step 22080: loss = 0.09598
Step 22085: loss = 0.10430
Step 22090: loss = 0.12851
Step 22095: loss = 0.16034
Step 22100: loss = 0.19495
Step 22105: loss = 0.14066
Step 22110: loss = 0.07810
Step 22115: loss = 0.22332
Step 22120: loss = 0.14367
Step 22125: loss = 0.14913
Step 22130: loss = 0.15345
Step 22135: loss = 0.13512
Step 22140: loss = 0.18205
Step 22145: loss = 0.13540
Step 22150: loss = 0.13670
Step 22155: loss = 0.11972
Step 22160: loss = 0.20945
Step 22165: loss = 0.13538
Step 22170: loss = 0.12879
Step 22175: loss = 0.16671
Step 22180: loss = 0.17615
Step 22185: loss = 0.13348
Step 22190: loss = 0.09464
Step 22195: loss = 0.16843
Step 22200: loss = 0.21805
Step 22205: loss = 0.24349
Step 22210: loss = 0.10262
Step 22215: loss = 0.12129
Step 22220: loss = 0.16346
Step 22225: loss = 0.10249
Step 22230: loss = 0.11713
Training Data Eval:
  Num examples: 49920, Num correct: 48174, Precision @ 1: 0.9650
('Testing Data Eval: EPOCH->', 58)
  Num examples: 9984, Num correct: 7311, Precision @ 1: 0.7323
Step 22235: loss = 0.09585
Step 22240: loss = 0.12519
Step 22245: loss = 0.12328
Step 22250: loss = 0.23651
Step 22255: loss = 0.05603
Step 22260: loss = 0.15070
Step 22265: loss = 0.15047
Step 22270: loss = 0.21706
Step 22275: loss = 0.16325
Step 22280: loss = 0.11304
Step 22285: loss = 0.11275
Step 22290: loss = 0.14276
Step 22295: loss = 0.15692
Step 22300: loss = 0.11960
Step 22305: loss = 0.11727
Step 22310: loss = 0.16376
Step 22315: loss = 0.20525
Step 22320: loss = 0.11687
Step 22325: loss = 0.12572
Step 22330: loss = 0.12573
Step 22335: loss = 0.12185
Step 22340: loss = 0.11992
Step 22345: loss = 0.14194
Step 22350: loss = 0.11419
Step 22355: loss = 0.09898
Step 22360: loss = 0.17224
Step 22365: loss = 0.07763
Step 22370: loss = 0.13382
Step 22375: loss = 0.14545
Step 22380: loss = 0.17326
Step 22385: loss = 0.16968
Step 22390: loss = 0.09305
Step 22395: loss = 0.14824
Step 22400: loss = 0.12042
Step 22405: loss = 0.11505
Step 22410: loss = 0.07870
Step 22415: loss = 0.08509
Step 22420: loss = 0.11288
Step 22425: loss = 0.16422
Step 22430: loss = 0.11757
Step 22435: loss = 0.12983
Step 22440: loss = 0.15279
Step 22445: loss = 0.13061
Step 22450: loss = 0.13640
Step 22455: loss = 0.12290
Step 22460: loss = 0.16522
Step 22465: loss = 0.12587
Step 22470: loss = 0.17506
Step 22475: loss = 0.14823
Step 22480: loss = 0.18458
Step 22485: loss = 0.11017
Step 22490: loss = 0.11039
Step 22495: loss = 0.12337
Step 22500: loss = 0.08898
Step 22505: loss = 0.11980
Step 22510: loss = 0.10471
Step 22515: loss = 0.22721
Step 22520: loss = 0.11834
Step 22525: loss = 0.09546
Step 22530: loss = 0.09871
Step 22535: loss = 0.07021
Step 22540: loss = 0.19756
Step 22545: loss = 0.07594
Step 22550: loss = 0.24700
Step 22555: loss = 0.13682
Step 22560: loss = 0.12658
Step 22565: loss = 0.20499
Step 22570: loss = 0.12807
Step 22575: loss = 0.10801
Step 22580: loss = 0.11662
Step 22585: loss = 0.16154
Step 22590: loss = 0.11941
Step 22595: loss = 0.12049
Step 22600: loss = 0.16969
Step 22605: loss = 0.16662
Step 22610: loss = 0.16697
Step 22615: loss = 0.14848
Step 22620: loss = 0.14179
Training Data Eval:
  Num examples: 49920, Num correct: 48127, Precision @ 1: 0.9641
('Testing Data Eval: EPOCH->', 59)
  Num examples: 9984, Num correct: 7358, Precision @ 1: 0.7370
Step 22625: loss = 0.15133
Step 22630: loss = 0.17557
Step 22635: loss = 0.08886
Step 22640: loss = 0.11453
Step 22645: loss = 0.08422
Step 22650: loss = 0.11550
Step 22655: loss = 0.12593
Step 22660: loss = 0.16246
Step 22665: loss = 0.14050
Step 22670: loss = 0.20230
Step 22675: loss = 0.19931
Step 22680: loss = 0.15828
Step 22685: loss = 0.13828
Step 22690: loss = 0.09868
Step 22695: loss = 0.15814
Step 22700: loss = 0.18617
Step 22705: loss = 0.11594
Step 22710: loss = 0.10719
Step 22715: loss = 0.12424
Step 22720: loss = 0.12462
Step 22725: loss = 0.13771
Step 22730: loss = 0.14068
Step 22735: loss = 0.11415
Step 22740: loss = 0.16571
Step 22745: loss = 0.19155
Step 22750: loss = 0.18571
Step 22755: loss = 0.13103
Step 22760: loss = 0.14632
Step 22765: loss = 0.17872
Step 22770: loss = 0.17029
Step 22775: loss = 0.12144
Step 22780: loss = 0.16379
Step 22785: loss = 0.10453
Step 22790: loss = 0.12568
Step 22795: loss = 0.12191
Step 22800: loss = 0.11484
Step 22805: loss = 0.19138
Step 22810: loss = 0.16208
Step 22815: loss = 0.14816
Step 22820: loss = 0.16803
Step 22825: loss = 0.11463
Step 22830: loss = 0.10013
Step 22835: loss = 0.14614
Step 22840: loss = 0.14184
Step 22845: loss = 0.12471
Step 22850: loss = 0.25515
Step 22855: loss = 0.15811
Step 22860: loss = 0.10671
Step 22865: loss = 0.11030
Step 22870: loss = 0.23099
Step 22875: loss = 0.16947
Step 22880: loss = 0.18644
Step 22885: loss = 0.12710
Step 22890: loss = 0.11485
Step 22895: loss = 0.17627
Step 22900: loss = 0.14171
Step 22905: loss = 0.14014
Step 22910: loss = 0.17135
Step 22915: loss = 0.12389
Step 22920: loss = 0.21202
Step 22925: loss = 0.10351
Step 22930: loss = 0.20119
Step 22935: loss = 0.08556
Step 22940: loss = 0.15655
Step 22945: loss = 0.20119
Step 22950: loss = 0.13243
Step 22955: loss = 0.11481
Step 22960: loss = 0.12747
Step 22965: loss = 0.14214
Step 22970: loss = 0.16025
Step 22975: loss = 0.10120
Step 22980: loss = 0.08575
Step 22985: loss = 0.13938
Step 22990: loss = 0.15838
Step 22995: loss = 0.07423
Step 23000: loss = 0.14461
Step 23005: loss = 0.12333
Step 23010: loss = 0.08145
Training Data Eval:
  Num examples: 49920, Num correct: 48374, Precision @ 1: 0.9690
('Testing Data Eval: EPOCH->', 60)
  Num examples: 9984, Num correct: 7341, Precision @ 1: 0.7353
Step 23015: loss = 0.10853
Step 23020: loss = 0.21081
Step 23025: loss = 0.08995
Step 23030: loss = 0.13656
Step 23035: loss = 0.15662
Step 23040: loss = 0.09769
Step 23045: loss = 0.09210
Step 23050: loss = 0.07915
Step 23055: loss = 0.08289
Step 23060: loss = 0.13998
Step 23065: loss = 0.10782
Step 23070: loss = 0.11723
Step 23075: loss = 0.18146
Step 23080: loss = 0.10123
Step 23085: loss = 0.14270
Step 23090: loss = 0.25025
Step 23095: loss = 0.11831
Step 23100: loss = 0.10842
Step 23105: loss = 0.10368
Step 23110: loss = 0.10361
Step 23115: loss = 0.12694
Step 23120: loss = 0.17628
Step 23125: loss = 0.09470
Step 23130: loss = 0.12853
Step 23135: loss = 0.11197
Step 23140: loss = 0.12900
Step 23145: loss = 0.14284
Step 23150: loss = 0.16538
Step 23155: loss = 0.16204
Step 23160: loss = 0.18252
Step 23165: loss = 0.16560
Step 23170: loss = 0.11911
Step 23175: loss = 0.18511
Step 23180: loss = 0.12821
Step 23185: loss = 0.15181
Step 23190: loss = 0.14590
Step 23195: loss = 0.14389
Step 23200: loss = 0.09370
Step 23205: loss = 0.12758
Step 23210: loss = 0.08812
Step 23215: loss = 0.16784
Step 23220: loss = 0.08090
Step 23225: loss = 0.09201
Step 23230: loss = 0.14484
Step 23235: loss = 0.15502
Step 23240: loss = 0.13275
Step 23245: loss = 0.10822
Step 23250: loss = 0.15099
Step 23255: loss = 0.09650
Step 23260: loss = 0.12927
Step 23265: loss = 0.07374
Step 23270: loss = 0.14103
Step 23275: loss = 0.17044
Step 23280: loss = 0.07043
Step 23285: loss = 0.09936
Step 23290: loss = 0.10659
Step 23295: loss = 0.09787
Step 23300: loss = 0.10205
Step 23305: loss = 0.15116
Step 23310: loss = 0.12780
Step 23315: loss = 0.19700
Step 23320: loss = 0.11885
Step 23325: loss = 0.08481
Step 23330: loss = 0.15008
Step 23335: loss = 0.08202
Step 23340: loss = 0.18559
Step 23345: loss = 0.13423
Step 23350: loss = 0.11596
Step 23355: loss = 0.07121
Step 23360: loss = 0.09963
Step 23365: loss = 0.15076
Step 23370: loss = 0.08009
Step 23375: loss = 0.17403
Step 23380: loss = 0.12238
Step 23385: loss = 0.14454
Step 23390: loss = 0.11093
Step 23395: loss = 0.09029
Step 23400: loss = 0.10399
Training Data Eval:
  Num examples: 49920, Num correct: 48608, Precision @ 1: 0.9737
('Testing Data Eval: EPOCH->', 61)
  Num examples: 9984, Num correct: 7413, Precision @ 1: 0.7425
Step 23405: loss = 0.09828
Step 23410: loss = 0.15851
Step 23415: loss = 0.11472
Step 23420: loss = 0.13378
Step 23425: loss = 0.04528
Step 23430: loss = 0.10950
Step 23435: loss = 0.09638
Step 23440: loss = 0.13360
Step 23445: loss = 0.09417
Step 23450: loss = 0.12007
Step 23455: loss = 0.15068
Step 23460: loss = 0.13598
Step 23465: loss = 0.17730
Step 23470: loss = 0.14563
Step 23475: loss = 0.13196
Step 23480: loss = 0.15025
Step 23485: loss = 0.09642
Step 23490: loss = 0.12930
Step 23495: loss = 0.14737
Step 23500: loss = 0.10739
Step 23505: loss = 0.09393
Step 23510: loss = 0.11249
Step 23515: loss = 0.16116
Step 23520: loss = 0.10838
Step 23525: loss = 0.11815
Step 23530: loss = 0.12694
Step 23535: loss = 0.09271
Step 23540: loss = 0.08715
Step 23545: loss = 0.08389
Step 23550: loss = 0.18103
Step 23555: loss = 0.24283
Step 23560: loss = 0.13695
Step 23565: loss = 0.11136
Step 23570: loss = 0.15594
Step 23575: loss = 0.07935
Step 23580: loss = 0.22566
Step 23585: loss = 0.10711
Step 23590: loss = 0.10118
Step 23595: loss = 0.14783
Step 23600: loss = 0.10933
Step 23605: loss = 0.07715
Step 23610: loss = 0.22487
Step 23615: loss = 0.20868
Step 23620: loss = 0.14614
Step 23625: loss = 0.11784
Step 23630: loss = 0.12554
Step 23635: loss = 0.12598
Step 23640: loss = 0.09385
Step 23645: loss = 0.15664
Step 23650: loss = 0.18399
Step 23655: loss = 0.12860
Step 23660: loss = 0.13510
Step 23665: loss = 0.14101
Step 23670: loss = 0.13962
Step 23675: loss = 0.10986
Step 23680: loss = 0.09458
Step 23685: loss = 0.15374
Step 23690: loss = 0.12551
Step 23695: loss = 0.12971
Step 23700: loss = 0.14479
Step 23705: loss = 0.09070
Step 23710: loss = 0.17584
Step 23715: loss = 0.15932
Step 23720: loss = 0.18326
Step 23725: loss = 0.09140
Step 23730: loss = 0.14353
Step 23735: loss = 0.08483
Step 23740: loss = 0.10749
Step 23745: loss = 0.15568
Step 23750: loss = 0.12832
Step 23755: loss = 0.12424
Step 23760: loss = 0.14814
Step 23765: loss = 0.15835
Step 23770: loss = 0.14376
Step 23775: loss = 0.12578
Step 23780: loss = 0.20208
Step 23785: loss = 0.16397
Step 23790: loss = 0.14152
Training Data Eval:
  Num examples: 49920, Num correct: 47999, Precision @ 1: 0.9615
('Testing Data Eval: EPOCH->', 62)
  Num examples: 9984, Num correct: 7160, Precision @ 1: 0.7171
Step 23795: loss = 0.14228
Step 23800: loss = 0.16720
Step 23805: loss = 0.15029
Step 23810: loss = 0.16157
Step 23815: loss = 0.12889
Step 23820: loss = 0.08733
Step 23825: loss = 0.18143
Step 23830: loss = 0.13646
Step 23835: loss = 0.10670
Step 23840: loss = 0.12445
Step 23845: loss = 0.16932
Step 23850: loss = 0.09499
Step 23855: loss = 0.21941
Step 23860: loss = 0.18266
Step 23865: loss = 0.11500
Step 23870: loss = 0.14237
Step 23875: loss = 0.21605
Step 23880: loss = 0.13075
Step 23885: loss = 0.13343
Step 23890: loss = 0.11854
Step 23895: loss = 0.09130
Step 23900: loss = 0.10864
Step 23905: loss = 0.18144
Step 23910: loss = 0.12274
Step 23915: loss = 0.12046
Step 23920: loss = 0.07473
Step 23925: loss = 0.10366
Step 23930: loss = 0.17612
Step 23935: loss = 0.16332
Step 23940: loss = 0.17420
Step 23945: loss = 0.11652
Step 23950: loss = 0.08976
Step 23955: loss = 0.09765
Step 23960: loss = 0.14181
Step 23965: loss = 0.18291
Step 23970: loss = 0.11221
Step 23975: loss = 0.14514
Step 23980: loss = 0.14476
Step 23985: loss = 0.12307
Step 23990: loss = 0.13245
Step 23995: loss = 0.08571
Step 24000: loss = 0.08918
Step 24005: loss = 0.08938
Step 24010: loss = 0.11352
Step 24015: loss = 0.16576
Step 24020: loss = 0.16036
Step 24025: loss = 0.09754
Step 24030: loss = 0.10206
Step 24035: loss = 0.16713
Step 24040: loss = 0.10207
Step 24045: loss = 0.08352
Step 24050: loss = 0.18611
Step 24055: loss = 0.07019
Step 24060: loss = 0.10520
Step 24065: loss = 0.12050
Step 24070: loss = 0.15301
Step 24075: loss = 0.12399
Step 24080: loss = 0.10480
Step 24085: loss = 0.16430
Step 24090: loss = 0.11728
Step 24095: loss = 0.12813
Step 24100: loss = 0.10005
Step 24105: loss = 0.09969
Step 24110: loss = 0.10431
Step 24115: loss = 0.11721
Step 24120: loss = 0.12355
Step 24125: loss = 0.09688
Step 24130: loss = 0.10518
Step 24135: loss = 0.18045
Step 24140: loss = 0.11205
Step 24145: loss = 0.15876
Step 24150: loss = 0.21811
Step 24155: loss = 0.09183
Step 24160: loss = 0.10297
Step 24165: loss = 0.13065
Step 24170: loss = 0.16679
Step 24175: loss = 0.13729
Step 24180: loss = 0.15625
Training Data Eval:
  Num examples: 49920, Num correct: 48458, Precision @ 1: 0.9707
('Testing Data Eval: EPOCH->', 63)
  Num examples: 9984, Num correct: 7379, Precision @ 1: 0.7391
Step 24185: loss = 0.10164
Step 24190: loss = 0.08752
Step 24195: loss = 0.11764
Step 24200: loss = 0.09609
Step 24205: loss = 0.15442
Step 24210: loss = 0.07814
Step 24215: loss = 0.11388
Step 24220: loss = 0.11150
Step 24225: loss = 0.13002
Step 24230: loss = 0.13547
Step 24235: loss = 0.09513
Step 24240: loss = 0.11621
Step 24245: loss = 0.11791
Step 24250: loss = 0.13642
Step 24255: loss = 0.15154
Step 24260: loss = 0.12860
Step 24265: loss = 0.10914
Step 24270: loss = 0.09480
Step 24275: loss = 0.09789
Step 24280: loss = 0.14079
Step 24285: loss = 0.07788
Step 24290: loss = 0.07620
Step 24295: loss = 0.14003
Step 24300: loss = 0.13352
Step 24305: loss = 0.07454
Step 24310: loss = 0.10514
Step 24315: loss = 0.13152
Step 24320: loss = 0.12676
Step 24325: loss = 0.12921
Step 24330: loss = 0.14955
Step 24335: loss = 0.11946
Step 24340: loss = 0.07098
Step 24345: loss = 0.15519
Step 24350: loss = 0.13406
Step 24355: loss = 0.12772
Step 24360: loss = 0.13229
Step 24365: loss = 0.09161
Step 24370: loss = 0.13474
Step 24375: loss = 0.12486
Step 24380: loss = 0.19428
Step 24385: loss = 0.17146
Step 24390: loss = 0.18095
Step 24395: loss = 0.07024
Step 24400: loss = 0.10608
Step 24405: loss = 0.14285
Step 24410: loss = 0.14849
Step 24415: loss = 0.14597
Step 24420: loss = 0.12593
Step 24425: loss = 0.13120
Step 24430: loss = 0.14038
Step 24435: loss = 0.10242
Step 24440: loss = 0.14117
Step 24445: loss = 0.08074
Step 24450: loss = 0.11115
Step 24455: loss = 0.08988
Step 24460: loss = 0.07732
Step 24465: loss = 0.07658
Step 24470: loss = 0.10307
Step 24475: loss = 0.10309
Step 24480: loss = 0.15969
Step 24485: loss = 0.12849
Step 24490: loss = 0.09473
Step 24495: loss = 0.08231
Step 24500: loss = 0.19595
Step 24505: loss = 0.18758
Step 24510: loss = 0.07880
Step 24515: loss = 0.18288
Step 24520: loss = 0.11340
Step 24525: loss = 0.11608
Step 24530: loss = 0.14093
Step 24535: loss = 0.10014
Step 24540: loss = 0.18198
Step 24545: loss = 0.07963
Step 24550: loss = 0.14808
Step 24555: loss = 0.08832
Step 24560: loss = 0.13159
Step 24565: loss = 0.23992
Step 24570: loss = 0.12140
Training Data Eval:
  Num examples: 49920, Num correct: 48635, Precision @ 1: 0.9743
('Testing Data Eval: EPOCH->', 64)
  Num examples: 9984, Num correct: 7399, Precision @ 1: 0.7411
Step 24575: loss = 0.06995
Step 24580: loss = 0.15639
Step 24585: loss = 0.07656
Step 24590: loss = 0.07489
Step 24595: loss = 0.13422
Step 24600: loss = 0.20374
Step 24605: loss = 0.09951
Step 24610: loss = 0.08679
Step 24615: loss = 0.07191
Step 24620: loss = 0.12156
Step 24625: loss = 0.14324
Step 24630: loss = 0.10119
Step 24635: loss = 0.16146
Step 24640: loss = 0.10716
Step 24645: loss = 0.13862
Step 24650: loss = 0.19542
Step 24655: loss = 0.12930
Step 24660: loss = 0.08369
Step 24665: loss = 0.11369
Step 24670: loss = 0.19869
Step 24675: loss = 0.15403
Step 24680: loss = 0.14763
Step 24685: loss = 0.11054
Step 24690: loss = 0.22955
Step 24695: loss = 0.08493
Step 24700: loss = 0.09445
Step 24705: loss = 0.09829
Step 24710: loss = 0.09136
Step 24715: loss = 0.11655
Step 24720: loss = 0.13422
Step 24725: loss = 0.13793
Step 24730: loss = 0.14844
Step 24735: loss = 0.10791
Step 24740: loss = 0.11256
Step 24745: loss = 0.07833
Step 24750: loss = 0.09744
Step 24755: loss = 0.21103
Step 24760: loss = 0.13715
Step 24765: loss = 0.15772
Step 24770: loss = 0.13795
Step 24775: loss = 0.13399
Step 24780: loss = 0.12929
Step 24785: loss = 0.12194
Step 24790: loss = 0.18760
Step 24795: loss = 0.18653
Step 24800: loss = 0.14388
Step 24805: loss = 0.11371
Step 24810: loss = 0.08975
Step 24815: loss = 0.16491
Step 24820: loss = 0.11151
Step 24825: loss = 0.18408
Step 24830: loss = 0.13900
Step 24835: loss = 0.16839
Step 24840: loss = 0.15325
Step 24845: loss = 0.12716
Step 24850: loss = 0.09509
Step 24855: loss = 0.10134
Step 24860: loss = 0.15707
Step 24865: loss = 0.17539
Step 24870: loss = 0.11611
Step 24875: loss = 0.14067
Step 24880: loss = 0.20584
Step 24885: loss = 0.15391
Step 24890: loss = 0.27503
Step 24895: loss = 0.13093
Step 24900: loss = 0.12411
Step 24905: loss = 0.13429
Step 24910: loss = 0.14945
Step 24915: loss = 0.13803
Step 24920: loss = 0.16735
Step 24925: loss = 0.09109
Step 24930: loss = 0.13528
Step 24935: loss = 0.12689
Step 24940: loss = 0.17288
Step 24945: loss = 0.14370
Step 24950: loss = 0.12491
Step 24955: loss = 0.19735
Step 24960: loss = 0.14064
Training Data Eval:
  Num examples: 49920, Num correct: 48430, Precision @ 1: 0.9702
('Testing Data Eval: EPOCH->', 65)
  Num examples: 9984, Num correct: 7321, Precision @ 1: 0.7333
Step 24965: loss = 0.17126
Step 24970: loss = 0.09365
Step 24975: loss = 0.12287
Step 24980: loss = 0.09473
Step 24985: loss = 0.08845
Step 24990: loss = 0.10595
Step 24995: loss = 0.07965
Step 25000: loss = 0.08685
Step 25005: loss = 0.07207
Step 25010: loss = 0.13138
Step 25015: loss = 0.16685
Step 25020: loss = 0.14507
Step 25025: loss = 0.09670
Step 25030: loss = 0.07812
Step 25035: loss = 0.07706
Step 25040: loss = 0.08958
Step 25045: loss = 0.19029
Step 25050: loss = 0.11896
Step 25055: loss = 0.16652
Step 25060: loss = 0.14589
Step 25065: loss = 0.18302
Step 25070: loss = 0.10184
Step 25075: loss = 0.15783
Step 25080: loss = 0.09624
Step 25085: loss = 0.15630
Step 25090: loss = 0.12865
Step 25095: loss = 0.17640
Step 25100: loss = 0.13382
Step 25105: loss = 0.13725
Step 25110: loss = 0.16379
Step 25115: loss = 0.11076
Step 25120: loss = 0.16407
Step 25125: loss = 0.24919
Step 25130: loss = 0.15225
Step 25135: loss = 0.09173
Step 25140: loss = 0.16350
Step 25145: loss = 0.09009
Step 25150: loss = 0.11897
Step 25155: loss = 0.11363
Step 25160: loss = 0.10143
Step 25165: loss = 0.14315
Step 25170: loss = 0.22062
Step 25175: loss = 0.14560
Step 25180: loss = 0.10862
Step 25185: loss = 0.10526
Step 25190: loss = 0.13384
Step 25195: loss = 0.10982
Step 25200: loss = 0.05967
Step 25205: loss = 0.09621
Step 25210: loss = 0.12286
Step 25215: loss = 0.11165
Step 25220: loss = 0.10295
Step 25225: loss = 0.07126
Step 25230: loss = 0.10257
Step 25235: loss = 0.14878
Step 25240: loss = 0.08164
Step 25245: loss = 0.15524
Step 25250: loss = 0.09475
Step 25255: loss = 0.06957
Step 25260: loss = 0.16597
Step 25265: loss = 0.09014
Step 25270: loss = 0.10105
Step 25275: loss = 0.07851
Step 25280: loss = 0.08628
Step 25285: loss = 0.07999
Step 25290: loss = 0.08919
Step 25295: loss = 0.09741
Step 25300: loss = 0.13659
Step 25305: loss = 0.11523
Step 25310: loss = 0.07830
Step 25315: loss = 0.08935
Step 25320: loss = 0.14812
Step 25325: loss = 0.12513
Step 25330: loss = 0.12249
Step 25335: loss = 0.09814
Step 25340: loss = 0.10800
Step 25345: loss = 0.09291
Step 25350: loss = 0.12511
Training Data Eval:
  Num examples: 49920, Num correct: 48649, Precision @ 1: 0.9745
('Testing Data Eval: EPOCH->', 66)
  Num examples: 9984, Num correct: 7364, Precision @ 1: 0.7376
Step 25355: loss = 0.13332
Step 25360: loss = 0.12576
Step 25365: loss = 0.13859
Step 25370: loss = 0.15148
Step 25375: loss = 0.08612
Step 25380: loss = 0.10812
Step 25385: loss = 0.07472
Step 25390: loss = 0.09222
Step 25395: loss = 0.12178
Step 25400: loss = 0.10166
Step 25405: loss = 0.13057
Step 25410: loss = 0.12698
Step 25415: loss = 0.24616
Step 25420: loss = 0.16442
Step 25425: loss = 0.18964
Step 25430: loss = 0.11863
Step 25435: loss = 0.23330
Step 25440: loss = 0.15102
Step 25445: loss = 0.13330
Step 25450: loss = 0.15171
Step 25455: loss = 0.24206
Step 25460: loss = 0.16702
Step 25465: loss = 0.12786
Step 25470: loss = 0.14144
Step 25475: loss = 0.16290
Step 25480: loss = 0.11306
Step 25485: loss = 0.16246
Step 25490: loss = 0.17405
Step 25495: loss = 0.12024
Step 25500: loss = 0.09648
Step 25505: loss = 0.15692
Step 25510: loss = 0.10591
Step 25515: loss = 0.13731
Step 25520: loss = 0.10518
Step 25525: loss = 0.09952
Step 25530: loss = 0.13166
Step 25535: loss = 0.09351
Step 25540: loss = 0.20428
Step 25545: loss = 0.20477
Step 25550: loss = 0.09707
Step 25555: loss = 0.10170
Step 25560: loss = 0.23495
Step 25565: loss = 0.17052
Step 25570: loss = 0.10624
Step 25575: loss = 0.11363
Step 25580: loss = 0.12978
Step 25585: loss = 0.10333
Step 25590: loss = 0.14057
Step 25595: loss = 0.23837
Step 25600: loss = 0.10886
Step 25605: loss = 0.12121
Step 25610: loss = 0.09626
Step 25615: loss = 0.10190
Step 25620: loss = 0.15356
Step 25625: loss = 0.11404
Step 25630: loss = 0.12504
Step 25635: loss = 0.15564
Step 25640: loss = 0.12396
Step 25645: loss = 0.12915
Step 25650: loss = 0.13774
Step 25655: loss = 0.06966
Step 25660: loss = 0.14343
Step 25665: loss = 0.09094
Step 25670: loss = 0.20435
Step 25675: loss = 0.16834
Step 25680: loss = 0.14306
Step 25685: loss = 0.17203
Step 25690: loss = 0.15626
Step 25695: loss = 0.18634
Step 25700: loss = 0.14594
Step 25705: loss = 0.15045
Step 25710: loss = 0.12442
Step 25715: loss = 0.09415
Step 25720: loss = 0.12781
Step 25725: loss = 0.15421
Step 25730: loss = 0.21275
Step 25735: loss = 0.12803
Step 25740: loss = 0.08638
Training Data Eval:
  Num examples: 49920, Num correct: 48553, Precision @ 1: 0.9726
('Testing Data Eval: EPOCH->', 67)
  Num examples: 9984, Num correct: 7329, Precision @ 1: 0.7341
Step 25745: loss = 0.14185
Step 25750: loss = 0.18674
Step 25755: loss = 0.14466
Step 25760: loss = 0.18031
Step 25765: loss = 0.11145
Step 25770: loss = 0.06348
Step 25775: loss = 0.08351
Step 25780: loss = 0.07364
Step 25785: loss = 0.17299
Step 25790: loss = 0.09441
Step 25795: loss = 0.11710
Step 25800: loss = 0.08687
Step 25805: loss = 0.17866
Step 25810: loss = 0.09439
Step 25815: loss = 0.11204
Step 25820: loss = 0.05518
Step 25825: loss = 0.09535
Step 25830: loss = 0.08734
Step 25835: loss = 0.09600
Step 25840: loss = 0.08004
Step 25845: loss = 0.11072
Step 25850: loss = 0.11333
Step 25855: loss = 0.09400
Step 25860: loss = 0.12671
Step 25865: loss = 0.11858
Step 25870: loss = 0.15003
Step 25875: loss = 0.16263
Step 25880: loss = 0.13021
Step 25885: loss = 0.10499
Step 25890: loss = 0.22471
Step 25895: loss = 0.11407
Step 25900: loss = 0.12567
Step 25905: loss = 0.12977
Step 25910: loss = 0.11433
Step 25915: loss = 0.24087
Step 25920: loss = 0.11059
Step 25925: loss = 0.10454
Step 25930: loss = 0.12001
Step 25935: loss = 0.12235
Step 25940: loss = 0.10697
Step 25945: loss = 0.10225
Step 25950: loss = 0.11353
Step 25955: loss = 0.09853
Step 25960: loss = 0.14598
Step 25965: loss = 0.10217
Step 25970: loss = 0.15785
Step 25975: loss = 0.11676
Step 25980: loss = 0.09679
Step 25985: loss = 0.14142
Step 25990: loss = 0.11583
Step 25995: loss = 0.13921
Step 26000: loss = 0.14672
Step 26005: loss = 0.08965
Step 26010: loss = 0.13710
Step 26015: loss = 0.13348
Step 26020: loss = 0.22203
Step 26025: loss = 0.09300
Step 26030: loss = 0.19954
Step 26035: loss = 0.07162
Step 26040: loss = 0.11303
Step 26045: loss = 0.14745
Step 26050: loss = 0.18566
Step 26055: loss = 0.17662
Step 26060: loss = 0.08690
Step 26065: loss = 0.14154
Step 26070: loss = 0.07103
Step 26075: loss = 0.12827
Step 26080: loss = 0.19891
Step 26085: loss = 0.08316
Step 26090: loss = 0.13626
Step 26095: loss = 0.11832
Step 26100: loss = 0.12985
Step 26105: loss = 0.32244
Step 26110: loss = 0.07779
Step 26115: loss = 0.12181
Step 26120: loss = 0.10959
Step 26125: loss = 0.09146
Step 26130: loss = 0.07984
Training Data Eval:
  Num examples: 49920, Num correct: 48658, Precision @ 1: 0.9747
('Testing Data Eval: EPOCH->', 68)
  Num examples: 9984, Num correct: 7438, Precision @ 1: 0.7450
Step 26135: loss = 0.07192
Step 26140: loss = 0.09096
Step 26145: loss = 0.08274
Step 26150: loss = 0.08424
Step 26155: loss = 0.07399
Step 26160: loss = 0.12933
Step 26165: loss = 0.10359
Step 26170: loss = 0.14159
Step 26175: loss = 0.08006
Step 26180: loss = 0.10781
Step 26185: loss = 0.07220
Step 26190: loss = 0.15958
Step 26195: loss = 0.10019
Step 26200: loss = 0.07101
Step 26205: loss = 0.06878
Step 26210: loss = 0.09342
Step 26215: loss = 0.13345
Step 26220: loss = 0.11702
Step 26225: loss = 0.10703
Step 26230: loss = 0.08427
Step 26235: loss = 0.23837
Step 26240: loss = 0.15352
Step 26245: loss = 0.08722
Step 26250: loss = 0.12284
Step 26255: loss = 0.08492
Step 26260: loss = 0.11148
Step 26265: loss = 0.08457
Step 26270: loss = 0.13598
Step 26275: loss = 0.10524
Step 26280: loss = 0.13580
Step 26285: loss = 0.07540
Step 26290: loss = 0.10580
Step 26295: loss = 0.13325
Step 26300: loss = 0.11860
Step 26305: loss = 0.07601
Step 26310: loss = 0.14746
Step 26315: loss = 0.13282
Step 26320: loss = 0.16375
Step 26325: loss = 0.08133
Step 26330: loss = 0.11833
Step 26335: loss = 0.09349
Step 26340: loss = 0.12243
Step 26345: loss = 0.19654
Step 26350: loss = 0.09307
Step 26355: loss = 0.18918
Step 26360: loss = 0.08327
Step 26365: loss = 0.10494
Step 26370: loss = 0.12477
Step 26375: loss = 0.09268
Step 26380: loss = 0.10934
Step 26385: loss = 0.08740
Step 26390: loss = 0.07040
Step 26395: loss = 0.06038
Step 26400: loss = 0.12906
Step 26405: loss = 0.09932
Step 26410: loss = 0.11350
Step 26415: loss = 0.11544
Step 26420: loss = 0.13400
Step 26425: loss = 0.10810
Step 26430: loss = 0.13853
Step 26435: loss = 0.08195
Step 26440: loss = 0.11602
Step 26445: loss = 0.06999
Step 26450: loss = 0.06693
Step 26455: loss = 0.13139
Step 26460: loss = 0.09371
Step 26465: loss = 0.12332
Step 26470: loss = 0.11093
Step 26475: loss = 0.06820
Step 26480: loss = 0.12111
Step 26485: loss = 0.10072
Step 26490: loss = 0.12014
Step 26495: loss = 0.07887
Step 26500: loss = 0.13750
Step 26505: loss = 0.13445
Step 26510: loss = 0.08855
Step 26515: loss = 0.17863
Step 26520: loss = 0.09664
Training Data Eval:
  Num examples: 49920, Num correct: 48716, Precision @ 1: 0.9759
('Testing Data Eval: EPOCH->', 69)
  Num examples: 9984, Num correct: 7391, Precision @ 1: 0.7403
Step 26525: loss = 0.08471
Step 26530: loss = 0.10451
Step 26535: loss = 0.07408
Step 26540: loss = 0.13526
Step 26545: loss = 0.10515
Step 26550: loss = 0.05185
Step 26555: loss = 0.08450
Step 26560: loss = 0.10776
Step 26565: loss = 0.11786
Step 26570: loss = 0.15375
Step 26575: loss = 0.14296
Step 26580: loss = 0.06342
Step 26585: loss = 0.11773
Step 26590: loss = 0.08108
Step 26595: loss = 0.07974
Step 26600: loss = 0.11261
Step 26605: loss = 0.08846
Step 26610: loss = 0.19677
Step 26615: loss = 0.10411
Step 26620: loss = 0.13093
Step 26625: loss = 0.11621
Step 26630: loss = 0.11824
Step 26635: loss = 0.13160
Step 26640: loss = 0.07448
Step 26645: loss = 0.14309
Step 26650: loss = 0.08098
Step 26655: loss = 0.09225
Step 26660: loss = 0.14008
Step 26665: loss = 0.06309
Step 26670: loss = 0.09094
Step 26675: loss = 0.15325
Step 26680: loss = 0.07892
Step 26685: loss = 0.12379
Step 26690: loss = 0.11547
Step 26695: loss = 0.14616
Step 26700: loss = 0.15920
Step 26705: loss = 0.09461
Step 26710: loss = 0.10179
Step 26715: loss = 0.15480
Step 26720: loss = 0.10985
Step 26725: loss = 0.09714
Step 26730: loss = 0.11138
Step 26735: loss = 0.11355
Step 26740: loss = 0.06845
Step 26745: loss = 0.10630
Step 26750: loss = 0.09192
Step 26755: loss = 0.09471
Step 26760: loss = 0.07911
Step 26765: loss = 0.12087
Step 26770: loss = 0.08742
Step 26775: loss = 0.11550
Step 26780: loss = 0.11344
Step 26785: loss = 0.08834
Step 26790: loss = 0.12578
Step 26795: loss = 0.20275
Step 26800: loss = 0.07554
Step 26805: loss = 0.13342
Step 26810: loss = 0.06095
Step 26815: loss = 0.11295
Step 26820: loss = 0.09092
Step 26825: loss = 0.09243
Step 26830: loss = 0.15748
Step 26835: loss = 0.08070
Step 26840: loss = 0.13857
Step 26845: loss = 0.09825
Step 26850: loss = 0.10421
Step 26855: loss = 0.12794
Step 26860: loss = 0.17546
Step 26865: loss = 0.09334
Step 26870: loss = 0.17089
Step 26875: loss = 0.15427
Step 26880: loss = 0.08376
Step 26885: loss = 0.11452
Step 26890: loss = 0.14251
Step 26895: loss = 0.09850
Step 26900: loss = 0.14451
Step 26905: loss = 0.10613
Step 26910: loss = 0.15418
Training Data Eval:
  Num examples: 49920, Num correct: 48512, Precision @ 1: 0.9718
('Testing Data Eval: EPOCH->', 70)
  Num examples: 9984, Num correct: 7195, Precision @ 1: 0.7207
Step 26915: loss = 0.12373
Step 26920: loss = 0.11824
Step 26925: loss = 0.07560
Step 26930: loss = 0.12626
Step 26935: loss = 0.13457
Step 26940: loss = 0.11564
Step 26945: loss = 0.09753
Step 26950: loss = 0.05226
Step 26955: loss = 0.11568
Step 26960: loss = 0.09233
Step 26965: loss = 0.06986
Step 26970: loss = 0.19617
Step 26975: loss = 0.06220
Step 26980: loss = 0.08111
Step 26985: loss = 0.11461
Step 26990: loss = 0.09170
Step 26995: loss = 0.10939
Step 27000: loss = 0.14804
Step 27005: loss = 0.07405
Step 27010: loss = 0.12499
Step 27015: loss = 0.11690
Step 27020: loss = 0.10130
Step 27025: loss = 0.08763
Step 27030: loss = 0.07860
Step 27035: loss = 0.08915
Step 27040: loss = 0.04933
Step 27045: loss = 0.15015
Step 27050: loss = 0.16843
Step 27055: loss = 0.12494
Step 27060: loss = 0.08819
Step 27065: loss = 0.08978
Step 27070: loss = 0.16155
Step 27075: loss = 0.11126
Step 27080: loss = 0.12276
Step 27085: loss = 0.13769
Step 27090: loss = 0.08444
Step 27095: loss = 0.11233
Step 27100: loss = 0.06791
Step 27105: loss = 0.11731
Step 27110: loss = 0.08334
Step 27115: loss = 0.14236
Step 27120: loss = 0.20266
Step 27125: loss = 0.06011
Step 27130: loss = 0.11117
Step 27135: loss = 0.14620
Step 27140: loss = 0.11501
Step 27145: loss = 0.09240
Step 27150: loss = 0.06638
Step 27155: loss = 0.08636
Step 27160: loss = 0.19730
Step 27165: loss = 0.17452
Step 27170: loss = 0.09055
Step 27175: loss = 0.15430
Step 27180: loss = 0.08406
Step 27185: loss = 0.14793
Step 27190: loss = 0.16447
Step 27195: loss = 0.10706
Step 27200: loss = 0.17035
Step 27205: loss = 0.09479
Step 27210: loss = 0.19368
Step 27215: loss = 0.14392
Step 27220: loss = 0.08094
Step 27225: loss = 0.09451
Step 27230: loss = 0.12017
Step 27235: loss = 0.13194
Step 27240: loss = 0.15047
Step 27245: loss = 0.10953
Step 27250: loss = 0.09954
Step 27255: loss = 0.10010
Step 27260: loss = 0.10721
Step 27265: loss = 0.10997
Step 27270: loss = 0.08953
Step 27275: loss = 0.12928
Step 27280: loss = 0.09322
Step 27285: loss = 0.14442
Step 27290: loss = 0.19067
Step 27295: loss = 0.09199
Step 27300: loss = 0.10961
Training Data Eval:
  Num examples: 49920, Num correct: 48467, Precision @ 1: 0.9709
('Testing Data Eval: EPOCH->', 71)
  Num examples: 9984, Num correct: 7282, Precision @ 1: 0.7294
Step 27305: loss = 0.09373
Step 27310: loss = 0.10246
Step 27315: loss = 0.07347
Step 27320: loss = 0.08801
Step 27325: loss = 0.07400
Step 27330: loss = 0.09037
Step 27335: loss = 0.12698
Step 27340: loss = 0.13771
Step 27345: loss = 0.08449
Step 27350: loss = 0.08907
Step 27355: loss = 0.10257
Step 27360: loss = 0.08787
Step 27365: loss = 0.22362
Step 27370: loss = 0.13665
Step 27375: loss = 0.07707
Step 27380: loss = 0.08927
Step 27385: loss = 0.13886
Step 27390: loss = 0.11879
Step 27395: loss = 0.13768
Step 27400: loss = 0.08395
Step 27405: loss = 0.09373
Step 27410: loss = 0.17491
Step 27415: loss = 0.09167
Step 27420: loss = 0.14951
Step 27425: loss = 0.08466
Step 27430: loss = 0.09708
Step 27435: loss = 0.09158
Step 27440: loss = 0.06301
Step 27445: loss = 0.08421
Step 27450: loss = 0.16811
Step 27455: loss = 0.06309
Step 27460: loss = 0.10381
Step 27465: loss = 0.05891
Step 27470: loss = 0.07881
Step 27475: loss = 0.12598
Step 27480: loss = 0.11579
Step 27485: loss = 0.06671
Step 27490: loss = 0.08416
Step 27495: loss = 0.09096
Step 27500: loss = 0.07064
Step 27505: loss = 0.19517
Step 27510: loss = 0.09275
Step 27515: loss = 0.11222
Step 27520: loss = 0.07757
Step 27525: loss = 0.18601
Step 27530: loss = 0.06389
Step 27535: loss = 0.10654
Step 27540: loss = 0.12536
Step 27545: loss = 0.11800
Step 27550: loss = 0.11807
Step 27555: loss = 0.14496
Step 27560: loss = 0.14042
Step 27565: loss = 0.09069
Step 27570: loss = 0.09219
Step 27575: loss = 0.12559
Step 27580: loss = 0.10443
Step 27585: loss = 0.14717
Step 27590: loss = 0.10578
Step 27595: loss = 0.09320
Step 27600: loss = 0.17253
Step 27605: loss = 0.10088
Step 27610: loss = 0.13140
Step 27615: loss = 0.08883
Step 27620: loss = 0.18457
Step 27625: loss = 0.11162
Step 27630: loss = 0.06736
Step 27635: loss = 0.22225
Step 27640: loss = 0.11222
Step 27645: loss = 0.11365
Step 27650: loss = 0.10900
Step 27655: loss = 0.22070
Step 27660: loss = 0.16417
Step 27665: loss = 0.14308
Step 27670: loss = 0.10567
Step 27675: loss = 0.15930
Step 27680: loss = 0.15908
Step 27685: loss = 0.13776
Step 27690: loss = 0.14743
Training Data Eval:
  Num examples: 49920, Num correct: 48609, Precision @ 1: 0.9737
('Testing Data Eval: EPOCH->', 72)
  Num examples: 9984, Num correct: 7358, Precision @ 1: 0.7370
Step 27695: loss = 0.09334
Step 27700: loss = 0.11673
Step 27705: loss = 0.13543
Step 27710: loss = 0.09459
Step 27715: loss = 0.10672
Step 27720: loss = 0.09511
Step 27725: loss = 0.06724
Step 27730: loss = 0.07387
Step 27735: loss = 0.12020
Step 27740: loss = 0.04184
Step 27745: loss = 0.13149
Step 27750: loss = 0.12572
Step 27755: loss = 0.11303
Step 27760: loss = 0.08914
Step 27765: loss = 0.14859
Step 27770: loss = 0.15132
Step 27775: loss = 0.09635
Step 27780: loss = 0.09642
Step 27785: loss = 0.09185
Step 27790: loss = 0.12993
Step 27795: loss = 0.14541
Step 27800: loss = 0.08853
Step 27805: loss = 0.13244
Step 27810: loss = 0.10944
Step 27815: loss = 0.10965
Step 27820: loss = 0.10445
Step 27825: loss = 0.16176
Step 27830: loss = 0.17766
Step 27835: loss = 0.09355
Step 27840: loss = 0.10849
Step 27845: loss = 0.16157
Step 27850: loss = 0.09680
Step 27855: loss = 0.14126
Step 27860: loss = 0.07780
Step 27865: loss = 0.10093
Step 27870: loss = 0.10531
Step 27875: loss = 0.19008
Step 27880: loss = 0.07531
Step 27885: loss = 0.13706
Step 27890: loss = 0.08667
Step 27895: loss = 0.10863
Step 27900: loss = 0.17809
Step 27905: loss = 0.11809
Step 27910: loss = 0.08767
Step 27915: loss = 0.12349
Step 27920: loss = 0.13060
Step 27925: loss = 0.11594
Step 27930: loss = 0.09338
Step 27935: loss = 0.11181
Step 27940: loss = 0.10051
Step 27945: loss = 0.06882
Step 27950: loss = 0.14236
Step 27955: loss = 0.10224
Step 27960: loss = 0.13164
Step 27965: loss = 0.11812
Step 27970: loss = 0.07617
Step 27975: loss = 0.10831
Step 27980: loss = 0.18397
Step 27985: loss = 0.06482
Step 27990: loss = 0.17270
Step 27995: loss = 0.09581
Step 28000: loss = 0.06587
Step 28005: loss = 0.12582
Step 28010: loss = 0.08764
Step 28015: loss = 0.07939
Step 28020: loss = 0.10095
Step 28025: loss = 0.07577
Step 28030: loss = 0.09806
Step 28035: loss = 0.13847
Step 28040: loss = 0.07267
Step 28045: loss = 0.09070
Step 28050: loss = 0.08012
Step 28055: loss = 0.10441
Step 28060: loss = 0.09521
Step 28065: loss = 0.10502
Step 28070: loss = 0.07733
Step 28075: loss = 0.05037
Step 28080: loss = 0.10628
Training Data Eval:
  Num examples: 49920, Num correct: 48884, Precision @ 1: 0.9792
('Testing Data Eval: EPOCH->', 73)
  Num examples: 9984, Num correct: 7380, Precision @ 1: 0.7392
Step 28085: loss = 0.11084
Step 28090: loss = 0.11682
Step 28095: loss = 0.09053
Step 28100: loss = 0.07663
Step 28105: loss = 0.07202
Step 28110: loss = 0.13106
Step 28115: loss = 0.07729
Step 28120: loss = 0.11905
Step 28125: loss = 0.17780
Step 28130: loss = 0.09543
Step 28135: loss = 0.13065
Step 28140: loss = 0.10083
Step 28145: loss = 0.08048
Step 28150: loss = 0.11377
Step 28155: loss = 0.08526
Step 28160: loss = 0.09808
Step 28165: loss = 0.13691
Step 28170: loss = 0.10652
Step 28175: loss = 0.11334
Step 28180: loss = 0.19761
Step 28185: loss = 0.07504
Step 28190: loss = 0.07223
Step 28195: loss = 0.12050
Step 28200: loss = 0.08969
Step 28205: loss = 0.06360
Step 28210: loss = 0.14382
Step 28215: loss = 0.14633
Step 28220: loss = 0.11182
Step 28225: loss = 0.11000
Step 28230: loss = 0.16257
Step 28235: loss = 0.15020
Step 28240: loss = 0.10334
Step 28245: loss = 0.15631
Step 28250: loss = 0.15812
Step 28255: loss = 0.15037
Step 28260: loss = 0.12270
Step 28265: loss = 0.11434
Step 28270: loss = 0.11304
Step 28275: loss = 0.12998
Step 28280: loss = 0.10066
Step 28285: loss = 0.08621
Step 28290: loss = 0.07769
Step 28295: loss = 0.16250
Step 28300: loss = 0.10162
Step 28305: loss = 0.09998
Step 28310: loss = 0.11875
Step 28315: loss = 0.10921
Step 28320: loss = 0.08906
Step 28325: loss = 0.07485
Step 28330: loss = 0.09258
Step 28335: loss = 0.06190
Step 28340: loss = 0.07981
Step 28345: loss = 0.09832
Step 28350: loss = 0.12916
Step 28355: loss = 0.09818
Step 28360: loss = 0.07777
Step 28365: loss = 0.08283
Step 28370: loss = 0.06129
Step 28375: loss = 0.14557
Step 28380: loss = 0.12136
Step 28385: loss = 0.07610
Step 28390: loss = 0.09816
Step 28395: loss = 0.10213
Step 28400: loss = 0.07812
Step 28405: loss = 0.07270
Step 28410: loss = 0.08676
Step 28415: loss = 0.08643
Step 28420: loss = 0.10380
Step 28425: loss = 0.11812
Step 28430: loss = 0.08200
Step 28435: loss = 0.11781
Step 28440: loss = 0.07145
Step 28445: loss = 0.06530
Step 28450: loss = 0.08293
Step 28455: loss = 0.08189
Step 28460: loss = 0.11092
Step 28465: loss = 0.14037
Step 28470: loss = 0.08194
Training Data Eval:
  Num examples: 49920, Num correct: 48951, Precision @ 1: 0.9806
('Testing Data Eval: EPOCH->', 74)
  Num examples: 9984, Num correct: 7457, Precision @ 1: 0.7469
Step 28475: loss = 0.10325
Step 28480: loss = 0.12077
Step 28485: loss = 0.07977
Step 28490: loss = 0.05859
Step 28495: loss = 0.06777
Step 28500: loss = 0.11433
Step 28505: loss = 0.07810
Step 28510: loss = 0.10318
Step 28515: loss = 0.08643
Step 28520: loss = 0.08866
Step 28525: loss = 0.10842
Step 28530: loss = 0.07802
Step 28535: loss = 0.07519
Step 28540: loss = 0.14305
Step 28545: loss = 0.10383
Step 28550: loss = 0.07949
Step 28555: loss = 0.15897
Step 28560: loss = 0.10740
Step 28565: loss = 0.12272
Step 28570: loss = 0.05534
Step 28575: loss = 0.09831
Step 28580: loss = 0.11603
Step 28585: loss = 0.05056
Step 28590: loss = 0.13197
Step 28595: loss = 0.07694
Step 28600: loss = 0.10581
Step 28605: loss = 0.07565
Step 28610: loss = 0.06898
Step 28615: loss = 0.06939
Step 28620: loss = 0.13857
Step 28625: loss = 0.07028
Step 28630: loss = 0.07951
Step 28635: loss = 0.07457
Step 28640: loss = 0.14570
Step 28645: loss = 0.14118
Step 28650: loss = 0.18437
Step 28655: loss = 0.06467
Step 28660: loss = 0.12316
Step 28665: loss = 0.07057
Step 28670: loss = 0.06973
Step 28675: loss = 0.13746
Step 28680: loss = 0.11254
Step 28685: loss = 0.13569
Step 28690: loss = 0.15557
Step 28695: loss = 0.07707
Step 28700: loss = 0.12142
Step 28705: loss = 0.09060
Step 28710: loss = 0.08704
Step 28715: loss = 0.09112
Step 28720: loss = 0.11777
Step 28725: loss = 0.09524
Step 28730: loss = 0.14453
Step 28735: loss = 0.10100
Step 28740: loss = 0.07659
Step 28745: loss = 0.07198
Step 28750: loss = 0.08067
Step 28755: loss = 0.13303
Step 28760: loss = 0.18040
Step 28765: loss = 0.09252
Step 28770: loss = 0.08739
Step 28775: loss = 0.09399
Step 28780: loss = 0.10607
Step 28785: loss = 0.09499
Step 28790: loss = 0.15568
Step 28795: loss = 0.06492
Step 28800: loss = 0.11064
Step 28805: loss = 0.14860
Step 28810: loss = 0.08943
Step 28815: loss = 0.13805
Step 28820: loss = 0.11763
Step 28825: loss = 0.08458
Step 28830: loss = 0.10425
Step 28835: loss = 0.11043
Step 28840: loss = 0.07672
Step 28845: loss = 0.07621
Step 28850: loss = 0.06545
Step 28855: loss = 0.13488
Step 28860: loss = 0.13749
Training Data Eval:
  Num examples: 49920, Num correct: 49059, Precision @ 1: 0.9828
('Testing Data Eval: EPOCH->', 75)
  Num examples: 9984, Num correct: 7361, Precision @ 1: 0.7373
Step 28865: loss = 0.08458
Step 28870: loss = 0.09857
Step 28875: loss = 0.07961
Step 28880: loss = 0.08438
Step 28885: loss = 0.06791
Step 28890: loss = 0.10192
Step 28895: loss = 0.09292
Step 28900: loss = 0.12079
Step 28905: loss = 0.04976
Step 28910: loss = 0.10518
Step 28915: loss = 0.10822
Step 28920: loss = 0.07995
Step 28925: loss = 0.06951
Step 28930: loss = 0.07867
Step 28935: loss = 0.07676
Step 28940: loss = 0.10125
Step 28945: loss = 0.07698
Step 28950: loss = 0.08173
Step 28955: loss = 0.12068
Step 28960: loss = 0.06051
Step 28965: loss = 0.07524
Step 28970: loss = 0.06643
Step 28975: loss = 0.09907
Step 28980: loss = 0.10255
Step 28985: loss = 0.07657
Step 28990: loss = 0.11191
Step 28995: loss = 0.12485
Step 29000: loss = 0.12760
Step 29005: loss = 0.10768
Step 29010: loss = 0.14915
Step 29015: loss = 0.09630
Step 29020: loss = 0.10612
Step 29025: loss = 0.11586
Step 29030: loss = 0.22109
Step 29035: loss = 0.13517
Step 29040: loss = 0.11986
Step 29045: loss = 0.13509
Step 29050: loss = 0.11724
Step 29055: loss = 0.11290
Step 29060: loss = 0.08429
Step 29065: loss = 0.12226
Step 29070: loss = 0.12192
Step 29075: loss = 0.11924
Step 29080: loss = 0.08369
Step 29085: loss = 0.08338
Step 29090: loss = 0.08592
Step 29095: loss = 0.12360
Step 29100: loss = 0.08677
Step 29105: loss = 0.10773
Step 29110: loss = 0.07864
Step 29115: loss = 0.08085
Step 29120: loss = 0.12469
Step 29125: loss = 0.15223
Step 29130: loss = 0.13368
Step 29135: loss = 0.11723
Step 29140: loss = 0.07196
Step 29145: loss = 0.09923
Step 29150: loss = 0.10368
Step 29155: loss = 0.08175
Step 29160: loss = 0.16561
Step 29165: loss = 0.08507
Step 29170: loss = 0.09754
Step 29175: loss = 0.12890
Step 29180: loss = 0.09771
Step 29185: loss = 0.08024
Step 29190: loss = 0.10068
Step 29195: loss = 0.08340
Step 29200: loss = 0.12249
Step 29205: loss = 0.09092
Step 29210: loss = 0.05280
Step 29215: loss = 0.09661
Step 29220: loss = 0.10245
Step 29225: loss = 0.09919
Step 29230: loss = 0.06957
Step 29235: loss = 0.11255
Step 29240: loss = 0.07147
Step 29245: loss = 0.06482
Step 29250: loss = 0.09654
Training Data Eval:
  Num examples: 49920, Num correct: 49035, Precision @ 1: 0.9823
('Testing Data Eval: EPOCH->', 76)
  Num examples: 9984, Num correct: 7406, Precision @ 1: 0.7418
Step 29255: loss = 0.11998
Step 29260: loss = 0.09538
Step 29265: loss = 0.10987
Step 29270: loss = 0.08334
Step 29275: loss = 0.07201
Step 29280: loss = 0.06927
Step 29285: loss = 0.08667
Step 29290: loss = 0.12563
Step 29295: loss = 0.12998
Step 29300: loss = 0.06893
Step 29305: loss = 0.11930
Step 29310: loss = 0.07665
Step 29315: loss = 0.09668
Step 29320: loss = 0.10729
Step 29325: loss = 0.12319
Step 29330: loss = 0.12574
Step 29335: loss = 0.07243
Step 29340: loss = 0.11029
Step 29345: loss = 0.09971
Step 29350: loss = 0.07657
Step 29355: loss = 0.10523
Step 29360: loss = 0.16856
Step 29365: loss = 0.11246
Step 29370: loss = 0.06511
Step 29375: loss = 0.08907
Step 29380: loss = 0.06327
Step 29385: loss = 0.05401
Step 29390: loss = 0.16838
Step 29395: loss = 0.09805
Step 29400: loss = 0.07207
Step 29405: loss = 0.17268
Step 29410: loss = 0.12209
Step 29415: loss = 0.13356
Step 29420: loss = 0.07278
Step 29425: loss = 0.14150
Step 29430: loss = 0.10200
Step 29435: loss = 0.10959
Step 29440: loss = 0.09557
Step 29445: loss = 0.10906
Step 29450: loss = 0.08114
Step 29455: loss = 0.13465
Step 29460: loss = 0.08372
Step 29465: loss = 0.11491
Step 29470: loss = 0.12261
Step 29475: loss = 0.07050
Step 29480: loss = 0.07279
Step 29485: loss = 0.09906
Step 29490: loss = 0.08060
Step 29495: loss = 0.21736
Step 29500: loss = 0.05017
Step 29505: loss = 0.10420
Step 29510: loss = 0.07483
Step 29515: loss = 0.06310
Step 29520: loss = 0.09373
Step 29525: loss = 0.04413
Step 29530: loss = 0.11000
Step 29535: loss = 0.06037
Step 29540: loss = 0.07029
Step 29545: loss = 0.05612
Step 29550: loss = 0.16282
Step 29555: loss = 0.10571
Step 29560: loss = 0.17608
Step 29565: loss = 0.13989
Step 29570: loss = 0.13189
Step 29575: loss = 0.08675
Step 29580: loss = 0.09606
Step 29585: loss = 0.16844
Step 29590: loss = 0.14035
Step 29595: loss = 0.07404
Step 29600: loss = 0.09301
Step 29605: loss = 0.10166
Step 29610: loss = 0.08849
Step 29615: loss = 0.14564
Step 29620: loss = 0.11755
Step 29625: loss = 0.10982
Step 29630: loss = 0.07896
Step 29635: loss = 0.07573
Step 29640: loss = 0.17700
Training Data Eval:
  Num examples: 49920, Num correct: 49055, Precision @ 1: 0.9827
('Testing Data Eval: EPOCH->', 77)
  Num examples: 9984, Num correct: 7356, Precision @ 1: 0.7368
Step 29645: loss = 0.09726
Step 29650: loss = 0.06863
Step 29655: loss = 0.09862
Step 29660: loss = 0.12926
Step 29665: loss = 0.06586
Step 29670: loss = 0.10403
Step 29675: loss = 0.19221
Step 29680: loss = 0.07130
Step 29685: loss = 0.10164
Step 29690: loss = 0.06213
Step 29695: loss = 0.06266
Step 29700: loss = 0.05856
Step 29705: loss = 0.07753
Step 29710: loss = 0.05123
Step 29715: loss = 0.11762
Step 29720: loss = 0.12474
Step 29725: loss = 0.07536
Step 29730: loss = 0.09621
Step 29735: loss = 0.06931
Step 29740: loss = 0.09802
Step 29745: loss = 0.04454
Step 29750: loss = 0.14573
Step 29755: loss = 0.14167
Step 29760: loss = 0.05177
Step 29765: loss = 0.10246
Step 29770: loss = 0.06759
Step 29775: loss = 0.06154
Step 29780: loss = 0.10626
Step 29785: loss = 0.07455
Step 29790: loss = 0.09239
Step 29795: loss = 0.15505
Step 29800: loss = 0.13341
Step 29805: loss = 0.07856
Step 29810: loss = 0.07158
Step 29815: loss = 0.11793
Step 29820: loss = 0.08527
Step 29825: loss = 0.14245
Step 29830: loss = 0.06927
Step 29835: loss = 0.11161
Step 29840: loss = 0.11962
Step 29845: loss = 0.09003
Step 29850: loss = 0.04811
Step 29855: loss = 0.07657
Step 29860: loss = 0.06939
Step 29865: loss = 0.12875
Step 29870: loss = 0.09753
Step 29875: loss = 0.10497
Step 29880: loss = 0.11758
Step 29885: loss = 0.08030
Step 29890: loss = 0.10718
Step 29895: loss = 0.05862
Step 29900: loss = 0.06886
Step 29905: loss = 0.07860
Step 29910: loss = 0.07220
Step 29915: loss = 0.16273
Step 29920: loss = 0.14295
Step 29925: loss = 0.14175
Step 29930: loss = 0.10350
Step 29935: loss = 0.10020
Step 29940: loss = 0.09326
Step 29945: loss = 0.08310
Step 29950: loss = 0.10686
Step 29955: loss = 0.10147
Step 29960: loss = 0.15580
Step 29965: loss = 0.13616
Step 29970: loss = 0.11477
Step 29975: loss = 0.09539
Step 29980: loss = 0.12392
Step 29985: loss = 0.10308
Step 29990: loss = 0.13983
Step 29995: loss = 0.10140
Step 30000: loss = 0.12352
Step 30005: loss = 0.05466
Step 30010: loss = 0.11735
Step 30015: loss = 0.06377
Step 30020: loss = 0.09341
Step 30025: loss = 0.10773
Step 30030: loss = 0.06213
Training Data Eval:
  Num examples: 49920, Num correct: 48835, Precision @ 1: 0.9783
('Testing Data Eval: EPOCH->', 78)
  Num examples: 9984, Num correct: 7358, Precision @ 1: 0.7370
Step 30035: loss = 0.12157
Step 30040: loss = 0.07887
Step 30045: loss = 0.06576
Step 30050: loss = 0.08595
Step 30055: loss = 0.09189
Step 30060: loss = 0.14920
Step 30065: loss = 0.05456
Step 30070: loss = 0.13613
Step 30075: loss = 0.07442
Step 30080: loss = 0.08382
Step 30085: loss = 0.09944
Step 30090: loss = 0.06754
Step 30095: loss = 0.11562
Step 30100: loss = 0.07527
Step 30105: loss = 0.10332
Step 30110: loss = 0.13832
Step 30115: loss = 0.15442
Step 30120: loss = 0.08240
Step 30125: loss = 0.10182
Step 30130: loss = 0.07801
Step 30135: loss = 0.09215
Step 30140: loss = 0.06838
Step 30145: loss = 0.11224
Step 30150: loss = 0.10198
Step 30155: loss = 0.13484
Step 30160: loss = 0.08573
Step 30165: loss = 0.12329
Step 30170: loss = 0.10633
Step 30175: loss = 0.09141
Step 30180: loss = 0.15589
Step 30185: loss = 0.05223
Step 30190: loss = 0.09071
Step 30195: loss = 0.06638
Step 30200: loss = 0.14669
Step 30205: loss = 0.09143
Step 30210: loss = 0.15016
Step 30215: loss = 0.08781
Step 30220: loss = 0.08948
Step 30225: loss = 0.11904
Step 30230: loss = 0.10928
Step 30235: loss = 0.07827
Step 30240: loss = 0.07379
Step 30245: loss = 0.16577
Step 30250: loss = 0.09412
Step 30255: loss = 0.11930
Step 30260: loss = 0.13215
Step 30265: loss = 0.08123
Step 30270: loss = 0.13142
Step 30275: loss = 0.08300
Step 30280: loss = 0.08860
Step 30285: loss = 0.16026
Step 30290: loss = 0.16009
Step 30295: loss = 0.10127
Step 30300: loss = 0.06931
Step 30305: loss = 0.20953
Step 30310: loss = 0.12625
Step 30315: loss = 0.12504
Step 30320: loss = 0.08625
Step 30325: loss = 0.06650
Step 30330: loss = 0.11453
Step 30335: loss = 0.05505
Step 30340: loss = 0.10999
Step 30345: loss = 0.10933
Step 30350: loss = 0.08241
Step 30355: loss = 0.14015
Step 30360: loss = 0.07508
Step 30365: loss = 0.11434
Step 30370: loss = 0.06082
Step 30375: loss = 0.10499
Step 30380: loss = 0.10387
Step 30385: loss = 0.08769
Step 30390: loss = 0.08811
Step 30395: loss = 0.09361
Step 30400: loss = 0.07402
Step 30405: loss = 0.11572
Step 30410: loss = 0.10830
Step 30415: loss = 0.07383
Step 30420: loss = 0.06818
Training Data Eval:
  Num examples: 49920, Num correct: 49027, Precision @ 1: 0.9821
('Testing Data Eval: EPOCH->', 79)
  Num examples: 9984, Num correct: 7382, Precision @ 1: 0.7394
Step 30425: loss = 0.12757
Step 30430: loss = 0.07051
Step 30435: loss = 0.04802
Step 30440: loss = 0.04992
Step 30445: loss = 0.08693
Step 30450: loss = 0.08848
Step 30455: loss = 0.11039
Step 30460: loss = 0.20283
Step 30465: loss = 0.09640
Step 30470: loss = 0.11352
Step 30475: loss = 0.08719
Step 30480: loss = 0.10244
Step 30485: loss = 0.08879
Step 30490: loss = 0.07457
Step 30495: loss = 0.08343
Step 30500: loss = 0.07358
Step 30505: loss = 0.12477
Step 30510: loss = 0.08927
Step 30515: loss = 0.09661
Step 30520: loss = 0.09931
Step 30525: loss = 0.11590
Step 30530: loss = 0.08838
Step 30535: loss = 0.09258
Step 30540: loss = 0.08389
Step 30545: loss = 0.17730
Step 30550: loss = 0.10373
Step 30555: loss = 0.06662
Step 30560: loss = 0.09991
Step 30565: loss = 0.14171
Step 30570: loss = 0.09225
Step 30575: loss = 0.07875
Step 30580: loss = 0.12961
Step 30585: loss = 0.15937
Step 30590: loss = 0.07177
Step 30595: loss = 0.10882
Step 30600: loss = 0.13544
Step 30605: loss = 0.09168
Step 30610: loss = 0.11955
Step 30615: loss = 0.10639
Step 30620: loss = 0.04899
Step 30625: loss = 0.12824
Step 30630: loss = 0.08336
Step 30635: loss = 0.09466
Step 30640: loss = 0.08312
Step 30645: loss = 0.06809
Step 30650: loss = 0.05688
Step 30655: loss = 0.06784
Step 30660: loss = 0.10213
Step 30665: loss = 0.17565
Step 30670: loss = 0.09968
Step 30675: loss = 0.15527
Step 30680: loss = 0.13797
Step 30685: loss = 0.06776
Step 30690: loss = 0.20078
Step 30695: loss = 0.07011
Step 30700: loss = 0.11784
Step 30705: loss = 0.08309
Step 30710: loss = 0.09706
Step 30715: loss = 0.10430
Step 30720: loss = 0.13758
Step 30725: loss = 0.07017
Step 30730: loss = 0.09630
Step 30735: loss = 0.10309
Step 30740: loss = 0.09151
Step 30745: loss = 0.09539
Step 30750: loss = 0.14145
Step 30755: loss = 0.10667
Step 30760: loss = 0.13123
Step 30765: loss = 0.07426
Step 30770: loss = 0.06251
Step 30775: loss = 0.06960
Step 30780: loss = 0.12750
Step 30785: loss = 0.07803
Step 30790: loss = 0.11136
Step 30795: loss = 0.13623
Step 30800: loss = 0.06560
Step 30805: loss = 0.10520
Step 30810: loss = 0.11173
Training Data Eval:
  Num examples: 49920, Num correct: 48980, Precision @ 1: 0.9812
('Testing Data Eval: EPOCH->', 80)
  Num examples: 9984, Num correct: 7445, Precision @ 1: 0.7457
Step 30815: loss = 0.06965
Step 30820: loss = 0.17539
Step 30825: loss = 0.10873
Step 30830: loss = 0.07881
Step 30835: loss = 0.05943
Step 30840: loss = 0.12515
Step 30845: loss = 0.13942
Step 30850: loss = 0.14346
Step 30855: loss = 0.12915
Step 30860: loss = 0.13036
Step 30865: loss = 0.11754
Step 30870: loss = 0.20537
Step 30875: loss = 0.11604
Step 30880: loss = 0.06556
Step 30885: loss = 0.07145
Step 30890: loss = 0.05905
Step 30895: loss = 0.08736
Step 30900: loss = 0.12945
Step 30905: loss = 0.12477
Step 30910: loss = 0.13074
Step 30915: loss = 0.06668
Step 30920: loss = 0.08444
Step 30925: loss = 0.07430
Step 30930: loss = 0.06969
Step 30935: loss = 0.09970
Step 30940: loss = 0.09953
Step 30945: loss = 0.12728
Step 30950: loss = 0.04404
Step 30955: loss = 0.12955
Step 30960: loss = 0.11999
Step 30965: loss = 0.09218
Step 30970: loss = 0.20354
Step 30975: loss = 0.09299
Step 30980: loss = 0.10168
Step 30985: loss = 0.14864
Step 30990: loss = 0.08993
Step 30995: loss = 0.09695
Step 31000: loss = 0.09238
Step 31005: loss = 0.09544
Step 31010: loss = 0.06293
Step 31015: loss = 0.09485
Step 31020: loss = 0.10901
Step 31025: loss = 0.14087
Step 31030: loss = 0.17725
Step 31035: loss = 0.07479
Step 31040: loss = 0.12694
Step 31045: loss = 0.05206
Step 31050: loss = 0.08825
Step 31055: loss = 0.10160
Step 31060: loss = 0.08357
Step 31065: loss = 0.06534
Step 31070: loss = 0.11889
Step 31075: loss = 0.06585
Step 31080: loss = 0.16015
Step 31085: loss = 0.11103
Step 31090: loss = 0.11080
Step 31095: loss = 0.07602
Step 31100: loss = 0.07179
Step 31105: loss = 0.06917
Step 31110: loss = 0.07125
Step 31115: loss = 0.09529
Step 31120: loss = 0.07457
Step 31125: loss = 0.11425
Step 31130: loss = 0.14695
Step 31135: loss = 0.14308
Step 31140: loss = 0.08633
Step 31145: loss = 0.17225
Step 31150: loss = 0.16546
Step 31155: loss = 0.06791
Step 31160: loss = 0.09888
Step 31165: loss = 0.07964
Step 31170: loss = 0.14572
Step 31175: loss = 0.08829
Step 31180: loss = 0.11349
Step 31185: loss = 0.14329
Step 31190: loss = 0.09946
Step 31195: loss = 0.13335
Step 31200: loss = 0.06440
Training Data Eval:
  Num examples: 49920, Num correct: 48761, Precision @ 1: 0.9768
('Testing Data Eval: EPOCH->', 81)
  Num examples: 9984, Num correct: 7427, Precision @ 1: 0.7439
Step 31205: loss = 0.11194
Step 31210: loss = 0.08156
Step 31215: loss = 0.10824
Step 31220: loss = 0.07937
Step 31225: loss = 0.12271
Step 31230: loss = 0.09190
Step 31235: loss = 0.09024
Step 31240: loss = 0.08561
Step 31245: loss = 0.09446
Step 31250: loss = 0.12520
Step 31255: loss = 0.12864
Step 31260: loss = 0.11693
Step 31265: loss = 0.18463
Step 31270: loss = 0.07000
Step 31275: loss = 0.11621
Step 31280: loss = 0.05559
Step 31285: loss = 0.07402
Step 31290: loss = 0.10804
Step 31295: loss = 0.07834
Step 31300: loss = 0.09627
Step 31305: loss = 0.08021
Step 31310: loss = 0.07925
Step 31315: loss = 0.09851
Step 31320: loss = 0.08881
Step 31325: loss = 0.08259
Step 31330: loss = 0.07762
Step 31335: loss = 0.09983
Step 31340: loss = 0.16598
Step 31345: loss = 0.10551
Step 31350: loss = 0.12536
Step 31355: loss = 0.12773
Step 31360: loss = 0.14583
Step 31365: loss = 0.09696
Step 31370: loss = 0.08367
Step 31375: loss = 0.11699
Step 31380: loss = 0.05442
Step 31385: loss = 0.06670
Step 31390: loss = 0.09729
Step 31395: loss = 0.05351
Step 31400: loss = 0.06706
Step 31405: loss = 0.10187
Step 31410: loss = 0.09842
Step 31415: loss = 0.09608
Step 31420: loss = 0.15635
Step 31425: loss = 0.18628
Step 31430: loss = 0.09523
Step 31435: loss = 0.17525
Step 31440: loss = 0.10001
Step 31445: loss = 0.11755
Step 31450: loss = 0.10926
Step 31455: loss = 0.05748
Step 31460: loss = 0.07644
Step 31465: loss = 0.10505
Step 31470: loss = 0.11631
Step 31475: loss = 0.16571
Step 31480: loss = 0.12646
Step 31485: loss = 0.11204
Step 31490: loss = 0.09977
Step 31495: loss = 0.10268
Step 31500: loss = 0.08628
Step 31505: loss = 0.05873
Step 31510: loss = 0.11115
Step 31515: loss = 0.08033
Step 31520: loss = 0.08291
Step 31525: loss = 0.12860
Step 31530: loss = 0.13135
Step 31535: loss = 0.09454
Step 31540: loss = 0.12786
Step 31545: loss = 0.08178
Step 31550: loss = 0.09157
Step 31555: loss = 0.08353
Step 31560: loss = 0.11774
Step 31565: loss = 0.07209
Step 31570: loss = 0.05017
Step 31575: loss = 0.09054
Step 31580: loss = 0.16789
Step 31585: loss = 0.08434
Step 31590: loss = 0.12287
Training Data Eval:
  Num examples: 49920, Num correct: 48900, Precision @ 1: 0.9796
('Testing Data Eval: EPOCH->', 82)
  Num examples: 9984, Num correct: 7456, Precision @ 1: 0.7468
Step 31595: loss = 0.06709
Step 31600: loss = 0.06676
Step 31605: loss = 0.05269
Step 31610: loss = 0.07333
Step 31615: loss = 0.11981
Step 31620: loss = 0.08393
Step 31625: loss = 0.07241
Step 31630: loss = 0.05848
Step 31635: loss = 0.10393
Step 31640: loss = 0.08687
Step 31645: loss = 0.08773
Step 31650: loss = 0.09078
Step 31655: loss = 0.07591
Step 31660: loss = 0.08058
Step 31665: loss = 0.08344
Step 31670: loss = 0.09288
Step 31675: loss = 0.11838
Step 31680: loss = 0.09175
Step 31685: loss = 0.08585
Step 31690: loss = 0.10825
Step 31695: loss = 0.06740
Step 31700: loss = 0.04979
Step 31705: loss = 0.05135
Step 31710: loss = 0.13650
Step 31715: loss = 0.13318
Step 31720: loss = 0.09025
Step 31725: loss = 0.08674
Step 31730: loss = 0.04944
Step 31735: loss = 0.12017
Step 31740: loss = 0.15032
Step 31745: loss = 0.24880
Step 31750: loss = 0.06116
Step 31755: loss = 0.11259
Step 31760: loss = 0.09604
Step 31765: loss = 0.07544
Step 31770: loss = 0.07960
Step 31775: loss = 0.05295
Step 31780: loss = 0.05679
Step 31785: loss = 0.05673
Step 31790: loss = 0.10034
Step 31795: loss = 0.07824
Step 31800: loss = 0.15900
Step 31805: loss = 0.04881
Step 31810: loss = 0.09728
Step 31815: loss = 0.09279
Step 31820: loss = 0.09493
Step 31825: loss = 0.14278
Step 31830: loss = 0.11591
Step 31835: loss = 0.09565
Step 31840: loss = 0.06810
Step 31845: loss = 0.08393
Step 31850: loss = 0.13034
Step 31855: loss = 0.10155
Step 31860: loss = 0.07757
Step 31865: loss = 0.06270
Step 31870: loss = 0.04255
Step 31875: loss = 0.07287
Step 31880: loss = 0.10725
Step 31885: loss = 0.12490
Step 31890: loss = 0.06574
Step 31895: loss = 0.09480
Step 31900: loss = 0.09670
Step 31905: loss = 0.17632
Step 31910: loss = 0.05946
Step 31915: loss = 0.05735
Step 31920: loss = 0.09623
Step 31925: loss = 0.06255
Step 31930: loss = 0.05976
Step 31935: loss = 0.12185
Step 31940: loss = 0.06400
Step 31945: loss = 0.09807
Step 31950: loss = 0.09273
Step 31955: loss = 0.09413
Step 31960: loss = 0.08298
Step 31965: loss = 0.11739
Step 31970: loss = 0.09273
Step 31975: loss = 0.06594
Step 31980: loss = 0.10786
Training Data Eval:
  Num examples: 49920, Num correct: 49197, Precision @ 1: 0.9855
('Testing Data Eval: EPOCH->', 83)
  Num examples: 9984, Num correct: 7449, Precision @ 1: 0.7461
Step 31985: loss = 0.08734
Step 31990: loss = 0.06514
Step 31995: loss = 0.06077
Step 32000: loss = 0.07555
Step 32005: loss = 0.08285
Step 32010: loss = 0.09938
Step 32015: loss = 0.08154
Step 32020: loss = 0.11937
Step 32025: loss = 0.04979
Step 32030: loss = 0.09632
Step 32035: loss = 0.12331
Step 32040: loss = 0.13110
Step 32045: loss = 0.07870
Step 32050: loss = 0.05743
Step 32055: loss = 0.06459
Step 32060: loss = 0.08645
Step 32065: loss = 0.09282
Step 32070: loss = 0.14472
Step 32075: loss = 0.07549
Step 32080: loss = 0.09820
Step 32085: loss = 0.13557
Step 32090: loss = 0.09414
Step 32095: loss = 0.06965
Step 32100: loss = 0.08757
Step 32105: loss = 0.06636
Step 32110: loss = 0.06542
Step 32115: loss = 0.06683
Step 32120: loss = 0.07382
Step 32125: loss = 0.12478
Step 32130: loss = 0.08678
Step 32135: loss = 0.12438
Step 32140: loss = 0.08263
Step 32145: loss = 0.07960
Step 32150: loss = 0.10218
Step 32155: loss = 0.08550
Step 32160: loss = 0.05391
Step 32165: loss = 0.07757
Step 32170: loss = 0.07167
Step 32175: loss = 0.10933
Step 32180: loss = 0.07293
Step 32185: loss = 0.13560
Step 32190: loss = 0.07945
Step 32195: loss = 0.09499
Step 32200: loss = 0.13063
Step 32205: loss = 0.13558
Step 32210: loss = 0.15002
Step 32215: loss = 0.12937
Step 32220: loss = 0.19801
Step 32225: loss = 0.16965
Step 32230: loss = 0.11209
Step 32235: loss = 0.14534
Step 32240: loss = 0.12496
Step 32245: loss = 0.07466
Step 32250: loss = 0.07591
Step 32255: loss = 0.08898
Step 32260: loss = 0.08437
Step 32265: loss = 0.09003
Step 32270: loss = 0.19032
Step 32275: loss = 0.08549
Step 32280: loss = 0.12929
Step 32285: loss = 0.06460
Step 32290: loss = 0.08719
Step 32295: loss = 0.05714
Step 32300: loss = 0.07561
Step 32305: loss = 0.11333
Step 32310: loss = 0.13081
Step 32315: loss = 0.08288
Step 32320: loss = 0.10197
Step 32325: loss = 0.07014
Step 32330: loss = 0.08304
Step 32335: loss = 0.09915
Step 32340: loss = 0.09763
Step 32345: loss = 0.11332
Step 32350: loss = 0.09845
Step 32355: loss = 0.09963
Step 32360: loss = 0.10516
Step 32365: loss = 0.12602
Step 32370: loss = 0.12998
Training Data Eval:
  Num examples: 49920, Num correct: 48818, Precision @ 1: 0.9779
('Testing Data Eval: EPOCH->', 84)
  Num examples: 9984, Num correct: 7342, Precision @ 1: 0.7354
Step 32375: loss = 0.09816
Step 32380: loss = 0.06946
Step 32385: loss = 0.08857
Step 32390: loss = 0.12167
Step 32395: loss = 0.09556
Step 32400: loss = 0.15997
Step 32405: loss = 0.09467
Step 32410: loss = 0.07349
Step 32415: loss = 0.05401
Step 32420: loss = 0.14451
Step 32425: loss = 0.12638
Step 32430: loss = 0.11501
Step 32435: loss = 0.12265
Step 32440: loss = 0.15055
Step 32445: loss = 0.10477
Step 32450: loss = 0.23259
Step 32455: loss = 0.11220
Step 32460: loss = 0.22414
Step 32465: loss = 0.09563
Step 32470: loss = 0.12107
Step 32475: loss = 0.12835
Step 32480: loss = 0.12281
Step 32485: loss = 0.12332
Step 32490: loss = 0.09684
Step 32495: loss = 0.13933
Step 32500: loss = 0.11372
Step 32505: loss = 0.07909
Step 32510: loss = 0.09054
Step 32515: loss = 0.12536
Step 32520: loss = 0.10215
Step 32525: loss = 0.08791
Step 32530: loss = 0.06668
Step 32535: loss = 0.11668
Step 32540: loss = 0.18005
Step 32545: loss = 0.12978
Step 32550: loss = 0.14125
Step 32555: loss = 0.13070
Step 32560: loss = 0.12683
Step 32565: loss = 0.08425
Step 32570: loss = 0.13727
Step 32575: loss = 0.10143
Step 32580: loss = 0.14573
Step 32585: loss = 0.07334
Step 32590: loss = 0.05721
Step 32595: loss = 0.11892
Step 32600: loss = 0.08915
Step 32605: loss = 0.13389
Step 32610: loss = 0.09370
Step 32615: loss = 0.08064
Step 32620: loss = 0.06704
Step 32625: loss = 0.10972
Step 32630: loss = 0.12373
Step 32635: loss = 0.08000
Step 32640: loss = 0.14749
Step 32645: loss = 0.08349
Step 32650: loss = 0.06920
Step 32655: loss = 0.12007
Step 32660: loss = 0.08082
Step 32665: loss = 0.10033
Step 32670: loss = 0.07708
Step 32675: loss = 0.07241
Step 32680: loss = 0.10642
Step 32685: loss = 0.10918
Step 32690: loss = 0.06258
Step 32695: loss = 0.13039
Step 32700: loss = 0.09414
Step 32705: loss = 0.13122
Step 32710: loss = 0.09954
Step 32715: loss = 0.05456
Step 32720: loss = 0.13018
Step 32725: loss = 0.11695
Step 32730: loss = 0.10011
Step 32735: loss = 0.10380
Step 32740: loss = 0.17089
Step 32745: loss = 0.05566
Step 32750: loss = 0.14317
Step 32755: loss = 0.06805
Step 32760: loss = 0.14864
Training Data Eval:
  Num examples: 49920, Num correct: 48849, Precision @ 1: 0.9785
('Testing Data Eval: EPOCH->', 85)
  Num examples: 9984, Num correct: 7380, Precision @ 1: 0.7392
Step 32765: loss = 0.07610
Step 32770: loss = 0.18721
Step 32775: loss = 0.07702
Step 32780: loss = 0.08921
Step 32785: loss = 0.07327
Step 32790: loss = 0.11639
Step 32795: loss = 0.10936
Step 32800: loss = 0.10735
Step 32805: loss = 0.06565
Step 32810: loss = 0.06796
Step 32815: loss = 0.09972
Step 32820: loss = 0.07810
Step 32825: loss = 0.09854
Step 32830: loss = 0.09310
Step 32835: loss = 0.11000
Step 32840: loss = 0.07453
Step 32845: loss = 0.14900
Step 32850: loss = 0.07115
Step 32855: loss = 0.07993
Step 32860: loss = 0.09436
Step 32865: loss = 0.12000
Step 32870: loss = 0.07512
Step 32875: loss = 0.08882
Step 32880: loss = 0.08246
Step 32885: loss = 0.08361
Step 32890: loss = 0.09033
Step 32895: loss = 0.05755
Step 32900: loss = 0.09453
Step 32905: loss = 0.13606
Step 32910: loss = 0.05700
Step 32915: loss = 0.09436
Step 32920: loss = 0.13930
Step 32925: loss = 0.06722
Step 32930: loss = 0.10950
Step 32935: loss = 0.18363
Step 32940: loss = 0.15326
Step 32945: loss = 0.07599
Step 32950: loss = 0.11612
Step 32955: loss = 0.06103
Step 32960: loss = 0.04937
Step 32965: loss = 0.13534
Step 32970: loss = 0.06298
Step 32975: loss = 0.13859
Step 32980: loss = 0.05195
Step 32985: loss = 0.06174
Step 32990: loss = 0.09642
Step 32995: loss = 0.08155
Step 33000: loss = 0.13981
Step 33005: loss = 0.11514
Step 33010: loss = 0.08992
Step 33015: loss = 0.09636
Step 33020: loss = 0.08601
Step 33025: loss = 0.11533
Step 33030: loss = 0.18322
Step 33035: loss = 0.11386
Step 33040: loss = 0.10525
Step 33045: loss = 0.14013
Step 33050: loss = 0.11277
Step 33055: loss = 0.14022
Step 33060: loss = 0.08980
Step 33065: loss = 0.11980
Step 33070: loss = 0.08240
Step 33075: loss = 0.08467
Step 33080: loss = 0.07316
Step 33085: loss = 0.14377
Step 33090: loss = 0.13683
Step 33095: loss = 0.07766
Step 33100: loss = 0.10684
Step 33105: loss = 0.13096
Step 33110: loss = 0.07353
Step 33115: loss = 0.08707
Step 33120: loss = 0.09303
Step 33125: loss = 0.13453
Step 33130: loss = 0.08802
Step 33135: loss = 0.09469
Step 33140: loss = 0.14520
Step 33145: loss = 0.13527
Step 33150: loss = 0.13630
Training Data Eval:
  Num examples: 49920, Num correct: 48848, Precision @ 1: 0.9785
('Testing Data Eval: EPOCH->', 86)
  Num examples: 9984, Num correct: 7344, Precision @ 1: 0.7356
Step 33155: loss = 0.07640
Step 33160: loss = 0.12675
Step 33165: loss = 0.08401
Step 33170: loss = 0.08100
Step 33175: loss = 0.10176
Step 33180: loss = 0.08856
Step 33185: loss = 0.08757
Step 33190: loss = 0.14578
Step 33195: loss = 0.09271
Step 33200: loss = 0.13332
Step 33205: loss = 0.11539
Step 33210: loss = 0.08488
Step 33215: loss = 0.09134
Step 33220: loss = 0.08726
Step 33225: loss = 0.12271
Step 33230: loss = 0.05016
Step 33235: loss = 0.07713
Step 33240: loss = 0.12952
Step 33245: loss = 0.14468
Step 33250: loss = 0.08188
Step 33255: loss = 0.09839
Step 33260: loss = 0.11216
Step 33265: loss = 0.08991
Step 33270: loss = 0.11754
Step 33275: loss = 0.06049
Step 33280: loss = 0.08430
Step 33285: loss = 0.07657
Step 33290: loss = 0.09351
Step 33295: loss = 0.10871
Step 33300: loss = 0.07505
Step 33305: loss = 0.09704
Step 33310: loss = 0.12868
Step 33315: loss = 0.22987
Step 33320: loss = 0.10355
Step 33325: loss = 0.11813
Step 33330: loss = 0.10493
Step 33335: loss = 0.08626
Step 33340: loss = 0.05451
Step 33345: loss = 0.07874
Step 33350: loss = 0.11410
Step 33355: loss = 0.11766
Step 33360: loss = 0.10763
Step 33365: loss = 0.11617
Step 33370: loss = 0.08502
Step 33375: loss = 0.13829
Step 33380: loss = 0.08488
Step 33385: loss = 0.05232
Step 33390: loss = 0.12305
Step 33395: loss = 0.07565
Step 33400: loss = 0.04902
Step 33405: loss = 0.09590
Step 33410: loss = 0.05126
Step 33415: loss = 0.09209
Step 33420: loss = 0.06535
Step 33425: loss = 0.08798
Step 33430: loss = 0.13679
Step 33435: loss = 0.10345
Step 33440: loss = 0.12520
Step 33445: loss = 0.06121
Step 33450: loss = 0.05602
Step 33455: loss = 0.07676
Step 33460: loss = 0.08538
Step 33465: loss = 0.10921
Step 33470: loss = 0.09719
Step 33475: loss = 0.09633
Step 33480: loss = 0.08270
Step 33485: loss = 0.11851
Step 33490: loss = 0.11889
Step 33495: loss = 0.14467
Step 33500: loss = 0.11558
Step 33505: loss = 0.11181
Step 33510: loss = 0.09686
Step 33515: loss = 0.07419
Step 33520: loss = 0.12504
Step 33525: loss = 0.08524
Step 33530: loss = 0.06205
Step 33535: loss = 0.17792
Step 33540: loss = 0.13891
Training Data Eval:
  Num examples: 49920, Num correct: 49022, Precision @ 1: 0.9820
('Testing Data Eval: EPOCH->', 87)
  Num examples: 9984, Num correct: 7433, Precision @ 1: 0.7445
Step 33545: loss = 0.08265
Step 33550: loss = 0.05637
Step 33555: loss = 0.07154
Step 33560: loss = 0.05974
Step 33565: loss = 0.07554
Step 33570: loss = 0.12998
Step 33575: loss = 0.09456
Step 33580: loss = 0.16037
Step 33585: loss = 0.06950
Step 33590: loss = 0.17183
Step 33595: loss = 0.09558
Step 33600: loss = 0.08786
Step 33605: loss = 0.10096
Step 33610: loss = 0.09260
Step 33615: loss = 0.06908
Step 33620: loss = 0.10873
Step 33625: loss = 0.18500
Step 33630: loss = 0.06704
Step 33635: loss = 0.08425
Step 33640: loss = 0.06530
Step 33645: loss = 0.09359
Step 33650: loss = 0.06582
Step 33655: loss = 0.08280
Step 33660: loss = 0.10174
Step 33665: loss = 0.09459
Step 33670: loss = 0.10962
Step 33675: loss = 0.08181
Step 33680: loss = 0.08072
Step 33685: loss = 0.07540
Step 33690: loss = 0.06320
Step 33695: loss = 0.10715
Step 33700: loss = 0.09607
Step 33705: loss = 0.10204
Step 33710: loss = 0.12087
Step 33715: loss = 0.10885
Step 33720: loss = 0.07519
Step 33725: loss = 0.06353
Step 33730: loss = 0.05769
Step 33735: loss = 0.05965
Step 33740: loss = 0.10324
Step 33745: loss = 0.08260
Step 33750: loss = 0.09854
Step 33755: loss = 0.06731
Step 33760: loss = 0.11610
Step 33765: loss = 0.17989
Step 33770: loss = 0.07372
Step 33775: loss = 0.08086
Step 33780: loss = 0.16454
Step 33785: loss = 0.08681
Step 33790: loss = 0.09074
Step 33795: loss = 0.06473
Step 33800: loss = 0.09144
Step 33805: loss = 0.09694
Step 33810: loss = 0.12772
Step 33815: loss = 0.08978
Step 33820: loss = 0.07844
Step 33825: loss = 0.16262
Step 33830: loss = 0.05526
Step 33835: loss = 0.12134
Step 33840: loss = 0.10009
Step 33845: loss = 0.05655
Step 33850: loss = 0.06752
Step 33855: loss = 0.06440
Step 33860: loss = 0.10827
Step 33865: loss = 0.07809
Step 33870: loss = 0.12868
Step 33875: loss = 0.11263
Step 33880: loss = 0.08153
Step 33885: loss = 0.14994
Step 33890: loss = 0.11733
Step 33895: loss = 0.09559
Step 33900: loss = 0.12371
Step 33905: loss = 0.05365
Step 33910: loss = 0.08137
Step 33915: loss = 0.13474
Step 33920: loss = 0.14049
Step 33925: loss = 0.10990
Step 33930: loss = 0.12197
Training Data Eval:
  Num examples: 49920, Num correct: 48889, Precision @ 1: 0.9793
('Testing Data Eval: EPOCH->', 88)
  Num examples: 9984, Num correct: 7432, Precision @ 1: 0.7444
Step 33935: loss = 0.09711
Step 33940: loss = 0.11202
Step 33945: loss = 0.07515
Step 33950: loss = 0.10051
Step 33955: loss = 0.10287
Step 33960: loss = 0.06021
Step 33965: loss = 0.07112
Step 33970: loss = 0.07836
Step 33975: loss = 0.12159
Step 33980: loss = 0.04700
Step 33985: loss = 0.11960
Step 33990: loss = 0.07567
Step 33995: loss = 0.11207
Step 34000: loss = 0.08614
Step 34005: loss = 0.05738
Step 34010: loss = 0.16310
Step 34015: loss = 0.10085
Step 34020: loss = 0.07405
Step 34025: loss = 0.07400
Step 34030: loss = 0.08339
Step 34035: loss = 0.06189
Step 34040: loss = 0.04976
Step 34045: loss = 0.11215
Step 34050: loss = 0.15376
Step 34055: loss = 0.06399
Step 34060: loss = 0.11114
Step 34065: loss = 0.10252
Step 34070: loss = 0.11812
Step 34075: loss = 0.12808
Step 34080: loss = 0.10669
Step 34085: loss = 0.09695
Step 34090: loss = 0.13710
Step 34095: loss = 0.09846
Step 34100: loss = 0.11667
Step 34105: loss = 0.04163
Step 34110: loss = 0.06961
Step 34115: loss = 0.08474
Step 34120: loss = 0.08180
Step 34125: loss = 0.06747
Step 34130: loss = 0.12300
Step 34135: loss = 0.05067
Step 34140: loss = 0.06238
Step 34145: loss = 0.05196
Step 34150: loss = 0.10721
Step 34155: loss = 0.07083
Step 34160: loss = 0.12472
Step 34165: loss = 0.07742
Step 34170: loss = 0.06102
Step 34175: loss = 0.07473
Step 34180: loss = 0.08694
Step 34185: loss = 0.13055
Step 34190: loss = 0.09523
Step 34195: loss = 0.06586
Step 34200: loss = 0.07841
Step 34205: loss = 0.05878
Step 34210: loss = 0.05598
Step 34215: loss = 0.11863
Step 34220: loss = 0.07876
Step 34225: loss = 0.09039
Step 34230: loss = 0.09717
Step 34235: loss = 0.08305
Step 34240: loss = 0.07518
Step 34245: loss = 0.06330
Step 34250: loss = 0.07520
Step 34255: loss = 0.13409
Step 34260: loss = 0.07933
Step 34265: loss = 0.06370
Step 34270: loss = 0.09965
Step 34275: loss = 0.08936
Step 34280: loss = 0.05195
Step 34285: loss = 0.11278
Step 34290: loss = 0.21321
Step 34295: loss = 0.09305
Step 34300: loss = 0.16726
Step 34305: loss = 0.11165
Step 34310: loss = 0.07425
Step 34315: loss = 0.07151
Step 34320: loss = 0.07691
Training Data Eval:
  Num examples: 49920, Num correct: 49187, Precision @ 1: 0.9853
('Testing Data Eval: EPOCH->', 89)
  Num examples: 9984, Num correct: 7382, Precision @ 1: 0.7394
Step 34325: loss = 0.07857
Step 34330: loss = 0.09247
Step 34335: loss = 0.10193
Step 34340: loss = 0.11484
Step 34345: loss = 0.05953
Step 34350: loss = 0.09291
Step 34355: loss = 0.08079
Step 34360: loss = 0.06062
Step 34365: loss = 0.05867
Step 34370: loss = 0.09225
Step 34375: loss = 0.06797
Step 34380: loss = 0.10321
Step 34385: loss = 0.11481
Step 34390: loss = 0.06493
Step 34395: loss = 0.11314
Step 34400: loss = 0.10615
Step 34405: loss = 0.11123
Step 34410: loss = 0.09676
Step 34415: loss = 0.07770
Step 34420: loss = 0.06717
Step 34425: loss = 0.08889
Step 34430: loss = 0.10029
Step 34435: loss = 0.07150
Step 34440: loss = 0.10790
Step 34445: loss = 0.07455
Step 34450: loss = 0.09811
Step 34455: loss = 0.07045
Step 34460: loss = 0.06598
Step 34465: loss = 0.08700
Step 34470: loss = 0.06236
Step 34475: loss = 0.12639
Step 34480: loss = 0.08170
Step 34485: loss = 0.10500
Step 34490: loss = 0.15036
Step 34495: loss = 0.11774
Step 34500: loss = 0.12408
Step 34505: loss = 0.08522
Step 34510: loss = 0.12042
Step 34515: loss = 0.08880
Step 34520: loss = 0.08139
Step 34525: loss = 0.06695
Step 34530: loss = 0.05303
Step 34535: loss = 0.08863
Step 34540: loss = 0.11180
Step 34545: loss = 0.07475
Step 34550: loss = 0.09849
Step 34555: loss = 0.07300
Step 34560: loss = 0.12815
Step 34565: loss = 0.09840
Step 34570: loss = 0.09634
Step 34575: loss = 0.08320
Step 34580: loss = 0.10139
Step 34585: loss = 0.12691
Step 34590: loss = 0.09769
Step 34595: loss = 0.11781
Step 34600: loss = 0.05829
Step 34605: loss = 0.08254
Step 34610: loss = 0.11085
Step 34615: loss = 0.09263
Step 34620: loss = 0.08746
Step 34625: loss = 0.07610
Step 34630: loss = 0.06107
Step 34635: loss = 0.06793
Step 34640: loss = 0.08018
Step 34645: loss = 0.10901
Step 34650: loss = 0.06266
Step 34655: loss = 0.05822
Step 34660: loss = 0.14103
Step 34665: loss = 0.07936
Step 34670: loss = 0.07237
Step 34675: loss = 0.09466
Step 34680: loss = 0.05140
Step 34685: loss = 0.10070
Step 34690: loss = 0.19083
Step 34695: loss = 0.10079
Step 34700: loss = 0.09331
Step 34705: loss = 0.07811
Step 34710: loss = 0.07318
Training Data Eval:
  Num examples: 49920, Num correct: 49150, Precision @ 1: 0.9846
('Testing Data Eval: EPOCH->', 90)
  Num examples: 9984, Num correct: 7431, Precision @ 1: 0.7443
Step 34715: loss = 0.06612
Step 34720: loss = 0.10624
Step 34725: loss = 0.05899
Step 34730: loss = 0.05884
Step 34735: loss = 0.07550
Step 34740: loss = 0.08595
Step 34745: loss = 0.08081
Step 34750: loss = 0.05290
Step 34755: loss = 0.05396
Step 34760: loss = 0.04447
Step 34765: loss = 0.04564
Step 34770: loss = 0.05494
Step 34775: loss = 0.09547
Step 34780: loss = 0.08828
Step 34785: loss = 0.05875
Step 34790: loss = 0.06716
Step 34795: loss = 0.08970
Step 34800: loss = 0.06515
Step 34805: loss = 0.08327
Step 34810: loss = 0.05205
Step 34815: loss = 0.04720
Step 34820: loss = 0.08852
Step 34825: loss = 0.06166
Step 34830: loss = 0.06305
Step 34835: loss = 0.09533
Step 34840: loss = 0.05921
Step 34845: loss = 0.13806
Step 34850: loss = 0.08138
Step 34855: loss = 0.15473
Step 34860: loss = 0.05220
Step 34865: loss = 0.10062
Step 34870: loss = 0.10052
Step 34875: loss = 0.11452
Step 34880: loss = 0.06349
Step 34885: loss = 0.04954
Step 34890: loss = 0.08110
Step 34895: loss = 0.07269
Step 34900: loss = 0.08408
Step 34905: loss = 0.07607
Step 34910: loss = 0.07147
Step 34915: loss = 0.08273
Step 34920: loss = 0.05210
Step 34925: loss = 0.09648
Step 34930: loss = 0.10464
Step 34935: loss = 0.07656
Step 34940: loss = 0.06516
Step 34945: loss = 0.06081
Step 34950: loss = 0.07802
Step 34955: loss = 0.07220
Step 34960: loss = 0.08305
Step 34965: loss = 0.06569
Step 34970: loss = 0.14649
Step 34975: loss = 0.09864
Step 34980: loss = 0.09422
Step 34985: loss = 0.08757
Step 34990: loss = 0.07283
Step 34995: loss = 0.05558
Step 35000: loss = 0.09047
Step 35005: loss = 0.04983
Step 35010: loss = 0.07028
Step 35015: loss = 0.07263
Step 35020: loss = 0.07137
Step 35025: loss = 0.06920
Step 35030: loss = 0.10868
Step 35035: loss = 0.10075
Step 35040: loss = 0.12271
Step 35045: loss = 0.18152
Step 35050: loss = 0.09580
Step 35055: loss = 0.08835
Step 35060: loss = 0.12860
Step 35065: loss = 0.07430
Step 35070: loss = 0.09006
Step 35075: loss = 0.09421
Step 35080: loss = 0.17581
Step 35085: loss = 0.08296
Step 35090: loss = 0.13647
Step 35095: loss = 0.08355
Step 35100: loss = 0.06364
Training Data Eval:
  Num examples: 49920, Num correct: 49151, Precision @ 1: 0.9846
('Testing Data Eval: EPOCH->', 91)
  Num examples: 9984, Num correct: 7331, Precision @ 1: 0.7343
Step 35105: loss = 0.04948
Step 35110: loss = 0.07889
Step 35115: loss = 0.08269
Step 35120: loss = 0.07203
Step 35125: loss = 0.07979
Step 35130: loss = 0.08969
Step 35135: loss = 0.08026
Step 35140: loss = 0.10042
Step 35145: loss = 0.08082
Step 35150: loss = 0.05321
Step 35155: loss = 0.07709
Step 35160: loss = 0.07966
Step 35165: loss = 0.12032
Step 35170: loss = 0.06250
Step 35175: loss = 0.06020
Step 35180: loss = 0.07555
Step 35185: loss = 0.07431
Step 35190: loss = 0.07563
Step 35195: loss = 0.04362
Step 35200: loss = 0.09620
Step 35205: loss = 0.07372
Step 35210: loss = 0.08363
Step 35215: loss = 0.11578
Step 35220: loss = 0.06974
Step 35225: loss = 0.05035
Step 35230: loss = 0.07525
Step 35235: loss = 0.05602
Step 35240: loss = 0.09352
Step 35245: loss = 0.09162
Step 35250: loss = 0.10434
Step 35255: loss = 0.05803
Step 35260: loss = 0.14806
Step 35265: loss = 0.09350
Step 35270: loss = 0.07808
Step 35275: loss = 0.12097
Step 35280: loss = 0.08257
Step 35285: loss = 0.06404
Step 35290: loss = 0.10311
Step 35295: loss = 0.10769
Step 35300: loss = 0.08717
Step 35305: loss = 0.11628
Step 35310: loss = 0.08667
Step 35315: loss = 0.08333
Step 35320: loss = 0.06868
Step 35325: loss = 0.06157
Step 35330: loss = 0.13098
Step 35335: loss = 0.10606
Step 35340: loss = 0.06450
Step 35345: loss = 0.06601
Step 35350: loss = 0.11420
Step 35355: loss = 0.08752
Step 35360: loss = 0.09307
Step 35365: loss = 0.13820
Step 35370: loss = 0.08855
Step 35375: loss = 0.06833
Step 35380: loss = 0.09357
Step 35385: loss = 0.11664
Step 35390: loss = 0.12633
Step 35395: loss = 0.12373
Step 35400: loss = 0.10354
Step 35405: loss = 0.10825
Step 35410: loss = 0.07618
Step 35415: loss = 0.07753
Step 35420: loss = 0.05984
Step 35425: loss = 0.08548
Step 35430: loss = 0.07993
Step 35435: loss = 0.08927
Step 35440: loss = 0.10446
Step 35445: loss = 0.06178
Step 35450: loss = 0.08018
Step 35455: loss = 0.09467
Step 35460: loss = 0.07655
Step 35465: loss = 0.08246
Step 35470: loss = 0.08053
Step 35475: loss = 0.06838
Step 35480: loss = 0.08678
Step 35485: loss = 0.07705
Step 35490: loss = 0.05525
Training Data Eval:
  Num examples: 49920, Num correct: 49062, Precision @ 1: 0.9828
('Testing Data Eval: EPOCH->', 92)
  Num examples: 9984, Num correct: 7395, Precision @ 1: 0.7407
Step 35495: loss = 0.05203
Step 35500: loss = 0.09926
Step 35505: loss = 0.06881
Step 35510: loss = 0.10231
Step 35515: loss = 0.07563
Step 35520: loss = 0.06493
Step 35525: loss = 0.10657
Step 35530: loss = 0.08599
Step 35535: loss = 0.09458
Step 35540: loss = 0.07136
Step 35545: loss = 0.09320
Step 35550: loss = 0.07919
Step 35555: loss = 0.17283
Step 35560: loss = 0.09224
Step 35565: loss = 0.07675
Step 35570: loss = 0.10055
Step 35575: loss = 0.05354
Step 35580: loss = 0.06118
Step 35585: loss = 0.12406
Step 35590: loss = 0.08187
Step 35595: loss = 0.16009
Step 35600: loss = 0.08076
Step 35605: loss = 0.07429
Step 35610: loss = 0.08742
Step 35615: loss = 0.06058
Step 35620: loss = 0.06779
Step 35625: loss = 0.06187
Step 35630: loss = 0.14555
Step 35635: loss = 0.12045
Step 35640: loss = 0.09591
Step 35645: loss = 0.17569
Step 35650: loss = 0.05065
Step 35655: loss = 0.08030
Step 35660: loss = 0.10377
Step 35665: loss = 0.16035
Step 35670: loss = 0.13004
Step 35675: loss = 0.11780
Step 35680: loss = 0.10552
Step 35685: loss = 0.06179
Step 35690: loss = 0.11686
Step 35695: loss = 0.09950
Step 35700: loss = 0.06779
Step 35705: loss = 0.08295
Step 35710: loss = 0.08806
Step 35715: loss = 0.06311
Step 35720: loss = 0.10422
Step 35725: loss = 0.07027
Step 35730: loss = 0.13841
Step 35735: loss = 0.08005
Step 35740: loss = 0.07906
Step 35745: loss = 0.06840
Step 35750: loss = 0.05838
Step 35755: loss = 0.05186
Step 35760: loss = 0.07622
Step 35765: loss = 0.06581
Step 35770: loss = 0.06357
Step 35775: loss = 0.10138
Step 35780: loss = 0.05144
Step 35785: loss = 0.09876
Step 35790: loss = 0.09219
Step 35795: loss = 0.09661
Step 35800: loss = 0.09498
Step 35805: loss = 0.06993
Step 35810: loss = 0.07873
Step 35815: loss = 0.09406
Step 35820: loss = 0.08045
Step 35825: loss = 0.04887
Step 35830: loss = 0.08515
Step 35835: loss = 0.09819
Step 35840: loss = 0.07323
Step 35845: loss = 0.08408
Step 35850: loss = 0.11097
Step 35855: loss = 0.06797
Step 35860: loss = 0.13542
Step 35865: loss = 0.12339
Step 35870: loss = 0.09487
Step 35875: loss = 0.08114
Step 35880: loss = 0.05828
Training Data Eval:
  Num examples: 49920, Num correct: 49239, Precision @ 1: 0.9864
('Testing Data Eval: EPOCH->', 93)
  Num examples: 9984, Num correct: 7419, Precision @ 1: 0.7431
Step 35885: loss = 0.07133
Step 35890: loss = 0.13160
Step 35895: loss = 0.07533
Step 35900: loss = 0.07807
Step 35905: loss = 0.04905
Step 35910: loss = 0.06812
Step 35915: loss = 0.05807
Step 35920: loss = 0.06226
Step 35925: loss = 0.05500
Step 35930: loss = 0.06009
Step 35935: loss = 0.06296
Step 35940: loss = 0.11882
Step 35945: loss = 0.08939
Step 35950: loss = 0.06140
Step 35955: loss = 0.16137
Step 35960: loss = 0.12587
Step 35965: loss = 0.04117
Step 35970: loss = 0.08684
Step 35975: loss = 0.03662
Step 35980: loss = 0.06616
Step 35985: loss = 0.05925
Step 35990: loss = 0.12440
Step 35995: loss = 0.07576
Step 36000: loss = 0.08671
Step 36005: loss = 0.07515
Step 36010: loss = 0.09735
Step 36015: loss = 0.06735
Step 36020: loss = 0.07308
Step 36025: loss = 0.08834
Step 36030: loss = 0.10835
Step 36035: loss = 0.07602
Step 36040: loss = 0.11700
Step 36045: loss = 0.05707
Step 36050: loss = 0.09942
Step 36055: loss = 0.11079
Step 36060: loss = 0.06922
Step 36065: loss = 0.07290
Step 36070: loss = 0.08107
Step 36075: loss = 0.07765
Step 36080: loss = 0.07547
Step 36085: loss = 0.07213
Step 36090: loss = 0.07517
Step 36095: loss = 0.04710
Step 36100: loss = 0.08659
Step 36105: loss = 0.08462
Step 36110: loss = 0.07643
Step 36115: loss = 0.06446
Step 36120: loss = 0.04369
Step 36125: loss = 0.09620
Step 36130: loss = 0.08164
Step 36135: loss = 0.09262
Step 36140: loss = 0.08408
Step 36145: loss = 0.11753
Step 36150: loss = 0.07236
Step 36155: loss = 0.04591
Step 36160: loss = 0.08096
Step 36165: loss = 0.08934
Step 36170: loss = 0.05635
Step 36175: loss = 0.07255
Step 36180: loss = 0.06607
Step 36185: loss = 0.05107
Step 36190: loss = 0.05564
Step 36195: loss = 0.08235
Step 36200: loss = 0.05101
Step 36205: loss = 0.07571
Step 36210: loss = 0.04859
Step 36215: loss = 0.07239
Step 36220: loss = 0.15040
Step 36225: loss = 0.06269
Step 36230: loss = 0.11167
Step 36235: loss = 0.12116
Step 36240: loss = 0.06859
Step 36245: loss = 0.10784
Step 36250: loss = 0.03443
Step 36255: loss = 0.09198
Step 36260: loss = 0.07269
Step 36265: loss = 0.07870
Step 36270: loss = 0.08069
Training Data Eval:
  Num examples: 49920, Num correct: 49056, Precision @ 1: 0.9827
('Testing Data Eval: EPOCH->', 94)
  Num examples: 9984, Num correct: 7390, Precision @ 1: 0.7402
Step 36275: loss = 0.10046
Step 36280: loss = 0.09335
Step 36285: loss = 0.06025
Step 36290: loss = 0.06136
Step 36295: loss = 0.08764
Step 36300: loss = 0.09095
Step 36305: loss = 0.06552
Step 36310: loss = 0.06335
Step 36315: loss = 0.07427
Step 36320: loss = 0.06580
Step 36325: loss = 0.07046
Step 36330: loss = 0.04441
Step 36335: loss = 0.05582
Step 36340: loss = 0.06846
Step 36345: loss = 0.07028
Step 36350: loss = 0.04655
Step 36355: loss = 0.07455
Step 36360: loss = 0.06717
Step 36365: loss = 0.16081
Step 36370: loss = 0.07619
Step 36375: loss = 0.06501
Step 36380: loss = 0.14150
Step 36385: loss = 0.08798
Step 36390: loss = 0.09580
Step 36395: loss = 0.10515
Step 36400: loss = 0.13562
Step 36405: loss = 0.13765
Step 36410: loss = 0.07920
Step 36415: loss = 0.15873
Step 36420: loss = 0.06462
Step 36425: loss = 0.08819
Step 36430: loss = 0.03529
Step 36435: loss = 0.12034
Step 36440: loss = 0.08122
Step 36445: loss = 0.10981
Step 36450: loss = 0.06428
Step 36455: loss = 0.10299
Step 36460: loss = 0.10221
Step 36465: loss = 0.10348
Step 36470: loss = 0.10160
Step 36475: loss = 0.07491
Step 36480: loss = 0.13407
Step 36485: loss = 0.08747
Step 36490: loss = 0.08316
Step 36495: loss = 0.05206
Step 36500: loss = 0.06095
Step 36505: loss = 0.07133
Step 36510: loss = 0.12084
Step 36515: loss = 0.07447
Step 36520: loss = 0.07012
Step 36525: loss = 0.06515
Step 36530: loss = 0.07276
Step 36535: loss = 0.08385
Step 36540: loss = 0.09105
Step 36545: loss = 0.08692
Step 36550: loss = 0.05818
Step 36555: loss = 0.11848
Step 36560: loss = 0.15634
Step 36565: loss = 0.06339
Step 36570: loss = 0.07526
Step 36575: loss = 0.06581
Step 36580: loss = 0.06829
Step 36585: loss = 0.09176
Step 36590: loss = 0.05914
Step 36595: loss = 0.11815
Step 36600: loss = 0.11633
Step 36605: loss = 0.09934
Step 36610: loss = 0.07245
Step 36615: loss = 0.14210
Step 36620: loss = 0.08053
Step 36625: loss = 0.06070
Step 36630: loss = 0.07772
Step 36635: loss = 0.03382
Step 36640: loss = 0.09102
Step 36645: loss = 0.06301
Step 36650: loss = 0.13468
Step 36655: loss = 0.06567
Step 36660: loss = 0.05441
Training Data Eval:
  Num examples: 49920, Num correct: 49043, Precision @ 1: 0.9824
('Testing Data Eval: EPOCH->', 95)
  Num examples: 9984, Num correct: 7418, Precision @ 1: 0.7430
Step 36665: loss = 0.05478
Step 36670: loss = 0.09469
Step 36675: loss = 0.07815
Step 36680: loss = 0.07034
Step 36685: loss = 0.08018
Step 36690: loss = 0.08596
Step 36695: loss = 0.11316
Step 36700: loss = 0.13251
Step 36705: loss = 0.07873
Step 36710: loss = 0.05778
Step 36715: loss = 0.12442
Step 36720: loss = 0.07689
Step 36725: loss = 0.06513
Step 36730: loss = 0.09664
Step 36735: loss = 0.07730
Step 36740: loss = 0.10782
Step 36745: loss = 0.12351
Step 36750: loss = 0.09038
Step 36755: loss = 0.13797
Step 36760: loss = 0.07119
Step 36765: loss = 0.11211
Step 36770: loss = 0.11824
Step 36775: loss = 0.08247
Step 36780: loss = 0.09792
Step 36785: loss = 0.06237
Step 36790: loss = 0.08496
Step 36795: loss = 0.11511
Step 36800: loss = 0.06610
Step 36805: loss = 0.07599
Step 36810: loss = 0.07763
Step 36815: loss = 0.07708
Step 36820: loss = 0.05566
Step 36825: loss = 0.08278
Step 36830: loss = 0.08546
Step 36835: loss = 0.06449
Step 36840: loss = 0.07890
Step 36845: loss = 0.07427
Step 36850: loss = 0.09745
Step 36855: loss = 0.09003
Step 36860: loss = 0.11363
Step 36865: loss = 0.09416
Step 36870: loss = 0.13687
Step 36875: loss = 0.06070
Step 36880: loss = 0.13783
Step 36885: loss = 0.09873
Step 36890: loss = 0.08725
Step 36895: loss = 0.05978
Step 36900: loss = 0.06534
Step 36905: loss = 0.06949
Step 36910: loss = 0.04709
Step 36915: loss = 0.08017
Step 36920: loss = 0.11680
Step 36925: loss = 0.08846
Step 36930: loss = 0.10962
Step 36935: loss = 0.11780
Step 36940: loss = 0.07426
Step 36945: loss = 0.11497
Step 36950: loss = 0.09619
Step 36955: loss = 0.07041
Step 36960: loss = 0.10245
Step 36965: loss = 0.06894
Step 36970: loss = 0.07332
Step 36975: loss = 0.07459
Step 36980: loss = 0.10110
Step 36985: loss = 0.14056
Step 36990: loss = 0.08529
Step 36995: loss = 0.10151
Step 37000: loss = 0.06636
Step 37005: loss = 0.10702
Step 37010: loss = 0.05523
Step 37015: loss = 0.05348
Step 37020: loss = 0.06317
Step 37025: loss = 0.07421
Step 37030: loss = 0.07670
Step 37035: loss = 0.09246
Step 37040: loss = 0.07981
Step 37045: loss = 0.09625
Step 37050: loss = 0.06254
Training Data Eval:
  Num examples: 49920, Num correct: 49273, Precision @ 1: 0.9870
('Testing Data Eval: EPOCH->', 96)
  Num examples: 9984, Num correct: 7493, Precision @ 1: 0.7505
Step 37055: loss = 0.11464
Step 37060: loss = 0.09254
Step 37065: loss = 0.05314
Step 37070: loss = 0.05776
Step 37075: loss = 0.08834
Step 37080: loss = 0.06032
Step 37085: loss = 0.07837
Step 37090: loss = 0.09013
Step 37095: loss = 0.03267
Step 37100: loss = 0.08301
Step 37105: loss = 0.07450
Step 37110: loss = 0.06261
Step 37115: loss = 0.12156
Step 37120: loss = 0.09670
Step 37125: loss = 0.05725
Step 37130: loss = 0.15815
Step 37135: loss = 0.04165
Step 37140: loss = 0.09461
Step 37145: loss = 0.05397
Step 37150: loss = 0.07604
Step 37155: loss = 0.06090
Step 37160: loss = 0.10095
Step 37165: loss = 0.08092
Step 37170: loss = 0.06785
Step 37175: loss = 0.08020
Step 37180: loss = 0.04448
Step 37185: loss = 0.07308
Step 37190: loss = 0.07985
Step 37195: loss = 0.09155
Step 37200: loss = 0.07296
Step 37205: loss = 0.10683
Step 37210: loss = 0.04438
Step 37215: loss = 0.05475
Step 37220: loss = 0.07598
Step 37225: loss = 0.06473
Step 37230: loss = 0.08525
Step 37235: loss = 0.10482
Step 37240: loss = 0.05946
Step 37245: loss = 0.07984
Step 37250: loss = 0.03548
Step 37255: loss = 0.07576
Step 37260: loss = 0.12857
Step 37265: loss = 0.08273
Step 37270: loss = 0.07803
Step 37275: loss = 0.03366
Step 37280: loss = 0.06094
Step 37285: loss = 0.09501
Step 37290: loss = 0.07382
Step 37295: loss = 0.08842
Step 37300: loss = 0.06828
Step 37305: loss = 0.10942
Step 37310: loss = 0.06861
Step 37315: loss = 0.06247
Step 37320: loss = 0.09486
Step 37325: loss = 0.04158
Step 37330: loss = 0.14164
Step 37335: loss = 0.04392
Step 37340: loss = 0.21611
Step 37345: loss = 0.10542
Step 37350: loss = 0.08201
Step 37355: loss = 0.06257
Step 37360: loss = 0.06380
Step 37365: loss = 0.09935
Step 37370: loss = 0.13466
Step 37375: loss = 0.08359
Step 37380: loss = 0.09614
Step 37385: loss = 0.05656
Step 37390: loss = 0.06106
Step 37395: loss = 0.07956
Step 37400: loss = 0.06275
Step 37405: loss = 0.08346
Step 37410: loss = 0.05254
Step 37415: loss = 0.03820
Step 37420: loss = 0.08454
Step 37425: loss = 0.08665
Step 37430: loss = 0.10241
Step 37435: loss = 0.08198
Step 37440: loss = 0.10741
Training Data Eval:
  Num examples: 49920, Num correct: 49008, Precision @ 1: 0.9817
('Testing Data Eval: EPOCH->', 97)
  Num examples: 9984, Num correct: 7436, Precision @ 1: 0.7448
Step 37445: loss = 0.08617
Step 37450: loss = 0.06337
Step 37455: loss = 0.13359
Step 37460: loss = 0.07646
Step 37465: loss = 0.10624
Step 37470: loss = 0.05418
Step 37475: loss = 0.08575
Step 37480: loss = 0.08290
Step 37485: loss = 0.08827
Step 37490: loss = 0.09957
Step 37495: loss = 0.08380
Step 37500: loss = 0.08339
Step 37505: loss = 0.03703
Step 37510: loss = 0.12964
Step 37515: loss = 0.06877
Step 37520: loss = 0.08889
Step 37525: loss = 0.08848
Step 37530: loss = 0.08283
Step 37535: loss = 0.05671
Step 37540: loss = 0.08136
Step 37545: loss = 0.07027
Step 37550: loss = 0.09911
Step 37555: loss = 0.09922
Step 37560: loss = 0.08118
Step 37565: loss = 0.08585
Step 37570: loss = 0.08133
Step 37575: loss = 0.07699
Step 37580: loss = 0.07066
Step 37585: loss = 0.09811
Step 37590: loss = 0.10715
Step 37595: loss = 0.09575
Step 37600: loss = 0.05500
Step 37605: loss = 0.09020
Step 37610: loss = 0.05782
Step 37615: loss = 0.11726
Step 37620: loss = 0.11169
Step 37625: loss = 0.09421
Step 37630: loss = 0.08928
Step 37635: loss = 0.07866
Step 37640: loss = 0.07742
Step 37645: loss = 0.06105
Step 37650: loss = 0.08427
Step 37655: loss = 0.07947
Step 37660: loss = 0.06511
Step 37665: loss = 0.12310
Step 37670: loss = 0.09008
Step 37675: loss = 0.07096
Step 37680: loss = 0.09923
Step 37685: loss = 0.08041
Step 37690: loss = 0.13215
Step 37695: loss = 0.10897
Step 37700: loss = 0.13682
Step 37705: loss = 0.07595
Step 37710: loss = 0.16496
Step 37715: loss = 0.08786
Step 37720: loss = 0.08657
Step 37725: loss = 0.13471
Step 37730: loss = 0.07343
Step 37735: loss = 0.08157
Step 37740: loss = 0.08863
Step 37745: loss = 0.10794
Step 37750: loss = 0.08517
Step 37755: loss = 0.11233
Step 37760: loss = 0.09097
Step 37765: loss = 0.08584
Step 37770: loss = 0.08070
Step 37775: loss = 0.09035
Step 37780: loss = 0.17086
Step 37785: loss = 0.07107
Step 37790: loss = 0.15991
Step 37795: loss = 0.05478
Step 37800: loss = 0.10732
Step 37805: loss = 0.10445
Step 37810: loss = 0.12229
Step 37815: loss = 0.12522
Step 37820: loss = 0.08763
Step 37825: loss = 0.05569
Step 37830: loss = 0.08246
Training Data Eval:
  Num examples: 49920, Num correct: 49034, Precision @ 1: 0.9823
('Testing Data Eval: EPOCH->', 98)
  Num examples: 9984, Num correct: 7319, Precision @ 1: 0.7331
Step 37835: loss = 0.06292
Step 37840: loss = 0.06643
Step 37845: loss = 0.06267
Step 37850: loss = 0.05615
Step 37855: loss = 0.07474
Step 37860: loss = 0.05874
Step 37865: loss = 0.05838
Step 37870: loss = 0.08344
Step 37875: loss = 0.09537
Step 37880: loss = 0.07035
Step 37885: loss = 0.07593
Step 37890: loss = 0.05523
Step 37895: loss = 0.07349
Step 37900: loss = 0.06218
Step 37905: loss = 0.05057
Step 37910: loss = 0.06741
Step 37915: loss = 0.06493
Step 37920: loss = 0.05838
Step 37925: loss = 0.05504
Step 37930: loss = 0.05930
Step 37935: loss = 0.09786
Step 37940: loss = 0.08738
Step 37945: loss = 0.09920
Step 37950: loss = 0.10416
Step 37955: loss = 0.09132
Step 37960: loss = 0.10334
Step 37965: loss = 0.08228
Step 37970: loss = 0.08620
Step 37975: loss = 0.06974
Step 37980: loss = 0.07823
Step 37985: loss = 0.07229
Step 37990: loss = 0.06648
Step 37995: loss = 0.08754
Step 38000: loss = 0.07880
Step 38005: loss = 0.06064
Step 38010: loss = 0.05707
Step 38015: loss = 0.14249
Step 38020: loss = 0.20659
Step 38025: loss = 0.13638
Step 38030: loss = 0.06962
Step 38035: loss = 0.12268
Step 38040: loss = 0.06527
Step 38045: loss = 0.07310
Step 38050: loss = 0.08117
Step 38055: loss = 0.06980
Step 38060: loss = 0.07804
Step 38065: loss = 0.09019
Step 38070: loss = 0.08205
Step 38075: loss = 0.09134
Step 38080: loss = 0.13391
Step 38085: loss = 0.09481
Step 38090: loss = 0.07875
Step 38095: loss = 0.08618
Step 38100: loss = 0.06139
Step 38105: loss = 0.10651
Step 38110: loss = 0.11409
Step 38115: loss = 0.05885
Step 38120: loss = 0.07500
Step 38125: loss = 0.10550
Step 38130: loss = 0.08135
Step 38135: loss = 0.07139
Step 38140: loss = 0.07043
Step 38145: loss = 0.05184
Step 38150: loss = 0.10134
Step 38155: loss = 0.09836
Step 38160: loss = 0.06568
Step 38165: loss = 0.10782
Step 38170: loss = 0.05715
Step 38175: loss = 0.12417
Step 38180: loss = 0.09563
Step 38185: loss = 0.07432
Step 38190: loss = 0.13506
Step 38195: loss = 0.07802
Step 38200: loss = 0.09587
Step 38205: loss = 0.07288
Step 38210: loss = 0.06830
Step 38215: loss = 0.04838
Step 38220: loss = 0.10342
Training Data Eval:
  Num examples: 49920, Num correct: 49162, Precision @ 1: 0.9848
('Testing Data Eval: EPOCH->', 99)
  Num examples: 9984, Num correct: 7450, Precision @ 1: 0.7462
Step 38225: loss = 0.07431
Step 38230: loss = 0.05612
Step 38235: loss = 0.11289
Step 38240: loss = 0.04830
Step 38245: loss = 0.13894
Step 38250: loss = 0.22176
Step 38255: loss = 0.10573
Step 38260: loss = 0.04265
Step 38265: loss = 0.11168
Step 38270: loss = 0.09561
Step 38275: loss = 0.08172
Step 38280: loss = 0.07602
Step 38285: loss = 0.13012
Step 38290: loss = 0.11506
Step 38295: loss = 0.08622
Step 38300: loss = 0.04712
Step 38305: loss = 0.10061
Step 38310: loss = 0.14794
Step 38315: loss = 0.06421
Step 38320: loss = 0.10137
Step 38325: loss = 0.08505
Step 38330: loss = 0.07432
Step 38335: loss = 0.06989
Step 38340: loss = 0.04952
Step 38345: loss = 0.05883
Step 38350: loss = 0.07427
Step 38355: loss = 0.09646
Step 38360: loss = 0.11074
Step 38365: loss = 0.05898
Step 38370: loss = 0.10836
Step 38375: loss = 0.10803
Step 38380: loss = 0.07813
Step 38385: loss = 0.08286
Step 38390: loss = 0.05614
Step 38395: loss = 0.20507
Step 38400: loss = 0.08016
Step 38405: loss = 0.07840
Step 38410: loss = 0.15945
Step 38415: loss = 0.06309
Step 38420: loss = 0.05277
Step 38425: loss = 0.05200
Step 38430: loss = 0.07911
Step 38435: loss = 0.05260
Step 38440: loss = 0.07965
Step 38445: loss = 0.05211
Step 38450: loss = 0.07133
Step 38455: loss = 0.06179
Step 38460: loss = 0.04789
Step 38465: loss = 0.04214
Step 38470: loss = 0.05653
Step 38475: loss = 0.06193
Step 38480: loss = 0.07671
Step 38485: loss = 0.09883
Step 38490: loss = 0.05076
Step 38495: loss = 0.10332
Step 38500: loss = 0.06985
Step 38505: loss = 0.10370
Step 38510: loss = 0.07979
Step 38515: loss = 0.07623
Step 38520: loss = 0.07201
Step 38525: loss = 0.05888
Step 38530: loss = 0.06253
Step 38535: loss = 0.07631
Step 38540: loss = 0.05297
Step 38545: loss = 0.10232
Step 38550: loss = 0.08584
Step 38555: loss = 0.08794
Step 38560: loss = 0.07947
Step 38565: loss = 0.06465
Step 38570: loss = 0.07518
Step 38575: loss = 0.09948
Step 38580: loss = 0.06572
Step 38585: loss = 0.06235
Step 38590: loss = 0.06760
Step 38595: loss = 0.07124
Step 38600: loss = 0.05552
Step 38605: loss = 0.06860
Step 38610: loss = 0.05835
Training Data Eval:
  Num examples: 49920, Num correct: 49302, Precision @ 1: 0.9876
('Testing Data Eval: EPOCH->', 100)
  Num examples: 9984, Num correct: 7430, Precision @ 1: 0.7442
Step 38615: loss = 0.04744
Step 38620: loss = 0.10990
Step 38625: loss = 0.09191
Step 38630: loss = 0.05452
Step 38635: loss = 0.13103
Step 38640: loss = 0.04136
Step 38645: loss = 0.07202
Step 38650: loss = 0.10290
Step 38655: loss = 0.08075
Step 38660: loss = 0.07997
Step 38665: loss = 0.14378
Step 38670: loss = 0.06396
Step 38675: loss = 0.10434
Step 38680: loss = 0.08950
Step 38685: loss = 0.12387
Step 38690: loss = 0.09543
Step 38695: loss = 0.08216
Step 38700: loss = 0.08344
Step 38705: loss = 0.05187
Step 38710: loss = 0.07859
Step 38715: loss = 0.09745
Step 38720: loss = 0.08938
Step 38725: loss = 0.13548
Step 38730: loss = 0.09409
Step 38735: loss = 0.08288
Step 38740: loss = 0.04535
Step 38745: loss = 0.07986
Step 38750: loss = 0.14428
Step 38755: loss = 0.04301
Step 38760: loss = 0.08822
Step 38765: loss = 0.12393
Step 38770: loss = 0.04779
Step 38775: loss = 0.05216
Step 38780: loss = 0.09668
Step 38785: loss = 0.04911
Step 38790: loss = 0.09257
Step 38795: loss = 0.09118
Step 38800: loss = 0.08100
Step 38805: loss = 0.09844
Step 38810: loss = 0.06224
Step 38815: loss = 0.05533
Step 38820: loss = 0.11358
Step 38825: loss = 0.07881
Step 38830: loss = 0.10988
Step 38835: loss = 0.09700
Step 38840: loss = 0.08176
Step 38845: loss = 0.06254
Step 38850: loss = 0.07514
Step 38855: loss = 0.13899
Step 38860: loss = 0.07383
Step 38865: loss = 0.17736
Step 38870: loss = 0.08895
Step 38875: loss = 0.13963
Step 38880: loss = 0.08182
Step 38885: loss = 0.07544
Step 38890: loss = 0.05894
Step 38895: loss = 0.10096
Step 38900: loss = 0.05483
Step 38905: loss = 0.07387
Step 38910: loss = 0.06211
Step 38915: loss = 0.06921
Step 38920: loss = 0.07242
Step 38925: loss = 0.08900
Step 38930: loss = 0.06647
Step 38935: loss = 0.08391
Step 38940: loss = 0.05746
Step 38945: loss = 0.07527
Step 38950: loss = 0.18537
Step 38955: loss = 0.05508
Step 38960: loss = 0.05226
Step 38965: loss = 0.05321
Step 38970: loss = 0.06903
Step 38975: loss = 0.05767
Step 38980: loss = 0.04041
Step 38985: loss = 0.08275
Step 38990: loss = 0.04637
Step 38995: loss = 0.06112
Step 39000: loss = 0.07739
Training Data Eval:
  Num examples: 49920, Num correct: 49219, Precision @ 1: 0.9860
('Testing Data Eval: EPOCH->', 101)
  Num examples: 9984, Num correct: 7398, Precision @ 1: 0.7410
Step 39005: loss = 0.07684
Step 39010: loss = 0.07122
Step 39015: loss = 0.06586
Step 39020: loss = 0.07680
Step 39025: loss = 0.07632
Step 39030: loss = 0.06829
Step 39035: loss = 0.08861
Step 39040: loss = 0.05464
Step 39045: loss = 0.07017
Step 39050: loss = 0.07328
Step 39055: loss = 0.06486
Step 39060: loss = 0.05228
Step 39065: loss = 0.04412
Step 39070: loss = 0.06722
Step 39075: loss = 0.04027
Step 39080: loss = 0.08706
Step 39085: loss = 0.08722
Step 39090: loss = 0.08899
Step 39095: loss = 0.07078
Step 39100: loss = 0.06789
Step 39105: loss = 0.08166
Step 39110: loss = 0.05478
Step 39115: loss = 0.11120
Step 39120: loss = 0.07643
Step 39125: loss = 0.06072
Step 39130: loss = 0.08334
Step 39135: loss = 0.08023
Step 39140: loss = 0.04271
Step 39145: loss = 0.08630
Step 39150: loss = 0.07229
Step 39155: loss = 0.13082
Step 39160: loss = 0.08168
Step 39165: loss = 0.07649
Step 39170: loss = 0.06867
Step 39175: loss = 0.09081
Step 39180: loss = 0.04770
Step 39185: loss = 0.06204
Step 39190: loss = 0.07327
Step 39195: loss = 0.05702
Step 39200: loss = 0.06847
Step 39205: loss = 0.04166
Step 39210: loss = 0.07926
Step 39215: loss = 0.03329
Step 39220: loss = 0.06727
Step 39225: loss = 0.09108
Step 39230: loss = 0.05440
Step 39235: loss = 0.05286
Step 39240: loss = 0.06961
Step 39245: loss = 0.04490
Step 39250: loss = 0.07040
Step 39255: loss = 0.06345
Step 39260: loss = 0.06357
Step 39265: loss = 0.04072
Step 39270: loss = 0.05693
Step 39275: loss = 0.04174
Step 39280: loss = 0.12031
Step 39285: loss = 0.09309
Step 39290: loss = 0.05912
Step 39295: loss = 0.06740
Step 39300: loss = 0.10622
Step 39305: loss = 0.06086
Step 39310: loss = 0.06063
Step 39315: loss = 0.06999
Step 39320: loss = 0.08257
Step 39325: loss = 0.11910
Step 39330: loss = 0.12519
Step 39335: loss = 0.08017
Step 39340: loss = 0.08815
Step 39345: loss = 0.08041
Step 39350: loss = 0.11772
Step 39355: loss = 0.08256
Step 39360: loss = 0.05440
Step 39365: loss = 0.06664
Step 39370: loss = 0.07010
Step 39375: loss = 0.05478
Step 39380: loss = 0.08874
Step 39385: loss = 0.06753
Step 39390: loss = 0.07370
Training Data Eval:
  Num examples: 49920, Num correct: 49211, Precision @ 1: 0.9858
('Testing Data Eval: EPOCH->', 102)
  Num examples: 9984, Num correct: 7472, Precision @ 1: 0.7484
Step 39395: loss = 0.06351
Step 39400: loss = 0.05240
Step 39405: loss = 0.04830
Step 39410: loss = 0.13860
Step 39415: loss = 0.07164
Step 39420: loss = 0.06161
Step 39425: loss = 0.07257
Step 39430: loss = 0.08475
Step 39435: loss = 0.04345
Step 39440: loss = 0.07687
Step 39445: loss = 0.08331
Step 39450: loss = 0.09664
Step 39455: loss = 0.05520
Step 39460: loss = 0.04389
Step 39465: loss = 0.10519
Step 39470: loss = 0.07547
Step 39475: loss = 0.04473
Step 39480: loss = 0.08747
Step 39485: loss = 0.07294
Step 39490: loss = 0.05963
Step 39495: loss = 0.08209
Step 39500: loss = 0.03036
Step 39505: loss = 0.04892
Step 39510: loss = 0.06925
Step 39515: loss = 0.05681
Step 39520: loss = 0.08369
Step 39525: loss = 0.10337
Step 39530: loss = 0.06895
Step 39535: loss = 0.06772
Step 39540: loss = 0.06590
Step 39545: loss = 0.07882
Step 39550: loss = 0.06175
Step 39555: loss = 0.07628
Step 39560: loss = 0.07091
Step 39565: loss = 0.04971
Step 39570: loss = 0.11492
Step 39575: loss = 0.08620
Step 39580: loss = 0.08829
Step 39585: loss = 0.06290
Step 39590: loss = 0.08405
Step 39595: loss = 0.06134
Step 39600: loss = 0.14939
Step 39605: loss = 0.06693
Step 39610: loss = 0.05676
Step 39615: loss = 0.08160
Step 39620: loss = 0.16782
Step 39625: loss = 0.04957
Step 39630: loss = 0.10242
Step 39635: loss = 0.05869
Step 39640: loss = 0.05110
Step 39645: loss = 0.05657
Step 39650: loss = 0.05915
Step 39655: loss = 0.10175
Step 39660: loss = 0.06720
Step 39665: loss = 0.13775
Step 39670: loss = 0.04725
Step 39675: loss = 0.03678
Step 39680: loss = 0.07716
Step 39685: loss = 0.06284
Step 39690: loss = 0.10506
Step 39695: loss = 0.06091
Step 39700: loss = 0.11330
Step 39705: loss = 0.05212
Step 39710: loss = 0.05950
Step 39715: loss = 0.08866
Step 39720: loss = 0.07354
Step 39725: loss = 0.12944
Step 39730: loss = 0.04694
Step 39735: loss = 0.06418
Step 39740: loss = 0.06011
Step 39745: loss = 0.13163
Step 39750: loss = 0.06093
Step 39755: loss = 0.04692
Step 39760: loss = 0.10404
Step 39765: loss = 0.10458
Step 39770: loss = 0.06236
Step 39775: loss = 0.05888
Step 39780: loss = 0.05448
Training Data Eval:
  Num examples: 49920, Num correct: 49198, Precision @ 1: 0.9855
('Testing Data Eval: EPOCH->', 103)
  Num examples: 9984, Num correct: 7403, Precision @ 1: 0.7415
Step 39785: loss = 0.06937
Step 39790: loss = 0.07597
Step 39795: loss = 0.05315
Step 39800: loss = 0.10644
Step 39805: loss = 0.07567
Step 39810: loss = 0.07691
Step 39815: loss = 0.04726
Step 39820: loss = 0.09377
Step 39825: loss = 0.05349
Step 39830: loss = 0.05359
Step 39835: loss = 0.05794
Step 39840: loss = 0.07784
Step 39845: loss = 0.06606
Step 39850: loss = 0.08735
Step 39855: loss = 0.11117
Step 39860: loss = 0.13427
Step 39865: loss = 0.09359
Step 39870: loss = 0.08327
Step 39875: loss = 0.08150
Step 39880: loss = 0.08587
Step 39885: loss = 0.07699
Step 39890: loss = 0.06433
Step 39895: loss = 0.08850
Step 39900: loss = 0.04861
Step 39905: loss = 0.10029
Step 39910: loss = 0.05436
Step 39915: loss = 0.08164
Step 39920: loss = 0.07156
Step 39925: loss = 0.06813
Step 39930: loss = 0.10510
Step 39935: loss = 0.05532
Step 39940: loss = 0.04732
Step 39945: loss = 0.07890
Step 39950: loss = 0.10697
Step 39955: loss = 0.07374
Step 39960: loss = 0.11034
Step 39965: loss = 0.03822
Step 39970: loss = 0.03888
Step 39975: loss = 0.08348
Step 39980: loss = 0.04418
Step 39985: loss = 0.06639
Step 39990: loss = 0.08245
Step 39995: loss = 0.11702
Step 40000: loss = 0.07860
Step 40005: loss = 0.06370
Step 40010: loss = 0.07142
Step 40015: loss = 0.12864
Step 40020: loss = 0.05895
Step 40025: loss = 0.05230
Step 40030: loss = 0.09959
Step 40035: loss = 0.09773
Step 40040: loss = 0.11220
Step 40045: loss = 0.06544
Step 40050: loss = 0.12683
Step 40055: loss = 0.04195
Step 40060: loss = 0.09449
Step 40065: loss = 0.07551
Step 40070: loss = 0.06196
Step 40075: loss = 0.05616
Step 40080: loss = 0.06676
Step 40085: loss = 0.05527
Step 40090: loss = 0.08640
Step 40095: loss = 0.07887
Step 40100: loss = 0.05834
Step 40105: loss = 0.08494
Step 40110: loss = 0.09040
Step 40115: loss = 0.06930
Step 40120: loss = 0.08088
Step 40125: loss = 0.08535
Step 40130: loss = 0.11118
Step 40135: loss = 0.16522
Step 40140: loss = 0.09005
Step 40145: loss = 0.08667
Step 40150: loss = 0.04860
Step 40155: loss = 0.10720
Step 40160: loss = 0.05791
Step 40165: loss = 0.07157
Step 40170: loss = 0.05499
Training Data Eval:
  Num examples: 49920, Num correct: 49273, Precision @ 1: 0.9870
('Testing Data Eval: EPOCH->', 104)
  Num examples: 9984, Num correct: 7390, Precision @ 1: 0.7402
Step 40175: loss = 0.04614
Step 40180: loss = 0.05426
Step 40185: loss = 0.05576
Step 40190: loss = 0.10103
Step 40195: loss = 0.07345
Step 40200: loss = 0.04500
Step 40205: loss = 0.04501
Step 40210: loss = 0.05150
Step 40215: loss = 0.05719
Step 40220: loss = 0.04968
Step 40225: loss = 0.09655
Step 40230: loss = 0.05415
Step 40235: loss = 0.10946
Step 40240: loss = 0.08426
Step 40245: loss = 0.04090
Step 40250: loss = 0.08608
Step 40255: loss = 0.13613
Step 40260: loss = 0.04285
Step 40265: loss = 0.08847
Step 40270: loss = 0.07154
Step 40275: loss = 0.09616
Step 40280: loss = 0.12440
Step 40285: loss = 0.11058
Step 40290: loss = 0.09755
Step 40295: loss = 0.06371
Step 40300: loss = 0.08515
Step 40305: loss = 0.09981
Step 40310: loss = 0.05071
Step 40315: loss = 0.03760
Step 40320: loss = 0.04838
Step 40325: loss = 0.05837
Step 40330: loss = 0.06627
Step 40335: loss = 0.09179
Step 40340: loss = 0.07307
Step 40345: loss = 0.06678
Step 40350: loss = 0.08693
Step 40355: loss = 0.06264
Step 40360: loss = 0.08503
Step 40365: loss = 0.10528
Step 40370: loss = 0.07149
Step 40375: loss = 0.07885
Step 40380: loss = 0.06336
Step 40385: loss = 0.08478
Step 40390: loss = 0.06774
Step 40395: loss = 0.06841
Step 40400: loss = 0.04035
Step 40405: loss = 0.05089
Step 40410: loss = 0.05038
Step 40415: loss = 0.08302
Step 40420: loss = 0.05643
Step 40425: loss = 0.04329
Step 40430: loss = 0.07270
Step 40435: loss = 0.05730
Step 40440: loss = 0.06859
Step 40445: loss = 0.08474
Step 40450: loss = 0.04448
Step 40455: loss = 0.04987
Step 40460: loss = 0.11410
Step 40465: loss = 0.06610
Step 40470: loss = 0.06297
Step 40475: loss = 0.04693
Step 40480: loss = 0.05683
Step 40485: loss = 0.12897
Step 40490: loss = 0.04886
Step 40495: loss = 0.06750
Step 40500: loss = 0.08533
Step 40505: loss = 0.09672
Step 40510: loss = 0.08998
Step 40515: loss = 0.04001
Step 40520: loss = 0.09213
Step 40525: loss = 0.10646
Step 40530: loss = 0.13869
Step 40535: loss = 0.05069
Step 40540: loss = 0.08375
Step 40545: loss = 0.11626
Step 40550: loss = 0.09587
Step 40555: loss = 0.05556
Step 40560: loss = 0.06969
Training Data Eval:
  Num examples: 49920, Num correct: 49257, Precision @ 1: 0.9867
('Testing Data Eval: EPOCH->', 105)
  Num examples: 9984, Num correct: 7399, Precision @ 1: 0.7411
Step 40565: loss = 0.08266
Step 40570: loss = 0.08743
Step 40575: loss = 0.08126
Step 40580: loss = 0.05246
Step 40585: loss = 0.05796
Step 40590: loss = 0.06462
Step 40595: loss = 0.03568
Step 40600: loss = 0.05184
Step 40605: loss = 0.04937
Step 40610: loss = 0.06099
Step 40615: loss = 0.05015
Step 40620: loss = 0.08047
Step 40625: loss = 0.07212
Step 40630: loss = 0.06781
Step 40635: loss = 0.06642
Step 40640: loss = 0.05900
Step 40645: loss = 0.08341
Step 40650: loss = 0.06376
Step 40655: loss = 0.10730
Step 40660: loss = 0.05293
Step 40665: loss = 0.05222
Step 40670: loss = 0.07129
Step 40675: loss = 0.06178
Step 40680: loss = 0.10891
Step 40685: loss = 0.08739
Step 40690: loss = 0.06212
Step 40695: loss = 0.05724
Step 40700: loss = 0.08143
Step 40705: loss = 0.07884
Step 40710: loss = 0.05443
Step 40715: loss = 0.06465
Step 40720: loss = 0.04778
Step 40725: loss = 0.05904
Step 40730: loss = 0.06342
Step 40735: loss = 0.06547
Step 40740: loss = 0.06274
Step 40745: loss = 0.04556
Step 40750: loss = 0.04366
Step 40755: loss = 0.06068
Step 40760: loss = 0.06021
Step 40765: loss = 0.08205
Step 40770: loss = 0.06892
Step 40775: loss = 0.04807
Step 40780: loss = 0.07483
Step 40785: loss = 0.09417
Step 40790: loss = 0.04351
Step 40795: loss = 0.03701
Step 40800: loss = 0.06489
Step 40805: loss = 0.05154
Step 40810: loss = 0.04482
Step 40815: loss = 0.05043
Step 40820: loss = 0.07499
Step 40825: loss = 0.08531
Step 40830: loss = 0.04955
Step 40835: loss = 0.07101
Step 40840: loss = 0.09163
Step 40845: loss = 0.06613
Step 40850: loss = 0.04259
Step 40855: loss = 0.04953
Step 40860: loss = 0.06042
Step 40865: loss = 0.03105
Step 40870: loss = 0.07357
Step 40875: loss = 0.07476
Step 40880: loss = 0.07407
Step 40885: loss = 0.04953
Step 40890: loss = 0.06698
Step 40895: loss = 0.06340
Step 40900: loss = 0.07870
Step 40905: loss = 0.08550
Step 40910: loss = 0.06038
Step 40915: loss = 0.13368
Step 40920: loss = 0.04201
Step 40925: loss = 0.05489
Step 40930: loss = 0.09285
Step 40935: loss = 0.05958
Step 40940: loss = 0.05748
Step 40945: loss = 0.05633
Step 40950: loss = 0.07741
Training Data Eval:
  Num examples: 49920, Num correct: 49268, Precision @ 1: 0.9869
('Testing Data Eval: EPOCH->', 106)
  Num examples: 9984, Num correct: 7360, Precision @ 1: 0.7372
Step 40955: loss = 0.09136
Step 40960: loss = 0.05445
Step 40965: loss = 0.10338
Step 40970: loss = 0.07470
Step 40975: loss = 0.10609
Step 40980: loss = 0.08771
Step 40985: loss = 0.07016
Step 40990: loss = 0.07936
Step 40995: loss = 0.06732
Step 41000: loss = 0.04789
Step 41005: loss = 0.07400
Step 41010: loss = 0.07594
Step 41015: loss = 0.05748
Step 41020: loss = 0.05302
Step 41025: loss = 0.07801
Step 41030: loss = 0.05295
Step 41035: loss = 0.06633
Step 41040: loss = 0.05603
Step 41045: loss = 0.08763
Step 41050: loss = 0.07300
Step 41055: loss = 0.04809
Step 41060: loss = 0.17536
Step 41065: loss = 0.07513
Step 41070: loss = 0.07075
Step 41075: loss = 0.07047
Step 41080: loss = 0.04870
Step 41085: loss = 0.10455
Step 41090: loss = 0.07229
Step 41095: loss = 0.12771
Step 41100: loss = 0.16769
Step 41105: loss = 0.08157
Step 41110: loss = 0.09911
Step 41115: loss = 0.07026
Step 41120: loss = 0.09270
Step 41125: loss = 0.08984
Step 41130: loss = 0.05707
Step 41135: loss = 0.14285
Step 41140: loss = 0.15874
Step 41145: loss = 0.12030
Step 41150: loss = 0.08622
Step 41155: loss = 0.06887
Step 41160: loss = 0.08173
Step 41165: loss = 0.10821
Step 41170: loss = 0.05745
Step 41175: loss = 0.06261
Step 41180: loss = 0.07634
Step 41185: loss = 0.09225
Step 41190: loss = 0.05129
Step 41195: loss = 0.06911
Step 41200: loss = 0.07703
Step 41205: loss = 0.06536
Step 41210: loss = 0.06889
Step 41215: loss = 0.06792
Step 41220: loss = 0.04996
Step 41225: loss = 0.06187
Step 41230: loss = 0.05906
Step 41235: loss = 0.08113
Step 41240: loss = 0.06425
Step 41245: loss = 0.08957
Step 41250: loss = 0.06751
Step 41255: loss = 0.07257
Step 41260: loss = 0.04040
Step 41265: loss = 0.07674
Step 41270: loss = 0.11774
Step 41275: loss = 0.10462
Step 41280: loss = 0.06518
Step 41285: loss = 0.06233
Step 41290: loss = 0.02986
Step 41295: loss = 0.06014
Step 41300: loss = 0.06320
Step 41305: loss = 0.07866
Step 41310: loss = 0.04640
Step 41315: loss = 0.07392
Step 41320: loss = 0.06113
Step 41325: loss = 0.10372
Step 41330: loss = 0.05210
Step 41335: loss = 0.08882
Step 41340: loss = 0.11773
Training Data Eval:
  Num examples: 49920, Num correct: 49363, Precision @ 1: 0.9888
('Testing Data Eval: EPOCH->', 107)
  Num examples: 9984, Num correct: 7474, Precision @ 1: 0.7486
Step 41345: loss = 0.08804
Step 41350: loss = 0.07075
Step 41355: loss = 0.07036
Step 41360: loss = 0.04317
Step 41365: loss = 0.06013
Step 41370: loss = 0.04529
Step 41375: loss = 0.05437
Step 41380: loss = 0.07966
Step 41385: loss = 0.07821
Step 41390: loss = 0.08299
Step 41395: loss = 0.07009
Step 41400: loss = 0.06368
Step 41405: loss = 0.09581
Step 41410: loss = 0.08296
Step 41415: loss = 0.11907
Step 41420: loss = 0.08628
Step 41425: loss = 0.07971
Step 41430: loss = 0.07492
Step 41435: loss = 0.11320
Step 41440: loss = 0.06701
Step 41445: loss = 0.09229
Step 41450: loss = 0.07086
Step 41455: loss = 0.09577
Step 41460: loss = 0.12527
Step 41465: loss = 0.09641
Step 41470: loss = 0.06693
Step 41475: loss = 0.11529
Step 41480: loss = 0.12462
Step 41485: loss = 0.07515
Step 41490: loss = 0.06108
Step 41495: loss = 0.09516
Step 41500: loss = 0.06993
Step 41505: loss = 0.07356
Step 41510: loss = 0.07282
Step 41515: loss = 0.04723
Step 41520: loss = 0.12022
Step 41525: loss = 0.07117
Step 41530: loss = 0.10395
Step 41535: loss = 0.05377
Step 41540: loss = 0.07323
Step 41545: loss = 0.08705
Step 41550: loss = 0.10053
Step 41555: loss = 0.05568
Step 41560: loss = 0.06712
Step 41565: loss = 0.06329
Step 41570: loss = 0.10472
Step 41575: loss = 0.10623
Step 41580: loss = 0.11820
Step 41585: loss = 0.08564
Step 41590: loss = 0.07235
Step 41595: loss = 0.08358
Step 41600: loss = 0.05962
Step 41605: loss = 0.06601
Step 41610: loss = 0.06580
Step 41615: loss = 0.04191
Step 41620: loss = 0.10887
Step 41625: loss = 0.09106
Step 41630: loss = 0.06222
Step 41635: loss = 0.11431
Step 41640: loss = 0.08921
Step 41645: loss = 0.05578
Step 41650: loss = 0.10275
Step 41655: loss = 0.07014
Step 41660: loss = 0.05905
Step 41665: loss = 0.10540
Step 41670: loss = 0.05305
Step 41675: loss = 0.09506
Step 41680: loss = 0.06398
Step 41685: loss = 0.08393
Step 41690: loss = 0.05987
Step 41695: loss = 0.04469
Step 41700: loss = 0.09094
Step 41705: loss = 0.09412
Step 41710: loss = 0.06045
Step 41715: loss = 0.10021
Step 41720: loss = 0.05531
Step 41725: loss = 0.08163
Step 41730: loss = 0.11593
Training Data Eval:
  Num examples: 49920, Num correct: 49098, Precision @ 1: 0.9835
('Testing Data Eval: EPOCH->', 108)
  Num examples: 9984, Num correct: 7331, Precision @ 1: 0.7343
Step 41735: loss = 0.08166
Step 41740: loss = 0.13845
Step 41745: loss = 0.14290
Step 41750: loss = 0.11157
Step 41755: loss = 0.05532
Step 41760: loss = 0.14134
Step 41765: loss = 0.09586
Step 41770: loss = 0.05013
Step 41775: loss = 0.08529
Step 41780: loss = 0.05063
Step 41785: loss = 0.10889
Step 41790: loss = 0.12180
Step 41795: loss = 0.09495
Step 41800: loss = 0.08611
Step 41805: loss = 0.06882
Step 41810: loss = 0.13001
Step 41815: loss = 0.06668
Step 41820: loss = 0.09263
Step 41825: loss = 0.10228
Step 41830: loss = 0.06603
Step 41835: loss = 0.06022
Step 41840: loss = 0.06092
Step 41845: loss = 0.09011
Step 41850: loss = 0.06553
Step 41855: loss = 0.09926
Step 41860: loss = 0.09094
Step 41865: loss = 0.10951
Step 41870: loss = 0.10912
Step 41875: loss = 0.10704
Step 41880: loss = 0.15091
Step 41885: loss = 0.12546
Step 41890: loss = 0.06810
Step 41895: loss = 0.10492
Step 41900: loss = 0.06944
Step 41905: loss = 0.11493
Step 41910: loss = 0.14438
Step 41915: loss = 0.05256
Step 41920: loss = 0.08415
Step 41925: loss = 0.06675
Step 41930: loss = 0.09074
Step 41935: loss = 0.12996
Step 41940: loss = 0.08687
Step 41945: loss = 0.14205
Step 41950: loss = 0.04449
Step 41955: loss = 0.04668
Step 41960: loss = 0.05234
Step 41965: loss = 0.06011
Step 41970: loss = 0.05399
Step 41975: loss = 0.05819
Step 41980: loss = 0.06855
Step 41985: loss = 0.06848
Step 41990: loss = 0.08914
Step 41995: loss = 0.06411
Step 42000: loss = 0.08558
Step 42005: loss = 0.04965
Step 42010: loss = 0.05450
Step 42015: loss = 0.07710
Step 42020: loss = 0.04517
Step 42025: loss = 0.06319
Step 42030: loss = 0.11761
Step 42035: loss = 0.05727
Step 42040: loss = 0.05942
Step 42045: loss = 0.06558
Step 42050: loss = 0.05846
Step 42055: loss = 0.10888
Step 42060: loss = 0.05616
Step 42065: loss = 0.05316
Step 42070: loss = 0.06902
Step 42075: loss = 0.13305
Step 42080: loss = 0.04824
Step 42085: loss = 0.08729
Step 42090: loss = 0.07735
Step 42095: loss = 0.09819
Step 42100: loss = 0.08994
Step 42105: loss = 0.08861
Step 42110: loss = 0.16169
Step 42115: loss = 0.11434
Step 42120: loss = 0.07404
Training Data Eval:
  Num examples: 49920, Num correct: 49297, Precision @ 1: 0.9875
('Testing Data Eval: EPOCH->', 109)
  Num examples: 9984, Num correct: 7473, Precision @ 1: 0.7485
Step 42125: loss = 0.06489
Step 42130: loss = 0.04791
Step 42135: loss = 0.06104
Step 42140: loss = 0.04215
Step 42145: loss = 0.07106
Step 42150: loss = 0.04873
Step 42155: loss = 0.08899
Step 42160: loss = 0.08062
Step 42165: loss = 0.08417
Step 42170: loss = 0.04372
Step 42175: loss = 0.06072
Step 42180: loss = 0.06617
Step 42185: loss = 0.08714
Step 42190: loss = 0.07381
Step 42195: loss = 0.08631
Step 42200: loss = 0.09588
Step 42205: loss = 0.05002
Step 42210: loss = 0.06817
Step 42215: loss = 0.06894
Step 42220: loss = 0.06911
Step 42225: loss = 0.06466
Step 42230: loss = 0.05554
Step 42235: loss = 0.06511
Step 42240: loss = 0.07832
Step 42245: loss = 0.07527
Step 42250: loss = 0.09774
Step 42255: loss = 0.05915
Step 42260: loss = 0.12093
Step 42265: loss = 0.06108
Step 42270: loss = 0.04169
Step 42275: loss = 0.08120
Step 42280: loss = 0.03942
Step 42285: loss = 0.04596
Step 42290: loss = 0.07393
Step 42295: loss = 0.05728
Step 42300: loss = 0.05228
Step 42305: loss = 0.10983
Step 42310: loss = 0.07336
Step 42315: loss = 0.06460
Step 42320: loss = 0.06974
Step 42325: loss = 0.09205
Step 42330: loss = 0.07634
Step 42335: loss = 0.04399
Step 42340: loss = 0.08506
Step 42345: loss = 0.08648
Step 42350: loss = 0.06943
Step 42355: loss = 0.03899
Step 42360: loss = 0.09518
Step 42365: loss = 0.04493
Step 42370: loss = 0.07075
Step 42375: loss = 0.08012
Step 42380: loss = 0.04579
Step 42385: loss = 0.06132
Step 42390: loss = 0.07708
Step 42395: loss = 0.05846
Step 42400: loss = 0.09723
Step 42405: loss = 0.03716
Step 42410: loss = 0.06319
Step 42415: loss = 0.04281
Step 42420: loss = 0.06419
Step 42425: loss = 0.04328
Step 42430: loss = 0.18134
Step 42435: loss = 0.05711
Step 42440: loss = 0.04376
Step 42445: loss = 0.08190
Step 42450: loss = 0.04473
Step 42455: loss = 0.06159
Step 42460: loss = 0.03681
Step 42465: loss = 0.08998
Step 42470: loss = 0.07160
Step 42475: loss = 0.06152
Step 42480: loss = 0.04102
Step 42485: loss = 0.05628
Step 42490: loss = 0.06491
Step 42495: loss = 0.04948
Step 42500: loss = 0.06813
Step 42505: loss = 0.08986
Step 42510: loss = 0.07964
Training Data Eval:
  Num examples: 49920, Num correct: 49251, Precision @ 1: 0.9866
('Testing Data Eval: EPOCH->', 110)
  Num examples: 9984, Num correct: 7352, Precision @ 1: 0.7364
Step 42515: loss = 0.09279
Step 42520: loss = 0.06990
Step 42525: loss = 0.10126
Step 42530: loss = 0.12605
Step 42535: loss = 0.07788
Step 42540: loss = 0.05268
Step 42545: loss = 0.08107
Step 42550: loss = 0.07057
Step 42555: loss = 0.07596
Step 42560: loss = 0.08213
Step 42565: loss = 0.07678
Step 42570: loss = 0.04533
Step 42575: loss = 0.07392
Step 42580: loss = 0.06585
Step 42585: loss = 0.08547
Step 42590: loss = 0.05680
Step 42595: loss = 0.08644
Step 42600: loss = 0.06205
Step 42605: loss = 0.04976
Step 42610: loss = 0.06873
Step 42615: loss = 0.06804
Step 42620: loss = 0.06926
Step 42625: loss = 0.04273
Step 42630: loss = 0.11261
Step 42635: loss = 0.03831
Step 42640: loss = 0.04553
Step 42645: loss = 0.03913
Step 42650: loss = 0.07790
Step 42655: loss = 0.05122
Step 42660: loss = 0.03991
Step 42665: loss = 0.05404
Step 42670: loss = 0.05628
Step 42675: loss = 0.06851
Step 42680: loss = 0.06877
Step 42685: loss = 0.04521
Step 42690: loss = 0.11374
Step 42695: loss = 0.06294
Step 42700: loss = 0.10728
Step 42705: loss = 0.07045
Step 42710: loss = 0.04438
Step 42715: loss = 0.08969
Step 42720: loss = 0.09808
Step 42725: loss = 0.09321
Step 42730: loss = 0.08650
Step 42735: loss = 0.05938
Step 42740: loss = 0.04907
Step 42745: loss = 0.06607
Step 42750: loss = 0.11127
Step 42755: loss = 0.10664
Step 42760: loss = 0.05026
Step 42765: loss = 0.08478
Step 42770: loss = 0.07612
Step 42775: loss = 0.06325
Step 42780: loss = 0.06351
Step 42785: loss = 0.06742
Step 42790: loss = 0.07591
Step 42795: loss = 0.05651
Step 42800: loss = 0.05178
Step 42805: loss = 0.09025
Step 42810: loss = 0.11966
Step 42815: loss = 0.05540
Step 42820: loss = 0.09940
Step 42825: loss = 0.06950
Step 42830: loss = 0.06639
Step 42835: loss = 0.05733
Step 42840: loss = 0.06237
Step 42845: loss = 0.07258
Step 42850: loss = 0.06708
Step 42855: loss = 0.05335
Step 42860: loss = 0.08136
Step 42865: loss = 0.08282
Step 42870: loss = 0.10756
Step 42875: loss = 0.03928
Step 42880: loss = 0.09105
Step 42885: loss = 0.05761
Step 42890: loss = 0.05263
Step 42895: loss = 0.07943
Step 42900: loss = 0.08375
Training Data Eval:
  Num examples: 49920, Num correct: 49408, Precision @ 1: 0.9897
('Testing Data Eval: EPOCH->', 111)
  Num examples: 9984, Num correct: 7435, Precision @ 1: 0.7447
Step 42905: loss = 0.06683
Step 42910: loss = 0.06165
Step 42915: loss = 0.09110
Step 42920: loss = 0.07632
Step 42925: loss = 0.05403
Step 42930: loss = 0.12091
Step 42935: loss = 0.05809
Step 42940: loss = 0.06818
Step 42945: loss = 0.08780
Step 42950: loss = 0.08362
Step 42955: loss = 0.06145
Step 42960: loss = 0.06585
Step 42965: loss = 0.08584
Step 42970: loss = 0.06417
Step 42975: loss = 0.05509
Step 42980: loss = 0.05436
Step 42985: loss = 0.05134
Step 42990: loss = 0.10619
Step 42995: loss = 0.07980
Step 43000: loss = 0.04628
Step 43005: loss = 0.06165
Step 43010: loss = 0.09059
Step 43015: loss = 0.05511
Step 43020: loss = 0.10319
Step 43025: loss = 0.08316
Step 43030: loss = 0.14436
Step 43035: loss = 0.07451
Step 43040: loss = 0.06233
Step 43045: loss = 0.06107
Step 43050: loss = 0.05783
Step 43055: loss = 0.12707
Step 43060: loss = 0.04724
Step 43065: loss = 0.05877
Step 43070: loss = 0.07209
Step 43075: loss = 0.05818
Step 43080: loss = 0.07708
Step 43085: loss = 0.07897
Step 43090: loss = 0.06889
Step 43095: loss = 0.06946
Step 43100: loss = 0.06407
Step 43105: loss = 0.06087
Step 43110: loss = 0.05817
Step 43115: loss = 0.07198
Step 43120: loss = 0.05796
Step 43125: loss = 0.05553
Step 43130: loss = 0.16314
Step 43135: loss = 0.13021
Step 43140: loss = 0.07860
Step 43145: loss = 0.05647
Step 43150: loss = 0.11273
Step 43155: loss = 0.10870
Step 43160: loss = 0.08545
Step 43165: loss = 0.12089
Step 43170: loss = 0.09660
Step 43175: loss = 0.06951
Step 43180: loss = 0.08802
Step 43185: loss = 0.11434
Step 43190: loss = 0.07912
Step 43195: loss = 0.05666
Step 43200: loss = 0.07954
Step 43205: loss = 0.10613
Step 43210: loss = 0.11927
Step 43215: loss = 0.05301
Step 43220: loss = 0.06480
Step 43225: loss = 0.06238
Step 43230: loss = 0.05408
Step 43235: loss = 0.05551
Step 43240: loss = 0.06030
Step 43245: loss = 0.08083
Step 43250: loss = 0.12674
Step 43255: loss = 0.09823
Step 43260: loss = 0.06823
Step 43265: loss = 0.06219
Step 43270: loss = 0.08371
Step 43275: loss = 0.11519
Step 43280: loss = 0.06613
Step 43285: loss = 0.04912
Step 43290: loss = 0.05639
Training Data Eval:
  Num examples: 49920, Num correct: 49324, Precision @ 1: 0.9881
('Testing Data Eval: EPOCH->', 112)
  Num examples: 9984, Num correct: 7447, Precision @ 1: 0.7459
Step 43295: loss = 0.06997
Step 43300: loss = 0.06099
Step 43305: loss = 0.05280
Step 43310: loss = 0.07023
Step 43315: loss = 0.07454
Step 43320: loss = 0.10834
Step 43325: loss = 0.05979
Step 43330: loss = 0.06082
Step 43335: loss = 0.07772
Step 43340: loss = 0.10266
Step 43345: loss = 0.03869
Step 43350: loss = 0.05144
Step 43355: loss = 0.05976
Step 43360: loss = 0.05820
Step 43365: loss = 0.09644
Step 43370: loss = 0.08556
Step 43375: loss = 0.04191
Step 43380: loss = 0.06575
Step 43385: loss = 0.07380
Step 43390: loss = 0.06621
Step 43395: loss = 0.09438
Step 43400: loss = 0.08972
Step 43405: loss = 0.06368
Step 43410: loss = 0.06538
Step 43415: loss = 0.06551
Step 43420: loss = 0.08670
Step 43425: loss = 0.04915
Step 43430: loss = 0.04146
Step 43435: loss = 0.05719
Step 43440: loss = 0.11033
Step 43445: loss = 0.06123
Step 43450: loss = 0.08345
Step 43455: loss = 0.08919
Step 43460: loss = 0.07904
Step 43465: loss = 0.05297
Step 43470: loss = 0.05088
Step 43475: loss = 0.06614
Step 43480: loss = 0.04636
Step 43485: loss = 0.10344
Step 43490: loss = 0.07048
Step 43495: loss = 0.05283
Step 43500: loss = 0.06838
Step 43505: loss = 0.05171
Step 43510: loss = 0.05377
Step 43515: loss = 0.04810
Step 43520: loss = 0.03988
Step 43525: loss = 0.04858
Step 43530: loss = 0.06546
Step 43535: loss = 0.04350
Step 43540: loss = 0.04668
Step 43545: loss = 0.09900
Step 43550: loss = 0.11519
Step 43555: loss = 0.08790
Step 43560: loss = 0.07203
Step 43565: loss = 0.09540
Step 43570: loss = 0.06093
Step 43575: loss = 0.06792
Step 43580: loss = 0.10138
Step 43585: loss = 0.05450
Step 43590: loss = 0.08740
Step 43595: loss = 0.05581
Step 43600: loss = 0.09668
Step 43605: loss = 0.04380
Step 43610: loss = 0.07066
Step 43615: loss = 0.05805
Step 43620: loss = 0.09659
Step 43625: loss = 0.06795
Step 43630: loss = 0.07291
Step 43635: loss = 0.07798
Step 43640: loss = 0.04603
Step 43645: loss = 0.07606
Step 43650: loss = 0.03918
Step 43655: loss = 0.08326
Step 43660: loss = 0.08676
Step 43665: loss = 0.06244
Step 43670: loss = 0.05586
Step 43675: loss = 0.09430
Step 43680: loss = 0.06206
Training Data Eval:
  Num examples: 49920, Num correct: 49349, Precision @ 1: 0.9886
('Testing Data Eval: EPOCH->', 113)
  Num examples: 9984, Num correct: 7418, Precision @ 1: 0.7430
Step 43685: loss = 0.04615
Step 43690: loss = 0.04349
Step 43695: loss = 0.03930
Step 43700: loss = 0.06529
Step 43705: loss = 0.04963
Step 43710: loss = 0.11402
Step 43715: loss = 0.07317
Step 43720: loss = 0.04851
Step 43725: loss = 0.06012
Step 43730: loss = 0.08902
Step 43735: loss = 0.06522
Step 43740: loss = 0.04900
Step 43745: loss = 0.08259
Step 43750: loss = 0.07404
Step 43755: loss = 0.09270
Step 43760: loss = 0.09926
Step 43765: loss = 0.05563
Step 43770: loss = 0.05581
Step 43775: loss = 0.06302
Step 43780: loss = 0.11223
Step 43785: loss = 0.07155
Step 43790: loss = 0.07740
Step 43795: loss = 0.08924
Step 43800: loss = 0.06941
Step 43805: loss = 0.09408
Step 43810: loss = 0.05551
Step 43815: loss = 0.04362
Step 43820: loss = 0.05806
Step 43825: loss = 0.07048
Step 43830: loss = 0.09399
Step 43835: loss = 0.06590
Step 43840: loss = 0.07982
Step 43845: loss = 0.05107
Step 43850: loss = 0.08917
Step 43855: loss = 0.08337
Step 43860: loss = 0.09332
Step 43865: loss = 0.07247
Step 43870: loss = 0.04307
Step 43875: loss = 0.06121
Step 43880: loss = 0.07733
Step 43885: loss = 0.04513
Step 43890: loss = 0.05081
Step 43895: loss = 0.07023
Step 43900: loss = 0.08190
Step 43905: loss = 0.07511
Step 43910: loss = 0.09635
Step 43915: loss = 0.04837
Step 43920: loss = 0.09458
Step 43925: loss = 0.06472
Step 43930: loss = 0.05926
Step 43935: loss = 0.06929
Step 43940: loss = 0.04972
Step 43945: loss = 0.07948
Step 43950: loss = 0.07916
Step 43955: loss = 0.04436
Step 43960: loss = 0.06299
Step 43965: loss = 0.09192
Step 43970: loss = 0.07896
Step 43975: loss = 0.04392
Step 43980: loss = 0.05671
Step 43985: loss = 0.05725
Step 43990: loss = 0.12330
Step 43995: loss = 0.05747
Step 44000: loss = 0.08829
Step 44005: loss = 0.15731
Step 44010: loss = 0.10338
Step 44015: loss = 0.05374
Step 44020: loss = 0.08614
Step 44025: loss = 0.08705
Step 44030: loss = 0.06381
Step 44035: loss = 0.04515
Step 44040: loss = 0.07655
Step 44045: loss = 0.06632
Step 44050: loss = 0.03896
Step 44055: loss = 0.07836
Step 44060: loss = 0.03847
Step 44065: loss = 0.08992
Step 44070: loss = 0.06140
Training Data Eval:
  Num examples: 49920, Num correct: 49457, Precision @ 1: 0.9907
('Testing Data Eval: EPOCH->', 114)
  Num examples: 9984, Num correct: 7394, Precision @ 1: 0.7406
Step 44075: loss = 0.10516
Step 44080: loss = 0.03419
Step 44085: loss = 0.04951
Step 44090: loss = 0.07640
Step 44095: loss = 0.07448
Step 44100: loss = 0.06461
Step 44105: loss = 0.09507
Step 44110: loss = 0.04693
Step 44115: loss = 0.07482
Step 44120: loss = 0.05055
Step 44125: loss = 0.04187
Step 44130: loss = 0.06459
Step 44135: loss = 0.09926
Step 44140: loss = 0.04531
Step 44145: loss = 0.06619
Step 44150: loss = 0.07068
Step 44155: loss = 0.04846
Step 44160: loss = 0.07152
Step 44165: loss = 0.06276
Step 44170: loss = 0.09806
Step 44175: loss = 0.09664
Step 44180: loss = 0.14750
Step 44185: loss = 0.11473
Step 44190: loss = 0.07988
Step 44195: loss = 0.16347
Step 44200: loss = 0.14275
Step 44205: loss = 0.04455
Step 44210: loss = 0.07135
Step 44215: loss = 0.05175
Step 44220: loss = 0.07560
Step 44225: loss = 0.07781
Step 44230: loss = 0.08220
Step 44235: loss = 0.08076
Step 44240: loss = 0.09954
Step 44245: loss = 0.08769
Step 44250: loss = 0.09391
Step 44255: loss = 0.05917
Step 44260: loss = 0.09795
Step 44265: loss = 0.10114
Step 44270: loss = 0.11511
Step 44275: loss = 0.08925
Step 44280: loss = 0.09029
Step 44285: loss = 0.07650
Step 44290: loss = 0.08652
Step 44295: loss = 0.10057
Step 44300: loss = 0.06119
Step 44305: loss = 0.06057
Step 44310: loss = 0.09923
Step 44315: loss = 0.03001
Step 44320: loss = 0.10853
Step 44325: loss = 0.07007
Step 44330: loss = 0.12875
Step 44335: loss = 0.11064
Step 44340: loss = 0.04610
Step 44345: loss = 0.06050
Step 44350: loss = 0.04449
Step 44355: loss = 0.04123
Step 44360: loss = 0.05007
Step 44365: loss = 0.03836
Step 44370: loss = 0.05903
Step 44375: loss = 0.04013
Step 44380: loss = 0.13294
Step 44385: loss = 0.09802
Step 44390: loss = 0.08147
Step 44395: loss = 0.06062
Step 44400: loss = 0.06618
Step 44405: loss = 0.10171
Step 44410: loss = 0.05776
Step 44415: loss = 0.08503
Step 44420: loss = 0.04058
Step 44425: loss = 0.08185
Step 44430: loss = 0.08594
Step 44435: loss = 0.04233
Step 44440: loss = 0.06950
Step 44445: loss = 0.05962
Step 44450: loss = 0.07298
Step 44455: loss = 0.07629
Step 44460: loss = 0.05702
Training Data Eval:
  Num examples: 49920, Num correct: 49310, Precision @ 1: 0.9878
('Testing Data Eval: EPOCH->', 115)
  Num examples: 9984, Num correct: 7484, Precision @ 1: 0.7496
Step 44465: loss = 0.05570
Step 44470: loss = 0.06543
Step 44475: loss = 0.06760
Step 44480: loss = 0.06761
Step 44485: loss = 0.09928
Step 44490: loss = 0.06745
Step 44495: loss = 0.06998
Step 44500: loss = 0.05976
Step 44505: loss = 0.06935
Step 44510: loss = 0.08155
Step 44515: loss = 0.04893
Step 44520: loss = 0.06578
Step 44525: loss = 0.06913
Step 44530: loss = 0.04112
Step 44535: loss = 0.09752
Step 44540: loss = 0.06521
Step 44545: loss = 0.06760
Step 44550: loss = 0.06327
Step 44555: loss = 0.08051
Step 44560: loss = 0.09748
Step 44565: loss = 0.06270
Step 44570: loss = 0.11469
Step 44575: loss = 0.07034
Step 44580: loss = 0.11248
Step 44585: loss = 0.08306
Step 44590: loss = 0.04835
Step 44595: loss = 0.07750
Step 44600: loss = 0.05452
Step 44605: loss = 0.07611
Step 44610: loss = 0.06977
Step 44615: loss = 0.05544
Step 44620: loss = 0.05436
Step 44625: loss = 0.05789
Step 44630: loss = 0.09392
Step 44635: loss = 0.08963
Step 44640: loss = 0.11646
Step 44645: loss = 0.06006
Step 44650: loss = 0.08286
Step 44655: loss = 0.05482
Step 44660: loss = 0.04288
Step 44665: loss = 0.10655
Step 44670: loss = 0.15107
Step 44675: loss = 0.08330
Step 44680: loss = 0.05982
Step 44685: loss = 0.06435
Step 44690: loss = 0.09031
Step 44695: loss = 0.03908
Step 44700: loss = 0.05680
Step 44705: loss = 0.05530
Step 44710: loss = 0.05188
Step 44715: loss = 0.08225
Step 44720: loss = 0.05516
Step 44725: loss = 0.07766
Step 44730: loss = 0.10441
Step 44735: loss = 0.08980
Step 44740: loss = 0.05095
Step 44745: loss = 0.07161
Step 44750: loss = 0.04124
Step 44755: loss = 0.04514
Step 44760: loss = 0.17522
Step 44765: loss = 0.09145
Step 44770: loss = 0.06499
Step 44775: loss = 0.05637
Step 44780: loss = 0.05100
Step 44785: loss = 0.07743
Step 44790: loss = 0.06079
Step 44795: loss = 0.06092
Step 44800: loss = 0.05555
Step 44805: loss = 0.05984
Step 44810: loss = 0.09767
Step 44815: loss = 0.06400
Step 44820: loss = 0.05598
Step 44825: loss = 0.05043
Step 44830: loss = 0.04744
Step 44835: loss = 0.04322
Step 44840: loss = 0.08659
Step 44845: loss = 0.05652
Step 44850: loss = 0.09113
Training Data Eval:
  Num examples: 49920, Num correct: 49451, Precision @ 1: 0.9906
('Testing Data Eval: EPOCH->', 116)
  Num examples: 9984, Num correct: 7413, Precision @ 1: 0.7425
Step 44855: loss = 0.08897
Step 44860: loss = 0.10668
Step 44865: loss = 0.04879
Step 44870: loss = 0.05498
Step 44875: loss = 0.07883
Step 44880: loss = 0.12936
Step 44885: loss = 0.09284
Step 44890: loss = 0.07834
Step 44895: loss = 0.13026
Step 44900: loss = 0.05958
Step 44905: loss = 0.08042
Step 44910: loss = 0.09328
Step 44915: loss = 0.08425
Step 44920: loss = 0.04833
Step 44925: loss = 0.06110
Step 44930: loss = 0.05735
Step 44935: loss = 0.04897
Step 44940: loss = 0.04881
Step 44945: loss = 0.10402
Step 44950: loss = 0.08081
Step 44955: loss = 0.06149
Step 44960: loss = 0.06875
Step 44965: loss = 0.05301
Step 44970: loss = 0.06969
Step 44975: loss = 0.09370
Step 44980: loss = 0.12703
Step 44985: loss = 0.10495
Step 44990: loss = 0.13167
Step 44995: loss = 0.06043
Step 45000: loss = 0.13627
Step 45005: loss = 0.11186
Step 45010: loss = 0.06520
Step 45015: loss = 0.07505
Step 45020: loss = 0.05212
Step 45025: loss = 0.06309
Step 45030: loss = 0.06208
Step 45035: loss = 0.05929
Step 45040: loss = 0.05224
Step 45045: loss = 0.08842
Step 45050: loss = 0.04792
Step 45055: loss = 0.10095
Step 45060: loss = 0.09392
Step 45065: loss = 0.07136
Step 45070: loss = 0.11130
Step 45075: loss = 0.08009
Step 45080: loss = 0.06159
Step 45085: loss = 0.04246
Step 45090: loss = 0.07196
Step 45095: loss = 0.11788
Step 45100: loss = 0.06888
Step 45105: loss = 0.05914
Step 45110: loss = 0.08606
Step 45115: loss = 0.05013
Step 45120: loss = 0.05719
Step 45125: loss = 0.11957
Step 45130: loss = 0.06001
Step 45135: loss = 0.04121
Step 45140: loss = 0.05165
Step 45145: loss = 0.06663
Step 45150: loss = 0.07158
Step 45155: loss = 0.08797
Step 45160: loss = 0.07429
Step 45165: loss = 0.05890
Step 45170: loss = 0.04673
Step 45175: loss = 0.04542
Step 45180: loss = 0.06636
Step 45185: loss = 0.07858
Step 45190: loss = 0.10054
Step 45195: loss = 0.09283
Step 45200: loss = 0.08565
Step 45205: loss = 0.06751
Step 45210: loss = 0.05365
Step 45215: loss = 0.04925
Step 45220: loss = 0.11169
Step 45225: loss = 0.09590
Step 45230: loss = 0.04784
Step 45235: loss = 0.05319
Step 45240: loss = 0.04482
Training Data Eval:
  Num examples: 49920, Num correct: 49363, Precision @ 1: 0.9888
('Testing Data Eval: EPOCH->', 117)
  Num examples: 9984, Num correct: 7386, Precision @ 1: 0.7398
Step 45245: loss = 0.06321
Step 45250: loss = 0.10535
Step 45255: loss = 0.06326
Step 45260: loss = 0.05858
Step 45265: loss = 0.04770
Step 45270: loss = 0.03836
Step 45275: loss = 0.02856
Step 45280: loss = 0.06817
Step 45285: loss = 0.03721
Step 45290: loss = 0.05989
Step 45295: loss = 0.06526
Step 45300: loss = 0.07553
Step 45305: loss = 0.04228
Step 45310: loss = 0.05085
Step 45315: loss = 0.09130
Step 45320: loss = 0.08420
Step 45325: loss = 0.09583
Step 45330: loss = 0.09534
Step 45335: loss = 0.07275
Step 45340: loss = 0.04645
Step 45345: loss = 0.05625
Step 45350: loss = 0.03896
Step 45355: loss = 0.05901
Step 45360: loss = 0.06126
Step 45365: loss = 0.03236
Step 45370: loss = 0.13060
Step 45375: loss = 0.05280
Step 45380: loss = 0.08610
Step 45385: loss = 0.06237
Step 45390: loss = 0.05274
Step 45395: loss = 0.03155
Step 45400: loss = 0.05186
Step 45405: loss = 0.05820
Step 45410: loss = 0.04145
Step 45415: loss = 0.04804
Step 45420: loss = 0.05279
Step 45425: loss = 0.05903
Step 45430: loss = 0.07282
Step 45435: loss = 0.07976
Step 45440: loss = 0.07621
Step 45445: loss = 0.07791
Step 45450: loss = 0.05680
Step 45455: loss = 0.05612
Step 45460: loss = 0.06274
Step 45465: loss = 0.05565
Step 45470: loss = 0.05401
Step 45475: loss = 0.04727
Step 45480: loss = 0.09354
Step 45485: loss = 0.08392
Step 45490: loss = 0.04997
Step 45495: loss = 0.05887
Step 45500: loss = 0.09450
Step 45505: loss = 0.04520
Step 45510: loss = 0.09679
Step 45515: loss = 0.06867
Step 45520: loss = 0.05736
Step 45525: loss = 0.06198
Step 45530: loss = 0.07268
Step 45535: loss = 0.05324
Step 45540: loss = 0.04515
Step 45545: loss = 0.04844
Step 45550: loss = 0.06592
Step 45555: loss = 0.03515
Step 45560: loss = 0.08192
Step 45565: loss = 0.04707
Step 45570: loss = 0.06117
Step 45575: loss = 0.05961
Step 45580: loss = 0.05713
Step 45585: loss = 0.08182
Step 45590: loss = 0.05177
Step 45595: loss = 0.09439
Step 45600: loss = 0.06473
Step 45605: loss = 0.07866
Step 45610: loss = 0.07049
Step 45615: loss = 0.07036
Step 45620: loss = 0.05281
Step 45625: loss = 0.05198
Step 45630: loss = 0.06338
Training Data Eval:
  Num examples: 49920, Num correct: 49601, Precision @ 1: 0.9936
('Testing Data Eval: EPOCH->', 118)
  Num examples: 9984, Num correct: 7572, Precision @ 1: 0.7584
Step 45635: loss = 0.04597
Step 45640: loss = 0.04968
Step 45645: loss = 0.06017
Step 45650: loss = 0.04750
Step 45655: loss = 0.03191
Step 45660: loss = 0.04671
Step 45665: loss = 0.05505
Step 45670: loss = 0.08771
Step 45675: loss = 0.04974
Step 45680: loss = 0.04607
Step 45685: loss = 0.07456
Step 45690: loss = 0.04937
Step 45695: loss = 0.07233
Step 45700: loss = 0.09300
Step 45705: loss = 0.05921
Step 45710: loss = 0.05065
Step 45715: loss = 0.03971
Step 45720: loss = 0.04678
Step 45725: loss = 0.07207
Step 45730: loss = 0.05712
Step 45735: loss = 0.04534
Step 45740: loss = 0.06859
Step 45745: loss = 0.04838
Step 45750: loss = 0.09468
Step 45755: loss = 0.07449
Step 45760: loss = 0.04889
Step 45765: loss = 0.03515
Step 45770: loss = 0.05444
Step 45775: loss = 0.06578
Step 45780: loss = 0.11294
Step 45785: loss = 0.04671
Step 45790: loss = 0.07822
Step 45795: loss = 0.05457
Step 45800: loss = 0.06163
Step 45805: loss = 0.05077
Step 45810: loss = 0.06357
Step 45815: loss = 0.05522
Step 45820: loss = 0.06131
Step 45825: loss = 0.08539
Step 45830: loss = 0.05194
Step 45835: loss = 0.06773
Step 45840: loss = 0.12508
Step 45845: loss = 0.07911
Step 45850: loss = 0.07010
Step 45855: loss = 0.03973
Step 45860: loss = 0.06220
Step 45865: loss = 0.09416
Step 45870: loss = 0.06284
Step 45875: loss = 0.05593
Step 45880: loss = 0.05733
Step 45885: loss = 0.09626
Step 45890: loss = 0.11702
Step 45895: loss = 0.06909
Step 45900: loss = 0.10083
Step 45905: loss = 0.03840
Step 45910: loss = 0.05020
Step 45915: loss = 0.05732
Step 45920: loss = 0.07007
Step 45925: loss = 0.06627
Step 45930: loss = 0.06233
Step 45935: loss = 0.04585
Step 45940: loss = 0.08789
Step 45945: loss = 0.10083
Step 45950: loss = 0.09658
Step 45955: loss = 0.06056
Step 45960: loss = 0.08577
Step 45965: loss = 0.06073
Step 45970: loss = 0.06425
Step 45975: loss = 0.04333
Step 45980: loss = 0.05105
Step 45985: loss = 0.06134
Step 45990: loss = 0.07435
Step 45995: loss = 0.15134
Step 46000: loss = 0.08853
Step 46005: loss = 0.05922
Step 46010: loss = 0.03832
Step 46015: loss = 0.05934
Step 46020: loss = 0.03830
Training Data Eval:
  Num examples: 49920, Num correct: 49540, Precision @ 1: 0.9924
('Testing Data Eval: EPOCH->', 119)
  Num examples: 9984, Num correct: 7348, Precision @ 1: 0.7360
Step 46025: loss = 0.05173
Step 46030: loss = 0.05999
Step 46035: loss = 0.03178
Step 46040: loss = 0.04803
Step 46045: loss = 0.05563
Step 46050: loss = 0.05202
Step 46055: loss = 0.05469
Step 46060: loss = 0.09861
Step 46065: loss = 0.05265
Step 46070: loss = 0.05393
Step 46075: loss = 0.04973
Step 46080: loss = 0.08408
Step 46085: loss = 0.08967
Step 46090: loss = 0.13994
Step 46095: loss = 0.09164
Step 46100: loss = 0.06968
Step 46105: loss = 0.07023
Step 46110: loss = 0.09398
Step 46115: loss = 0.08217
Step 46120: loss = 0.03932
Step 46125: loss = 0.05484
Step 46130: loss = 0.04516
Step 46135: loss = 0.05442
Step 46140: loss = 0.10532
Step 46145: loss = 0.05950
Step 46150: loss = 0.12311
Step 46155: loss = 0.07372
Step 46160: loss = 0.11083
Step 46165: loss = 0.07350
Step 46170: loss = 0.06320
Step 46175: loss = 0.05753
Step 46180: loss = 0.07284
Step 46185: loss = 0.08958
Step 46190: loss = 0.07045
Step 46195: loss = 0.07988
Step 46200: loss = 0.08373
Step 46205: loss = 0.10439
Step 46210: loss = 0.10173
Step 46215: loss = 0.05830
Step 46220: loss = 0.06578
Step 46225: loss = 0.05914
Step 46230: loss = 0.05101
Step 46235: loss = 0.07642
Step 46240: loss = 0.05807
Step 46245: loss = 0.06164
Step 46250: loss = 0.05554
Step 46255: loss = 0.06030
Step 46260: loss = 0.12054
Step 46265: loss = 0.05381
Step 46270: loss = 0.08586
Step 46275: loss = 0.06647
Step 46280: loss = 0.10065
Step 46285: loss = 0.10616
Step 46290: loss = 0.02996
Step 46295: loss = 0.04758
Step 46300: loss = 0.06745
Step 46305: loss = 0.05470
Step 46310: loss = 0.05342
Step 46315: loss = 0.05064
Step 46320: loss = 0.11338
Step 46325: loss = 0.08874
Step 46330: loss = 0.07695
Step 46335: loss = 0.04514
Step 46340: loss = 0.08077
Step 46345: loss = 0.07219
Step 46350: loss = 0.05091
Step 46355: loss = 0.05666
Step 46360: loss = 0.06627
Step 46365: loss = 0.05895
Step 46370: loss = 0.03367
Step 46375: loss = 0.04159
Step 46380: loss = 0.06415
Step 46385: loss = 0.05325
Step 46390: loss = 0.05514
Step 46395: loss = 0.05130
Step 46400: loss = 0.03894
Step 46405: loss = 0.06039
Step 46410: loss = 0.04433
Training Data Eval:
  Num examples: 49920, Num correct: 49493, Precision @ 1: 0.9914
('Testing Data Eval: EPOCH->', 120)
  Num examples: 9984, Num correct: 7432, Precision @ 1: 0.7444
Step 46415: loss = 0.03284
Step 46420: loss = 0.07363
Step 46425: loss = 0.05842
Step 46430: loss = 0.09226
Step 46435: loss = 0.12796
Step 46440: loss = 0.09389
Step 46445: loss = 0.04880
Step 46450: loss = 0.06032
Step 46455: loss = 0.06020
Step 46460: loss = 0.04910
Step 46465: loss = 0.08475
Step 46470: loss = 0.07248
Step 46475: loss = 0.05268
Step 46480: loss = 0.05215
Step 46485: loss = 0.08363
Step 46490: loss = 0.05828
Step 46495: loss = 0.07182
Step 46500: loss = 0.04754
Step 46505: loss = 0.05958
Step 46510: loss = 0.10792
Step 46515: loss = 0.05494
Step 46520: loss = 0.07478
Step 46525: loss = 0.07104
Step 46530: loss = 0.06295
Step 46535: loss = 0.08521
Step 46540: loss = 0.05213
Step 46545: loss = 0.03982
Step 46550: loss = 0.04590
Step 46555: loss = 0.06813
Step 46560: loss = 0.07470
Step 46565: loss = 0.04471
Step 46570: loss = 0.06345
Step 46575: loss = 0.08086
Step 46580: loss = 0.04189
Step 46585: loss = 0.06464
Step 46590: loss = 0.05206
Step 46595: loss = 0.07089
Step 46600: loss = 0.04415
Step 46605: loss = 0.05518
Step 46610: loss = 0.07747
Step 46615: loss = 0.07689
Step 46620: loss = 0.05322
Step 46625: loss = 0.07532
Step 46630: loss = 0.07443
Step 46635: loss = 0.04995
Step 46640: loss = 0.08863
Step 46645: loss = 0.06555
Step 46650: loss = 0.12047
Step 46655: loss = 0.05992
Step 46660: loss = 0.06791
Step 46665: loss = 0.09474
Step 46670: loss = 0.07413
Step 46675: loss = 0.06519
Step 46680: loss = 0.06347
Step 46685: loss = 0.08112
Step 46690: loss = 0.05201
Step 46695: loss = 0.06194
Step 46700: loss = 0.03407
Step 46705: loss = 0.04994
Step 46710: loss = 0.05274
Step 46715: loss = 0.12106
Step 46720: loss = 0.08727
Step 46725: loss = 0.08107
Step 46730: loss = 0.08602
Step 46735: loss = 0.08726
Step 46740: loss = 0.06127
Step 46745: loss = 0.03567
Step 46750: loss = 0.09905
Step 46755: loss = 0.04031
Step 46760: loss = 0.06350
Step 46765: loss = 0.04993
Step 46770: loss = 0.07966
Step 46775: loss = 0.11996
Step 46780: loss = 0.04810
Step 46785: loss = 0.06010
Step 46790: loss = 0.12025
Step 46795: loss = 0.05581
Step 46800: loss = 0.09429
Training Data Eval:
  Num examples: 49920, Num correct: 49443, Precision @ 1: 0.9904
('Testing Data Eval: EPOCH->', 121)
  Num examples: 9984, Num correct: 7484, Precision @ 1: 0.7496
Step 46805: loss = 0.10073
Step 46810: loss = 0.05272
Step 46815: loss = 0.05918
Step 46820: loss = 0.07651
Step 46825: loss = 0.04800
Step 46830: loss = 0.05115
Step 46835: loss = 0.05085
Step 46840: loss = 0.03657
Step 46845: loss = 0.06218
Step 46850: loss = 0.04145
Step 46855: loss = 0.04644
Step 46860: loss = 0.03837
Step 46865: loss = 0.05788
Step 46870: loss = 0.04445
Step 46875: loss = 0.06276
Step 46880: loss = 0.07487
Step 46885: loss = 0.03970
Step 46890: loss = 0.04137
Step 46895: loss = 0.04194
Step 46900: loss = 0.05838
Step 46905: loss = 0.10820
Step 46910: loss = 0.05784
Step 46915: loss = 0.06254
Step 46920: loss = 0.06284
Step 46925: loss = 0.06534
Step 46930: loss = 0.07236
Step 46935: loss = 0.07474
Step 46940: loss = 0.07988
Step 46945: loss = 0.05783
Step 46950: loss = 0.09053
Step 46955: loss = 0.09254
Step 46960: loss = 0.04699
Step 46965: loss = 0.10899
Step 46970: loss = 0.05201
Step 46975: loss = 0.09316
Step 46980: loss = 0.13077
Step 46985: loss = 0.08014
Step 46990: loss = 0.06189
Step 46995: loss = 0.09848
Step 47000: loss = 0.10085
Step 47005: loss = 0.07779
Step 47010: loss = 0.12590
Step 47015: loss = 0.06814
Step 47020: loss = 0.05041
Step 47025: loss = 0.07292
Step 47030: loss = 0.09601
Step 47035: loss = 0.06376
Step 47040: loss = 0.05690
Step 47045: loss = 0.05151
Step 47050: loss = 0.11652
Step 47055: loss = 0.04520
Step 47060: loss = 0.05030
Step 47065: loss = 0.04339
Step 47070: loss = 0.06296
Step 47075: loss = 0.08214
Step 47080: loss = 0.06287
Step 47085: loss = 0.03901
Step 47090: loss = 0.06969
Step 47095: loss = 0.05745
Step 47100: loss = 0.04581
Step 47105: loss = 0.03968
Step 47110: loss = 0.05635
Step 47115: loss = 0.06150
Step 47120: loss = 0.03866
Step 47125: loss = 0.03987
Step 47130: loss = 0.05379
Step 47135: loss = 0.05772
Step 47140: loss = 0.05001
Step 47145: loss = 0.06696
Step 47150: loss = 0.06669
Step 47155: loss = 0.05634
Step 47160: loss = 0.07456
Step 47165: loss = 0.07561
Step 47170: loss = 0.08792
Step 47175: loss = 0.05988
Step 47180: loss = 0.12600
Step 47185: loss = 0.04642
Step 47190: loss = 0.11711
Training Data Eval:
  Num examples: 49920, Num correct: 49253, Precision @ 1: 0.9866
('Testing Data Eval: EPOCH->', 122)
  Num examples: 9984, Num correct: 7385, Precision @ 1: 0.7397
Step 47195: loss = 0.05507
Step 47200: loss = 0.08334
Step 47205: loss = 0.06475
Step 47210: loss = 0.06240
Step 47215: loss = 0.06018
Step 47220: loss = 0.08404
Step 47225: loss = 0.07194
Step 47230: loss = 0.04086
Step 47235: loss = 0.05622
Step 47240: loss = 0.05760
Step 47245: loss = 0.04983
Step 47250: loss = 0.05827
Step 47255: loss = 0.04211
Step 47260: loss = 0.08760
Step 47265: loss = 0.06855
Step 47270: loss = 0.04001
Step 47275: loss = 0.05338
Step 47280: loss = 0.05662
Step 47285: loss = 0.06802
Step 47290: loss = 0.06146
Step 47295: loss = 0.05390
Step 47300: loss = 0.09007
Step 47305: loss = 0.12000
Step 47310: loss = 0.08044
Step 47315: loss = 0.07026
Step 47320: loss = 0.06117
Step 47325: loss = 0.04280
Step 47330: loss = 0.08032
Step 47335: loss = 0.07247
Step 47340: loss = 0.04192
Step 47345: loss = 0.06855
Step 47350: loss = 0.05525
Step 47355: loss = 0.05039
Step 47360: loss = 0.03548
Step 47365: loss = 0.04780
Step 47370: loss = 0.05767
Step 47375: loss = 0.05517
Step 47380: loss = 0.06666
Step 47385: loss = 0.06380
Step 47390: loss = 0.07894
Step 47395: loss = 0.05143
Step 47400: loss = 0.05065
Step 47405: loss = 0.04222
Step 47410: loss = 0.06010
Step 47415: loss = 0.05622
Step 47420: loss = 0.07895
Step 47425: loss = 0.06126
Step 47430: loss = 0.04791
Step 47435: loss = 0.04136
Step 47440: loss = 0.06264
Step 47445: loss = 0.04164
Step 47450: loss = 0.05205
Step 47455: loss = 0.04648
Step 47460: loss = 0.06957
Step 47465: loss = 0.04258
Step 47470: loss = 0.08062
Step 47475: loss = 0.05853
Step 47480: loss = 0.06469
Step 47485: loss = 0.03765
Step 47490: loss = 0.04937
Step 47495: loss = 0.09821
Step 47500: loss = 0.08824
Step 47505: loss = 0.17977
Step 47510: loss = 0.04667
Step 47515: loss = 0.04712
Step 47520: loss = 0.03724
Step 47525: loss = 0.08435
Step 47530: loss = 0.05686
Step 47535: loss = 0.09864
Step 47540: loss = 0.05164
Step 47545: loss = 0.07422
Step 47550: loss = 0.04839
Step 47555: loss = 0.10652
Step 47560: loss = 0.05505
Step 47565: loss = 0.06473
Step 47570: loss = 0.05076
Step 47575: loss = 0.11179
Step 47580: loss = 0.06316
Training Data Eval:
  Num examples: 49920, Num correct: 49418, Precision @ 1: 0.9899
('Testing Data Eval: EPOCH->', 123)
  Num examples: 9984, Num correct: 7500, Precision @ 1: 0.7512
Step 47585: loss = 0.15138
Step 47590: loss = 0.04420
Step 47595: loss = 0.08708
Step 47600: loss = 0.06150
Step 47605: loss = 0.04571
Step 47610: loss = 0.09150
Step 47615: loss = 0.04824
Step 47620: loss = 0.04995
Step 47625: loss = 0.04666
Step 47630: loss = 0.04254
Step 47635: loss = 0.06810
Step 47640: loss = 0.03374
Step 47645: loss = 0.07877
Step 47650: loss = 0.05519
Step 47655: loss = 0.06298
Step 47660: loss = 0.05222
Step 47665: loss = 0.07023
Step 47670: loss = 0.06800
Step 47675: loss = 0.03658
Step 47680: loss = 0.05072
Step 47685: loss = 0.04444
Step 47690: loss = 0.07930
Step 47695: loss = 0.07664
Step 47700: loss = 0.07978
Step 47705: loss = 0.05621
Step 47710: loss = 0.07948
Step 47715: loss = 0.06246
Step 47720: loss = 0.05065
Step 47725: loss = 0.05492
Step 47730: loss = 0.06901
Step 47735: loss = 0.06740
Step 47740: loss = 0.08340
Step 47745: loss = 0.05441
Step 47750: loss = 0.07099
Step 47755: loss = 0.05102
Step 47760: loss = 0.04840
Step 47765: loss = 0.05595
Step 47770: loss = 0.07698
Step 47775: loss = 0.05743
Step 47780: loss = 0.09424
Step 47785: loss = 0.03398
Step 47790: loss = 0.05123
Step 47795: loss = 0.09069
Step 47800: loss = 0.09621
Step 47805: loss = 0.05267
Step 47810: loss = 0.07972
Step 47815: loss = 0.07235
Step 47820: loss = 0.06824
Step 47825: loss = 0.05051
Step 47830: loss = 0.05692
Step 47835: loss = 0.07500
Step 47840: loss = 0.06160
Step 47845: loss = 0.05946
Step 47850: loss = 0.05902
Step 47855: loss = 0.08751
Step 47860: loss = 0.05445
Step 47865: loss = 0.07220
Step 47870: loss = 0.03749
Step 47875: loss = 0.14104
Step 47880: loss = 0.08244
Step 47885: loss = 0.06504
Step 47890: loss = 0.05419
Step 47895: loss = 0.07399
Step 47900: loss = 0.05979
Step 47905: loss = 0.10289
Step 47910: loss = 0.10402
Step 47915: loss = 0.06463
Step 47920: loss = 0.05362
Step 47925: loss = 0.06493
Step 47930: loss = 0.03805
Step 47935: loss = 0.08288
Step 47940: loss = 0.05229
Step 47945: loss = 0.08836
Step 47950: loss = 0.08500
Step 47955: loss = 0.09285
Step 47960: loss = 0.06865
Step 47965: loss = 0.06563
Step 47970: loss = 0.08982
Training Data Eval:
  Num examples: 49920, Num correct: 49505, Precision @ 1: 0.9917
('Testing Data Eval: EPOCH->', 124)
  Num examples: 9984, Num correct: 7475, Precision @ 1: 0.7487
Step 47975: loss = 0.10798
Step 47980: loss = 0.08905
Step 47985: loss = 0.05956
Step 47990: loss = 0.04468
Step 47995: loss = 0.06301
Step 48000: loss = 0.07331
Step 48005: loss = 0.05053
Step 48010: loss = 0.04933
Step 48015: loss = 0.06188
Step 48020: loss = 0.03647
Step 48025: loss = 0.05983
Step 48030: loss = 0.05137
Step 48035: loss = 0.05272
Step 48040: loss = 0.05087
Step 48045: loss = 0.05337
Step 48050: loss = 0.06165
Step 48055: loss = 0.04413
Step 48060: loss = 0.05936
Step 48065: loss = 0.08069
Step 48070: loss = 0.05568
Step 48075: loss = 0.06101
Step 48080: loss = 0.06598
Step 48085: loss = 0.03796
Step 48090: loss = 0.03968
Step 48095: loss = 0.07061
Step 48100: loss = 0.06110
Step 48105: loss = 0.07414
Step 48110: loss = 0.05769
Step 48115: loss = 0.06038
Step 48120: loss = 0.09609
Step 48125: loss = 0.04728
Step 48130: loss = 0.05586
Step 48135: loss = 0.09265
Step 48140: loss = 0.03685
Step 48145: loss = 0.06059
Step 48150: loss = 0.06155
Step 48155: loss = 0.05941
Step 48160: loss = 0.08287
Step 48165: loss = 0.04639
Step 48170: loss = 0.10162
Step 48175: loss = 0.06083
Step 48180: loss = 0.04946
Step 48185: loss = 0.07353
Step 48190: loss = 0.05450
Step 48195: loss = 0.05154
Step 48200: loss = 0.08338
Step 48205: loss = 0.08481
Step 48210: loss = 0.05807
Step 48215: loss = 0.05757
Step 48220: loss = 0.04455
Step 48225: loss = 0.05346
Step 48230: loss = 0.07736
Step 48235: loss = 0.06105
Step 48240: loss = 0.04261
Step 48245: loss = 0.04379
Step 48250: loss = 0.07255
Step 48255: loss = 0.06419
Step 48260: loss = 0.05389
Step 48265: loss = 0.05980
Step 48270: loss = 0.04387
Step 48275: loss = 0.10209
Step 48280: loss = 0.03887
Step 48285: loss = 0.06532
Step 48290: loss = 0.05681
Step 48295: loss = 0.05451
Step 48300: loss = 0.08099
Step 48305: loss = 0.07134
Step 48310: loss = 0.05714
Step 48315: loss = 0.06902
Step 48320: loss = 0.05447
Step 48325: loss = 0.06457
Step 48330: loss = 0.05239
Step 48335: loss = 0.06138
Step 48340: loss = 0.04302
Step 48345: loss = 0.05036
Step 48350: loss = 0.04653
Step 48355: loss = 0.05982
Step 48360: loss = 0.05030
Training Data Eval:
  Num examples: 49920, Num correct: 49497, Precision @ 1: 0.9915
('Testing Data Eval: EPOCH->', 125)
  Num examples: 9984, Num correct: 7520, Precision @ 1: 0.7532
Step 48365: loss = 0.04643
Step 48370: loss = 0.04809
Step 48375: loss = 0.04601
Step 48380: loss = 0.08997
Step 48385: loss = 0.06663
Step 48390: loss = 0.05079
Step 48395: loss = 0.06215
Step 48400: loss = 0.04699
Step 48405: loss = 0.05164
Step 48410: loss = 0.08454
Step 48415: loss = 0.04514
Step 48420: loss = 0.03713
Step 48425: loss = 0.08691
Step 48430: loss = 0.05120
Step 48435: loss = 0.06389
Step 48440: loss = 0.05217
Step 48445: loss = 0.05742
Step 48450: loss = 0.05072
Step 48455: loss = 0.06100
Step 48460: loss = 0.04738
Step 48465: loss = 0.05662
Step 48470: loss = 0.06235
Step 48475: loss = 0.04467
Step 48480: loss = 0.05294
Step 48485: loss = 0.04803
Step 48490: loss = 0.05833
Step 48495: loss = 0.03879
Step 48500: loss = 0.06030
Step 48505: loss = 0.05356
Step 48510: loss = 0.04367
Step 48515: loss = 0.05547
Step 48520: loss = 0.04685
Step 48525: loss = 0.05542
Step 48530: loss = 0.08328
Step 48535: loss = 0.05371
Step 48540: loss = 0.06466
Step 48545: loss = 0.04374
Step 48550: loss = 0.08256
Step 48555: loss = 0.05825
Step 48560: loss = 0.08162
Step 48565: loss = 0.05862
Step 48570: loss = 0.05410
Step 48575: loss = 0.03398
Step 48580: loss = 0.07678
Step 48585: loss = 0.05394
Step 48590: loss = 0.04871
Step 48595: loss = 0.04965
Step 48600: loss = 0.04920
Step 48605: loss = 0.04869
Step 48610: loss = 0.07139
Step 48615: loss = 0.06954
Step 48620: loss = 0.06209
Step 48625: loss = 0.07112
Step 48630: loss = 0.05489
Step 48635: loss = 0.04553
Step 48640: loss = 0.09485
Step 48645: loss = 0.03986
Step 48650: loss = 0.03866
Step 48655: loss = 0.06594
Step 48660: loss = 0.05561
Step 48665: loss = 0.07215
Step 48670: loss = 0.06201
Step 48675: loss = 0.07002
Step 48680: loss = 0.05684
Step 48685: loss = 0.09116
Step 48690: loss = 0.07393
Step 48695: loss = 0.04759
Step 48700: loss = 0.07222
Step 48705: loss = 0.08283
Step 48710: loss = 0.07283
Step 48715: loss = 0.05407
Step 48720: loss = 0.08800
Step 48725: loss = 0.07888
Step 48730: loss = 0.07211
Step 48735: loss = 0.07120
Step 48740: loss = 0.06164
Step 48745: loss = 0.07545
Step 48750: loss = 0.04715
Training Data Eval:
  Num examples: 49920, Num correct: 49414, Precision @ 1: 0.9899
('Testing Data Eval: EPOCH->', 126)
  Num examples: 9984, Num correct: 7465, Precision @ 1: 0.7477
Step 48755: loss = 0.04699
Step 48760: loss = 0.10407
Step 48765: loss = 0.09201
Step 48770: loss = 0.05155
Step 48775: loss = 0.04845
Step 48780: loss = 0.04537
Step 48785: loss = 0.05381
Step 48790: loss = 0.05986
Step 48795: loss = 0.05815
Step 48800: loss = 0.05808
Step 48805: loss = 0.04126
Step 48810: loss = 0.06644
Step 48815: loss = 0.10710
Step 48820: loss = 0.04979
Step 48825: loss = 0.06677
Step 48830: loss = 0.04497
Step 48835: loss = 0.04153
Step 48840: loss = 0.03885
Step 48845: loss = 0.05736
Step 48850: loss = 0.06381
Step 48855: loss = 0.03309
Step 48860: loss = 0.06499
Step 48865: loss = 0.05188
Step 48870: loss = 0.04254
Step 48875: loss = 0.04923
Step 48880: loss = 0.04826
Step 48885: loss = 0.04877
Step 48890: loss = 0.03226
Step 48895: loss = 0.06713
Step 48900: loss = 0.06333
Step 48905: loss = 0.04771
Step 48910: loss = 0.06993
Step 48915: loss = 0.05430
Step 48920: loss = 0.07934
Step 48925: loss = 0.05776
Step 48930: loss = 0.03997
Step 48935: loss = 0.04105
Step 48940: loss = 0.08943
Step 48945: loss = 0.04797
Step 48950: loss = 0.05030
Step 48955: loss = 0.04362
Step 48960: loss = 0.10164
Step 48965: loss = 0.05087
Step 48970: loss = 0.04600
Step 48975: loss = 0.07305
Step 48980: loss = 0.14265
Step 48985: loss = 0.05366
Step 48990: loss = 0.09037
Step 48995: loss = 0.07187
Step 49000: loss = 0.09866
Step 49005: loss = 0.04675
Step 49010: loss = 0.05694
Step 49015: loss = 0.05888
Step 49020: loss = 0.05638
Step 49025: loss = 0.07395
Step 49030: loss = 0.04401
Step 49035: loss = 0.07314
Step 49040: loss = 0.06369
Step 49045: loss = 0.03337
Step 49050: loss = 0.05288
Step 49055: loss = 0.04458
Step 49060: loss = 0.14291
Step 49065: loss = 0.04843
Step 49070: loss = 0.05511
Step 49075: loss = 0.06892
Step 49080: loss = 0.08195
Step 49085: loss = 0.06257
Step 49090: loss = 0.08292
Step 49095: loss = 0.13262
Step 49100: loss = 0.06821
Step 49105: loss = 0.07733
Step 49110: loss = 0.07850
Step 49115: loss = 0.02885
Step 49120: loss = 0.08193
Step 49125: loss = 0.05057
Step 49130: loss = 0.03848
Step 49135: loss = 0.12066
Step 49140: loss = 0.05952
Training Data Eval:
  Num examples: 49920, Num correct: 49422, Precision @ 1: 0.9900
('Testing Data Eval: EPOCH->', 127)
  Num examples: 9984, Num correct: 7400, Precision @ 1: 0.7412
Step 49145: loss = 0.08875
Step 49150: loss = 0.09498
Step 49155: loss = 0.06671
Step 49160: loss = 0.04396
Step 49165: loss = 0.04637
Step 49170: loss = 0.07096
Step 49175: loss = 0.06062
Step 49180: loss = 0.03712
Step 49185: loss = 0.05857
Step 49190: loss = 0.05515
Step 49195: loss = 0.04619
Step 49200: loss = 0.05900
Step 49205: loss = 0.03784
Step 49210: loss = 0.04494
Step 49215: loss = 0.04873
Step 49220: loss = 0.05692
Step 49225: loss = 0.07316
Step 49230: loss = 0.06051
Step 49235: loss = 0.05108
Step 49240: loss = 0.04605
Step 49245: loss = 0.05641
Step 49250: loss = 0.11711
Step 49255: loss = 0.05690
Step 49260: loss = 0.06745
Step 49265: loss = 0.11079
Step 49270: loss = 0.06526
Step 49275: loss = 0.05071
Step 49280: loss = 0.10115
Step 49285: loss = 0.04261
Step 49290: loss = 0.03735
Step 49295: loss = 0.03777
Step 49300: loss = 0.08074
Step 49305: loss = 0.05284
Step 49310: loss = 0.05400
Step 49315: loss = 0.05937
Step 49320: loss = 0.04613
Step 49325: loss = 0.09492
Step 49330: loss = 0.05953
Step 49335: loss = 0.07040
Step 49340: loss = 0.05099
Step 49345: loss = 0.06800
Step 49350: loss = 0.03901
Step 49355: loss = 0.07658
Step 49360: loss = 0.08918
Step 49365: loss = 0.06001
Step 49370: loss = 0.06274
Step 49375: loss = 0.09411
Step 49380: loss = 0.08955
Step 49385: loss = 0.07517
Step 49390: loss = 0.06167
Step 49395: loss = 0.07320
Step 49400: loss = 0.04568
Step 49405: loss = 0.04929
Step 49410: loss = 0.04273
Step 49415: loss = 0.05250
Step 49420: loss = 0.10551
Step 49425: loss = 0.09174
Step 49430: loss = 0.09301
Step 49435: loss = 0.03803
Step 49440: loss = 0.05778
Step 49445: loss = 0.04160
Step 49450: loss = 0.06409
Step 49455: loss = 0.07188
Step 49460: loss = 0.08439
Step 49465: loss = 0.08177
Step 49470: loss = 0.07337
Step 49475: loss = 0.05324
Step 49480: loss = 0.04410
Step 49485: loss = 0.04527
Step 49490: loss = 0.08949
Step 49495: loss = 0.04401
Step 49500: loss = 0.09034
Step 49505: loss = 0.05878
Step 49510: loss = 0.05180
Step 49515: loss = 0.08565
Step 49520: loss = 0.06455
Step 49525: loss = 0.08837
Step 49530: loss = 0.07258
Training Data Eval:
  Num examples: 49920, Num correct: 49499, Precision @ 1: 0.9916
('Testing Data Eval: EPOCH->', 128)
  Num examples: 9984, Num correct: 7454, Precision @ 1: 0.7466
Step 49535: loss = 0.11500
Step 49540: loss = 0.08180
Step 49545: loss = 0.07360
Step 49550: loss = 0.07407
Step 49555: loss = 0.07077
Step 49560: loss = 0.05478
Step 49565: loss = 0.08258
Step 49570: loss = 0.09893
Step 49575: loss = 0.08345
Step 49580: loss = 0.05084
Step 49585: loss = 0.09837
Step 49590: loss = 0.07020
Step 49595: loss = 0.04035
Step 49600: loss = 0.05013
Step 49605: loss = 0.06035
Step 49610: loss = 0.07671
Step 49615: loss = 0.08157
Step 49620: loss = 0.11601
Step 49625: loss = 0.08165
Step 49630: loss = 0.04464
Step 49635: loss = 0.05630
Step 49640: loss = 0.06267
Step 49645: loss = 0.06190
Step 49650: loss = 0.04724
Step 49655: loss = 0.06735
Step 49660: loss = 0.05624
Step 49665: loss = 0.05151
Step 49670: loss = 0.04931
Step 49675: loss = 0.06253
Step 49680: loss = 0.04548
Step 49685: loss = 0.06540
Step 49690: loss = 0.05833
Step 49695: loss = 0.06922
Step 49700: loss = 0.06867
Step 49705: loss = 0.07504
Step 49710: loss = 0.05989
Step 49715: loss = 0.05066
Step 49720: loss = 0.04769
Step 49725: loss = 0.09144
Step 49730: loss = 0.07452
Step 49735: loss = 0.09472
Step 49740: loss = 0.06374
Step 49745: loss = 0.12679
Step 49750: loss = 0.04998
Step 49755: loss = 0.06046
Step 49760: loss = 0.09023
Step 49765: loss = 0.05236
Step 49770: loss = 0.05553
Step 49775: loss = 0.04513
Step 49780: loss = 0.05606
Step 49785: loss = 0.05197
Step 49790: loss = 0.09958
Step 49795: loss = 0.06122
Step 49800: loss = 0.07347
Step 49805: loss = 0.11878
Step 49810: loss = 0.07736
Step 49815: loss = 0.07218
Step 49820: loss = 0.08451
Step 49825: loss = 0.05923
Step 49830: loss = 0.05362
Step 49835: loss = 0.04650
Step 49840: loss = 0.07458
Step 49845: loss = 0.07558
Step 49850: loss = 0.06307
Step 49855: loss = 0.09307
Step 49860: loss = 0.15890
Step 49865: loss = 0.05800
Step 49870: loss = 0.07293
Step 49875: loss = 0.03665
Step 49880: loss = 0.06142
Step 49885: loss = 0.09868
Step 49890: loss = 0.04529
Step 49895: loss = 0.12612
Step 49900: loss = 0.04510
Step 49905: loss = 0.09981
Step 49910: loss = 0.06851
Step 49915: loss = 0.08540
Step 49920: loss = 0.07302
Training Data Eval:
  Num examples: 49920, Num correct: 49365, Precision @ 1: 0.9889
('Testing Data Eval: EPOCH->', 129)
  Num examples: 9984, Num correct: 7503, Precision @ 1: 0.7515
Step 49925: loss = 0.06382
Step 49930: loss = 0.11122
Step 49935: loss = 0.03951
Step 49940: loss = 0.05524
Step 49945: loss = 0.04171
Step 49950: loss = 0.04908
Step 49955: loss = 0.05426
Step 49960: loss = 0.06631
Step 49965: loss = 0.05133
Step 49970: loss = 0.05724
Step 49975: loss = 0.04033
Step 49980: loss = 0.11808
Step 49985: loss = 0.05122
Step 49990: loss = 0.05491
Step 49995: loss = 0.08210
Step 50000: loss = 0.03882
Step 50005: loss = 0.07223
Step 50010: loss = 0.04836
Step 50015: loss = 0.07581
Step 50020: loss = 0.07041
Step 50025: loss = 0.04859
Step 50030: loss = 0.07875
Step 50035: loss = 0.07086
Step 50040: loss = 0.03913
Step 50045: loss = 0.08424
Step 50050: loss = 0.04095
Step 50055: loss = 0.07937
Step 50060: loss = 0.05819
Step 50065: loss = 0.05660
Step 50070: loss = 0.04170
Step 50075: loss = 0.05968
Step 50080: loss = 0.05226
Step 50085: loss = 0.08718
Step 50090: loss = 0.10303
Step 50095: loss = 0.11745
Step 50100: loss = 0.03650
Step 50105: loss = 0.06112
Step 50110: loss = 0.10508
Step 50115: loss = 0.04481
Step 50120: loss = 0.04795
Step 50125: loss = 0.09603
Step 50130: loss = 0.10666
Step 50135: loss = 0.04014
Step 50140: loss = 0.06946
Step 50145: loss = 0.07450
Step 50150: loss = 0.10755
Step 50155: loss = 0.06930
Step 50160: loss = 0.09495
Step 50165: loss = 0.08654
Step 50170: loss = 0.04549
Step 50175: loss = 0.04349
Step 50180: loss = 0.04265
Step 50185: loss = 0.06199
Step 50190: loss = 0.05668
Step 50195: loss = 0.04119
Step 50200: loss = 0.04482
Step 50205: loss = 0.05391
Step 50210: loss = 0.10397
Step 50215: loss = 0.05553
Step 50220: loss = 0.08356
Step 50225: loss = 0.06375
Step 50230: loss = 0.07267
Step 50235: loss = 0.07156
Step 50240: loss = 0.05893
Step 50245: loss = 0.05145
Step 50250: loss = 0.04158
Step 50255: loss = 0.09962
Step 50260: loss = 0.05232
Step 50265: loss = 0.10642
Step 50270: loss = 0.03611
Step 50275: loss = 0.03520
Step 50280: loss = 0.04307
Step 50285: loss = 0.05807
Step 50290: loss = 0.06687
Step 50295: loss = 0.05442
Step 50300: loss = 0.04600
Step 50305: loss = 0.08521
Step 50310: loss = 0.05303
Training Data Eval:
  Num examples: 49920, Num correct: 49458, Precision @ 1: 0.9907
('Testing Data Eval: EPOCH->', 130)
  Num examples: 9984, Num correct: 7431, Precision @ 1: 0.7443
Step 50315: loss = 0.05802
Step 50320: loss = 0.07609
Step 50325: loss = 0.07542
Step 50330: loss = 0.06806
Step 50335: loss = 0.05332
Step 50340: loss = 0.04125
Step 50345: loss = 0.09653
Step 50350: loss = 0.07606
Step 50355: loss = 0.05483
Step 50360: loss = 0.08622
Step 50365: loss = 0.04284
Step 50370: loss = 0.04473
Step 50375: loss = 0.06071
Step 50380: loss = 0.04317
Step 50385: loss = 0.05171
Step 50390: loss = 0.04113
Step 50395: loss = 0.11226
Step 50400: loss = 0.05182
Step 50405: loss = 0.06806
Step 50410: loss = 0.03557
Step 50415: loss = 0.05173
Step 50420: loss = 0.05096
Step 50425: loss = 0.03238
Step 50430: loss = 0.03194
Step 50435: loss = 0.05904
Step 50440: loss = 0.06703
Step 50445: loss = 0.04598
Step 50450: loss = 0.06824
Step 50455: loss = 0.07341
Step 50460: loss = 0.04908
Step 50465: loss = 0.07711
Step 50470: loss = 0.05164
Step 50475: loss = 0.08408
Step 50480: loss = 0.10421
Step 50485: loss = 0.06981
Step 50490: loss = 0.08104
Step 50495: loss = 0.10071
Step 50500: loss = 0.06806
Step 50505: loss = 0.06360
Step 50510: loss = 0.04696
Step 50515: loss = 0.09369
Step 50520: loss = 0.07213
Step 50525: loss = 0.04743
Step 50530: loss = 0.09475
Step 50535: loss = 0.05284
Step 50540: loss = 0.07157
Step 50545: loss = 0.04928
Step 50550: loss = 0.06455
Step 50555: loss = 0.04505
Step 50560: loss = 0.13537
Step 50565: loss = 0.05513
Step 50570: loss = 0.03976
Step 50575: loss = 0.06979
Step 50580: loss = 0.06295
Step 50585: loss = 0.11336
Step 50590: loss = 0.07004
Step 50595: loss = 0.05771
Step 50600: loss = 0.08748
Step 50605: loss = 0.06592
Step 50610: loss = 0.06629
Step 50615: loss = 0.05901
Step 50620: loss = 0.03745
Step 50625: loss = 0.08469
Step 50630: loss = 0.04202
Step 50635: loss = 0.10746
Step 50640: loss = 0.07329
Step 50645: loss = 0.06917
Step 50650: loss = 0.06026
Step 50655: loss = 0.04779
Step 50660: loss = 0.09690
Step 50665: loss = 0.10408
Step 50670: loss = 0.04001
Step 50675: loss = 0.05321
Step 50680: loss = 0.06558
Step 50685: loss = 0.04437
Step 50690: loss = 0.05335
Step 50695: loss = 0.07603
Step 50700: loss = 0.05775
Training Data Eval:
  Num examples: 49920, Num correct: 49487, Precision @ 1: 0.9913
('Testing Data Eval: EPOCH->', 131)
  Num examples: 9984, Num correct: 7461, Precision @ 1: 0.7473
Step 50705: loss = 0.04839
Step 50710: loss = 0.05866
Step 50715: loss = 0.09325
Step 50720: loss = 0.07104
Step 50725: loss = 0.06224
Step 50730: loss = 0.05970
Step 50735: loss = 0.08525
Step 50740: loss = 0.07942
Step 50745: loss = 0.06458
Step 50750: loss = 0.09685
Step 50755: loss = 0.04833
Step 50760: loss = 0.05551
Step 50765: loss = 0.07753
Step 50770: loss = 0.04457
Step 50775: loss = 0.05057
Step 50780: loss = 0.05679
Step 50785: loss = 0.05039
Step 50790: loss = 0.05078
Step 50795: loss = 0.05801
Step 50800: loss = 0.05210
Step 50805: loss = 0.05350
Step 50810: loss = 0.03735
Step 50815: loss = 0.07834
Step 50820: loss = 0.05804
Step 50825: loss = 0.06610
Step 50830: loss = 0.05853
Step 50835: loss = 0.08598
Step 50840: loss = 0.04892
Step 50845: loss = 0.07576
Step 50850: loss = 0.07634
Step 50855: loss = 0.07882
Step 50860: loss = 0.09334
Step 50865: loss = 0.09452
Step 50870: loss = 0.05339
Step 50875: loss = 0.05229
Step 50880: loss = 0.05254
Step 50885: loss = 0.03926
Step 50890: loss = 0.03868
Step 50895: loss = 0.07913
Step 50900: loss = 0.05726
Step 50905: loss = 0.07173
Step 50910: loss = 0.05161
Step 50915: loss = 0.07214
Step 50920: loss = 0.05437
Step 50925: loss = 0.10426
Step 50930: loss = 0.10073
Step 50935: loss = 0.04311
Step 50940: loss = 0.14718
Step 50945: loss = 0.06710
Step 50950: loss = 0.04905
Step 50955: loss = 0.13211
Step 50960: loss = 0.06705
Step 50965: loss = 0.09068
Step 50970: loss = 0.05027
Step 50975: loss = 0.08097
Step 50980: loss = 0.08978
Step 50985: loss = 0.04982
Step 50990: loss = 0.05689
Step 50995: loss = 0.04120
Step 51000: loss = 0.04609
Step 51005: loss = 0.06908
Step 51010: loss = 0.04085
Step 51015: loss = 0.12808
Step 51020: loss = 0.04978
Step 51025: loss = 0.04929
Step 51030: loss = 0.04963
Step 51035: loss = 0.04363
Step 51040: loss = 0.06793
Step 51045: loss = 0.04663
Step 51050: loss = 0.04923
Step 51055: loss = 0.05111
Step 51060: loss = 0.08507
Step 51065: loss = 0.04341
Step 51070: loss = 0.07409
Step 51075: loss = 0.04689
Step 51080: loss = 0.03117
Step 51085: loss = 0.03239
Step 51090: loss = 0.23013
Training Data Eval:
  Num examples: 49920, Num correct: 49540, Precision @ 1: 0.9924
('Testing Data Eval: EPOCH->', 132)
  Num examples: 9984, Num correct: 7540, Precision @ 1: 0.7552
Step 51095: loss = 0.04919
Step 51100: loss = 0.03383
Step 51105: loss = 0.09329
Step 51110: loss = 0.06579
Step 51115: loss = 0.08607
Step 51120: loss = 0.03535
Step 51125: loss = 0.04225
Step 51130: loss = 0.05500
Step 51135: loss = 0.08282
Step 51140: loss = 0.06674
Step 51145: loss = 0.05275
Step 51150: loss = 0.07861
Step 51155: loss = 0.04432
Step 51160: loss = 0.04459
Step 51165: loss = 0.05311
Step 51170: loss = 0.07205
Step 51175: loss = 0.05148
Step 51180: loss = 0.03736
Step 51185: loss = 0.04473
Step 51190: loss = 0.07921
Step 51195: loss = 0.03543
Step 51200: loss = 0.06326
Step 51205: loss = 0.05010
Step 51210: loss = 0.08453
Step 51215: loss = 0.05949
Step 51220: loss = 0.06420
Step 51225: loss = 0.06119
Step 51230: loss = 0.07984
Step 51235: loss = 0.07611
Step 51240: loss = 0.05261
Step 51245: loss = 0.05479
Step 51250: loss = 0.04740
Step 51255: loss = 0.07698
Step 51260: loss = 0.08658
Step 51265: loss = 0.03894
Step 51270: loss = 0.06390
Step 51275: loss = 0.08319
Step 51280: loss = 0.06171
Step 51285: loss = 0.04701
Step 51290: loss = 0.06613
Step 51295: loss = 0.06259
Step 51300: loss = 0.10964
Step 51305: loss = 0.09377
Step 51310: loss = 0.03488
Step 51315: loss = 0.05616
Step 51320: loss = 0.11245
Step 51325: loss = 0.06152
Step 51330: loss = 0.08965
Step 51335: loss = 0.07251
Step 51340: loss = 0.06243
Step 51345: loss = 0.12688
Step 51350: loss = 0.05351
Step 51355: loss = 0.05154
Step 51360: loss = 0.07696
Step 51365: loss = 0.09281
Step 51370: loss = 0.04613
Step 51375: loss = 0.08677
Step 51380: loss = 0.06534
Step 51385: loss = 0.03274
Step 51390: loss = 0.05575
Step 51395: loss = 0.03958
Step 51400: loss = 0.05104
Step 51405: loss = 0.06791
Step 51410: loss = 0.03598
Step 51415: loss = 0.10312
Step 51420: loss = 0.07620
Step 51425: loss = 0.05016
Step 51430: loss = 0.06663
Step 51435: loss = 0.06796
Step 51440: loss = 0.07042
Step 51445: loss = 0.04508
Step 51450: loss = 0.05636
Step 51455: loss = 0.04191
Step 51460: loss = 0.06890
Step 51465: loss = 0.03863
Step 51470: loss = 0.04929
Step 51475: loss = 0.05536
Step 51480: loss = 0.10731
Training Data Eval:
  Num examples: 49920, Num correct: 49398, Precision @ 1: 0.9895
('Testing Data Eval: EPOCH->', 133)
  Num examples: 9984, Num correct: 7421, Precision @ 1: 0.7433
Step 51485: loss = 0.02942
Step 51490: loss = 0.05458
Step 51495: loss = 0.05754
Step 51500: loss = 0.03543
Step 51505: loss = 0.03304
Step 51510: loss = 0.10094
Step 51515: loss = 0.14210
Step 51520: loss = 0.06451
Step 51525: loss = 0.04993
Step 51530: loss = 0.03932
Step 51535: loss = 0.06609
Step 51540: loss = 0.05421
Step 51545: loss = 0.04570
Step 51550: loss = 0.08605
Step 51555: loss = 0.03928
Step 51560: loss = 0.04283
Step 51565: loss = 0.05340
Step 51570: loss = 0.07715
Step 51575: loss = 0.05746
Step 51580: loss = 0.15425
Step 51585: loss = 0.07339
Step 51590: loss = 0.05210
Step 51595: loss = 0.04145
Step 51600: loss = 0.04222
Step 51605: loss = 0.06833
Step 51610: loss = 0.06099
Step 51615: loss = 0.04820
Step 51620: loss = 0.07774
Step 51625: loss = 0.06415
Step 51630: loss = 0.05656
Step 51635: loss = 0.05923
Step 51640: loss = 0.05158
Step 51645: loss = 0.05259
Step 51650: loss = 0.07345
Step 51655: loss = 0.06884
Step 51660: loss = 0.05313
Step 51665: loss = 0.04857
Step 51670: loss = 0.05644
Step 51675: loss = 0.06324
Step 51680: loss = 0.08670
Step 51685: loss = 0.04399
Step 51690: loss = 0.04146
Step 51695: loss = 0.05148
Step 51700: loss = 0.06426
Step 51705: loss = 0.04890
Step 51710: loss = 0.05989
Step 51715: loss = 0.05348
Step 51720: loss = 0.04462
Step 51725: loss = 0.14894
Step 51730: loss = 0.07967
Step 51735: loss = 0.06486
Step 51740: loss = 0.12119
Step 51745: loss = 0.06341
Step 51750: loss = 0.05538
Step 51755: loss = 0.05281
Step 51760: loss = 0.06073
Step 51765: loss = 0.15414
Step 51770: loss = 0.04083
Step 51775: loss = 0.04943
Step 51780: loss = 0.06891
Step 51785: loss = 0.07048
Step 51790: loss = 0.05192
Step 51795: loss = 0.08399
Step 51800: loss = 0.03967
Step 51805: loss = 0.07201
Step 51810: loss = 0.05777
Step 51815: loss = 0.07308
Step 51820: loss = 0.07339
Step 51825: loss = 0.06868
Step 51830: loss = 0.03999
Step 51835: loss = 0.04725
Step 51840: loss = 0.04221
Step 51845: loss = 0.05921
Step 51850: loss = 0.08838
Step 51855: loss = 0.06375
Step 51860: loss = 0.04000
Step 51865: loss = 0.07966
Step 51870: loss = 0.08898
Training Data Eval:
  Num examples: 49920, Num correct: 49373, Precision @ 1: 0.9890
('Testing Data Eval: EPOCH->', 134)
  Num examples: 9984, Num correct: 7454, Precision @ 1: 0.7466
Step 51875: loss = 0.03944
Step 51880: loss = 0.10320
Step 51885: loss = 0.05723
Step 51890: loss = 0.07303
Step 51895: loss = 0.07627
Step 51900: loss = 0.06214
Step 51905: loss = 0.04161
Step 51910: loss = 0.06374
Step 51915: loss = 0.03044
Step 51920: loss = 0.05775
Step 51925: loss = 0.04405
Step 51930: loss = 0.04773
Step 51935: loss = 0.05783
Step 51940: loss = 0.05307
Step 51945: loss = 0.04864
Step 51950: loss = 0.04891
Step 51955: loss = 0.05139
Step 51960: loss = 0.04040
Step 51965: loss = 0.03942
Step 51970: loss = 0.06067
Step 51975: loss = 0.05711
Step 51980: loss = 0.08068
Step 51985: loss = 0.03834
Step 51990: loss = 0.05828
Step 51995: loss = 0.06027
Step 52000: loss = 0.05692
Step 52005: loss = 0.04559
Step 52010: loss = 0.04611
Step 52015: loss = 0.08056
Step 52020: loss = 0.05589
Step 52025: loss = 0.04647
Step 52030: loss = 0.05733
Step 52035: loss = 0.05207
Step 52040: loss = 0.04877
Step 52045: loss = 0.03860
Step 52050: loss = 0.04620
Step 52055: loss = 0.05091
Step 52060: loss = 0.04846
Step 52065: loss = 0.03500
Step 52070: loss = 0.03197
Step 52075: loss = 0.03994
Step 52080: loss = 0.05947
Step 52085: loss = 0.04111
Step 52090: loss = 0.05647
Step 52095: loss = 0.02896
Step 52100: loss = 0.09186
Step 52105: loss = 0.04793
Step 52110: loss = 0.06612
Step 52115: loss = 0.05838
Step 52120: loss = 0.06606
Step 52125: loss = 0.04643
Step 52130: loss = 0.05917
Step 52135: loss = 0.05745
Step 52140: loss = 0.08307
Step 52145: loss = 0.06267
Step 52150: loss = 0.05681
Step 52155: loss = 0.05930
Step 52160: loss = 0.29849
Step 52165: loss = 0.07001
Step 52170: loss = 0.05979
Step 52175: loss = 0.08633
Step 52180: loss = 0.09848
Step 52185: loss = 0.06011
Step 52190: loss = 0.06690
Step 52195: loss = 0.06473
Step 52200: loss = 0.09218
Step 52205: loss = 0.09767
Step 52210: loss = 0.07143
Step 52215: loss = 0.09363
Step 52220: loss = 0.15952
Step 52225: loss = 0.11449
Step 52230: loss = 0.09767
Step 52235: loss = 0.10443
Step 52240: loss = 0.08921
Step 52245: loss = 0.09317
Step 52250: loss = 0.13623
Step 52255: loss = 0.10710
Step 52260: loss = 0.04176
Training Data Eval:
  Num examples: 49920, Num correct: 49129, Precision @ 1: 0.9842
('Testing Data Eval: EPOCH->', 135)
  Num examples: 9984, Num correct: 7328, Precision @ 1: 0.7340
Step 52265: loss = 0.08855
Step 52270: loss = 0.07025
Step 52275: loss = 0.07236
Step 52280: loss = 0.03312
Step 52285: loss = 0.07702
Step 52290: loss = 0.05878
Step 52295: loss = 0.04612
Step 52300: loss = 0.07603
Step 52305: loss = 0.03787
Step 52310: loss = 0.07137
Step 52315: loss = 0.07087
Step 52320: loss = 0.05480
Step 52325: loss = 0.04070
Step 52330: loss = 0.08361
Step 52335: loss = 0.05547
Step 52340: loss = 0.03998
Step 52345: loss = 0.04544
Step 52350: loss = 0.06222
Step 52355: loss = 0.05871
Step 52360: loss = 0.07254
Step 52365: loss = 0.05530
Step 52370: loss = 0.05227
Step 52375: loss = 0.08265
Step 52380: loss = 0.03828
Step 52385: loss = 0.04033
Step 52390: loss = 0.07489
Step 52395: loss = 0.07425
Step 52400: loss = 0.05985
Step 52405: loss = 0.13339
Step 52410: loss = 0.06595
Step 52415: loss = 0.05712
Step 52420: loss = 0.05401
Step 52425: loss = 0.05836
Step 52430: loss = 0.08993
Step 52435: loss = 0.05644
Step 52440: loss = 0.06062
Step 52445: loss = 0.05891
Step 52450: loss = 0.09869
Step 52455: loss = 0.05305
Step 52460: loss = 0.05190
Step 52465: loss = 0.07167
Step 52470: loss = 0.09061
Step 52475: loss = 0.05980
Step 52480: loss = 0.05378
Step 52485: loss = 0.06871
Step 52490: loss = 0.07776
Step 52495: loss = 0.05329
Step 52500: loss = 0.07016
Step 52505: loss = 0.09004
Step 52510: loss = 0.04908
Step 52515: loss = 0.07924
Step 52520: loss = 0.08086
Step 52525: loss = 0.04677
Step 52530: loss = 0.04624
Step 52535: loss = 0.05227
Step 52540: loss = 0.13687
Step 52545: loss = 0.04480
Step 52550: loss = 0.05699
Step 52555: loss = 0.06378
Step 52560: loss = 0.12477
Step 52565: loss = 0.07559
Step 52570: loss = 0.05689
Step 52575: loss = 0.04915
Step 52580: loss = 0.06018
Step 52585: loss = 0.06470
Step 52590: loss = 0.04136
Step 52595: loss = 0.08432
Step 52600: loss = 0.06444
Step 52605: loss = 0.06993
Step 52610: loss = 0.08024
Step 52615: loss = 0.06765
Step 52620: loss = 0.05487
Step 52625: loss = 0.04036
Step 52630: loss = 0.04451
Step 52635: loss = 0.07823
Step 52640: loss = 0.08196
Step 52645: loss = 0.06266
Step 52650: loss = 0.13767
Training Data Eval:
  Num examples: 49920, Num correct: 49544, Precision @ 1: 0.9925
('Testing Data Eval: EPOCH->', 136)
  Num examples: 9984, Num correct: 7532, Precision @ 1: 0.7544
Step 52655: loss = 0.04378
Step 52660: loss = 0.09036
Step 52665: loss = 0.05176
Step 52670: loss = 0.03766
Step 52675: loss = 0.03830
Step 52680: loss = 0.06803
Step 52685: loss = 0.06655
Step 52690: loss = 0.05197
Step 52695: loss = 0.04532
Step 52700: loss = 0.07080
Step 52705: loss = 0.05759
Step 52710: loss = 0.04843
Step 52715: loss = 0.03886
Step 52720: loss = 0.04935
Step 52725: loss = 0.06836
Step 52730: loss = 0.06717
Step 52735: loss = 0.05643
Step 52740: loss = 0.03047
Step 52745: loss = 0.02846
Step 52750: loss = 0.04414
Step 52755: loss = 0.06217
Step 52760: loss = 0.05587
Step 52765: loss = 0.06779
Step 52770: loss = 0.05634
Step 52775: loss = 0.04962
Step 52780: loss = 0.06972
Step 52785: loss = 0.06162
Step 52790: loss = 0.05544
Step 52795: loss = 0.03955
Step 52800: loss = 0.06340
Step 52805: loss = 0.03553
Step 52810: loss = 0.05863
Step 52815: loss = 0.04179
Step 52820: loss = 0.06772
Step 52825: loss = 0.03866
Step 52830: loss = 0.03360
Step 52835: loss = 0.06584
Step 52840: loss = 0.04107
Step 52845: loss = 0.05839
Step 52850: loss = 0.03281
Step 52855: loss = 0.03451
Step 52860: loss = 0.04885
Step 52865: loss = 0.07511
Step 52870: loss = 0.06677
Step 52875: loss = 0.06184
Step 52880: loss = 0.05105
Step 52885: loss = 0.04903
Step 52890: loss = 0.06244
Step 52895: loss = 0.03986
Step 52900: loss = 0.08404
Step 52905: loss = 0.03793
Step 52910: loss = 0.08722
Step 52915: loss = 0.05189
Step 52920: loss = 0.05064
Step 52925: loss = 0.04022
Step 52930: loss = 0.13636
Step 52935: loss = 0.07161
Step 52940: loss = 0.05186
Step 52945: loss = 0.07072
Step 52950: loss = 0.06403
Step 52955: loss = 0.04364
Step 52960: loss = 0.05429
Step 52965: loss = 0.05983
Step 52970: loss = 0.08030
Step 52975: loss = 0.04980
Step 52980: loss = 0.04544
Step 52985: loss = 0.10013
Step 52990: loss = 0.04693
Step 52995: loss = 0.08185
Step 53000: loss = 0.04900
Step 53005: loss = 0.03095
Step 53010: loss = 0.05434
Step 53015: loss = 0.06276
Step 53020: loss = 0.08070
Step 53025: loss = 0.05635
Step 53030: loss = 0.05529
Step 53035: loss = 0.05514
Step 53040: loss = 0.09441
Training Data Eval:
  Num examples: 49920, Num correct: 49513, Precision @ 1: 0.9918
('Testing Data Eval: EPOCH->', 137)
  Num examples: 9984, Num correct: 7406, Precision @ 1: 0.7418
Step 53045: loss = 0.05926
Step 53050: loss = 0.07655
Step 53055: loss = 0.03368
Step 53060: loss = 0.09839
Step 53065: loss = 0.06867
Step 53070: loss = 0.05486
Step 53075: loss = 0.07210
Step 53080: loss = 0.04883
Step 53085: loss = 0.05787
Step 53090: loss = 0.04559
Step 53095: loss = 0.03541
Step 53100: loss = 0.08761
Step 53105: loss = 0.04914
Step 53110: loss = 0.08145
Step 53115: loss = 0.04494
Step 53120: loss = 0.07892
Step 53125: loss = 0.03255
Step 53130: loss = 0.07693
Step 53135: loss = 0.06228
Step 53140: loss = 0.03713
Step 53145: loss = 0.09104
Step 53150: loss = 0.05482
Step 53155: loss = 0.06402
Step 53160: loss = 0.07854
Step 53165: loss = 0.03419
Step 53170: loss = 0.03220
Step 53175: loss = 0.05279
Step 53180: loss = 0.06836
Step 53185: loss = 0.08117
Step 53190: loss = 0.03874
Step 53195: loss = 0.06451
Step 53200: loss = 0.08945
Step 53205: loss = 0.05586
Step 53210: loss = 0.03835
Step 53215: loss = 0.04053
Step 53220: loss = 0.04983
Step 53225: loss = 0.05182
Step 53230: loss = 0.08152
Step 53235: loss = 0.03931
Step 53240: loss = 0.04757
Step 53245: loss = 0.03506
Step 53250: loss = 0.06702
Step 53255: loss = 0.04538
Step 53260: loss = 0.03617
Step 53265: loss = 0.06624
Step 53270: loss = 0.08000
Step 53275: loss = 0.06285
Step 53280: loss = 0.07667
Step 53285: loss = 0.03535
Step 53290: loss = 0.08681
Step 53295: loss = 0.03633
Step 53300: loss = 0.04096
Step 53305: loss = 0.04344
Step 53310: loss = 0.04615
Step 53315: loss = 0.04898
Step 53320: loss = 0.06747
Step 53325: loss = 0.06154
Step 53330: loss = 0.04171
Step 53335: loss = 0.04869
Step 53340: loss = 0.07077
Step 53345: loss = 0.06113
Step 53350: loss = 0.04985
Step 53355: loss = 0.08340
Step 53360: loss = 0.07889
Step 53365: loss = 0.03201
Step 53370: loss = 0.04094
Step 53375: loss = 0.04404
Step 53380: loss = 0.06300
Step 53385: loss = 0.05794
Step 53390: loss = 0.03873
Step 53395: loss = 0.07782
Step 53400: loss = 0.09837
Step 53405: loss = 0.05334
Step 53410: loss = 0.06590
Step 53415: loss = 0.13156
Step 53420: loss = 0.05979
Step 53425: loss = 0.04806
Step 53430: loss = 0.09426
Training Data Eval:
  Num examples: 49920, Num correct: 49463, Precision @ 1: 0.9908
('Testing Data Eval: EPOCH->', 138)
  Num examples: 9984, Num correct: 7494, Precision @ 1: 0.7506
Step 53435: loss = 0.09602
Step 53440: loss = 0.03939
Step 53445: loss = 0.05090
Step 53450: loss = 0.05367
Step 53455: loss = 0.05686
Step 53460: loss = 0.08875
Step 53465: loss = 0.03812
Step 53470: loss = 0.04550
Step 53475: loss = 0.06127
Step 53480: loss = 0.04622
Step 53485: loss = 0.03700
Step 53490: loss = 0.05145
Step 53495: loss = 0.03729
Step 53500: loss = 0.07587
Step 53505: loss = 0.08194
Step 53510: loss = 0.04923
Step 53515: loss = 0.05103
Step 53520: loss = 0.04308
Step 53525: loss = 0.05205
Step 53530: loss = 0.03727
Step 53535: loss = 0.09566
Step 53540: loss = 0.04215
Step 53545: loss = 0.06211
Step 53550: loss = 0.05106
Step 53555: loss = 0.06427
Step 53560: loss = 0.04224
Step 53565: loss = 0.03649
Step 53570: loss = 0.05105
Step 53575: loss = 0.03453
Step 53580: loss = 0.05701
Step 53585: loss = 0.05195
Step 53590: loss = 0.09683
Step 53595: loss = 0.03448
Step 53600: loss = 0.05960
Step 53605: loss = 0.10698
Step 53610: loss = 0.04614
Step 53615: loss = 0.05195
Step 53620: loss = 0.10018
Step 53625: loss = 0.04622
Step 53630: loss = 0.06740
Step 53635: loss = 0.03978
Step 53640: loss = 0.13204
Step 53645: loss = 0.03491
Step 53650: loss = 0.03108
Step 53655: loss = 0.06619
Step 53660: loss = 0.09905
Step 53665: loss = 0.08752
Step 53670: loss = 0.05140
Step 53675: loss = 0.04798
Step 53680: loss = 0.08375
Step 53685: loss = 0.08633
Step 53690: loss = 0.05989
Step 53695: loss = 0.04164
Step 53700: loss = 0.04119
Step 53705: loss = 0.04655
Step 53710: loss = 0.09710
Step 53715: loss = 0.04700
Step 53720: loss = 0.06686
Step 53725: loss = 0.09400
Step 53730: loss = 0.06640
Step 53735: loss = 0.03914
Step 53740: loss = 0.04205
Step 53745: loss = 0.07549
Step 53750: loss = 0.04529
Step 53755: loss = 0.06634
Step 53760: loss = 0.04656
Step 53765: loss = 0.06663
Step 53770: loss = 0.03858
Step 53775: loss = 0.05367
Step 53780: loss = 0.10210
Step 53785: loss = 0.04455
Step 53790: loss = 0.04713
Step 53795: loss = 0.05802
Step 53800: loss = 0.05673
Step 53805: loss = 0.04200
Step 53810: loss = 0.08160
Step 53815: loss = 0.07141
Step 53820: loss = 0.06525
Training Data Eval:
  Num examples: 49920, Num correct: 49480, Precision @ 1: 0.9912
('Testing Data Eval: EPOCH->', 139)
  Num examples: 9984, Num correct: 7506, Precision @ 1: 0.7518
Step 53825: loss = 0.08119
Step 53830: loss = 0.04788
Step 53835: loss = 0.08355
Step 53840: loss = 0.06365
Step 53845: loss = 0.05777
Step 53850: loss = 0.10369
Step 53855: loss = 0.05869
Step 53860: loss = 0.04913
Step 53865: loss = 0.03749
Step 53870: loss = 0.04854
Step 53875: loss = 0.05583
Step 53880: loss = 0.05142
Step 53885: loss = 0.05285
Step 53890: loss = 0.07921
Step 53895: loss = 0.08498
Step 53900: loss = 0.04897
Step 53905: loss = 0.04317
Step 53910: loss = 0.10463
Step 53915: loss = 0.03251
Step 53920: loss = 0.05953
Step 53925: loss = 0.04607
Step 53930: loss = 0.04803
Step 53935: loss = 0.04676
Step 53940: loss = 0.07292
Step 53945: loss = 0.08693
Step 53950: loss = 0.06043
Step 53955: loss = 0.03774
Step 53960: loss = 0.03686
Step 53965: loss = 0.02937
Step 53970: loss = 0.03218
Step 53975: loss = 0.05526
Step 53980: loss = 0.07040
Step 53985: loss = 0.06866
Step 53990: loss = 0.05403
Step 53995: loss = 0.09472
Step 54000: loss = 0.08900
Step 54005: loss = 0.06059
Step 54010: loss = 0.05599
Step 54015: loss = 0.04903
Step 54020: loss = 0.12476
Step 54025: loss = 0.03952
Step 54030: loss = 0.06047
Step 54035: loss = 0.05125
Step 54040: loss = 0.04342
Step 54045: loss = 0.04922
Step 54050: loss = 0.11745
Step 54055: loss = 0.06316
Step 54060: loss = 0.04617
Step 54065: loss = 0.06279
Step 54070: loss = 0.06820
Step 54075: loss = 0.06433
Step 54080: loss = 0.09060
Step 54085: loss = 0.05686
Step 54090: loss = 0.04163
Step 54095: loss = 0.08281
Step 54100: loss = 0.08578
Step 54105: loss = 0.11412
Step 54110: loss = 0.04413
Step 54115: loss = 0.04246
Step 54120: loss = 0.04267
Step 54125: loss = 0.03787
Step 54130: loss = 0.04225
Step 54135: loss = 0.06021
Step 54140: loss = 0.06061
Step 54145: loss = 0.04247
Step 54150: loss = 0.05670
Step 54155: loss = 0.05180
Step 54160: loss = 0.05746
Step 54165: loss = 0.05476
Step 54170: loss = 0.06504
Step 54175: loss = 0.04776
Step 54180: loss = 0.07046
Step 54185: loss = 0.05257
Step 54190: loss = 0.05587
Step 54195: loss = 0.05280
Step 54200: loss = 0.03395
Step 54205: loss = 0.14388
Step 54210: loss = 0.05893
Training Data Eval:
  Num examples: 49920, Num correct: 49524, Precision @ 1: 0.9921
('Testing Data Eval: EPOCH->', 140)
  Num examples: 9984, Num correct: 7394, Precision @ 1: 0.7406
Step 54215: loss = 0.06149
Step 54220: loss = 0.04872
Step 54225: loss = 0.07940
Step 54230: loss = 0.11106
Step 54235: loss = 0.03820
Step 54240: loss = 0.05043
Step 54245: loss = 0.05852
Step 54250: loss = 0.05064
Step 54255: loss = 0.11629
Step 54260: loss = 0.07857
Step 54265: loss = 0.05456
Step 54270: loss = 0.07150
Step 54275: loss = 0.06968
Step 54280: loss = 0.04774
Step 54285: loss = 0.04166
Step 54290: loss = 0.06564
Step 54295: loss = 0.03605
Step 54300: loss = 0.05997
Step 54305: loss = 0.06045
Step 54310: loss = 0.04465
Step 54315: loss = 0.05436
Step 54320: loss = 0.06019
Step 54325: loss = 0.06485
Step 54330: loss = 0.08713
Step 54335: loss = 0.03614
Step 54340: loss = 0.06587
Step 54345: loss = 0.10160
Step 54350: loss = 0.06564
Step 54355: loss = 0.09033
Step 54360: loss = 0.05879
Step 54365: loss = 0.03711
Step 54370: loss = 0.04561
Step 54375: loss = 0.10258
Step 54380: loss = 0.04236
Step 54385: loss = 0.05152
Step 54390: loss = 0.06507
Step 54395: loss = 0.10151
Step 54400: loss = 0.06773
Step 54405: loss = 0.08747
Step 54410: loss = 0.04145
Step 54415: loss = 0.06356
Step 54420: loss = 0.07926
Step 54425: loss = 0.05313
Step 54430: loss = 0.09329
Step 54435: loss = 0.07616
Step 54440: loss = 0.06793
Step 54445: loss = 0.06169
Step 54450: loss = 0.06717
Step 54455: loss = 0.09166
Step 54460: loss = 0.04799
Step 54465: loss = 0.05974
Step 54470: loss = 0.03360
Step 54475: loss = 0.08265
Step 54480: loss = 0.05017
Step 54485: loss = 0.08934
Step 54490: loss = 0.10852
Step 54495: loss = 0.03684
Step 54500: loss = 0.06727
Step 54505: loss = 0.08721
Step 54510: loss = 0.07633
Step 54515: loss = 0.08042
Step 54520: loss = 0.04189
Step 54525: loss = 0.06880
Step 54530: loss = 0.03457
Step 54535: loss = 0.06570
Step 54540: loss = 0.04037
Step 54545: loss = 0.06752
Step 54550: loss = 0.04887
Step 54555: loss = 0.07709
Step 54560: loss = 0.10453
Step 54565: loss = 0.07700
Step 54570: loss = 0.06290
Step 54575: loss = 0.10117
Step 54580: loss = 0.06829
Step 54585: loss = 0.04699
Step 54590: loss = 0.03654
Step 54595: loss = 0.11610
Step 54600: loss = 0.09030
Training Data Eval:
  Num examples: 49920, Num correct: 49551, Precision @ 1: 0.9926
('Testing Data Eval: EPOCH->', 141)
  Num examples: 9984, Num correct: 7598, Precision @ 1: 0.7610
Step 54605: loss = 0.05451
Step 54610: loss = 0.04186
Step 54615: loss = 0.04321
Step 54620: loss = 0.04957
Step 54625: loss = 0.05902
Step 54630: loss = 0.06176
Step 54635: loss = 0.06384
Step 54640: loss = 0.10325
Step 54645: loss = 0.04886
Step 54650: loss = 0.05634
Step 54655: loss = 0.04205
Step 54660: loss = 0.04313
Step 54665: loss = 0.06027
Step 54670: loss = 0.07559
Step 54675: loss = 0.04342
Step 54680: loss = 0.05426
Step 54685: loss = 0.04153
Step 54690: loss = 0.05848
Step 54695: loss = 0.10531
Step 54700: loss = 0.08681
Step 54705: loss = 0.04536
Step 54710: loss = 0.08421
Step 54715: loss = 0.03488
Step 54720: loss = 0.06436
Step 54725: loss = 0.08038
Step 54730: loss = 0.08056
Step 54735: loss = 0.05470
Step 54740: loss = 0.09459
Step 54745: loss = 0.05982
Step 54750: loss = 0.05761
Step 54755: loss = 0.03978
Step 54760: loss = 0.05670
Step 54765: loss = 0.06557
Step 54770: loss = 0.05799
Step 54775: loss = 0.06937
Step 54780: loss = 0.07963
Step 54785: loss = 0.07680
Step 54790: loss = 0.03329
Step 54795: loss = 0.04534
Step 54800: loss = 0.08190
Step 54805: loss = 0.05626
Step 54810: loss = 0.05939
Step 54815: loss = 0.05894
Step 54820: loss = 0.05515
Step 54825: loss = 0.03799
Step 54830: loss = 0.03483
Step 54835: loss = 0.06201
Step 54840: loss = 0.05031
Step 54845: loss = 0.06819
Step 54850: loss = 0.06288
Step 54855: loss = 0.04323
Step 54860: loss = 0.07770
Step 54865: loss = 0.03497
Step 54870: loss = 0.06144
Step 54875: loss = 0.04205
Step 54880: loss = 0.04611
Step 54885: loss = 0.04500
Step 54890: loss = 0.07249
Step 54895: loss = 0.03869
Step 54900: loss = 0.09365
Step 54905: loss = 0.04262
Step 54910: loss = 0.06966
Step 54915: loss = 0.04186
Step 54920: loss = 0.05260
Step 54925: loss = 0.04356
Step 54930: loss = 0.03909
Step 54935: loss = 0.03128
Step 54940: loss = 0.05484
Step 54945: loss = 0.04448
Step 54950: loss = 0.05254
Step 54955: loss = 0.10338
Step 54960: loss = 0.07758
Step 54965: loss = 0.04188
Step 54970: loss = 0.04997
Step 54975: loss = 0.06060
Step 54980: loss = 0.05004
Step 54985: loss = 0.04643
Step 54990: loss = 0.04736
Training Data Eval:
  Num examples: 49920, Num correct: 49628, Precision @ 1: 0.9942
('Testing Data Eval: EPOCH->', 142)
  Num examples: 9984, Num correct: 7518, Precision @ 1: 0.7530
Step 54995: loss = 0.04749
Step 55000: loss = 0.05177
Step 55005: loss = 0.03083
Step 55010: loss = 0.07765
Step 55015: loss = 0.04226
Step 55020: loss = 0.04385
Step 55025: loss = 0.06919
Step 55030: loss = 0.05577
Step 55035: loss = 0.10225
Step 55040: loss = 0.14050
Step 55045: loss = 0.09162
Step 55050: loss = 0.08077
Step 55055: loss = 0.12933
Step 55060: loss = 0.03379
Step 55065: loss = 0.06321
Step 55070: loss = 0.05001
Step 55075: loss = 0.08547
Step 55080: loss = 0.05594
Step 55085: loss = 0.05155
Step 55090: loss = 0.09411
Step 55095: loss = 0.07828
Step 55100: loss = 0.07782
Step 55105: loss = 0.08227
Step 55110: loss = 0.04763
Step 55115: loss = 0.08638
Step 55120: loss = 0.04103
Step 55125: loss = 0.03942
Step 55130: loss = 0.08267
Step 55135: loss = 0.04618
Step 55140: loss = 0.06066
Step 55145: loss = 0.08496
Step 55150: loss = 0.05239
Step 55155: loss = 0.05682
Step 55160: loss = 0.07057
Step 55165: loss = 0.04513
Step 55170: loss = 0.05440
Step 55175: loss = 0.06674
Step 55180: loss = 0.14531
Step 55185: loss = 0.05677
Step 55190: loss = 0.04841
Step 55195: loss = 0.09239
Step 55200: loss = 0.05776
Step 55205: loss = 0.03749
Step 55210: loss = 0.05480
Step 55215: loss = 0.06185
Step 55220: loss = 0.06640
Step 55225: loss = 0.04390
Step 55230: loss = 0.08793
Step 55235: loss = 0.05552
Step 55240: loss = 0.07478
Step 55245: loss = 0.05271
Step 55250: loss = 0.04940
Step 55255: loss = 0.04521
Step 55260: loss = 0.08751
Step 55265: loss = 0.03165
Step 55270: loss = 0.09935
Step 55275: loss = 0.03970
Step 55280: loss = 0.06138
Step 55285: loss = 0.06107
Step 55290: loss = 0.05937
Step 55295: loss = 0.03842
Step 55300: loss = 0.04045
Step 55305: loss = 0.04190
Step 55310: loss = 0.04765
Step 55315: loss = 0.05342
Step 55320: loss = 0.03938
Step 55325: loss = 0.05333
Step 55330: loss = 0.10926
Step 55335: loss = 0.05124
Step 55340: loss = 0.05866
Step 55345: loss = 0.03536
Step 55350: loss = 0.07010
Step 55355: loss = 0.04837
Step 55360: loss = 0.04274
Step 55365: loss = 0.04756
Step 55370: loss = 0.04322
Step 55375: loss = 0.04634
Step 55380: loss = 0.03668
Training Data Eval:
  Num examples: 49920, Num correct: 49560, Precision @ 1: 0.9928
('Testing Data Eval: EPOCH->', 143)
  Num examples: 9984, Num correct: 7450, Precision @ 1: 0.7462
Step 55385: loss = 0.04491
Step 55390: loss = 0.10752
Step 55395: loss = 0.06518
Step 55400: loss = 0.06402
Step 55405: loss = 0.06974
Step 55410: loss = 0.05088
Step 55415: loss = 0.05877
Step 55420: loss = 0.06242
Step 55425: loss = 0.04698
Step 55430: loss = 0.06678
Step 55435: loss = 0.06177
Step 55440: loss = 0.04906
Step 55445: loss = 0.03167
Step 55450: loss = 0.05510
Step 55455: loss = 0.05399
Step 55460: loss = 0.05577
Step 55465: loss = 0.04865
Step 55470: loss = 0.09196
Step 55475: loss = 0.05150
Step 55480: loss = 0.05032
Step 55485: loss = 0.08188
Step 55490: loss = 0.04397
Step 55495: loss = 0.03238
Step 55500: loss = 0.06406
Step 55505: loss = 0.10682
Step 55510: loss = 0.08964
Step 55515: loss = 0.06754
Step 55520: loss = 0.06104
Step 55525: loss = 0.06568
Step 55530: loss = 0.09240
Step 55535: loss = 0.06974
Step 55540: loss = 0.04377
Step 55545: loss = 0.04341
Step 55550: loss = 0.10225
Step 55555: loss = 0.06155
Step 55560: loss = 0.03798
Step 55565: loss = 0.03712
Step 55570: loss = 0.06489
Step 55575: loss = 0.04234
Step 55580: loss = 0.06404
Step 55585: loss = 0.05177
Step 55590: loss = 0.05896
Step 55595: loss = 0.05945
Step 55600: loss = 0.05208
Step 55605: loss = 0.03963
Step 55610: loss = 0.03973
Step 55615: loss = 0.09244
Step 55620: loss = 0.06420
Step 55625: loss = 0.03920
Step 55630: loss = 0.05056
Step 55635: loss = 0.05375
Step 55640: loss = 0.03322
Step 55645: loss = 0.05235
Step 55650: loss = 0.08179
Step 55655: loss = 0.05676
Step 55660: loss = 0.07328
Step 55665: loss = 0.05333
Step 55670: loss = 0.06852
Step 55675: loss = 0.04597
Step 55680: loss = 0.08198
Step 55685: loss = 0.07417
Step 55690: loss = 0.06052
Step 55695: loss = 0.05562
Step 55700: loss = 0.03675
Step 55705: loss = 0.04011
Step 55710: loss = 0.06968
Step 55715: loss = 0.05919
Step 55720: loss = 0.04929
Step 55725: loss = 0.03824
Step 55730: loss = 0.03675
Step 55735: loss = 0.03878
Step 55740: loss = 0.04653
Step 55745: loss = 0.03743
Step 55750: loss = 0.08024
Step 55755: loss = 0.03492
Step 55760: loss = 0.06366
Step 55765: loss = 0.06298
Step 55770: loss = 0.06188
Training Data Eval:
  Num examples: 49920, Num correct: 49547, Precision @ 1: 0.9925
('Testing Data Eval: EPOCH->', 144)
  Num examples: 9984, Num correct: 7498, Precision @ 1: 0.7510
Step 55775: loss = 0.07184
Step 55780: loss = 0.07968
Step 55785: loss = 0.04698
Step 55790: loss = 0.08095
Step 55795: loss = 0.07631
Step 55800: loss = 0.03867
Step 55805: loss = 0.08010
Step 55810: loss = 0.03428
Step 55815: loss = 0.07983
Step 55820: loss = 0.04942
Step 55825: loss = 0.06792
Step 55830: loss = 0.03984
Step 55835: loss = 0.05301
Step 55840: loss = 0.06278
Step 55845: loss = 0.04099
Step 55850: loss = 0.05306
Step 55855: loss = 0.05244
Step 55860: loss = 0.04498
Step 55865: loss = 0.04420
Step 55870: loss = 0.05359
Step 55875: loss = 0.03766
Step 55880: loss = 0.05286
Step 55885: loss = 0.05931
Step 55890: loss = 0.06433
Step 55895: loss = 0.08394
Step 55900: loss = 0.03420
Step 55905: loss = 0.04562
Step 55910: loss = 0.03939
Step 55915: loss = 0.03508
Step 55920: loss = 0.07325
Step 55925: loss = 0.04099
Step 55930: loss = 0.04063
Step 55935: loss = 0.07933
Step 55940: loss = 0.04870
Step 55945: loss = 0.05499
Step 55950: loss = 0.04377
Step 55955: loss = 0.03379
Step 55960: loss = 0.05532
Step 55965: loss = 0.05456
Step 55970: loss = 0.04728
Step 55975: loss = 0.04061
Step 55980: loss = 0.04091
Step 55985: loss = 0.05483
Step 55990: loss = 0.06999
Step 55995: loss = 0.04580
Step 56000: loss = 0.09174
Step 56005: loss = 0.10645
Step 56010: loss = 0.05769
Step 56015: loss = 0.06079
Step 56020: loss = 0.12305
Step 56025: loss = 0.03797
Step 56030: loss = 0.05786
Step 56035: loss = 0.04172
Step 56040: loss = 0.04033
Step 56045: loss = 0.08125
Step 56050: loss = 0.11336
Step 56055: loss = 0.04349
Step 56060: loss = 0.05014
Step 56065: loss = 0.11400
Step 56070: loss = 0.03335
Step 56075: loss = 0.05409
Step 56080: loss = 0.08054
Step 56085: loss = 0.06560
Step 56090: loss = 0.06394
Step 56095: loss = 0.06929
Step 56100: loss = 0.06829
Step 56105: loss = 0.09163
Step 56110: loss = 0.03631
Step 56115: loss = 0.05603
Step 56120: loss = 0.06062
Step 56125: loss = 0.05325
Step 56130: loss = 0.03557
Step 56135: loss = 0.07886
Step 56140: loss = 0.08198
Step 56145: loss = 0.07727
Step 56150: loss = 0.05850
Step 56155: loss = 0.07652
Step 56160: loss = 0.04412
Training Data Eval:
  Num examples: 49920, Num correct: 49558, Precision @ 1: 0.9927
('Testing Data Eval: EPOCH->', 145)
  Num examples: 9984, Num correct: 7446, Precision @ 1: 0.7458
Step 56165: loss = 0.06088
Step 56170: loss = 0.03718
Step 56175: loss = 0.05436
Step 56180: loss = 0.06286
Step 56185: loss = 0.06384
Step 56190: loss = 0.05908
Step 56195: loss = 0.05219
Step 56200: loss = 0.03959
Step 56205: loss = 0.04488
Step 56210: loss = 0.03969
Step 56215: loss = 0.05912
Step 56220: loss = 0.06546
Step 56225: loss = 0.06261
Step 56230: loss = 0.07795
Step 56235: loss = 0.06679
Step 56240: loss = 0.04592
Step 56245: loss = 0.06532
Step 56250: loss = 0.06202
Step 56255: loss = 0.04133
Step 56260: loss = 0.05471
Step 56265: loss = 0.03985
Step 56270: loss = 0.04124
Step 56275: loss = 0.05022
Step 56280: loss = 0.04416
Step 56285: loss = 0.06318
Step 56290: loss = 0.03421
Step 56295: loss = 0.05512
Step 56300: loss = 0.05183
Step 56305: loss = 0.05750
Step 56310: loss = 0.03782
Step 56315: loss = 0.04820
Step 56320: loss = 0.04953
Step 56325: loss = 0.03694
Step 56330: loss = 0.04045
Step 56335: loss = 0.04343
Step 56340: loss = 0.04637
Step 56345: loss = 0.06470
Step 56350: loss = 0.05813
Step 56355: loss = 0.07951
Step 56360: loss = 0.03874
Step 56365: loss = 0.07109
Step 56370: loss = 0.10164
Step 56375: loss = 0.07648
Step 56380: loss = 0.16427
Step 56385: loss = 0.08724
Step 56390: loss = 0.06925
Step 56395: loss = 0.05797
Step 56400: loss = 0.06333
Step 56405: loss = 0.08696
Step 56410: loss = 0.04233
Step 56415: loss = 0.07648
Step 56420: loss = 0.11247
Step 56425: loss = 0.05261
Step 56430: loss = 0.06511
Step 56435: loss = 0.08256
Step 56440: loss = 0.05726
Step 56445: loss = 0.06598
Step 56450: loss = 0.03696
Step 56455: loss = 0.04704
Step 56460: loss = 0.04031
Step 56465: loss = 0.07364
Step 56470: loss = 0.07455
Step 56475: loss = 0.08207
Step 56480: loss = 0.07216
Step 56485: loss = 0.04432
Step 56490: loss = 0.04631
Step 56495: loss = 0.07462
Step 56500: loss = 0.05161
Step 56505: loss = 0.03770
Step 56510: loss = 0.07073
Step 56515: loss = 0.08679
Step 56520: loss = 0.12225
Step 56525: loss = 0.04054
Step 56530: loss = 0.05321
Step 56535: loss = 0.06949
Step 56540: loss = 0.05339
Step 56545: loss = 0.12728
Step 56550: loss = 0.06434
Training Data Eval:
  Num examples: 49920, Num correct: 49491, Precision @ 1: 0.9914
('Testing Data Eval: EPOCH->', 146)
  Num examples: 9984, Num correct: 7390, Precision @ 1: 0.7402
Step 56555: loss = 0.03881
Step 56560: loss = 0.07778
Step 56565: loss = 0.06308
Step 56570: loss = 0.06973
Step 56575: loss = 0.06399
Step 56580: loss = 0.04152
Step 56585: loss = 0.06739
Step 56590: loss = 0.04183
Step 56595: loss = 0.05717
Step 56600: loss = 0.02951
Step 56605: loss = 0.04350
Step 56610: loss = 0.05648
Step 56615: loss = 0.07957
Step 56620: loss = 0.12212
Step 56625: loss = 0.08653
Step 56630: loss = 0.04915
Step 56635: loss = 0.07880
Step 56640: loss = 0.04770
Step 56645: loss = 0.03558
Step 56650: loss = 0.03758
Step 56655: loss = 0.05747
Step 56660: loss = 0.06628
Step 56665: loss = 0.04222
Step 56670: loss = 0.04972
Step 56675: loss = 0.07711
Step 56680: loss = 0.06794
Step 56685: loss = 0.12020
Step 56690: loss = 0.06913
Step 56695: loss = 0.04831
Step 56700: loss = 0.04884
Step 56705: loss = 0.06244
Step 56710: loss = 0.05000
Step 56715: loss = 0.06460
Step 56720: loss = 0.07713
Step 56725: loss = 0.08056
Step 56730: loss = 0.07220
Step 56735: loss = 0.07167
Step 56740: loss = 0.04743
Step 56745: loss = 0.04719
Step 56750: loss = 0.05121
Step 56755: loss = 0.04363
Step 56760: loss = 0.08932
Step 56765: loss = 0.04325
Step 56770: loss = 0.06547
Step 56775: loss = 0.05459
Step 56780: loss = 0.06083
Step 56785: loss = 0.06579
Step 56790: loss = 0.03626
Step 56795: loss = 0.05004
Step 56800: loss = 0.04420
Step 56805: loss = 0.02531
Step 56810: loss = 0.08036
Step 56815: loss = 0.04213
Step 56820: loss = 0.06027
Step 56825: loss = 0.06427
Step 56830: loss = 0.04414
Step 56835: loss = 0.06366
Step 56840: loss = 0.08648
Step 56845: loss = 0.06532
Step 56850: loss = 0.15136
Step 56855: loss = 0.03947
Step 56860: loss = 0.07840
Step 56865: loss = 0.04911
Step 56870: loss = 0.03499
Step 56875: loss = 0.05092
Step 56880: loss = 0.03199
Step 56885: loss = 0.08831
Step 56890: loss = 0.06061
Step 56895: loss = 0.07859
Step 56900: loss = 0.05130
Step 56905: loss = 0.05480
Step 56910: loss = 0.05053
Step 56915: loss = 0.05961
Step 56920: loss = 0.08583
Step 56925: loss = 0.04789
Step 56930: loss = 0.04532
Step 56935: loss = 0.05933
Step 56940: loss = 0.04745
Training Data Eval:
  Num examples: 49920, Num correct: 49554, Precision @ 1: 0.9927
('Testing Data Eval: EPOCH->', 147)
  Num examples: 9984, Num correct: 7483, Precision @ 1: 0.7495
Step 56945: loss = 0.05858
Step 56950: loss = 0.09018
Step 56955: loss = 0.05026
Step 56960: loss = 0.07676
Step 56965: loss = 0.04076
Step 56970: loss = 0.05763
Step 56975: loss = 0.04309
Step 56980: loss = 0.09582
Step 56985: loss = 0.04083
Step 56990: loss = 0.08071
Step 56995: loss = 0.11467
Step 57000: loss = 0.07205
Step 57005: loss = 0.08247
Step 57010: loss = 0.12082
Step 57015: loss = 0.05366
Step 57020: loss = 0.03589
Step 57025: loss = 0.05971
Step 57030: loss = 0.06367
Step 57035: loss = 0.05165
Step 57040: loss = 0.03271
Step 57045: loss = 0.03293
Step 57050: loss = 0.09576
Step 57055: loss = 0.07255
Step 57060: loss = 0.04513
Step 57065: loss = 0.04299
Step 57070: loss = 0.05649
Step 57075: loss = 0.05243
Step 57080: loss = 0.03285
Step 57085: loss = 0.07420
Step 57090: loss = 0.06362
Step 57095: loss = 0.06336
Step 57100: loss = 0.05186
Step 57105: loss = 0.04186
Step 57110: loss = 0.05491
Step 57115: loss = 0.03841
Step 57120: loss = 0.05113
Step 57125: loss = 0.06284
Step 57130: loss = 0.04245
Step 57135: loss = 0.04622
Step 57140: loss = 0.05452
Step 57145: loss = 0.03143
Step 57150: loss = 0.06340
Step 57155: loss = 0.04470
Step 57160: loss = 0.06487
Step 57165: loss = 0.04339
Step 57170: loss = 0.06686
Step 57175: loss = 0.04125
Step 57180: loss = 0.07183
Step 57185: loss = 0.03389
Step 57190: loss = 0.04383
Step 57195: loss = 0.14380
Step 57200: loss = 0.05478
Step 57205: loss = 0.03410
Step 57210: loss = 0.04768
Step 57215: loss = 0.05446
Step 57220: loss = 0.05632
Step 57225: loss = 0.04887
Step 57230: loss = 0.04122
Step 57235: loss = 0.06864
Step 57240: loss = 0.05213
Step 57245: loss = 0.05272
Step 57250: loss = 0.04656
Step 57255: loss = 0.08232
Step 57260: loss = 0.08603
Step 57265: loss = 0.03857
Step 57270: loss = 0.05104
Step 57275: loss = 0.05574
Step 57280: loss = 0.06316
Step 57285: loss = 0.04790
Step 57290: loss = 0.07410
Step 57295: loss = 0.04790
Step 57300: loss = 0.04330
Step 57305: loss = 0.06316
Step 57310: loss = 0.04546
Step 57315: loss = 0.06125
Step 57320: loss = 0.05266
Step 57325: loss = 0.04490
Step 57330: loss = 0.05489
Training Data Eval:
  Num examples: 49920, Num correct: 49478, Precision @ 1: 0.9911
('Testing Data Eval: EPOCH->', 148)
  Num examples: 9984, Num correct: 7419, Precision @ 1: 0.7431
Step 57335: loss = 0.05959
Step 57340: loss = 0.04396
Step 57345: loss = 0.05080
Step 57350: loss = 0.06046
Step 57355: loss = 0.03724
Step 57360: loss = 0.05923
Step 57365: loss = 0.06958
Step 57370: loss = 0.03405
Step 57375: loss = 0.06740
Step 57380: loss = 0.04974
Step 57385: loss = 0.05294
Step 57390: loss = 0.06722
Step 57395: loss = 0.04574
Step 57400: loss = 0.08380
Step 57405: loss = 0.05808
Step 57410: loss = 0.06533
Step 57415: loss = 0.05093
Step 57420: loss = 0.05028
Step 57425: loss = 0.03370
Step 57430: loss = 0.05473
Step 57435: loss = 0.06452
Step 57440: loss = 0.03312
Step 57445: loss = 0.06805
Step 57450: loss = 0.06939
Step 57455: loss = 0.06762
Step 57460: loss = 0.05285
Step 57465: loss = 0.08466
Step 57470: loss = 0.07153
Step 57475: loss = 0.05336
Step 57480: loss = 0.06338
Step 57485: loss = 0.12318
Step 57490: loss = 0.08636
Step 57495: loss = 0.04975
Step 57500: loss = 0.03528
Step 57505: loss = 0.04327
Step 57510: loss = 0.04677
Step 57515: loss = 0.06951
Step 57520: loss = 0.04401
Step 57525: loss = 0.07813
Step 57530: loss = 0.05179
Step 57535: loss = 0.03878
Step 57540: loss = 0.04857
Step 57545: loss = 0.05487
Step 57550: loss = 0.04379
Step 57555: loss = 0.03889
Step 57560: loss = 0.08350
Step 57565: loss = 0.03894
Step 57570: loss = 0.03902
Step 57575: loss = 0.05204
Step 57580: loss = 0.05082
Step 57585: loss = 0.08683
Step 57590: loss = 0.05251
Step 57595: loss = 0.04711
Step 57600: loss = 0.04698
Step 57605: loss = 0.06428
Step 57610: loss = 0.06253
Step 57615: loss = 0.03524
Step 57620: loss = 0.04851
Step 57625: loss = 0.07628
Step 57630: loss = 0.04095
Step 57635: loss = 0.03433
Step 57640: loss = 0.06270
Step 57645: loss = 0.04127
Step 57650: loss = 0.04476
Step 57655: loss = 0.06517
Step 57660: loss = 0.03512
Step 57665: loss = 0.05416
Step 57670: loss = 0.10127
Step 57675: loss = 0.05535
Step 57680: loss = 0.03799
Step 57685: loss = 0.05463
Step 57690: loss = 0.10204
Step 57695: loss = 0.04739
Step 57700: loss = 0.05048
Step 57705: loss = 0.09309
Step 57710: loss = 0.04257
Step 57715: loss = 0.05839
Step 57720: loss = 0.06926
Training Data Eval:
  Num examples: 49920, Num correct: 49583, Precision @ 1: 0.9932
('Testing Data Eval: EPOCH->', 149)
  Num examples: 9984, Num correct: 7503, Precision @ 1: 0.7515
Step 57725: loss = 0.03684
Step 57730: loss = 0.03582
Step 57735: loss = 0.04242
Step 57740: loss = 0.04484
Step 57745: loss = 0.05312
Step 57750: loss = 0.04469
Step 57755: loss = 0.08551
Step 57760: loss = 0.05294
Step 57765: loss = 0.03585
Step 57770: loss = 0.05160
Step 57775: loss = 0.05830
Step 57780: loss = 0.07256
Step 57785: loss = 0.07371
Step 57790: loss = 0.03673
Step 57795: loss = 0.04555
Step 57800: loss = 0.04178
Step 57805: loss = 0.04306
Step 57810: loss = 0.08110
Step 57815: loss = 0.04777
Step 57820: loss = 0.03932
Step 57825: loss = 0.04315
Step 57830: loss = 0.07882
Step 57835: loss = 0.08868
Step 57840: loss = 0.03889
Step 57845: loss = 0.03855
Step 57850: loss = 0.04795
Step 57855: loss = 0.07265
Step 57860: loss = 0.04531
Step 57865: loss = 0.06915
Step 57870: loss = 0.03749
Step 57875: loss = 0.06266
Step 57880: loss = 0.05282
Step 57885: loss = 0.03853
Step 57890: loss = 0.06595
Step 57895: loss = 0.04666
Step 57900: loss = 0.04296
Step 57905: loss = 0.04675
Step 57910: loss = 0.04130
Step 57915: loss = 0.08580
Step 57920: loss = 0.06804
Step 57925: loss = 0.05989
Step 57930: loss = 0.03725
Step 57935: loss = 0.05826
Step 57940: loss = 0.05105
Step 57945: loss = 0.06248
Step 57950: loss = 0.03669
Step 57955: loss = 0.06787
Step 57960: loss = 0.04301
Step 57965: loss = 0.04676
Step 57970: loss = 0.04136
Step 57975: loss = 0.04428
Step 57980: loss = 0.06513
Step 57985: loss = 0.06314
Step 57990: loss = 0.05406
Step 57995: loss = 0.03543
Step 58000: loss = 0.04696
Step 58005: loss = 0.05221
Step 58010: loss = 0.07849
Step 58015: loss = 0.04752
Step 58020: loss = 0.04821
Step 58025: loss = 0.06202
Step 58030: loss = 0.05336
Step 58035: loss = 0.05707
Step 58040: loss = 0.03893
Step 58045: loss = 0.03856
Step 58050: loss = 0.02994
Step 58055: loss = 0.03353
Step 58060: loss = 0.04551
Step 58065: loss = 0.04693
Step 58070: loss = 0.04976
Step 58075: loss = 0.06359
Step 58080: loss = 0.05190
Step 58085: loss = 0.05720
Step 58090: loss = 0.03794
Step 58095: loss = 0.06019
Step 58100: loss = 0.04589
Step 58105: loss = 0.05325
Step 58110: loss = 0.03553
Training Data Eval:
  Num examples: 49920, Num correct: 49604, Precision @ 1: 0.9937
('Testing Data Eval: EPOCH->', 150)
  Num examples: 9984, Num correct: 7450, Precision @ 1: 0.7462
Step 58115: loss = 0.08401
Step 58120: loss = 0.05258
Step 58125: loss = 0.06499
Step 58130: loss = 0.06925
Step 58135: loss = 0.03232
Step 58140: loss = 0.07294
Step 58145: loss = 0.12408
Step 58150: loss = 0.07351
Step 58155: loss = 0.06942
Step 58160: loss = 0.06138
Step 58165: loss = 0.09679
Step 58170: loss = 0.03942
Step 58175: loss = 0.03447
Step 58180: loss = 0.04363
Step 58185: loss = 0.06230
Step 58190: loss = 0.09433
Step 58195: loss = 0.04611
Step 58200: loss = 0.05254
Step 58205: loss = 0.05607
Step 58210: loss = 0.04465
Step 58215: loss = 0.09682
Step 58220: loss = 0.04248
Step 58225: loss = 0.08561
Step 58230: loss = 0.04792
Step 58235: loss = 0.04980
Step 58240: loss = 0.04691
Step 58245: loss = 0.07214
Step 58250: loss = 0.04498
Step 58255: loss = 0.07507
Step 58260: loss = 0.09561
Step 58265: loss = 0.04004
Step 58270: loss = 0.03527
Step 58275: loss = 0.03386
Step 58280: loss = 0.04404
Step 58285: loss = 0.04252
Step 58290: loss = 0.06082
Step 58295: loss = 0.03874
Step 58300: loss = 0.04456
Step 58305: loss = 0.07466
Step 58310: loss = 0.05168
Step 58315: loss = 0.06272
Step 58320: loss = 0.05912
Step 58325: loss = 0.05540
Step 58330: loss = 0.05965
Step 58335: loss = 0.11356
Step 58340: loss = 0.04467
Step 58345: loss = 0.04994
Step 58350: loss = 0.04129
Step 58355: loss = 0.05043
Step 58360: loss = 0.04878
Step 58365: loss = 0.05553
Step 58370: loss = 0.04421
Step 58375: loss = 0.06418
Step 58380: loss = 0.02800
Step 58385: loss = 0.04529
Step 58390: loss = 0.03870
Step 58395: loss = 0.03878
Step 58400: loss = 0.05861
Step 58405: loss = 0.05415
Step 58410: loss = 0.06675
Step 58415: loss = 0.06454
Step 58420: loss = 0.04770
Step 58425: loss = 0.06224
Step 58430: loss = 0.03889
Step 58435: loss = 0.05175
Step 58440: loss = 0.17592
Step 58445: loss = 0.06553
Step 58450: loss = 0.04819
Step 58455: loss = 0.05141
Step 58460: loss = 0.04268
Step 58465: loss = 0.03631
Step 58470: loss = 0.03757
Step 58475: loss = 0.06356
Step 58480: loss = 0.06331
Step 58485: loss = 0.04955
Step 58490: loss = 0.05320
Step 58495: loss = 0.03776
Step 58500: loss = 0.07798
Training Data Eval:
  Num examples: 49920, Num correct: 49557, Precision @ 1: 0.9927
('Testing Data Eval: EPOCH->', 151)
  Num examples: 9984, Num correct: 7497, Precision @ 1: 0.7509
Step 58505: loss = 0.09296
Step 58510: loss = 0.10006
Step 58515: loss = 0.09317
Step 58520: loss = 0.07310
Step 58525: loss = 0.09424
Step 58530: loss = 0.07631
Step 58535: loss = 0.04750
Step 58540: loss = 0.05984
Step 58545: loss = 0.04804
Step 58550: loss = 0.03706
Step 58555: loss = 0.04258
Step 58560: loss = 0.04692
Step 58565: loss = 0.04362
Step 58570: loss = 0.03909
Step 58575: loss = 0.05128
Step 58580: loss = 0.07448
Step 58585: loss = 0.04509
Step 58590: loss = 0.05113
Step 58595: loss = 0.09979
Step 58600: loss = 0.04710
Step 58605: loss = 0.04492
Step 58610: loss = 0.05304
Step 58615: loss = 0.04385
Step 58620: loss = 0.03175
Step 58625: loss = 0.11116
Step 58630: loss = 0.05813
Step 58635: loss = 0.05579
Step 58640: loss = 0.05308
Step 58645: loss = 0.04366
Step 58650: loss = 0.05786
Step 58655: loss = 0.05047
Step 58660: loss = 0.10835
Step 58665: loss = 0.07344
Step 58670: loss = 0.04544
Step 58675: loss = 0.07080
Step 58680: loss = 0.05022
Step 58685: loss = 0.04308
Step 58690: loss = 0.04624
Step 58695: loss = 0.08764
Step 58700: loss = 0.04425
Step 58705: loss = 0.06143
Step 58710: loss = 0.03929
Step 58715: loss = 0.09268
Step 58720: loss = 0.06316
Step 58725: loss = 0.05035
Step 58730: loss = 0.07633
Step 58735: loss = 0.05100
Step 58740: loss = 0.04019
Step 58745: loss = 0.08454
Step 58750: loss = 0.10094
Step 58755: loss = 0.04057
Step 58760: loss = 0.07757
Step 58765: loss = 0.06560
Step 58770: loss = 0.06465
Step 58775: loss = 0.06578
Step 58780: loss = 0.08490
Step 58785: loss = 0.06162
Step 58790: loss = 0.08239
Step 58795: loss = 0.06326
Step 58800: loss = 0.05574
Step 58805: loss = 0.05173
Step 58810: loss = 0.06103
Step 58815: loss = 0.05834
Step 58820: loss = 0.05026
Step 58825: loss = 0.04057
Step 58830: loss = 0.07680
Step 58835: loss = 0.14816
Step 58840: loss = 0.08656
Step 58845: loss = 0.05092
Step 58850: loss = 0.04053
Step 58855: loss = 0.08479
Step 58860: loss = 0.08063
Step 58865: loss = 0.05179
Step 58870: loss = 0.04739
Step 58875: loss = 0.05832
Step 58880: loss = 0.03936
Step 58885: loss = 0.11763
Step 58890: loss = 0.06787
Training Data Eval:
  Num examples: 49920, Num correct: 49501, Precision @ 1: 0.9916
('Testing Data Eval: EPOCH->', 152)
  Num examples: 9984, Num correct: 7479, Precision @ 1: 0.7491
Step 58895: loss = 0.04770
Step 58900: loss = 0.05525
Step 58905: loss = 0.08501
Step 58910: loss = 0.06875
Step 58915: loss = 0.04079
Step 58920: loss = 0.08205
Step 58925: loss = 0.05576
Step 58930: loss = 0.04942
Step 58935: loss = 0.03909
Step 58940: loss = 0.05932
Step 58945: loss = 0.08014
Step 58950: loss = 0.05274
Step 58955: loss = 0.04598
Step 58960: loss = 0.05026
Step 58965: loss = 0.04831
Step 58970: loss = 0.04385
Step 58975: loss = 0.05326
Step 58980: loss = 0.07228
Step 58985: loss = 0.05333
Step 58990: loss = 0.04385
Step 58995: loss = 0.05801
Step 59000: loss = 0.05069
Step 59005: loss = 0.05494
Step 59010: loss = 0.09638
Step 59015: loss = 0.06815
Step 59020: loss = 0.08804
Step 59025: loss = 0.04947
Step 59030: loss = 0.03943
Step 59035: loss = 0.03631
Step 59040: loss = 0.05048
Step 59045: loss = 0.06938
Step 59050: loss = 0.06948
Step 59055: loss = 0.04876
Step 59060: loss = 0.03899
Step 59065: loss = 0.07671
Step 59070: loss = 0.05186
Step 59075: loss = 0.06813
Step 59080: loss = 0.05723
Step 59085: loss = 0.05973
Step 59090: loss = 0.08993
Step 59095: loss = 0.08965
Step 59100: loss = 0.04380
Step 59105: loss = 0.12194
Step 59110: loss = 0.04955
Step 59115: loss = 0.08363
Step 59120: loss = 0.05226
Step 59125: loss = 0.04960
Step 59130: loss = 0.06385
Step 59135: loss = 0.03544
Step 59140: loss = 0.03383
Step 59145: loss = 0.04726
Step 59150: loss = 0.06771
Step 59155: loss = 0.04094
Step 59160: loss = 0.11391
Step 59165: loss = 0.08839
Step 59170: loss = 0.07277
Step 59175: loss = 0.06071
Step 59180: loss = 0.04992
Step 59185: loss = 0.02941
Step 59190: loss = 0.03339
Step 59195: loss = 0.04130
Step 59200: loss = 0.08256
Step 59205: loss = 0.04996
Step 59210: loss = 0.07948
Step 59215: loss = 0.03333
Step 59220: loss = 0.05886
Step 59225: loss = 0.04654
Step 59230: loss = 0.07716
Step 59235: loss = 0.05945
Step 59240: loss = 0.07024
Step 59245: loss = 0.03937
Step 59250: loss = 0.07106
Step 59255: loss = 0.04971
Step 59260: loss = 0.10567
Step 59265: loss = 0.07894
Step 59270: loss = 0.05046
Step 59275: loss = 0.04778
Step 59280: loss = 0.04206
Training Data Eval:
  Num examples: 49920, Num correct: 49430, Precision @ 1: 0.9902
('Testing Data Eval: EPOCH->', 153)
  Num examples: 9984, Num correct: 7468, Precision @ 1: 0.7480
Step 59285: loss = 0.04836
Step 59290: loss = 0.04728
Step 59295: loss = 0.05856
Step 59300: loss = 0.03970
Step 59305: loss = 0.07777
Step 59310: loss = 0.05242
Step 59315: loss = 0.06311
Step 59320: loss = 0.04632
Step 59325: loss = 0.09079
Step 59330: loss = 0.04736
Step 59335: loss = 0.04999
Step 59340: loss = 0.06152
Step 59345: loss = 0.06249
Step 59350: loss = 0.04650
Step 59355: loss = 0.04079
Step 59360: loss = 0.03844
Step 59365: loss = 0.04461
Step 59370: loss = 0.12203
Step 59375: loss = 0.04969
Step 59380: loss = 0.07854
Step 59385: loss = 0.06614
Step 59390: loss = 0.07095
Step 59395: loss = 0.06518
Step 59400: loss = 0.05303
Step 59405: loss = 0.06335
Step 59410: loss = 0.03744
Step 59415: loss = 0.08232
Step 59420: loss = 0.05413
Step 59425: loss = 0.05347
Step 59430: loss = 0.04747
Step 59435: loss = 0.06119
Step 59440: loss = 0.04276
Step 59445: loss = 0.07467
Step 59450: loss = 0.03130
Step 59455: loss = 0.04436
Step 59460: loss = 0.05248
Step 59465: loss = 0.04617
Step 59470: loss = 0.03247
Step 59475: loss = 0.05064
Step 59480: loss = 0.06925
Step 59485: loss = 0.08529
Step 59490: loss = 0.03466
Step 59495: loss = 0.04551
Step 59500: loss = 0.05652
Step 59505: loss = 0.08721
Step 59510: loss = 0.05736
Step 59515: loss = 0.03190
Step 59520: loss = 0.07186
Step 59525: loss = 0.04277
Step 59530: loss = 0.08505
Step 59535: loss = 0.07308
Step 59540: loss = 0.07658
Step 59545: loss = 0.05694
Step 59550: loss = 0.05311
Step 59555: loss = 0.04384
Step 59560: loss = 0.08241
Step 59565: loss = 0.04647
Step 59570: loss = 0.04314
Step 59575: loss = 0.05040
Step 59580: loss = 0.04945
Step 59585: loss = 0.08859
Step 59590: loss = 0.07228
Step 59595: loss = 0.12076
Step 59600: loss = 0.04615
Step 59605: loss = 0.07915
Step 59610: loss = 0.06999
Step 59615: loss = 0.06287
Step 59620: loss = 0.05975
Step 59625: loss = 0.04876
Step 59630: loss = 0.07381
Step 59635: loss = 0.10919
Step 59640: loss = 0.07562
Step 59645: loss = 0.06265
Step 59650: loss = 0.05949
Step 59655: loss = 0.10065
Step 59660: loss = 0.05959
Step 59665: loss = 0.10102
Step 59670: loss = 0.06008
Training Data Eval:
  Num examples: 49920, Num correct: 49483, Precision @ 1: 0.9912
('Testing Data Eval: EPOCH->', 154)
  Num examples: 9984, Num correct: 7414, Precision @ 1: 0.7426
Step 59675: loss = 0.03736
Step 59680: loss = 0.06165
Step 59685: loss = 0.06206
Step 59690: loss = 0.06563
Step 59695: loss = 0.05081
Step 59700: loss = 0.05650
Step 59705: loss = 0.07820
Step 59710: loss = 0.08505
Step 59715: loss = 0.05423
Step 59720: loss = 0.07555
Step 59725: loss = 0.03855
Step 59730: loss = 0.06041
Step 59735: loss = 0.05341
Step 59740: loss = 0.09880
Step 59745: loss = 0.05846
Step 59750: loss = 0.07162
Step 59755: loss = 0.07993
Step 59760: loss = 0.05521
Step 59765: loss = 0.03302
Step 59770: loss = 0.05043
Step 59775: loss = 0.03955
Step 59780: loss = 0.05203
Step 59785: loss = 0.06240
Step 59790: loss = 0.08004
Step 59795: loss = 0.06142
Step 59800: loss = 0.04462
Step 59805: loss = 0.07767
Step 59810: loss = 0.04724
Step 59815: loss = 0.05268
Step 59820: loss = 0.07324
Step 59825: loss = 0.06793
Step 59830: loss = 0.07554
Step 59835: loss = 0.06141
Step 59840: loss = 0.04935
Step 59845: loss = 0.06833
Step 59850: loss = 0.05309
Step 59855: loss = 0.03737
Step 59860: loss = 0.04537
Step 59865: loss = 0.07459
Step 59870: loss = 0.06160
Step 59875: loss = 0.04834
Step 59880: loss = 0.04571
Step 59885: loss = 0.06011
Step 59890: loss = 0.07574
Step 59895: loss = 0.09348
Step 59900: loss = 0.06067
Step 59905: loss = 0.05413
Step 59910: loss = 0.10415
Step 59915: loss = 0.09137
Step 59920: loss = 0.09448
Step 59925: loss = 0.07049
Step 59930: loss = 0.07494
Step 59935: loss = 0.07807
Step 59940: loss = 0.04045
Step 59945: loss = 0.08063
Step 59950: loss = 0.06771
Step 59955: loss = 0.05000
Step 59960: loss = 0.04959
Step 59965: loss = 0.03932
Step 59970: loss = 0.09984
Step 59975: loss = 0.05562
Step 59980: loss = 0.06438
Step 59985: loss = 0.10784
Step 59990: loss = 0.07207
Step 59995: loss = 0.09796
Step 60000: loss = 0.07672
Step 60005: loss = 0.07790
Step 60010: loss = 0.05647
Step 60015: loss = 0.03228
Step 60020: loss = 0.03977
Step 60025: loss = 0.05023
Step 60030: loss = 0.08467
Step 60035: loss = 0.03626
Step 60040: loss = 0.05716
Step 60045: loss = 0.06874
Step 60050: loss = 0.08337
Step 60055: loss = 0.05793
Step 60060: loss = 0.04322
Training Data Eval:
  Num examples: 49920, Num correct: 49578, Precision @ 1: 0.9931
('Testing Data Eval: EPOCH->', 155)
  Num examples: 9984, Num correct: 7431, Precision @ 1: 0.7443
Step 60065: loss = 0.03856
Step 60070: loss = 0.05046
Step 60075: loss = 0.04861
Step 60080: loss = 0.04886
Step 60085: loss = 0.07751
Step 60090: loss = 0.04850
Step 60095: loss = 0.05840
Step 60100: loss = 0.03739
Step 60105: loss = 0.04208
Step 60110: loss = 0.03289
Step 60115: loss = 0.05300
Step 60120: loss = 0.03436
Step 60125: loss = 0.05760
Step 60130: loss = 0.05892
Step 60135: loss = 0.03757
Step 60140: loss = 0.04275
Step 60145: loss = 0.05242
Step 60150: loss = 0.03910
Step 60155: loss = 0.04898
Step 60160: loss = 0.04943
Step 60165: loss = 0.03875
Step 60170: loss = 0.05348
Step 60175: loss = 0.04557
Step 60180: loss = 0.09576
Step 60185: loss = 0.04157
Step 60190: loss = 0.06393
Step 60195: loss = 0.08916
Step 60200: loss = 0.06210
Step 60205: loss = 0.06155
Step 60210: loss = 0.06782
Step 60215: loss = 0.05584
Step 60220: loss = 0.03378
Step 60225: loss = 0.05264
Step 60230: loss = 0.03143
Step 60235: loss = 0.08394
Step 60240: loss = 0.05534
Step 60245: loss = 0.04639
Step 60250: loss = 0.04044
Step 60255: loss = 0.05944
Step 60260: loss = 0.04738
Step 60265: loss = 0.11286
Step 60270: loss = 0.04534
Step 60275: loss = 0.04567
Step 60280: loss = 0.12066
Step 60285: loss = 0.05823
Step 60290: loss = 0.07788
Step 60295: loss = 0.06014
Step 60300: loss = 0.06687
Step 60305: loss = 0.05031
Step 60310: loss = 0.04862
Step 60315: loss = 0.09453
Step 60320: loss = 0.07346
Step 60325: loss = 0.05997
Step 60330: loss = 0.06893
Step 60335: loss = 0.06042
Step 60340: loss = 0.04997
Step 60345: loss = 0.03385
Step 60350: loss = 0.05510
Step 60355: loss = 0.04776
Step 60360: loss = 0.06117
Step 60365: loss = 0.03241
Step 60370: loss = 0.05300
Step 60375: loss = 0.03824
Step 60380: loss = 0.05792
Step 60385: loss = 0.07378
Step 60390: loss = 0.06955
Step 60395: loss = 0.04578
Step 60400: loss = 0.06245
Step 60405: loss = 0.04794
Step 60410: loss = 0.04567
Step 60415: loss = 0.04538
Step 60420: loss = 0.05184
Step 60425: loss = 0.05551
Step 60430: loss = 0.03826
Step 60435: loss = 0.06071
Step 60440: loss = 0.08958
Step 60445: loss = 0.03869
Step 60450: loss = 0.06733
Training Data Eval:
  Num examples: 49920, Num correct: 49569, Precision @ 1: 0.9930
('Testing Data Eval: EPOCH->', 156)
  Num examples: 9984, Num correct: 7470, Precision @ 1: 0.7482
Step 60455: loss = 0.03131
Step 60460: loss = 0.04112
Step 60465: loss = 0.06658
Step 60470: loss = 0.08697
Step 60475: loss = 0.05159
Step 60480: loss = 0.04271
Step 60485: loss = 0.04569
Step 60490: loss = 0.04400
Step 60495: loss = 0.03538
Step 60500: loss = 0.03149
Step 60505: loss = 0.05587
Step 60510: loss = 0.03899
Step 60515: loss = 0.04479
Step 60520: loss = 0.04205
Step 60525: loss = 0.06058
Step 60530: loss = 0.06389
Step 60535: loss = 0.03356
Step 60540: loss = 0.04485
Step 60545: loss = 0.03526
Step 60550: loss = 0.03611
Step 60555: loss = 0.05767
Step 60560: loss = 0.06529
Step 60565: loss = 0.05939
Step 60570: loss = 0.04592
Step 60575: loss = 0.07238
Step 60580: loss = 0.06545
Step 60585: loss = 0.05816
Step 60590: loss = 0.05024
Step 60595: loss = 0.06860
Step 60600: loss = 0.04314
Step 60605: loss = 0.06376
Step 60610: loss = 0.04287
Step 60615: loss = 0.04123
Step 60620: loss = 0.04738
Step 60625: loss = 0.03027
Step 60630: loss = 0.04754
Step 60635: loss = 0.06053
Step 60640: loss = 0.05343
Step 60645: loss = 0.04907
Step 60650: loss = 0.03766
Step 60655: loss = 0.06713
Step 60660: loss = 0.08264
Step 60665: loss = 0.05215
Step 60670: loss = 0.04676
Step 60675: loss = 0.04753
Step 60680: loss = 0.06716
Step 60685: loss = 0.03439
Step 60690: loss = 0.03747
Step 60695: loss = 0.04534
Step 60700: loss = 0.03895
Step 60705: loss = 0.05406
Step 60710: loss = 0.04207
Step 60715: loss = 0.04195
Step 60720: loss = 0.10008
Step 60725: loss = 0.05875
Step 60730: loss = 0.07676
Step 60735: loss = 0.06481
Step 60740: loss = 0.05544
Step 60745: loss = 0.05020
Step 60750: loss = 0.03587
Step 60755: loss = 0.06936
Step 60760: loss = 0.07828
Step 60765: loss = 0.05276
Step 60770: loss = 0.05006
Step 60775: loss = 0.04980
Step 60780: loss = 0.03976
Step 60785: loss = 0.03611
Step 60790: loss = 0.08636
Step 60795: loss = 0.06296
Step 60800: loss = 0.08031
Step 60805: loss = 0.05105
Step 60810: loss = 0.05757
Step 60815: loss = 0.03662
Step 60820: loss = 0.11436
Step 60825: loss = 0.04202
Step 60830: loss = 0.04001
Step 60835: loss = 0.07181
Step 60840: loss = 0.05862
Training Data Eval:
  Num examples: 49920, Num correct: 49514, Precision @ 1: 0.9919
('Testing Data Eval: EPOCH->', 157)
  Num examples: 9984, Num correct: 7447, Precision @ 1: 0.7459
Step 60845: loss = 0.07206
Step 60850: loss = 0.03758
Step 60855: loss = 0.03630
Step 60860: loss = 0.04762
Step 60865: loss = 0.04304
Step 60870: loss = 0.06579
Step 60875: loss = 0.05974
Step 60880: loss = 0.04642
Step 60885: loss = 0.06311
Step 60890: loss = 0.06544
Step 60895: loss = 0.03849
Step 60900: loss = 0.05670
Step 60905: loss = 0.08654
Step 60910: loss = 0.05277
Step 60915: loss = 0.04110
Step 60920: loss = 0.04849
Step 60925: loss = 0.05901
Step 60930: loss = 0.06152
Step 60935: loss = 0.07055
Step 60940: loss = 0.05605
Step 60945: loss = 0.07675
Step 60950: loss = 0.05385
Step 60955: loss = 0.07299
Step 60960: loss = 0.06289
Step 60965: loss = 0.08641
Step 60970: loss = 0.06559
Step 60975: loss = 0.05271
Step 60980: loss = 0.07952
Step 60985: loss = 0.03730
Step 60990: loss = 0.07610
Step 60995: loss = 0.05796
Step 61000: loss = 0.06351
Step 61005: loss = 0.05496
Step 61010: loss = 0.04367
Step 61015: loss = 0.07410
Step 61020: loss = 0.05290
Step 61025: loss = 0.06456
Step 61030: loss = 0.03293
Step 61035: loss = 0.06676
Step 61040: loss = 0.08134
Step 61045: loss = 0.05307
Step 61050: loss = 0.07551
Step 61055: loss = 0.06136
Step 61060: loss = 0.04410
Step 61065: loss = 0.04789
Step 61070: loss = 0.07587
Step 61075: loss = 0.06124
Step 61080: loss = 0.03865
Step 61085: loss = 0.04334
Step 61090: loss = 0.07267
Step 61095: loss = 0.08009
Step 61100: loss = 0.05359
Step 61105: loss = 0.04908
Step 61110: loss = 0.04345
Step 61115: loss = 0.06877
Step 61120: loss = 0.06548
Step 61125: loss = 0.08261
Step 61130: loss = 0.05142
Step 61135: loss = 0.03908
Step 61140: loss = 0.07098
Step 61145: loss = 0.03519
Step 61150: loss = 0.08736
Step 61155: loss = 0.07099
Step 61160: loss = 0.05215
Step 61165: loss = 0.03021
Step 61170: loss = 0.04531
Step 61175: loss = 0.04774
Step 61180: loss = 0.09467
Step 61185: loss = 0.07178
Step 61190: loss = 0.11813
Step 61195: loss = 0.08503
Step 61200: loss = 0.07853
Step 61205: loss = 0.07459
Step 61210: loss = 0.09749
Step 61215: loss = 0.08045
Step 61220: loss = 0.06198
Step 61225: loss = 0.04916
Step 61230: loss = 0.08109
Training Data Eval:
  Num examples: 49920, Num correct: 49466, Precision @ 1: 0.9909
('Testing Data Eval: EPOCH->', 158)
  Num examples: 9984, Num correct: 7429, Precision @ 1: 0.7441
Step 61235: loss = 0.06130
Step 61240: loss = 0.03406
Step 61245: loss = 0.03649
Step 61250: loss = 0.06591
Step 61255: loss = 0.08201
Step 61260: loss = 0.06743
Step 61265: loss = 0.05555
Step 61270: loss = 0.04649
Step 61275: loss = 0.05344
Step 61280: loss = 0.05180
Step 61285: loss = 0.04625
Step 61290: loss = 0.03986
Step 61295: loss = 0.03175
Step 61300: loss = 0.04490
Step 61305: loss = 0.05565
Step 61310: loss = 0.04289
Step 61315: loss = 0.03530
Step 61320: loss = 0.04228
Step 61325: loss = 0.06756
Step 61330: loss = 0.04194
Step 61335: loss = 0.06888
Step 61340: loss = 0.09085
Step 61345: loss = 0.05196
Step 61350: loss = 0.04604
Step 61355: loss = 0.05030
Step 61360: loss = 0.06014
Step 61365: loss = 0.04879
Step 61370: loss = 0.13304
Step 61375: loss = 0.05773
Step 61380: loss = 0.05052
Step 61385: loss = 0.05217
Step 61390: loss = 0.07494
Step 61395: loss = 0.08948
Step 61400: loss = 0.04222
Step 61405: loss = 0.03880
Step 61410: loss = 0.09181
Step 61415: loss = 0.07019
Step 61420: loss = 0.06479
Step 61425: loss = 0.06141
Step 61430: loss = 0.15998
Step 61435: loss = 0.12516
Step 61440: loss = 0.05145
Step 61445: loss = 0.06567
Step 61450: loss = 0.07123
Step 61455: loss = 0.07471
Step 61460: loss = 0.07132
Step 61465: loss = 0.13841
Step 61470: loss = 0.03794
Step 61475: loss = 0.06989
Step 61480: loss = 0.05706
Step 61485: loss = 0.04125
Step 61490: loss = 0.04313
Step 61495: loss = 0.03823
Step 61500: loss = 0.07111
Step 61505: loss = 0.03536
Step 61510: loss = 0.06061
Step 61515: loss = 0.06365
Step 61520: loss = 0.03740
Step 61525: loss = 0.09482
Step 61530: loss = 0.07405
Step 61535: loss = 0.07902
Step 61540: loss = 0.02708
Step 61545: loss = 0.04630
Step 61550: loss = 0.06172
Step 61555: loss = 0.07805
Step 61560: loss = 0.05099
Step 61565: loss = 0.07386
Step 61570: loss = 0.05091
Step 61575: loss = 0.09947
Step 61580: loss = 0.05855
Step 61585: loss = 0.09297
Step 61590: loss = 0.09286
Step 61595: loss = 0.03418
Step 61600: loss = 0.06422
Step 61605: loss = 0.06457
Step 61610: loss = 0.06997
Step 61615: loss = 0.03840
Step 61620: loss = 0.06978
Training Data Eval:
  Num examples: 49920, Num correct: 49418, Precision @ 1: 0.9899
('Testing Data Eval: EPOCH->', 159)
  Num examples: 9984, Num correct: 7436, Precision @ 1: 0.7448
Step 61625: loss = 0.03271
Step 61630: loss = 0.06857
Step 61635: loss = 0.08331
Step 61640: loss = 0.06801
Step 61645: loss = 0.06058
Step 61650: loss = 0.05410
Step 61655: loss = 0.12589
Step 61660: loss = 0.04618
Step 61665: loss = 0.09898
Step 61670: loss = 0.05903
Step 61675: loss = 0.05094
Step 61680: loss = 0.04009
Step 61685: loss = 0.08197
Step 61690: loss = 0.03328
Step 61695: loss = 0.06845
Step 61700: loss = 0.06981
Step 61705: loss = 0.05319
Step 61710: loss = 0.04176
Step 61715: loss = 0.05909
Step 61720: loss = 0.05814
Step 61725: loss = 0.07257
Step 61730: loss = 0.08889
Step 61735: loss = 0.03595
Step 61740: loss = 0.05635
Step 61745: loss = 0.04873
Step 61750: loss = 0.05708
Step 61755: loss = 0.11512
Step 61760: loss = 0.07761
Step 61765: loss = 0.04665
Step 61770: loss = 0.11408
Step 61775: loss = 0.03967
Step 61780: loss = 0.05729
Step 61785: loss = 0.05109
Step 61790: loss = 0.10184
Step 61795: loss = 0.04548
Step 61800: loss = 0.06501
Step 61805: loss = 0.06046
Step 61810: loss = 0.07208
Step 61815: loss = 0.04832
Step 61820: loss = 0.04501
Step 61825: loss = 0.05234
Step 61830: loss = 0.04401
Step 61835: loss = 0.03525
Step 61840: loss = 0.03707
Step 61845: loss = 0.03801
Step 61850: loss = 0.04714
Step 61855: loss = 0.05504
Step 61860: loss = 0.04610
Step 61865: loss = 0.03293
Step 61870: loss = 0.04346
Step 61875: loss = 0.07072
Step 61880: loss = 0.05754
Step 61885: loss = 0.10008
Step 61890: loss = 0.03560
Step 61895: loss = 0.04659
Step 61900: loss = 0.05540
Step 61905: loss = 0.05904
Step 61910: loss = 0.06042
Step 61915: loss = 0.04710
Step 61920: loss = 0.06154
Step 61925: loss = 0.09159
Step 61930: loss = 0.03503
Step 61935: loss = 0.04494
Step 61940: loss = 0.04763
Step 61945: loss = 0.04313
Step 61950: loss = 0.04829
Step 61955: loss = 0.05648
Step 61960: loss = 0.04175
Step 61965: loss = 0.04770
Step 61970: loss = 0.06012
Step 61975: loss = 0.04737
Step 61980: loss = 0.05693
Step 61985: loss = 0.06890
Step 61990: loss = 0.03753
Step 61995: loss = 0.06188
Step 62000: loss = 0.04416
Step 62005: loss = 0.06887
Step 62010: loss = 0.03709
Training Data Eval:
  Num examples: 49920, Num correct: 49539, Precision @ 1: 0.9924
('Testing Data Eval: EPOCH->', 160)
  Num examples: 9984, Num correct: 7385, Precision @ 1: 0.7397
Step 62015: loss = 0.06190
Step 62020: loss = 0.04503
Step 62025: loss = 0.07117
Step 62030: loss = 0.03970
Step 62035: loss = 0.06042
Step 62040: loss = 0.04098
Step 62045: loss = 0.06470
Step 62050: loss = 0.04677
Step 62055: loss = 0.04891
Step 62060: loss = 0.07721
Step 62065: loss = 0.03350
Step 62070: loss = 0.05245
Step 62075: loss = 0.03539
Step 62080: loss = 0.06486
Step 62085: loss = 0.04427
Step 62090: loss = 0.03190
Step 62095: loss = 0.03896
Step 62100: loss = 0.04171
Step 62105: loss = 0.03825
Step 62110: loss = 0.04167
Step 62115: loss = 0.04831
Step 62120: loss = 0.03764
Step 62125: loss = 0.05306
Step 62130: loss = 0.04856
Step 62135: loss = 0.09829
Step 62140: loss = 0.04359
Step 62145: loss = 0.05562
Step 62150: loss = 0.03579
Step 62155: loss = 0.06976
Step 62160: loss = 0.03227
Step 62165: loss = 0.05140
Step 62170: loss = 0.03940
Step 62175: loss = 0.07659
Step 62180: loss = 0.07352
Step 62185: loss = 0.04232
Step 62190: loss = 0.04884
Step 62195: loss = 0.04982
Step 62200: loss = 0.05451
Step 62205: loss = 0.02986
Step 62210: loss = 0.06351
Step 62215: loss = 0.04540
Step 62220: loss = 0.08635
Step 62225: loss = 0.04282
Step 62230: loss = 0.04136
Step 62235: loss = 0.04040
Step 62240: loss = 0.03735
Step 62245: loss = 0.05456
Step 62250: loss = 0.06181
Step 62255: loss = 0.06577
Step 62260: loss = 0.05735
Step 62265: loss = 0.04383
Step 62270: loss = 0.08427
Step 62275: loss = 0.05428
Step 62280: loss = 0.06518
Step 62285: loss = 0.10626
Step 62290: loss = 0.03059
Step 62295: loss = 0.06500
Step 62300: loss = 0.05791
Step 62305: loss = 0.08116
Step 62310: loss = 0.04177
Step 62315: loss = 0.05353
Step 62320: loss = 0.02988
Step 62325: loss = 0.04040
Step 62330: loss = 0.03725
Step 62335: loss = 0.05249
Step 62340: loss = 0.04692
Step 62345: loss = 0.03502
Step 62350: loss = 0.05310
Step 62355: loss = 0.04975
Step 62360: loss = 0.04476
Step 62365: loss = 0.05870
Step 62370: loss = 0.08641
Step 62375: loss = 0.03468
Step 62380: loss = 0.03878
Step 62385: loss = 0.05154
Step 62390: loss = 0.05778
Step 62395: loss = 0.04028
Step 62400: loss = 0.05203
Training Data Eval:
  Num examples: 49920, Num correct: 49565, Precision @ 1: 0.9929
('Testing Data Eval: EPOCH->', 161)
  Num examples: 9984, Num correct: 7471, Precision @ 1: 0.7483
Step 62405: loss = 0.05392
Step 62410: loss = 0.03522
Step 62415: loss = 0.04462
Step 62420: loss = 0.04029
Step 62425: loss = 0.08200
Step 62430: loss = 0.06682
Step 62435: loss = 0.03274
Step 62440: loss = 0.04813
Step 62445: loss = 0.05493
Step 62450: loss = 0.04823
Step 62455: loss = 0.06755
Step 62460: loss = 0.05051
Step 62465: loss = 0.03691
Step 62470: loss = 0.04017
Step 62475: loss = 0.07861
Step 62480: loss = 0.03679
Step 62485: loss = 0.04815
Step 62490: loss = 0.06980
Step 62495: loss = 0.03781
Step 62500: loss = 0.03866
Step 62505: loss = 0.04607
Step 62510: loss = 0.03951
Step 62515: loss = 0.05780
Step 62520: loss = 0.03632
Step 62525: loss = 0.04452
Step 62530: loss = 0.03949
Step 62535: loss = 0.07078
Step 62540: loss = 0.05349
Step 62545: loss = 0.03703
Step 62550: loss = 0.04204
Step 62555: loss = 0.06565
Step 62560: loss = 0.07196
Step 62565: loss = 0.05473
Step 62570: loss = 0.04994
Step 62575: loss = 0.03770
Step 62580: loss = 0.03865
Step 62585: loss = 0.04718
Step 62590: loss = 0.05046
Step 62595: loss = 0.03942
Step 62600: loss = 0.05558
Step 62605: loss = 0.05008
Step 62610: loss = 0.07448
Step 62615: loss = 0.04534
Step 62620: loss = 0.04065
Step 62625: loss = 0.10891
Step 62630: loss = 0.07921
Step 62635: loss = 0.05113
Step 62640: loss = 0.07797
Step 62645: loss = 0.04963
Step 62650: loss = 0.03973
Step 62655: loss = 0.05176
Step 62660: loss = 0.05101
Step 62665: loss = 0.04954
Step 62670: loss = 0.05662
Step 62675: loss = 0.06080
Step 62680: loss = 0.06312
Step 62685: loss = 0.05817
Step 62690: loss = 0.07248
Step 62695: loss = 0.10164
Step 62700: loss = 0.04573
Step 62705: loss = 0.05836
Step 62710: loss = 0.05606
Step 62715: loss = 0.05841
Step 62720: loss = 0.05565
Step 62725: loss = 0.05039
Step 62730: loss = 0.08850
Step 62735: loss = 0.06039
Step 62740: loss = 0.06957
Step 62745: loss = 0.08719
Step 62750: loss = 0.04269
Step 62755: loss = 0.05179
Step 62760: loss = 0.02931
Step 62765: loss = 0.05173
Step 62770: loss = 0.05146
Step 62775: loss = 0.05405
Step 62780: loss = 0.07255
Step 62785: loss = 0.02948
Step 62790: loss = 0.10288
Training Data Eval:
  Num examples: 49920, Num correct: 49412, Precision @ 1: 0.9898
('Testing Data Eval: EPOCH->', 162)
  Num examples: 9984, Num correct: 7330, Precision @ 1: 0.7342
Step 62795: loss = 0.08533
Step 62800: loss = 0.05392
Step 62805: loss = 0.04948
Step 62810: loss = 0.07964
Step 62815: loss = 0.06345
Step 62820: loss = 0.05697
Step 62825: loss = 0.06844
Step 62830: loss = 0.07568
Step 62835: loss = 0.04255
Step 62840: loss = 0.03998
Step 62845: loss = 0.05832
Step 62850: loss = 0.05274
Step 62855: loss = 0.05798
Step 62860: loss = 0.05101
Step 62865: loss = 0.04524
Step 62870: loss = 0.03906
Step 62875: loss = 0.05847
Step 62880: loss = 0.03917
Step 62885: loss = 0.03455
Step 62890: loss = 0.05618
Step 62895: loss = 0.04233
Step 62900: loss = 0.05364
Step 62905: loss = 0.06533
Step 62910: loss = 0.03303
Step 62915: loss = 0.09134
Step 62920: loss = 0.06733
Step 62925: loss = 0.08374
Step 62930: loss = 0.06047
Step 62935: loss = 0.03456
Step 62940: loss = 0.04501
Step 62945: loss = 0.03886
Step 62950: loss = 0.03842
Step 62955: loss = 0.04116
Step 62960: loss = 0.06166
Step 62965: loss = 0.06271
Step 62970: loss = 0.04389
Step 62975: loss = 0.06748
Step 62980: loss = 0.05074
Step 62985: loss = 0.05473
Step 62990: loss = 0.03468
Step 62995: loss = 0.03505
Step 63000: loss = 0.04435
Step 63005: loss = 0.06583
Step 63010: loss = 0.04091
Step 63015: loss = 0.06674
Step 63020: loss = 0.06022
Step 63025: loss = 0.04762
Step 63030: loss = 0.04182
Step 63035: loss = 0.06892
Step 63040: loss = 0.03945
Step 63045: loss = 0.03390
Step 63050: loss = 0.04070
Step 63055: loss = 0.04612
Step 63060: loss = 0.10080
Step 63065: loss = 0.06239
Step 63070: loss = 0.05359
Step 63075: loss = 0.03260
Step 63080: loss = 0.05170
Step 63085: loss = 0.04850
Step 63090: loss = 0.05321
Step 63095: loss = 0.03889
Step 63100: loss = 0.05533
Step 63105: loss = 0.05781
Step 63110: loss = 0.08087
Step 63115: loss = 0.05199
Step 63120: loss = 0.05413
Step 63125: loss = 0.07191
Step 63130: loss = 0.04429
Step 63135: loss = 0.05727
Step 63140: loss = 0.04563
Step 63145: loss = 0.07945
Step 63150: loss = 0.05345
Step 63155: loss = 0.04503
Step 63160: loss = 0.04366
Step 63165: loss = 0.05456
Step 63170: loss = 0.07067
Step 63175: loss = 0.03439
Step 63180: loss = 0.04129
Training Data Eval:
  Num examples: 49920, Num correct: 49595, Precision @ 1: 0.9935
('Testing Data Eval: EPOCH->', 163)
  Num examples: 9984, Num correct: 7481, Precision @ 1: 0.7493
Step 63185: loss = 0.03325
Step 63190: loss = 0.05622
Step 63195: loss = 0.08142
Step 63200: loss = 0.04879
Step 63205: loss = 0.04578
Step 63210: loss = 0.04496
Step 63215: loss = 0.05075
Step 63220: loss = 0.03512
Step 63225: loss = 0.04276
Step 63230: loss = 0.09520
Step 63235: loss = 0.04512
Step 63240: loss = 0.07332
Step 63245: loss = 0.04206
Step 63250: loss = 0.04287
Step 63255: loss = 0.03926
Step 63260: loss = 0.04565
Step 63265: loss = 0.05469
Step 63270: loss = 0.06893
Step 63275: loss = 0.04157
Step 63280: loss = 0.04635
Step 63285: loss = 0.03582
Step 63290: loss = 0.03969
Step 63295: loss = 0.03972
Step 63300: loss = 0.04830
Step 63305: loss = 0.03481
Step 63310: loss = 0.05224
Step 63315: loss = 0.05147
Step 63320: loss = 0.03383
Step 63325: loss = 0.04724
Step 63330: loss = 0.06208
Step 63335: loss = 0.04266
Step 63340: loss = 0.05316
Step 63345: loss = 0.06503
Step 63350: loss = 0.04366
Step 63355: loss = 0.05911
Step 63360: loss = 0.04542
Step 63365: loss = 0.03188
Step 63370: loss = 0.06034
Step 63375: loss = 0.03022
Step 63380: loss = 0.03675
Step 63385: loss = 0.06767
Step 63390: loss = 0.06597
Step 63395: loss = 0.04878
Step 63400: loss = 0.03292
Step 63405: loss = 0.04555
Step 63410: loss = 0.03220
Step 63415: loss = 0.06599
Step 63420: loss = 0.04312
Step 63425: loss = 0.05109
Step 63430: loss = 0.05913
Step 63435: loss = 0.09899
Step 63440: loss = 0.06334
Step 63445: loss = 0.06534
Step 63450: loss = 0.06005
Step 63455: loss = 0.07625
Step 63460: loss = 0.04062
Step 63465: loss = 0.04917
Step 63470: loss = 0.06810
Step 63475: loss = 0.05042
Step 63480: loss = 0.07976
Step 63485: loss = 0.07733
Step 63490: loss = 0.04129
Step 63495: loss = 0.06254
Step 63500: loss = 0.07873
Step 63505: loss = 0.03461
Step 63510: loss = 0.04423
Step 63515: loss = 0.04790
Step 63520: loss = 0.04213
Step 63525: loss = 0.04824
Step 63530: loss = 0.06994
Step 63535: loss = 0.03199
Step 63540: loss = 0.07656
Step 63545: loss = 0.04808
Step 63550: loss = 0.05815
Step 63555: loss = 0.04392
Step 63560: loss = 0.04194
Step 63565: loss = 0.06837
Step 63570: loss = 0.05594
Training Data Eval:
  Num examples: 49920, Num correct: 49691, Precision @ 1: 0.9954
('Testing Data Eval: EPOCH->', 164)
  Num examples: 9984, Num correct: 7516, Precision @ 1: 0.7528
Step 63575: loss = 0.04293
Step 63580: loss = 0.02814
Step 63585: loss = 0.03799
Step 63590: loss = 0.03216
Step 63595: loss = 0.04512
Step 63600: loss = 0.03870
Step 63605: loss = 0.04109
Step 63610: loss = 0.04291
Step 63615: loss = 0.08447
Step 63620: loss = 0.03699
Step 63625: loss = 0.03114
Step 63630: loss = 0.04623
Step 63635: loss = 0.04811
Step 63640: loss = 0.04702
Step 63645: loss = 0.04415
Step 63650: loss = 0.07384
Step 63655: loss = 0.04970
Step 63660: loss = 0.05031
Step 63665: loss = 0.04723
Step 63670: loss = 0.07634
Step 63675: loss = 0.03592
Step 63680: loss = 0.04144
Step 63685: loss = 0.08776
Step 63690: loss = 0.03894
Step 63695: loss = 0.06201
Step 63700: loss = 0.05109
Step 63705: loss = 0.03692
Step 63710: loss = 0.05833
Step 63715: loss = 0.05071
Step 63720: loss = 0.04198
Step 63725: loss = 0.03322
Step 63730: loss = 0.03511
Step 63735: loss = 0.04355
Step 63740: loss = 0.06092
Step 63745: loss = 0.03545
Step 63750: loss = 0.05756
Step 63755: loss = 0.05319
Step 63760: loss = 0.04789
Step 63765: loss = 0.06370
Step 63770: loss = 0.03939
Step 63775: loss = 0.04991
Step 63780: loss = 0.04797
Step 63785: loss = 0.06168
Step 63790: loss = 0.05523
Step 63795: loss = 0.05460
Step 63800: loss = 0.05372
Step 63805: loss = 0.07269
Step 63810: loss = 0.08110
Step 63815: loss = 0.05101
Step 63820: loss = 0.06764
Step 63825: loss = 0.05905
Step 63830: loss = 0.05148
Step 63835: loss = 0.05126
Step 63840: loss = 0.05420
Step 63845: loss = 0.10548
Step 63850: loss = 0.09647
Step 63855: loss = 0.05519
Step 63860: loss = 0.08699
Step 63865: loss = 0.09652
Step 63870: loss = 0.07398
Step 63875: loss = 0.05824
Step 63880: loss = 0.04136
Step 63885: loss = 0.11375
Step 63890: loss = 0.05151
Step 63895: loss = 0.07179
Step 63900: loss = 0.05667
Step 63905: loss = 0.08253
Step 63910: loss = 0.06587
Step 63915: loss = 0.05170
Step 63920: loss = 0.05624
Step 63925: loss = 0.04470
Step 63930: loss = 0.03799
Step 63935: loss = 0.04264
Step 63940: loss = 0.05559
Step 63945: loss = 0.03195
Step 63950: loss = 0.09701
Step 63955: loss = 0.06958
Step 63960: loss = 0.05115
Training Data Eval:
  Num examples: 49920, Num correct: 49533, Precision @ 1: 0.9922
('Testing Data Eval: EPOCH->', 165)
  Num examples: 9984, Num correct: 7455, Precision @ 1: 0.7467
Step 63965: loss = 0.04701
Step 63970: loss = 0.04282
Step 63975: loss = 0.05626
Step 63980: loss = 0.05144
Step 63985: loss = 0.05417
Step 63990: loss = 0.05147
Step 63995: loss = 0.07370
Step 64000: loss = 0.05641
Step 64005: loss = 0.11255
Step 64010: loss = 0.04584
Step 64015: loss = 0.04711
Step 64020: loss = 0.05001
Step 64025: loss = 0.07904
Step 64030: loss = 0.09517
Step 64035: loss = 0.05306
Step 64040: loss = 0.05251
Step 64045: loss = 0.08415
Step 64050: loss = 0.09543
Step 64055: loss = 0.04228
Step 64060: loss = 0.06082
Step 64065: loss = 0.06891
Step 64070: loss = 0.05883
Step 64075: loss = 0.06986
Step 64080: loss = 0.04048
Step 64085: loss = 0.07548
Step 64090: loss = 0.05274
Step 64095: loss = 0.08377
Step 64100: loss = 0.04065
Step 64105: loss = 0.03853
Step 64110: loss = 0.05326
Step 64115: loss = 0.04625
Step 64120: loss = 0.08818
Step 64125: loss = 0.06943
Step 64130: loss = 0.04053
Step 64135: loss = 0.07919
Step 64140: loss = 0.06781
Step 64145: loss = 0.07938
Step 64150: loss = 0.03768
Step 64155: loss = 0.03865
Step 64160: loss = 0.07704
Step 64165: loss = 0.04863
Step 64170: loss = 0.04570
Step 64175: loss = 0.05135
Step 64180: loss = 0.04917
Step 64185: loss = 0.05342
Step 64190: loss = 0.05958
Step 64195: loss = 0.05961
Step 64200: loss = 0.04970
Step 64205: loss = 0.04565
Step 64210: loss = 0.04816
Step 64215: loss = 0.04918
Step 64220: loss = 0.06921
Step 64225: loss = 0.05676
Step 64230: loss = 0.03955
Step 64235: loss = 0.07055
Step 64240: loss = 0.07570
Step 64245: loss = 0.04757
Step 64250: loss = 0.05111
Step 64255: loss = 0.04328
Step 64260: loss = 0.05878
Step 64265: loss = 0.06304
Step 64270: loss = 0.08318
Step 64275: loss = 0.03555
Step 64280: loss = 0.07131
Step 64285: loss = 0.04376
Step 64290: loss = 0.05024
Step 64295: loss = 0.04288
Step 64300: loss = 0.03291
Step 64305: loss = 0.06731
Step 64310: loss = 0.04446
Step 64315: loss = 0.04718
Step 64320: loss = 0.04123
Step 64325: loss = 0.07543
Step 64330: loss = 0.05341
Step 64335: loss = 0.04417
Step 64340: loss = 0.04634
Step 64345: loss = 0.09554
Step 64350: loss = 0.05760
Training Data Eval:
  Num examples: 49920, Num correct: 49565, Precision @ 1: 0.9929
('Testing Data Eval: EPOCH->', 166)
  Num examples: 9984, Num correct: 7368, Precision @ 1: 0.7380
Step 64355: loss = 0.07318
Step 64360: loss = 0.03805
Step 64365: loss = 0.10296
Step 64370: loss = 0.06050
Step 64375: loss = 0.03925
Step 64380: loss = 0.06839
Step 64385: loss = 0.06544
Step 64390: loss = 0.03840
Step 64395: loss = 0.05209
Step 64400: loss = 0.04104
Step 64405: loss = 0.04855
Step 64410: loss = 0.04299
Step 64415: loss = 0.07145
Step 64420: loss = 0.04708
Step 64425: loss = 0.05849
Step 64430: loss = 0.05726
Step 64435: loss = 0.04688
Step 64440: loss = 0.06499
Step 64445: loss = 0.04772
Step 64450: loss = 0.03839
Step 64455: loss = 0.05127
Step 64460: loss = 0.04790
Step 64465: loss = 0.05968
Step 64470: loss = 0.03260
Step 64475: loss = 0.03823
Step 64480: loss = 0.04609
Step 64485: loss = 0.04789
Step 64490: loss = 0.05276
Step 64495: loss = 0.04583
Step 64500: loss = 0.07322
Step 64505: loss = 0.04090
Step 64510: loss = 0.04384
Step 64515: loss = 0.06303
Step 64520: loss = 0.07804
Step 64525: loss = 0.06631
Step 64530: loss = 0.03674
Step 64535: loss = 0.03456
Step 64540: loss = 0.09703
Step 64545: loss = 0.03758
Step 64550: loss = 0.04113
Step 64555: loss = 0.05585
Step 64560: loss = 0.06589
Step 64565: loss = 0.04579
Step 64570: loss = 0.04460
Step 64575: loss = 0.04376
Step 64580: loss = 0.06620
Step 64585: loss = 0.04473
Step 64590: loss = 0.04878
Step 64595: loss = 0.06418
Step 64600: loss = 0.03523
Step 64605: loss = 0.05148
Step 64610: loss = 0.03558
Step 64615: loss = 0.04777
Step 64620: loss = 0.05210
Step 64625: loss = 0.05938
Step 64630: loss = 0.05105
Step 64635: loss = 0.05833
Step 64640: loss = 0.03969
Step 64645: loss = 0.06212
Step 64650: loss = 0.03412
Step 64655: loss = 0.04245
Step 64660: loss = 0.11292
Step 64665: loss = 0.03997
Step 64670: loss = 0.04551
Step 64675: loss = 0.08715
Step 64680: loss = 0.04050
Step 64685: loss = 0.04234
Step 64690: loss = 0.07304
Step 64695: loss = 0.04038
Step 64700: loss = 0.06655
Step 64705: loss = 0.05187
Step 64710: loss = 0.06287
Step 64715: loss = 0.05160
Step 64720: loss = 0.03961
Step 64725: loss = 0.03619
Step 64730: loss = 0.03290
Step 64735: loss = 0.04920
Step 64740: loss = 0.10169
Training Data Eval:
  Num examples: 49920, Num correct: 49642, Precision @ 1: 0.9944
('Testing Data Eval: EPOCH->', 167)
  Num examples: 9984, Num correct: 7523, Precision @ 1: 0.7535
Step 64745: loss = 0.04854
Step 64750: loss = 0.05439
Step 64755: loss = 0.02730
Step 64760: loss = 0.05381
Step 64765: loss = 0.03925
Step 64770: loss = 0.04491
Step 64775: loss = 0.03803
Step 64780: loss = 0.03978
Step 64785: loss = 0.04069
Step 64790: loss = 0.05680
Step 64795: loss = 0.04494
Step 64800: loss = 0.03926
Step 64805: loss = 0.04254
Step 64810: loss = 0.04047
Step 64815: loss = 0.03081
Step 64820: loss = 0.04145
Step 64825: loss = 0.03040
Step 64830: loss = 0.04024
Step 64835: loss = 0.04700
Step 64840: loss = 0.04719
Step 64845: loss = 0.05453
Step 64850: loss = 0.04852
Step 64855: loss = 0.11934
Step 64860: loss = 0.03985
Step 64865: loss = 0.11943
Step 64870: loss = 0.05053
Step 64875: loss = 0.07869
Step 64880: loss = 0.05224
Step 64885: loss = 0.03265
Step 64890: loss = 0.06175
Step 64895: loss = 0.07011
Step 64900: loss = 0.05542
Step 64905: loss = 0.03640
Step 64910: loss = 0.08965
Step 64915: loss = 0.05504
Step 64920: loss = 0.04324
Step 64925: loss = 0.04150
Step 64930: loss = 0.03922
Step 64935: loss = 0.03510
Step 64940: loss = 0.05307
Step 64945: loss = 0.04325
Step 64950: loss = 0.10030
Step 64955: loss = 0.05826
Step 64960: loss = 0.05235
Step 64965: loss = 0.06647
Step 64970: loss = 0.04835
Step 64975: loss = 0.04325
Step 64980: loss = 0.06135
Step 64985: loss = 0.09082
Step 64990: loss = 0.06840
Step 64995: loss = 0.06303
Step 65000: loss = 0.07095
Step 65005: loss = 0.04262
Step 65010: loss = 0.09354
Step 65015: loss = 0.05136
Step 65020: loss = 0.09577
Step 65025: loss = 0.04596
Step 65030: loss = 0.06232
Step 65035: loss = 0.07818
Step 65040: loss = 0.08998
Step 65045: loss = 0.10981
Step 65050: loss = 0.04458
Step 65055: loss = 0.06033
Step 65060: loss = 0.05265
Step 65065: loss = 0.05829
Step 65070: loss = 0.06815
Step 65075: loss = 0.06766
Step 65080: loss = 0.04942
Step 65085: loss = 0.07899
Step 65090: loss = 0.05866
Step 65095: loss = 0.05899
Step 65100: loss = 0.06845
Step 65105: loss = 0.04022
Step 65110: loss = 0.07728
Step 65115: loss = 0.05810
Step 65120: loss = 0.07041
Step 65125: loss = 0.03765
Step 65130: loss = 0.04964
Training Data Eval:
  Num examples: 49920, Num correct: 49438, Precision @ 1: 0.9903
('Testing Data Eval: EPOCH->', 168)
  Num examples: 9984, Num correct: 7407, Precision @ 1: 0.7419
Step 65135: loss = 0.06862
Step 65140: loss = 0.05058
Step 65145: loss = 0.06427
Step 65150: loss = 0.05915
Step 65155: loss = 0.09602
Step 65160: loss = 0.04414
Step 65165: loss = 0.03826
Step 65170: loss = 0.05751
Step 65175: loss = 0.05231
Step 65180: loss = 0.05810
Step 65185: loss = 0.15424
Step 65190: loss = 0.03806
Step 65195: loss = 0.05385
Step 65200: loss = 0.05496
Step 65205: loss = 0.07184
Step 65210: loss = 0.06838
Step 65215: loss = 0.03708
Step 65220: loss = 0.04570
Step 65225: loss = 0.04901
Step 65230: loss = 0.03276
Step 65235: loss = 0.05479
Step 65240: loss = 0.09667
Step 65245: loss = 0.08587
Step 65250: loss = 0.05815
Step 65255: loss = 0.05503
Step 65260: loss = 0.04744
Step 65265: loss = 0.04241
Step 65270: loss = 0.04456
Step 65275: loss = 0.05325
Step 65280: loss = 0.05609
Step 65285: loss = 0.04383
Step 65290: loss = 0.07299
Step 65295: loss = 0.05660
Step 65300: loss = 0.08176
Step 65305: loss = 0.12311
Step 65310: loss = 0.04077
Step 65315: loss = 0.04797
Step 65320: loss = 0.04968
Step 65325: loss = 0.06536
Step 65330: loss = 0.05301
Step 65335: loss = 0.06621
Step 65340: loss = 0.03216
Step 65345: loss = 0.05290
Step 65350: loss = 0.06741
Step 65355: loss = 0.03740
Step 65360: loss = 0.06595
Step 65365: loss = 0.05434
Step 65370: loss = 0.09967
Step 65375: loss = 0.08972
Step 65380: loss = 0.07391
Step 65385: loss = 0.04742
Step 65390: loss = 0.06757
Step 65395: loss = 0.05195
Step 65400: loss = 0.03756
Step 65405: loss = 0.03353
Step 65410: loss = 0.08163
Step 65415: loss = 0.04895
Step 65420: loss = 0.04108
Step 65425: loss = 0.07150
Step 65430: loss = 0.04279
Step 65435: loss = 0.05336
Step 65440: loss = 0.04185
Step 65445: loss = 0.06628
Step 65450: loss = 0.03253
Step 65455: loss = 0.07540
Step 65460: loss = 0.04190
Step 65465: loss = 0.04405
Step 65470: loss = 0.07015
Step 65475: loss = 0.04000
Step 65480: loss = 0.08472
Step 65485: loss = 0.06636
Step 65490: loss = 0.05116
Step 65495: loss = 0.05136
Step 65500: loss = 0.06999
Step 65505: loss = 0.07846
Step 65510: loss = 0.08392
Step 65515: loss = 0.04340
Step 65520: loss = 0.06926
Training Data Eval:
  Num examples: 49920, Num correct: 49524, Precision @ 1: 0.9921
('Testing Data Eval: EPOCH->', 169)
  Num examples: 9984, Num correct: 7439, Precision @ 1: 0.7451
Step 65525: loss = 0.04012
Step 65530: loss = 0.04321
Step 65535: loss = 0.03915
Step 65540: loss = 0.05044
Step 65545: loss = 0.03777
Step 65550: loss = 0.06321
Step 65555: loss = 0.06263
Step 65560: loss = 0.04488
Step 65565: loss = 0.07913
Step 65570: loss = 0.04498
Step 65575: loss = 0.04832
Step 65580: loss = 0.05747
Step 65585: loss = 0.05186
Step 65590: loss = 0.03682
Step 65595: loss = 0.07583
Step 65600: loss = 0.06621
Step 65605: loss = 0.06644
Step 65610: loss = 0.04720
Step 65615: loss = 0.05774
Step 65620: loss = 0.06574
Step 65625: loss = 0.04332
Step 65630: loss = 0.03826
Step 65635: loss = 0.05348
Step 65640: loss = 0.06400
Step 65645: loss = 0.05020
Step 65650: loss = 0.04809
Step 65655: loss = 0.04061
Step 65660: loss = 0.06725
Step 65665: loss = 0.05515
Step 65670: loss = 0.05066
Step 65675: loss = 0.05343
Step 65680: loss = 0.06977
Step 65685: loss = 0.04514
Step 65690: loss = 0.06206
Step 65695: loss = 0.06457
Step 65700: loss = 0.06139
Step 65705: loss = 0.03881
Step 65710: loss = 0.04492
Step 65715: loss = 0.08424
Step 65720: loss = 0.05799
Step 65725: loss = 0.06212
Step 65730: loss = 0.04289
Step 65735: loss = 0.07604
Step 65740: loss = 0.03613
Step 65745: loss = 0.07547
Step 65750: loss = 0.08376
Step 65755: loss = 0.04995
Step 65760: loss = 0.06327
Step 65765: loss = 0.05093
Step 65770: loss = 0.05886
Step 65775: loss = 0.04323
Step 65780: loss = 0.07478
Step 65785: loss = 0.06296
Step 65790: loss = 0.05052
Step 65795: loss = 0.04972
Step 65800: loss = 0.04765
Step 65805: loss = 0.04157
Step 65810: loss = 0.05335
Step 65815: loss = 0.04401
Step 65820: loss = 0.07119
Step 65825: loss = 0.05809
Step 65830: loss = 0.04664
Step 65835: loss = 0.04233
Step 65840: loss = 0.05693
Step 65845: loss = 0.05987
Step 65850: loss = 0.04354
Step 65855: loss = 0.06280
Step 65860: loss = 0.04324
Step 65865: loss = 0.08946
Step 65870: loss = 0.05240
Step 65875: loss = 0.04818
Step 65880: loss = 0.04361
Step 65885: loss = 0.10222
Step 65890: loss = 0.04657
Step 65895: loss = 0.03256
Step 65900: loss = 0.04294
Step 65905: loss = 0.07605
Step 65910: loss = 0.04348
Training Data Eval:
  Num examples: 49920, Num correct: 49610, Precision @ 1: 0.9938
('Testing Data Eval: EPOCH->', 170)
  Num examples: 9984, Num correct: 7443, Precision @ 1: 0.7455
Step 65915: loss = 0.06356
Step 65920: loss = 0.04631
Step 65925: loss = 0.05229
Step 65930: loss = 0.09033
Step 65935: loss = 0.04356
Step 65940: loss = 0.06270
Step 65945: loss = 0.08360
Step 65950: loss = 0.03840
Step 65955: loss = 0.04212
Step 65960: loss = 0.04518
Step 65965: loss = 0.08810
Step 65970: loss = 0.06384
Step 65975: loss = 0.06153
Step 65980: loss = 0.04799
Step 65985: loss = 0.14962
Step 65990: loss = 0.04327
Step 65995: loss = 0.08211
Step 66000: loss = 0.05592
Step 66005: loss = 0.05411
Step 66010: loss = 0.05929
Step 66015: loss = 0.05849
Step 66020: loss = 0.06500
Step 66025: loss = 0.03201
Step 66030: loss = 0.03882
Step 66035: loss = 0.04624
Step 66040: loss = 0.04974
Step 66045: loss = 0.05937
Step 66050: loss = 0.05857
Step 66055: loss = 0.04283
Step 66060: loss = 0.06397
Step 66065: loss = 0.05573
Step 66070: loss = 0.06908
Step 66075: loss = 0.04748
Step 66080: loss = 0.04618
Step 66085: loss = 0.15574
Step 66090: loss = 0.03916
Step 66095: loss = 0.04694
Step 66100: loss = 0.08700
Step 66105: loss = 0.06975
Step 66110: loss = 0.04949
Step 66115: loss = 0.08817
Step 66120: loss = 0.07030
Step 66125: loss = 0.07161
Step 66130: loss = 0.05052
Step 66135: loss = 0.03688
Step 66140: loss = 0.06191
Step 66145: loss = 0.08259
Step 66150: loss = 0.04876
Step 66155: loss = 0.08688
Step 66160: loss = 0.06266
Step 66165: loss = 0.04613
Step 66170: loss = 0.09638
Step 66175: loss = 0.09271
Step 66180: loss = 0.05429
Step 66185: loss = 0.09897
Step 66190: loss = 0.05921
Step 66195: loss = 0.03538
Step 66200: loss = 0.10623
Step 66205: loss = 0.06296
Step 66210: loss = 0.07150
Step 66215: loss = 0.06997
Step 66220: loss = 0.06186
Step 66225: loss = 0.07221
Step 66230: loss = 0.06070
Step 66235: loss = 0.05050
Step 66240: loss = 0.04223
Step 66245: loss = 0.05351
Step 66250: loss = 0.05581
Step 66255: loss = 0.13549
Step 66260: loss = 0.08635
Step 66265: loss = 0.04782
Step 66270: loss = 0.04355
Step 66275: loss = 0.05312
Step 66280: loss = 0.06777
Step 66285: loss = 0.08734
Step 66290: loss = 0.05054
Step 66295: loss = 0.06019
Step 66300: loss = 0.06214
Training Data Eval:
  Num examples: 49920, Num correct: 49506, Precision @ 1: 0.9917
('Testing Data Eval: EPOCH->', 171)
  Num examples: 9984, Num correct: 7406, Precision @ 1: 0.7418
Step 66305: loss = 0.04779
Step 66310: loss = 0.05154
Step 66315: loss = 0.04841
Step 66320: loss = 0.05856
Step 66325: loss = 0.04395
Step 66330: loss = 0.10845
Step 66335: loss = 0.05280
Step 66340: loss = 0.05026
Step 66345: loss = 0.07377
Step 66350: loss = 0.05225
Step 66355: loss = 0.06738
Step 66360: loss = 0.09140
Step 66365: loss = 0.04241
Step 66370: loss = 0.04725
Step 66375: loss = 0.03552
Step 66380: loss = 0.09788
Step 66385: loss = 0.05115
Step 66390: loss = 0.04537
Step 66395: loss = 0.05257
Step 66400: loss = 0.08687
Step 66405: loss = 0.04408
Step 66410: loss = 0.05809
Step 66415: loss = 0.04650
Step 66420: loss = 0.04458
Step 66425: loss = 0.04328
Step 66430: loss = 0.06685
Step 66435: loss = 0.13635
Step 66440: loss = 0.06869
Step 66445: loss = 0.06476
Step 66450: loss = 0.06498
Step 66455: loss = 0.04883
Step 66460: loss = 0.05334
Step 66465: loss = 0.04423
Step 66470: loss = 0.05839
Step 66475: loss = 0.06307
Step 66480: loss = 0.08472
Step 66485: loss = 0.05041
Step 66490: loss = 0.07483
Step 66495: loss = 0.04144
Step 66500: loss = 0.06720
Step 66505: loss = 0.11755
Step 66510: loss = 0.03719
Step 66515: loss = 0.04996
Step 66520: loss = 0.05288
Step 66525: loss = 0.04629
Step 66530: loss = 0.04701
Step 66535: loss = 0.05914
Step 66540: loss = 0.04942
Step 66545: loss = 0.04695
Step 66550: loss = 0.04085
Step 66555: loss = 0.06145
Step 66560: loss = 0.04952
Step 66565: loss = 0.10834
Step 66570: loss = 0.05094
Step 66575: loss = 0.06053
Step 66580: loss = 0.06686
Step 66585: loss = 0.05253
Step 66590: loss = 0.08513
Step 66595: loss = 0.03683
Step 66600: loss = 0.06754
Step 66605: loss = 0.05690
Step 66610: loss = 0.04848
Step 66615: loss = 0.03888
Step 66620: loss = 0.04440
Step 66625: loss = 0.04192
Step 66630: loss = 0.03700
Step 66635: loss = 0.08063
Step 66640: loss = 0.04318
Step 66645: loss = 0.04078
Step 66650: loss = 0.04084
Step 66655: loss = 0.04208
Step 66660: loss = 0.04309
Step 66665: loss = 0.05482
Step 66670: loss = 0.07342
Step 66675: loss = 0.03665
Step 66680: loss = 0.05221
Step 66685: loss = 0.07292
Step 66690: loss = 0.06861
Training Data Eval:
  Num examples: 49920, Num correct: 49598, Precision @ 1: 0.9935
('Testing Data Eval: EPOCH->', 172)
  Num examples: 9984, Num correct: 7409, Precision @ 1: 0.7421
Step 66695: loss = 0.04496
Step 66700: loss = 0.04843
Step 66705: loss = 0.06688
Step 66710: loss = 0.07507
Step 66715: loss = 0.03817
Step 66720: loss = 0.04729
Step 66725: loss = 0.04638
Step 66730: loss = 0.04567
Step 66735: loss = 0.04544
Step 66740: loss = 0.05717
Step 66745: loss = 0.04792
Step 66750: loss = 0.07143
Step 66755: loss = 0.08323
Step 66760: loss = 0.06486
Step 66765: loss = 0.03377
Step 66770: loss = 0.04034
Step 66775: loss = 0.03531
Step 66780: loss = 0.07360
Step 66785: loss = 0.04373
Step 66790: loss = 0.06087
Step 66795: loss = 0.04178
Step 66800: loss = 0.03873
Step 66805: loss = 0.05723
Step 66810: loss = 0.03927
Step 66815: loss = 0.04337
Step 66820: loss = 0.04656
Step 66825: loss = 0.03199
Step 66830: loss = 0.03942
Step 66835: loss = 0.08188
Step 66840: loss = 0.07088
Step 66845: loss = 0.03483
Step 66850: loss = 0.04204
Step 66855: loss = 0.06325
Step 66860: loss = 0.05748
Step 66865: loss = 0.12571
Step 66870: loss = 0.04463
Step 66875: loss = 0.03890
Step 66880: loss = 0.05182
Step 66885: loss = 0.04703
Step 66890: loss = 0.05791
Step 66895: loss = 0.03999
Step 66900: loss = 0.05812
Step 66905: loss = 0.03731
Step 66910: loss = 0.04586
Step 66915: loss = 0.05521
Step 66920: loss = 0.03312
Step 66925: loss = 0.03742
Step 66930: loss = 0.04367
Step 66935: loss = 0.02997
Step 66940: loss = 0.03516
Step 66945: loss = 0.03928
Step 66950: loss = 0.03991
Step 66955: loss = 0.04789
Step 66960: loss = 0.05817
Step 66965: loss = 0.05915
Step 66970: loss = 0.04281
Step 66975: loss = 0.03479
Step 66980: loss = 0.07669
Step 66985: loss = 0.12188
Step 66990: loss = 0.05174
Step 66995: loss = 0.06767
Step 67000: loss = 0.06394
Step 67005: loss = 0.03682
Step 67010: loss = 0.04428
Step 67015: loss = 0.04659
Step 67020: loss = 0.07084
Step 67025: loss = 0.04148
Step 67030: loss = 0.03089
Step 67035: loss = 0.06075
Step 67040: loss = 0.05195
Step 67045: loss = 0.04221
Step 67050: loss = 0.04838
Step 67055: loss = 0.04748
Step 67060: loss = 0.08491
Step 67065: loss = 0.04261
Step 67070: loss = 0.05760
Step 67075: loss = 0.06549
Step 67080: loss = 0.07001
Training Data Eval:
  Num examples: 49920, Num correct: 49666, Precision @ 1: 0.9949
('Testing Data Eval: EPOCH->', 173)
  Num examples: 9984, Num correct: 7537, Precision @ 1: 0.7549
Step 67085: loss = 0.05262
Step 67090: loss = 0.04791
Step 67095: loss = 0.05406
Step 67100: loss = 0.04668
Step 67105: loss = 0.05779
Step 67110: loss = 0.04394
Step 67115: loss = 0.04460
Step 67120: loss = 0.04071
Step 67125: loss = 0.05377
Step 67130: loss = 0.03711
Step 67135: loss = 0.03824
Step 67140: loss = 0.06821
Step 67145: loss = 0.05162
Step 67150: loss = 0.03529
Step 67155: loss = 0.07792
Step 67160: loss = 0.04487
Step 67165: loss = 0.05494
Step 67170: loss = 0.08249
Step 67175: loss = 0.03398
Step 67180: loss = 0.05825
Step 67185: loss = 0.04802
Step 67190: loss = 0.03350
Step 67195: loss = 0.05041
Step 67200: loss = 0.03800
Step 67205: loss = 0.03797
Step 67210: loss = 0.07823
Step 67215: loss = 0.04428
Step 67220: loss = 0.05973
Step 67225: loss = 0.04047
Step 67230: loss = 0.03698
Step 67235: loss = 0.04393
Step 67240: loss = 0.05989
Step 67245: loss = 0.03632
Step 67250: loss = 0.03313
Step 67255: loss = 0.09550
Step 67260: loss = 0.03673
Step 67265: loss = 0.03603
Step 67270: loss = 0.04840
Step 67275: loss = 0.05331
Step 67280: loss = 0.03639
Step 67285: loss = 0.05702
Step 67290: loss = 0.05533
Step 67295: loss = 0.04675
Step 67300: loss = 0.07948
Step 67305: loss = 0.05227
Step 67310: loss = 0.08228
Step 67315: loss = 0.04701
Step 67320: loss = 0.07662
Step 67325: loss = 0.05557
Step 67330: loss = 0.07295
Step 67335: loss = 0.04884
Step 67340: loss = 0.06948
Step 67345: loss = 0.03518
Step 67350: loss = 0.04828
Step 67355: loss = 0.03170
Step 67360: loss = 0.03674
Step 67365: loss = 0.03804
Step 67370: loss = 0.04137
Step 67375: loss = 0.04027
Step 67380: loss = 0.03734
Step 67385: loss = 0.06443
Step 67390: loss = 0.04284
Step 67395: loss = 0.04021
Step 67400: loss = 0.05298
Step 67405: loss = 0.03888
Step 67410: loss = 0.05111
Step 67415: loss = 0.07146
Step 67420: loss = 0.05469
Step 67425: loss = 0.06530
Step 67430: loss = 0.05063
Step 67435: loss = 0.06535
Step 67440: loss = 0.03249
Step 67445: loss = 0.04115
Step 67450: loss = 0.03437
Step 67455: loss = 0.05268
Step 67460: loss = 0.05406
Step 67465: loss = 0.06154
Step 67470: loss = 0.04892
Training Data Eval:
  Num examples: 49920, Num correct: 49663, Precision @ 1: 0.9949
('Testing Data Eval: EPOCH->', 174)
  Num examples: 9984, Num correct: 7423, Precision @ 1: 0.7435
Step 67475: loss = 0.06015
Step 67480: loss = 0.05085
Step 67485: loss = 0.04484
Step 67490: loss = 0.03482
Step 67495: loss = 0.03762
Step 67500: loss = 0.03533
Step 67505: loss = 0.03496
Step 67510: loss = 0.05035
Step 67515: loss = 0.03585
Step 67520: loss = 0.06558
Step 67525: loss = 0.05072
Step 67530: loss = 0.03666
Step 67535: loss = 0.03111
Step 67540: loss = 0.05443
Step 67545: loss = 0.04443
Step 67550: loss = 0.04172
Step 67555: loss = 0.03125
Step 67560: loss = 0.04041
Step 67565: loss = 0.03144
Step 67570: loss = 0.03545
Step 67575: loss = 0.06124
Step 67580: loss = 0.06480
Step 67585: loss = 0.04794
Step 67590: loss = 0.04049
Step 67595: loss = 0.06445
Step 67600: loss = 0.08422
Step 67605: loss = 0.03579
Step 67610: loss = 0.04737
Step 67615: loss = 0.03666
Step 67620: loss = 0.04841
Step 67625: loss = 0.05477
Step 67630: loss = 0.04453
Step 67635: loss = 0.05451
Step 67640: loss = 0.08720
Step 67645: loss = 0.10818
Step 67650: loss = 0.04862
Step 67655: loss = 0.07169
Step 67660: loss = 0.03428
Step 67665: loss = 0.04027
Step 67670: loss = 0.07153
Step 67675: loss = 0.03113
Step 67680: loss = 0.04640
Step 67685: loss = 0.07099
Step 67690: loss = 0.05365
Step 67695: loss = 0.07134
Step 67700: loss = 0.07966
Step 67705: loss = 0.04594
Step 67710: loss = 0.03339
Step 67715: loss = 0.06160
Step 67720: loss = 0.05713
Step 67725: loss = 0.07291
Step 67730: loss = 0.05644
Step 67735: loss = 0.04210
Step 67740: loss = 0.04703
Step 67745: loss = 0.06013
Step 67750: loss = 0.05824
Step 67755: loss = 0.03807
Step 67760: loss = 0.04194
Step 67765: loss = 0.04026
Step 67770: loss = 0.05125
Step 67775: loss = 0.04080
Step 67780: loss = 0.04640
Step 67785: loss = 0.04684
Step 67790: loss = 0.06595
Step 67795: loss = 0.04596
Step 67800: loss = 0.06042
Step 67805: loss = 0.04673
Step 67810: loss = 0.11752
Step 67815: loss = 0.05061
Step 67820: loss = 0.05637
Step 67825: loss = 0.05071
Step 67830: loss = 0.06857
Step 67835: loss = 0.09870
Step 67840: loss = 0.05540
Step 67845: loss = 0.04425
Step 67850: loss = 0.10491
Step 67855: loss = 0.04112
Step 67860: loss = 0.04226
Training Data Eval:
  Num examples: 49920, Num correct: 49542, Precision @ 1: 0.9924
('Testing Data Eval: EPOCH->', 175)
  Num examples: 9984, Num correct: 7444, Precision @ 1: 0.7456
Step 67865: loss = 0.04283
Step 67870: loss = 0.03509
Step 67875: loss = 0.06112
Step 67880: loss = 0.04257
Step 67885: loss = 0.09082
Step 67890: loss = 0.05334
Step 67895: loss = 0.04665
Step 67900: loss = 0.04724
Step 67905: loss = 0.04423
Step 67910: loss = 0.06975
Step 67915: loss = 0.05760
Step 67920: loss = 0.06572
Step 67925: loss = 0.05148
Step 67930: loss = 0.06005
Step 67935: loss = 0.11023
Step 67940: loss = 0.06062
Step 67945: loss = 0.04797
Step 67950: loss = 0.05610
Step 67955: loss = 0.05865
Step 67960: loss = 0.05726
Step 67965: loss = 0.04173
Step 67970: loss = 0.04362
Step 67975: loss = 0.05951
Step 67980: loss = 0.05308
Step 67985: loss = 0.05261
Step 67990: loss = 0.06431
Step 67995: loss = 0.04603
Step 68000: loss = 0.04954
Step 68005: loss = 0.09635
Step 68010: loss = 0.06711
Step 68015: loss = 0.06731
Step 68020: loss = 0.03317
Step 68025: loss = 0.04418
Step 68030: loss = 0.08310
Step 68035: loss = 0.04066
Step 68040: loss = 0.04764
Step 68045: loss = 0.05646
Step 68050: loss = 0.10216
Step 68055: loss = 0.05834
Step 68060: loss = 0.03532
Step 68065: loss = 0.03789
Step 68070: loss = 0.06233
Step 68075: loss = 0.04136
Step 68080: loss = 0.06029
Step 68085: loss = 0.07706
Step 68090: loss = 0.05031
Step 68095: loss = 0.02892
Step 68100: loss = 0.04053
Step 68105: loss = 0.05039
Step 68110: loss = 0.04526
Step 68115: loss = 0.07008
Step 68120: loss = 0.05573
Step 68125: loss = 0.07647
Step 68130: loss = 0.04212
Step 68135: loss = 0.07648
Step 68140: loss = 0.03630
Step 68145: loss = 0.07334
Step 68150: loss = 0.03377
Step 68155: loss = 0.03943
Step 68160: loss = 0.06454
Step 68165: loss = 0.04926
Step 68170: loss = 0.03411
Step 68175: loss = 0.04102
Step 68180: loss = 0.05779
Step 68185: loss = 0.04343
Step 68190: loss = 0.04385
Step 68195: loss = 0.09999
Step 68200: loss = 0.07600
Step 68205: loss = 0.04779
Step 68210: loss = 0.03193
Step 68215: loss = 0.05000
Step 68220: loss = 0.07682
Step 68225: loss = 0.06523
Step 68230: loss = 0.04530
Step 68235: loss = 0.04830
Step 68240: loss = 0.03390
Step 68245: loss = 0.03338
Step 68250: loss = 0.06973
Training Data Eval:
  Num examples: 49920, Num correct: 49555, Precision @ 1: 0.9927
('Testing Data Eval: EPOCH->', 176)
  Num examples: 9984, Num correct: 7542, Precision @ 1: 0.7554
Step 68255: loss = 0.06070
Step 68260: loss = 0.03722
Step 68265: loss = 0.04379
Step 68270: loss = 0.03276
Step 68275: loss = 0.08838
Step 68280: loss = 0.04827
Step 68285: loss = 0.05036
Step 68290: loss = 0.07674
Step 68295: loss = 0.07415
Step 68300: loss = 0.03227
Step 68305: loss = 0.05705
Step 68310: loss = 0.02830
Step 68315: loss = 0.05337
Step 68320: loss = 0.05080
Step 68325: loss = 0.03221
Step 68330: loss = 0.04614
Step 68335: loss = 0.05080
Step 68340: loss = 0.06442
Step 68345: loss = 0.04918
Step 68350: loss = 0.06226
Step 68355: loss = 0.05296
Step 68360: loss = 0.03258
Step 68365: loss = 0.06820
Step 68370: loss = 0.04262
Step 68375: loss = 0.05185
Step 68380: loss = 0.04866
Step 68385: loss = 0.03475
Step 68390: loss = 0.06438
Step 68395: loss = 0.05056
Step 68400: loss = 0.08598
Step 68405: loss = 0.05195
Step 68410: loss = 0.06285
Step 68415: loss = 0.05585
Step 68420: loss = 0.09062
Step 68425: loss = 0.06646
Step 68430: loss = 0.10810
Step 68435: loss = 0.04198
Step 68440: loss = 0.07301
Step 68445: loss = 0.10462
Step 68450: loss = 0.05144
Step 68455: loss = 0.04406
Step 68460: loss = 0.09131
Step 68465: loss = 0.04669
Step 68470: loss = 0.08269
Step 68475: loss = 0.06169
Step 68480: loss = 0.05834
Step 68485: loss = 0.06386
Step 68490: loss = 0.04411
Step 68495: loss = 0.05329
Step 68500: loss = 0.09159
Step 68505: loss = 0.07495
Step 68510: loss = 0.07906
Step 68515: loss = 0.04876
Step 68520: loss = 0.04492
Step 68525: loss = 0.05955
Step 68530: loss = 0.06457
Step 68535: loss = 0.03851
Step 68540: loss = 0.09249
Step 68545: loss = 0.07211
Step 68550: loss = 0.06142
Step 68555: loss = 0.06988
Step 68560: loss = 0.10625
Step 68565: loss = 0.06296
Step 68570: loss = 0.05544
Step 68575: loss = 0.09696
Step 68580: loss = 0.07891
Step 68585: loss = 0.06508
Step 68590: loss = 0.04638
Step 68595: loss = 0.04600
Step 68600: loss = 0.04135
Step 68605: loss = 0.04869
Step 68610: loss = 0.10681
Step 68615: loss = 0.10169
Step 68620: loss = 0.02739
Step 68625: loss = 0.10018
Step 68630: loss = 0.05826
Step 68635: loss = 0.05897
Step 68640: loss = 0.06323
Training Data Eval:
  Num examples: 49920, Num correct: 49576, Precision @ 1: 0.9931
('Testing Data Eval: EPOCH->', 177)
  Num examples: 9984, Num correct: 7454, Precision @ 1: 0.7466
Step 68645: loss = 0.04786
Step 68650: loss = 0.07724
Step 68655: loss = 0.03422
Step 68660: loss = 0.14617
Step 68665: loss = 0.04385
Step 68670: loss = 0.03612
Step 68675: loss = 0.07542
Step 68680: loss = 0.07230
Step 68685: loss = 0.04128
Step 68690: loss = 0.05384
Step 68695: loss = 0.04426
Step 68700: loss = 0.06885
Step 68705: loss = 0.03245
Step 68710: loss = 0.04482
Step 68715: loss = 0.05512
Step 68720: loss = 0.10223
Step 68725: loss = 0.05923
Step 68730: loss = 0.03676
Step 68735: loss = 0.05773
Step 68740: loss = 0.04085
Step 68745: loss = 0.13097
Step 68750: loss = 0.09441
Step 68755: loss = 0.05614
Step 68760: loss = 0.08936
Step 68765: loss = 0.05030
Step 68770: loss = 0.06172
Step 68775: loss = 0.05196
Step 68780: loss = 0.06047
Step 68785: loss = 0.05945
Step 68790: loss = 0.07712
Step 68795: loss = 0.05087
Step 68800: loss = 0.06723
Step 68805: loss = 0.06390
Step 68810: loss = 0.03351
Step 68815: loss = 0.09278
Step 68820: loss = 0.07460
Step 68825: loss = 0.08389
Step 68830: loss = 0.04621
Step 68835: loss = 0.04814
Step 68840: loss = 0.05606
Step 68845: loss = 0.06267
Step 68850: loss = 0.08382
Step 68855: loss = 0.04400
Step 68860: loss = 0.06041
Step 68865: loss = 0.03370
Step 68870: loss = 0.03006
Step 68875: loss = 0.03984
Step 68880: loss = 0.05016
Step 68885: loss = 0.06873
Step 68890: loss = 0.08930
Step 68895: loss = 0.08389
Step 68900: loss = 0.03183
Step 68905: loss = 0.12311
Step 68910: loss = 0.11232
Step 68915: loss = 0.05961
Step 68920: loss = 0.04666
Step 68925: loss = 0.06506
Step 68930: loss = 0.06553
Step 68935: loss = 0.07339
Step 68940: loss = 0.04272
Step 68945: loss = 0.07411
Step 68950: loss = 0.05058
Step 68955: loss = 0.03733
Step 68960: loss = 0.05462
Step 68965: loss = 0.07970
Step 68970: loss = 0.06426
Step 68975: loss = 0.06407
Step 68980: loss = 0.05195
Step 68985: loss = 0.06672
Step 68990: loss = 0.04361
Step 68995: loss = 0.07948
Step 69000: loss = 0.06479
Step 69005: loss = 0.05564
Step 69010: loss = 0.04486
Step 69015: loss = 0.03637
Step 69020: loss = 0.04826
Step 69025: loss = 0.05883
Step 69030: loss = 0.03828
Training Data Eval:
  Num examples: 49920, Num correct: 49691, Precision @ 1: 0.9954
('Testing Data Eval: EPOCH->', 178)
  Num examples: 9984, Num correct: 7497, Precision @ 1: 0.7509
Step 69035: loss = 0.04139
Step 69040: loss = 0.03705
Step 69045: loss = 0.04322
Step 69050: loss = 0.05417
Step 69055: loss = 0.06045
Step 69060: loss = 0.04309
Step 69065: loss = 0.03994
Step 69070: loss = 0.04494
Step 69075: loss = 0.05009
Step 69080: loss = 0.03237
Step 69085: loss = 0.05133
Step 69090: loss = 0.04765
Step 69095: loss = 0.04850
Step 69100: loss = 0.03822
Step 69105: loss = 0.03734
Step 69110: loss = 0.09178
Step 69115: loss = 0.09482
Step 69120: loss = 0.03340
Step 69125: loss = 0.04971
Step 69130: loss = 0.05164
Step 69135: loss = 0.04460
Step 69140: loss = 0.04809
Step 69145: loss = 0.03939
Step 69150: loss = 0.05002
Step 69155: loss = 0.03540
Step 69160: loss = 0.07041
Step 69165: loss = 0.04256
Step 69170: loss = 0.05411
Step 69175: loss = 0.05364
Step 69180: loss = 0.06011
Step 69185: loss = 0.03750
Step 69190: loss = 0.05596
Step 69195: loss = 0.03719
Step 69200: loss = 0.04844
Step 69205: loss = 0.03925
Step 69210: loss = 0.03192
Step 69215: loss = 0.04290
Step 69220: loss = 0.03388
Step 69225: loss = 0.04249
Step 69230: loss = 0.06858
Step 69235: loss = 0.04027
Step 69240: loss = 0.05504
Step 69245: loss = 0.05681
Step 69250: loss = 0.07685
Step 69255: loss = 0.08361
Step 69260: loss = 0.03448
Step 69265: loss = 0.08648
Step 69270: loss = 0.08266
Step 69275: loss = 0.07567
Step 69280: loss = 0.05493
Step 69285: loss = 0.06261
Step 69290: loss = 0.03649
Step 69295: loss = 0.04091
Step 69300: loss = 0.05060
Step 69305: loss = 0.03560
Step 69310: loss = 0.03304
Step 69315: loss = 0.04744
Step 69320: loss = 0.03944
Step 69325: loss = 0.03744
Step 69330: loss = 0.04542
Step 69335: loss = 0.06355
Step 69340: loss = 0.03994
Step 69345: loss = 0.03299
Step 69350: loss = 0.03971
Step 69355: loss = 0.04465
Step 69360: loss = 0.04676
Step 69365: loss = 0.04601
Step 69370: loss = 0.04398
Step 69375: loss = 0.05946
Step 69380: loss = 0.05072
Step 69385: loss = 0.05248
Step 69390: loss = 0.08621
Step 69395: loss = 0.04114
Step 69400: loss = 0.05535
Step 69405: loss = 0.07116
Step 69410: loss = 0.08559
Step 69415: loss = 0.03150
Step 69420: loss = 0.03788
Training Data Eval:
  Num examples: 49920, Num correct: 49629, Precision @ 1: 0.9942
('Testing Data Eval: EPOCH->', 179)
  Num examples: 9984, Num correct: 7446, Precision @ 1: 0.7458
Step 69425: loss = 0.06413
Step 69430: loss = 0.10208
Step 69435: loss = 0.05701
Step 69440: loss = 0.02860
Step 69445: loss = 0.04790
Step 69450: loss = 0.03391
Step 69455: loss = 0.07997
Step 69460: loss = 0.04788
Step 69465: loss = 0.03977
Step 69470: loss = 0.05165
Step 69475: loss = 0.08867
Step 69480: loss = 0.06118
Step 69485: loss = 0.04026
Step 69490: loss = 0.06137
Step 69495: loss = 0.05006
Step 69500: loss = 0.03451
Step 69505: loss = 0.03489
Step 69510: loss = 0.07218
Step 69515: loss = 0.03456
Step 69520: loss = 0.04302
Step 69525: loss = 0.04213
Step 69530: loss = 0.03541
Step 69535: loss = 0.04867
Step 69540: loss = 0.10484
Step 69545: loss = 0.05187
Step 69550: loss = 0.03732
Step 69555: loss = 0.07649
Step 69560: loss = 0.04258
Step 69565: loss = 0.03037
Step 69570: loss = 0.03848
Step 69575: loss = 0.03160
Step 69580: loss = 0.05315
Step 69585: loss = 0.04190
Step 69590: loss = 0.03615
Step 69595: loss = 0.07775
Step 69600: loss = 0.05223
Step 69605: loss = 0.07915
Step 69610: loss = 0.03134
Step 69615: loss = 0.06048
Step 69620: loss = 0.06715
Step 69625: loss = 0.03889
Step 69630: loss = 0.03720
Step 69635: loss = 0.04308
Step 69640: loss = 0.04132
Step 69645: loss = 0.03752
Step 69650: loss = 0.04340
Step 69655: loss = 0.05896
Step 69660: loss = 0.05879
Step 69665: loss = 0.04539
Step 69670: loss = 0.04773
Step 69675: loss = 0.06046
Step 69680: loss = 0.04445
Step 69685: loss = 0.08979
Step 69690: loss = 0.03850
Step 69695: loss = 0.03813
Step 69700: loss = 0.03948
Step 69705: loss = 0.05793
Step 69710: loss = 0.04512
Step 69715: loss = 0.08078
Step 69720: loss = 0.03877
Step 69725: loss = 0.03788
Step 69730: loss = 0.03028
Step 69735: loss = 0.09614
Step 69740: loss = 0.07762
Step 69745: loss = 0.04944
Step 69750: loss = 0.07637
Step 69755: loss = 0.03659
Step 69760: loss = 0.05593
Step 69765: loss = 0.03659
Step 69770: loss = 0.04421
Step 69775: loss = 0.03261
Step 69780: loss = 0.05317
Step 69785: loss = 0.04113
Step 69790: loss = 0.03534
Step 69795: loss = 0.05008
Step 69800: loss = 0.05635
Step 69805: loss = 0.03273
Step 69810: loss = 0.07316
Training Data Eval:
  Num examples: 49920, Num correct: 49661, Precision @ 1: 0.9948
('Testing Data Eval: EPOCH->', 180)
  Num examples: 9984, Num correct: 7560, Precision @ 1: 0.7572
Step 69815: loss = 0.03147
Step 69820: loss = 0.07560
Step 69825: loss = 0.06646
Step 69830: loss = 0.05926
Step 69835: loss = 0.04369
Step 69840: loss = 0.03324
Step 69845: loss = 0.10070
Step 69850: loss = 0.05048
Step 69855: loss = 0.03239
Step 69860: loss = 0.05175
Step 69865: loss = 0.04982
Step 69870: loss = 0.03450
Step 69875: loss = 0.09978
Step 69880: loss = 0.10629
Step 69885: loss = 0.04667
Step 69890: loss = 0.05659
Step 69895: loss = 0.03363
Step 69900: loss = 0.04707
Step 69905: loss = 0.08014
Step 69910: loss = 0.06927
Step 69915: loss = 0.04641
Step 69920: loss = 0.06779
Step 69925: loss = 0.03311
Step 69930: loss = 0.03823
Step 69935: loss = 0.03441
Step 69940: loss = 0.03960
Step 69945: loss = 0.03862
Step 69950: loss = 0.06805
Step 69955: loss = 0.08843
Step 69960: loss = 0.05345
Step 69965: loss = 0.06117
Step 69970: loss = 0.04206
Step 69975: loss = 0.04726
Step 69980: loss = 0.04279
Step 69985: loss = 0.03869
Step 69990: loss = 0.10737
Step 69995: loss = 0.04389
Step 70000: loss = 0.04446
Step 70005: loss = 0.04290
Step 70010: loss = 0.04196
Step 70015: loss = 0.04059
Step 70020: loss = 0.05522
Step 70025: loss = 0.04789
Step 70030: loss = 0.04361
Step 70035: loss = 0.03785
Step 70040: loss = 0.05306
Step 70045: loss = 0.03861
Step 70050: loss = 0.04473
Step 70055: loss = 0.03189
Step 70060: loss = 0.03824
Step 70065: loss = 0.03691
Step 70070: loss = 0.07199
Step 70075: loss = 0.07918
Step 70080: loss = 0.04013
Step 70085: loss = 0.04401
Step 70090: loss = 0.04029
Step 70095: loss = 0.04045
Step 70100: loss = 0.03119
Step 70105: loss = 0.03688
Step 70110: loss = 0.08432
Step 70115: loss = 0.04175
Step 70120: loss = 0.04150
Step 70125: loss = 0.03734
Step 70130: loss = 0.02901
Step 70135: loss = 0.03884
Step 70140: loss = 0.03880
Step 70145: loss = 0.03730
Step 70150: loss = 0.03385
Step 70155: loss = 0.03913
Step 70160: loss = 0.07817
Step 70165: loss = 0.05439
Step 70170: loss = 0.05836
Step 70175: loss = 0.04419
Step 70180: loss = 0.06122
Step 70185: loss = 0.04485
Step 70190: loss = 0.04712
Step 70195: loss = 0.05624
Step 70200: loss = 0.05227
Training Data Eval:
  Num examples: 49920, Num correct: 49703, Precision @ 1: 0.9957
('Testing Data Eval: EPOCH->', 181)
  Num examples: 9984, Num correct: 7494, Precision @ 1: 0.7506
Step 70205: loss = 0.05741
Step 70210: loss = 0.03865
Step 70215: loss = 0.06679
Step 70220: loss = 0.05136
Step 70225: loss = 0.05547
Step 70230: loss = 0.03742
Step 70235: loss = 0.03480
Step 70240: loss = 0.05265
Step 70245: loss = 0.04405
Step 70250: loss = 0.04259
Step 70255: loss = 0.04268
Step 70260: loss = 0.03725
Step 70265: loss = 0.04625
Step 70270: loss = 0.04012
Step 70275: loss = 0.05676
Step 70280: loss = 0.04702
Step 70285: loss = 0.09394
Step 70290: loss = 0.04820
Step 70295: loss = 0.03976
Step 70300: loss = 0.05272
Step 70305: loss = 0.07345
Step 70310: loss = 0.05590
Step 70315: loss = 0.05069
Step 70320: loss = 0.02784
Step 70325: loss = 0.04288
Step 70330: loss = 0.05572
Step 70335: loss = 0.08434
Step 70340: loss = 0.03375
Step 70345: loss = 0.03283
Step 70350: loss = 0.03950
Step 70355: loss = 0.04051
Step 70360: loss = 0.04695
Step 70365: loss = 0.04088
Step 70370: loss = 0.04338
Step 70375: loss = 0.03561
Step 70380: loss = 0.05919
Step 70385: loss = 0.03622
Step 70390: loss = 0.06429
Step 70395: loss = 0.03333
Step 70400: loss = 0.04753
Step 70405: loss = 0.05190
Step 70410: loss = 0.03391
Step 70415: loss = 0.04804
Step 70420: loss = 0.04892
Step 70425: loss = 0.03807
Step 70430: loss = 0.03907
Step 70435: loss = 0.04951
Step 70440: loss = 0.03437
Step 70445: loss = 0.03890
Step 70450: loss = 0.03782
Step 70455: loss = 0.03951
Step 70460: loss = 0.04644
Step 70465: loss = 0.03295
Step 70470: loss = 0.04128
Step 70475: loss = 0.09063
Step 70480: loss = 0.05356
Step 70485: loss = 0.05661
Step 70490: loss = 0.04746
Step 70495: loss = 0.06355
Step 70500: loss = 0.08929
Step 70505: loss = 0.05785
Step 70510: loss = 0.05972
Step 70515: loss = 0.09416
Step 70520: loss = 0.05359
Step 70525: loss = 0.04689
Step 70530: loss = 0.08232
Step 70535: loss = 0.05263
Step 70540: loss = 0.06126
Step 70545: loss = 0.04774
Step 70550: loss = 0.04649
Step 70555: loss = 0.04713
Step 70560: loss = 0.04197
Step 70565: loss = 0.03026
Step 70570: loss = 0.03861
Step 70575: loss = 0.04931
Step 70580: loss = 0.08085
Step 70585: loss = 0.04185
Step 70590: loss = 0.03250
Training Data Eval:
  Num examples: 49920, Num correct: 49653, Precision @ 1: 0.9947
('Testing Data Eval: EPOCH->', 182)
  Num examples: 9984, Num correct: 7458, Precision @ 1: 0.7470
Step 70595: loss = 0.04340
Step 70600: loss = 0.04016
Step 70605: loss = 0.04440
Step 70610: loss = 0.03620
Step 70615: loss = 0.04477
Step 70620: loss = 0.04242
Step 70625: loss = 0.03570
Step 70630: loss = 0.03849
Step 70635: loss = 0.05766
Step 70640: loss = 0.05378
Step 70645: loss = 0.05012
Step 70650: loss = 0.11126
Step 70655: loss = 0.04933
Step 70660: loss = 0.03725
Step 70665: loss = 0.10262
Step 70670: loss = 0.04673
Step 70675: loss = 0.04978
Step 70680: loss = 0.04596
Step 70685: loss = 0.07813
Step 70690: loss = 0.05061
Step 70695: loss = 0.05911
Step 70700: loss = 0.07904
Step 70705: loss = 0.05944
Step 70710: loss = 0.06263
Step 70715: loss = 0.04015
Step 70720: loss = 0.08847
Step 70725: loss = 0.05762
Step 70730: loss = 0.03176
Step 70735: loss = 0.05768
Step 70740: loss = 0.03729
Step 70745: loss = 0.04847
Step 70750: loss = 0.05373
Step 70755: loss = 0.06492
Step 70760: loss = 0.07126
Step 70765: loss = 0.04564
Step 70770: loss = 0.06407
Step 70775: loss = 0.04615
Step 70780: loss = 0.03893
Step 70785: loss = 0.05532
Step 70790: loss = 0.04789
Step 70795: loss = 0.06734
Step 70800: loss = 0.04971
Step 70805: loss = 0.04325
Step 70810: loss = 0.07227
Step 70815: loss = 0.05480
Step 70820: loss = 0.07934
Step 70825: loss = 0.04610
Step 70830: loss = 0.06432
Step 70835: loss = 0.04638
Step 70840: loss = 0.07693
Step 70845: loss = 0.04318
Step 70850: loss = 0.03487
Step 70855: loss = 0.04313
Step 70860: loss = 0.03616
Step 70865: loss = 0.09762
Step 70870: loss = 0.09428
Step 70875: loss = 0.05467
Step 70880: loss = 0.04928
Step 70885: loss = 0.05445
Step 70890: loss = 0.04111
Step 70895: loss = 0.03566
Step 70900: loss = 0.04961
Step 70905: loss = 0.09682
Step 70910: loss = 0.04168
Step 70915: loss = 0.04192
Step 70920: loss = 0.05060
Step 70925: loss = 0.03676
Step 70930: loss = 0.05426
Step 70935: loss = 0.05363
Step 70940: loss = 0.06422
Step 70945: loss = 0.05403
Step 70950: loss = 0.05442
Step 70955: loss = 0.06176
Step 70960: loss = 0.05555
Step 70965: loss = 0.06461
Step 70970: loss = 0.04087
Step 70975: loss = 0.04659
Step 70980: loss = 0.05283
Training Data Eval:
  Num examples: 49920, Num correct: 49676, Precision @ 1: 0.9951
('Testing Data Eval: EPOCH->', 183)
  Num examples: 9984, Num correct: 7401, Precision @ 1: 0.7413
Step 70985: loss = 0.03509
Step 70990: loss = 0.03364
Step 70995: loss = 0.05844
Step 71000: loss = 0.04612
Step 71005: loss = 0.05213
Step 71010: loss = 0.04376
Step 71015: loss = 0.04820
Step 71020: loss = 0.07626
Step 71025: loss = 0.03542
Step 71030: loss = 0.04029
Step 71035: loss = 0.04393
Step 71040: loss = 0.05621
Step 71045: loss = 0.06693
Step 71050: loss = 0.06113
Step 71055: loss = 0.04978
Step 71060: loss = 0.03037
Step 71065: loss = 0.03383
Step 71070: loss = 0.08829
Step 71075: loss = 0.04532
Step 71080: loss = 0.07133
Step 71085: loss = 0.03506
Step 71090: loss = 0.04046
Step 71095: loss = 0.03377
Step 71100: loss = 0.03127
Step 71105: loss = 0.03127
Step 71110: loss = 0.03840
Step 71115: loss = 0.04033
Step 71120: loss = 0.05178
Step 71125: loss = 0.06718
Step 71130: loss = 0.05271
Step 71135: loss = 0.05153
Step 71140: loss = 0.03795
Step 71145: loss = 0.05978
Step 71150: loss = 0.04276
Step 71155: loss = 0.03215
Step 71160: loss = 0.06802
Step 71165: loss = 0.04316
Step 71170: loss = 0.03127
Step 71175: loss = 0.05272
Step 71180: loss = 0.07101
Step 71185: loss = 0.09239
Step 71190: loss = 0.03877
Step 71195: loss = 0.03283
Step 71200: loss = 0.06654
Step 71205: loss = 0.04396
Step 71210: loss = 0.03753
Step 71215: loss = 0.03889
Step 71220: loss = 0.11077
Step 71225: loss = 0.05565
Step 71230: loss = 0.04870
Step 71235: loss = 0.06425
Step 71240: loss = 0.04086
Step 71245: loss = 0.08092
Step 71250: loss = 0.06045
Step 71255: loss = 0.04490
Step 71260: loss = 0.08008
Step 71265: loss = 0.05625
Step 71270: loss = 0.09209
Step 71275: loss = 0.05188
Step 71280: loss = 0.03947
Step 71285: loss = 0.04289
Step 71290: loss = 0.10535
Step 71295: loss = 0.03937
Step 71300: loss = 0.04035
Step 71305: loss = 0.08289
Step 71310: loss = 0.04343
Step 71315: loss = 0.06364
Step 71320: loss = 0.04867
Step 71325: loss = 0.03826
Step 71330: loss = 0.05370
Step 71335: loss = 0.06863
Step 71340: loss = 0.08694
Step 71345: loss = 0.04625
Step 71350: loss = 0.03925
Step 71355: loss = 0.05853
Step 71360: loss = 0.03208
Step 71365: loss = 0.04113
Step 71370: loss = 0.03945
Training Data Eval:
  Num examples: 49920, Num correct: 49563, Precision @ 1: 0.9928
('Testing Data Eval: EPOCH->', 184)
  Num examples: 9984, Num correct: 7550, Precision @ 1: 0.7562
Step 71375: loss = 0.05256
Step 71380: loss = 0.03663
Step 71385: loss = 0.07759
Step 71390: loss = 0.03965
Step 71395: loss = 0.05748
Step 71400: loss = 0.03164
Step 71405: loss = 0.04777
Step 71410: loss = 0.06253
Step 71415: loss = 0.06203
Step 71420: loss = 0.03738
Step 71425: loss = 0.07368
Step 71430: loss = 0.05313
Step 71435: loss = 0.02880
Step 71440: loss = 0.03185
Step 71445: loss = 0.05421
Step 71450: loss = 0.04177
Step 71455: loss = 0.09997
Step 71460: loss = 0.05062
Step 71465: loss = 0.05196
Step 71470: loss = 0.03546
Step 71475: loss = 0.09029
Step 71480: loss = 0.05090
Step 71485: loss = 0.03463
Step 71490: loss = 0.05515
Step 71495: loss = 0.09331
Step 71500: loss = 0.03034
Step 71505: loss = 0.04361
Step 71510: loss = 0.05128
Step 71515: loss = 0.04180
Step 71520: loss = 0.05955
Step 71525: loss = 0.03875
Step 71530: loss = 0.04657
Step 71535: loss = 0.06774
Step 71540: loss = 0.03611
Step 71545: loss = 0.03388
Step 71550: loss = 0.04439
Step 71555: loss = 0.04227
Step 71560: loss = 0.05142
Step 71565: loss = 0.04534
Step 71570: loss = 0.03284
Step 71575: loss = 0.04582
Step 71580: loss = 0.03266
Step 71585: loss = 0.05265
Step 71590: loss = 0.09237
Step 71595: loss = 0.05530
Step 71600: loss = 0.03445
Step 71605: loss = 0.04367
Step 71610: loss = 0.02774
Step 71615: loss = 0.07572
Step 71620: loss = 0.05943
Step 71625: loss = 0.05093
Step 71630: loss = 0.03857
Step 71635: loss = 0.03694
Step 71640: loss = 0.05376
Step 71645: loss = 0.06407
Step 71650: loss = 0.05422
Step 71655: loss = 0.04045
Step 71660: loss = 0.03351
Step 71665: loss = 0.05666
Step 71670: loss = 0.03720
Step 71675: loss = 0.05563
Step 71680: loss = 0.06770
Step 71685: loss = 0.05251
Step 71690: loss = 0.04286
Step 71695: loss = 0.03092
Step 71700: loss = 0.05897
Step 71705: loss = 0.05604
Step 71710: loss = 0.02955
Step 71715: loss = 0.06504
Step 71720: loss = 0.04437
Step 71725: loss = 0.03788
Step 71730: loss = 0.04375
Step 71735: loss = 0.03801
Step 71740: loss = 0.04651
Step 71745: loss = 0.04557
Step 71750: loss = 0.04667
Step 71755: loss = 0.05207
Step 71760: loss = 0.05973
Training Data Eval:
  Num examples: 49920, Num correct: 49650, Precision @ 1: 0.9946
('Testing Data Eval: EPOCH->', 185)
  Num examples: 9984, Num correct: 7447, Precision @ 1: 0.7459
Step 71765: loss = 0.04024
Step 71770: loss = 0.04664
Step 71775: loss = 0.04501
Step 71780: loss = 0.03501
Step 71785: loss = 0.04211
Step 71790: loss = 0.03987
Step 71795: loss = 0.04275
Step 71800: loss = 0.03781
Step 71805: loss = 0.04469
Step 71810: loss = 0.03716
Step 71815: loss = 0.05298
Step 71820: loss = 0.12180
Step 71825: loss = 0.03121
Step 71830: loss = 0.03609
Step 71835: loss = 0.03203
Step 71840: loss = 0.04811
Step 71845: loss = 0.05063
Step 71850: loss = 0.04560
Step 71855: loss = 0.03294
Step 71860: loss = 0.05299
Step 71865: loss = 0.03834
Step 71870: loss = 0.05599
Step 71875: loss = 0.03011
Step 71880: loss = 0.04507
Step 71885: loss = 0.04045
Step 71890: loss = 0.06942
Step 71895: loss = 0.06344
Step 71900: loss = 0.04240
Step 71905: loss = 0.03908
Step 71910: loss = 0.05340
Step 71915: loss = 0.04266
Step 71920: loss = 0.04935
Step 71925: loss = 0.08481
Step 71930: loss = 0.06573
Step 71935: loss = 0.05165
Step 71940: loss = 0.05488
Step 71945: loss = 0.04481
Step 71950: loss = 0.05216
Step 71955: loss = 0.03700
Step 71960: loss = 0.05426
Step 71965: loss = 0.06750
Step 71970: loss = 0.04623
Step 71975: loss = 0.06456
Step 71980: loss = 0.03581
Step 71985: loss = 0.03201
Step 71990: loss = 0.04436
Step 71995: loss = 0.03617
Step 72000: loss = 0.04300
Step 72005: loss = 0.05283
Step 72010: loss = 0.03553
Step 72015: loss = 0.05414
Step 72020: loss = 0.07708
Step 72025: loss = 0.03543
Step 72030: loss = 0.05088
Step 72035: loss = 0.03788
Step 72040: loss = 0.03961
Step 72045: loss = 0.03737
Step 72050: loss = 0.04520
Step 72055: loss = 0.05626
Step 72060: loss = 0.05777
Step 72065: loss = 0.04568
Step 72070: loss = 0.04096
Step 72075: loss = 0.03409
Step 72080: loss = 0.04285
Step 72085: loss = 0.03706
Step 72090: loss = 0.04493
Step 72095: loss = 0.04145
Step 72100: loss = 0.05098
Step 72105: loss = 0.04191
Step 72110: loss = 0.06012
Step 72115: loss = 0.03244
Step 72120: loss = 0.04455
Step 72125: loss = 0.03714
Step 72130: loss = 0.04160
Step 72135: loss = 0.04429
Step 72140: loss = 0.02965
Step 72145: loss = 0.02577
Step 72150: loss = 0.06131
Training Data Eval:
  Num examples: 49920, Num correct: 49600, Precision @ 1: 0.9936
('Testing Data Eval: EPOCH->', 186)
  Num examples: 9984, Num correct: 7462, Precision @ 1: 0.7474
Step 72155: loss = 0.06142
Step 72160: loss = 0.09037
Step 72165: loss = 0.06467
Step 72170: loss = 0.03390
Step 72175: loss = 0.04437
Step 72180: loss = 0.04046
Step 72185: loss = 0.05604
Step 72190: loss = 0.06190
Step 72195: loss = 0.04554
Step 72200: loss = 0.10246
Step 72205: loss = 0.06086
Step 72210: loss = 0.04877
Step 72215: loss = 0.03618
Step 72220: loss = 0.03484
Step 72225: loss = 0.07854
Step 72230: loss = 0.06958
Step 72235: loss = 0.03799
Step 72240: loss = 0.05549
Step 72245: loss = 0.05352
Step 72250: loss = 0.06147
Step 72255: loss = 0.03940
Step 72260: loss = 0.05147
Step 72265: loss = 0.03747
Step 72270: loss = 0.03824
Step 72275: loss = 0.05880
Step 72280: loss = 0.05931
Step 72285: loss = 0.11455
Step 72290: loss = 0.04585
Step 72295: loss = 0.05307
Step 72300: loss = 0.04851
Step 72305: loss = 0.04050
Step 72310: loss = 0.04551
Step 72315: loss = 0.03138
Step 72320: loss = 0.03912
Step 72325: loss = 0.03975
Step 72330: loss = 0.03268
Step 72335: loss = 0.03356
Step 72340: loss = 0.04318
Step 72345: loss = 0.05926
Step 72350: loss = 0.05310
Step 72355: loss = 0.03242
Step 72360: loss = 0.03649
Step 72365: loss = 0.04008
Step 72370: loss = 0.04229
Step 72375: loss = 0.04148
Step 72380: loss = 0.03720
Step 72385: loss = 0.14438
Step 72390: loss = 0.03862
Step 72395: loss = 0.04124
Step 72400: loss = 0.04526
Step 72405: loss = 0.06262
Step 72410: loss = 0.03713
Step 72415: loss = 0.06340
Step 72420: loss = 0.04406
Step 72425: loss = 0.03201
Step 72430: loss = 0.06443
Step 72435: loss = 0.05636
Step 72440: loss = 0.04688
Step 72445: loss = 0.04469
Step 72450: loss = 0.03387
Step 72455: loss = 0.04351
Step 72460: loss = 0.04749
Step 72465: loss = 0.04698
Step 72470: loss = 0.04414
Step 72475: loss = 0.03027
Step 72480: loss = 0.03759
Step 72485: loss = 0.05689
Step 72490: loss = 0.03835
Step 72495: loss = 0.03046
Step 72500: loss = 0.08351
Step 72505: loss = 0.05637
Step 72510: loss = 0.03626
Step 72515: loss = 0.04622
Step 72520: loss = 0.05292
Step 72525: loss = 0.03663
Step 72530: loss = 0.10109
Step 72535: loss = 0.04389
Step 72540: loss = 0.03682
Training Data Eval:
  Num examples: 49920, Num correct: 49676, Precision @ 1: 0.9951
('Testing Data Eval: EPOCH->', 187)
  Num examples: 9984, Num correct: 7469, Precision @ 1: 0.7481
Step 72545: loss = 0.03800
Step 72550: loss = 0.03821
Step 72555: loss = 0.04493
Step 72560: loss = 0.04027
Step 72565: loss = 0.03665
Step 72570: loss = 0.03114
Step 72575: loss = 0.06348
Step 72580: loss = 0.05368
Step 72585: loss = 0.04966
Step 72590: loss = 0.04831
Step 72595: loss = 0.05804
Step 72600: loss = 0.03265
Step 72605: loss = 0.09371
Step 72610: loss = 0.06810
Step 72615: loss = 0.04571
Step 72620: loss = 0.05382
Step 72625: loss = 0.03933
Step 72630: loss = 0.02944
Step 72635: loss = 0.07035
Step 72640: loss = 0.05936
Step 72645: loss = 0.09879
Step 72650: loss = 0.02920
Step 72655: loss = 0.04791
Step 72660: loss = 0.04293
Step 72665: loss = 0.06330
Step 72670: loss = 0.08572
Step 72675: loss = 0.04851
Step 72680: loss = 0.10074
Step 72685: loss = 0.05012
Step 72690: loss = 0.04687
Step 72695: loss = 0.05066
Step 72700: loss = 0.05911
Step 72705: loss = 0.08744
Step 72710: loss = 0.04184
Step 72715: loss = 0.07540
Step 72720: loss = 0.04680
Step 72725: loss = 0.04865
Step 72730: loss = 0.03738
Step 72735: loss = 0.04741
Step 72740: loss = 0.05859
Step 72745: loss = 0.06389
Step 72750: loss = 0.04801
Step 72755: loss = 0.06064
Step 72760: loss = 0.04451
Step 72765: loss = 0.04261
Step 72770: loss = 0.05663
Step 72775: loss = 0.07319
Step 72780: loss = 0.05619
Step 72785: loss = 0.04671
Step 72790: loss = 0.03469
Step 72795: loss = 0.04687
Step 72800: loss = 0.05721
Step 72805: loss = 0.04894
Step 72810: loss = 0.05180
Step 72815: loss = 0.05274
Step 72820: loss = 0.06447
Step 72825: loss = 0.05704
Step 72830: loss = 0.04086
Step 72835: loss = 0.09382
Step 72840: loss = 0.06327
Step 72845: loss = 0.03894
Step 72850: loss = 0.10087
Step 72855: loss = 0.04677
Step 72860: loss = 0.06135
Step 72865: loss = 0.06751
Step 72870: loss = 0.05407
Step 72875: loss = 0.05175
Step 72880: loss = 0.04784
Step 72885: loss = 0.06345
Step 72890: loss = 0.06455
Step 72895: loss = 0.03845
Step 72900: loss = 0.05197
Step 72905: loss = 0.04429
Step 72910: loss = 0.04814
Step 72915: loss = 0.08838
Step 72920: loss = 0.03850
Step 72925: loss = 0.04876
Step 72930: loss = 0.04374
Training Data Eval:
  Num examples: 49920, Num correct: 49611, Precision @ 1: 0.9938
('Testing Data Eval: EPOCH->', 188)
  Num examples: 9984, Num correct: 7477, Precision @ 1: 0.7489
Step 72935: loss = 0.03303
Step 72940: loss = 0.03266
Step 72945: loss = 0.05506
Step 72950: loss = 0.05611
Step 72955: loss = 0.03347
Step 72960: loss = 0.08804
Step 72965: loss = 0.03894
Step 72970: loss = 0.04548
Step 72975: loss = 0.03501
Step 72980: loss = 0.03497
Step 72985: loss = 0.05131
Step 72990: loss = 0.06124
Step 72995: loss = 0.04094
Step 73000: loss = 0.04829
Step 73005: loss = 0.04354
Step 73010: loss = 0.03246
Step 73015: loss = 0.05745
Step 73020: loss = 0.03572
Step 73025: loss = 0.04701
Step 73030: loss = 0.04466
Step 73035: loss = 0.04117
Step 73040: loss = 0.06092
Step 73045: loss = 0.03685
Step 73050: loss = 0.03423
Step 73055: loss = 0.04029
Step 73060: loss = 0.04569
Step 73065: loss = 0.04604
Step 73070: loss = 0.04408
Step 73075: loss = 0.04175
Step 73080: loss = 0.04509
Step 73085: loss = 0.03267
Step 73090: loss = 0.03381
Step 73095: loss = 0.04234
Step 73100: loss = 0.05109
Step 73105: loss = 0.03641
Step 73110: loss = 0.05464
Step 73115: loss = 0.05799
Step 73120: loss = 0.03978
Step 73125: loss = 0.02755
Step 73130: loss = 0.03999
Step 73135: loss = 0.06380
Step 73140: loss = 0.04280
Step 73145: loss = 0.07888
Step 73150: loss = 0.07368
Step 73155: loss = 0.05475
Step 73160: loss = 0.09686
Step 73165: loss = 0.03433
Step 73170: loss = 0.05235
Step 73175: loss = 0.03806
Step 73180: loss = 0.03721
Step 73185: loss = 0.05719
Step 73190: loss = 0.03380
Step 73195: loss = 0.04088
Step 73200: loss = 0.04659
Step 73205: loss = 0.05433
Step 73210: loss = 0.04792
Step 73215: loss = 0.06071
Step 73220: loss = 0.04330
Step 73225: loss = 0.05347
Step 73230: loss = 0.04494
Step 73235: loss = 0.04705
Step 73240: loss = 0.05480
Step 73245: loss = 0.04817
Step 73250: loss = 0.03202
Step 73255: loss = 0.08852
Step 73260: loss = 0.04269
Step 73265: loss = 0.08983
Step 73270: loss = 0.04144
Step 73275: loss = 0.04259
Step 73280: loss = 0.06166
Step 73285: loss = 0.04423
Step 73290: loss = 0.06271
Step 73295: loss = 0.04765
Step 73300: loss = 0.04923
Step 73305: loss = 0.03356
Step 73310: loss = 0.03780
Step 73315: loss = 0.03911
Step 73320: loss = 0.05059
Training Data Eval:
  Num examples: 49920, Num correct: 49701, Precision @ 1: 0.9956
('Testing Data Eval: EPOCH->', 189)
  Num examples: 9984, Num correct: 7515, Precision @ 1: 0.7527
Step 73325: loss = 0.03979
Step 73330: loss = 0.07313
Step 73335: loss = 0.03486
Step 73340: loss = 0.03993
Step 73345: loss = 0.05892
Step 73350: loss = 0.04834
Step 73355: loss = 0.03534
Step 73360: loss = 0.03513
Step 73365: loss = 0.04262
Step 73370: loss = 0.05067
Step 73375: loss = 0.03892
Step 73380: loss = 0.04833
Step 73385: loss = 0.04265
Step 73390: loss = 0.04159
Step 73395: loss = 0.08810
Step 73400: loss = 0.05604
Step 73405: loss = 0.05453
Step 73410: loss = 0.08038
Step 73415: loss = 0.04712
Step 73420: loss = 0.03458
Step 73425: loss = 0.04574
Step 73430: loss = 0.05694
Step 73435: loss = 0.03283
Step 73440: loss = 0.06961
Step 73445: loss = 0.06281
Step 73450: loss = 0.03622
Step 73455: loss = 0.05088
Step 73460: loss = 0.08441
Step 73465: loss = 0.04200
Step 73470: loss = 0.03192
Step 73475: loss = 0.03653
Step 73480: loss = 0.06472
Step 73485: loss = 0.04960
Step 73490: loss = 0.05681
Step 73495: loss = 0.03911
Step 73500: loss = 0.03458
Step 73505: loss = 0.04173
Step 73510: loss = 0.06295
Step 73515: loss = 0.04516
Step 73520: loss = 0.03808
Step 73525: loss = 0.05311
Step 73530: loss = 0.05354
Step 73535: loss = 0.03606
Step 73540: loss = 0.03097
Step 73545: loss = 0.05770
Step 73550: loss = 0.05338
Step 73555: loss = 0.07318
Step 73560: loss = 0.03322
Step 73565: loss = 0.03808
Step 73570: loss = 0.08074
Step 73575: loss = 0.05034
Step 73580: loss = 0.05322
Step 73585: loss = 0.04334
Step 73590: loss = 0.04490
Step 73595: loss = 0.05102
Step 73600: loss = 0.04403
Step 73605: loss = 0.05415
Step 73610: loss = 0.04155
Step 73615: loss = 0.04244
Step 73620: loss = 0.03649
Step 73625: loss = 0.04064
Step 73630: loss = 0.05577
Step 73635: loss = 0.04696
Step 73640: loss = 0.04087
Step 73645: loss = 0.04866
Step 73650: loss = 0.05790
Step 73655: loss = 0.04125
Step 73660: loss = 0.11510
Step 73665: loss = 0.05095
Step 73670: loss = 0.04457
Step 73675: loss = 0.05934
Step 73680: loss = 0.06906
Step 73685: loss = 0.06739
Step 73690: loss = 0.05501
Step 73695: loss = 0.04962
Step 73700: loss = 0.06038
Step 73705: loss = 0.04473
Step 73710: loss = 0.04296
Training Data Eval:
  Num examples: 49920, Num correct: 49672, Precision @ 1: 0.9950
('Testing Data Eval: EPOCH->', 190)
  Num examples: 9984, Num correct: 7476, Precision @ 1: 0.7488
Step 73715: loss = 0.03818
Step 73720: loss = 0.07234
Step 73725: loss = 0.03392
Step 73730: loss = 0.07310
Step 73735: loss = 0.03917
Step 73740: loss = 0.07467
Step 73745: loss = 0.03729
Step 73750: loss = 0.07685
Step 73755: loss = 0.05036
Step 73760: loss = 0.03052
Step 73765: loss = 0.04478
Step 73770: loss = 0.05081
Step 73775: loss = 0.03638
Step 73780: loss = 0.03592
Step 73785: loss = 0.04640
Step 73790: loss = 0.03496
Step 73795: loss = 0.03816
Step 73800: loss = 0.04504
Step 73805: loss = 0.05754
Step 73810: loss = 0.05299
Step 73815: loss = 0.05241
Step 73820: loss = 0.03423
Step 73825: loss = 0.03558
Step 73830: loss = 0.03751
Step 73835: loss = 0.06694
Step 73840: loss = 0.04503
Step 73845: loss = 0.04625
Step 73850: loss = 0.04454
Step 73855: loss = 0.03588
Step 73860: loss = 0.03630
Step 73865: loss = 0.07236
Step 73870: loss = 0.03140
Step 73875: loss = 0.05035
Step 73880: loss = 0.05546
Step 73885: loss = 0.08265
Step 73890: loss = 0.04462
Step 73895: loss = 0.05522
Step 73900: loss = 0.05164
Step 73905: loss = 0.07113
Step 73910: loss = 0.04192
Step 73915: loss = 0.04261
Step 73920: loss = 0.08782
Step 73925: loss = 0.03368
Step 73930: loss = 0.04565
Step 73935: loss = 0.05742
Step 73940: loss = 0.08114
Step 73945: loss = 0.05952
Step 73950: loss = 0.03275
Step 73955: loss = 0.03937
Step 73960: loss = 0.07561
Step 73965: loss = 0.03708
Step 73970: loss = 0.07917
Step 73975: loss = 0.06929
Step 73980: loss = 0.03656
Step 73985: loss = 0.04509
Step 73990: loss = 0.06372
Step 73995: loss = 0.07559
Step 74000: loss = 0.03954
Step 74005: loss = 0.03963
Step 74010: loss = 0.04033
Step 74015: loss = 0.03650
Step 74020: loss = 0.06655
Step 74025: loss = 0.03908
Step 74030: loss = 0.06891
Step 74035: loss = 0.03942
Step 74040: loss = 0.04088
Step 74045: loss = 0.04827
Step 74050: loss = 0.05642
Step 74055: loss = 0.05637
Step 74060: loss = 0.03855
Step 74065: loss = 0.06149
Step 74070: loss = 0.04201
Step 74075: loss = 0.04660
Step 74080: loss = 0.05021
Step 74085: loss = 0.06417
Step 74090: loss = 0.08793
Step 74095: loss = 0.11081
Step 74100: loss = 0.03695
Training Data Eval:
  Num examples: 49920, Num correct: 49553, Precision @ 1: 0.9926
('Testing Data Eval: EPOCH->', 191)
  Num examples: 9984, Num correct: 7516, Precision @ 1: 0.7528
Step 74105: loss = 0.04334
Step 74110: loss = 0.05369
Step 74115: loss = 0.03912
Step 74120: loss = 0.04501
Step 74125: loss = 0.03758
Step 74130: loss = 0.03412
Step 74135: loss = 0.04423
Step 74140: loss = 0.06112
Step 74145: loss = 0.05318
Step 74150: loss = 0.06641
Step 74155: loss = 0.05935
Step 74160: loss = 0.06905
Step 74165: loss = 0.03594
Step 74170: loss = 0.11384
Step 74175: loss = 0.04931
Step 74180: loss = 0.03565
Step 74185: loss = 0.04028
Step 74190: loss = 0.04454
Step 74195: loss = 0.04743
Step 74200: loss = 0.03476
Step 74205: loss = 0.10210
Step 74210: loss = 0.04234
Step 74215: loss = 0.06450
Step 74220: loss = 0.04890
Step 74225: loss = 0.05245
Step 74230: loss = 0.04992
Step 74235: loss = 0.05416
Step 74240: loss = 0.05729
Step 74245: loss = 0.04704
Step 74250: loss = 0.04680
Step 74255: loss = 0.03819
Step 74260: loss = 0.06922
Step 74265: loss = 0.03367
Step 74270: loss = 0.04150
Step 74275: loss = 0.05224
Step 74280: loss = 0.04857
Step 74285: loss = 0.04562
Step 74290: loss = 0.04291
Step 74295: loss = 0.08327
Step 74300: loss = 0.04179
Step 74305: loss = 0.07090
Step 74310: loss = 0.03432
Step 74315: loss = 0.03614
Step 74320: loss = 0.04757
Step 74325: loss = 0.10127
Step 74330: loss = 0.03628
Step 74335: loss = 0.06532
Step 74340: loss = 0.03978
Step 74345: loss = 0.05374
Step 74350: loss = 0.04524
Step 74355: loss = 0.04938
Step 74360: loss = 0.05285
Step 74365: loss = 0.03983
Step 74370: loss = 0.03005
Step 74375: loss = 0.05370
Step 74380: loss = 0.10095
Step 74385: loss = 0.06132
Step 74390: loss = 0.07317
Step 74395: loss = 0.06023
Step 74400: loss = 0.04255
Step 74405: loss = 0.05778
Step 74410: loss = 0.03419
Step 74415: loss = 0.04189
Step 74420: loss = 0.04653
Step 74425: loss = 0.06046
Step 74430: loss = 0.06374
Step 74435: loss = 0.05647
Step 74440: loss = 0.04992
Step 74445: loss = 0.10333
Step 74450: loss = 0.05494
Step 74455: loss = 0.03705
Step 74460: loss = 0.07947
Step 74465: loss = 0.06971
Step 74470: loss = 0.04518
Step 74475: loss = 0.04808
Step 74480: loss = 0.04318
Step 74485: loss = 0.05821
Step 74490: loss = 0.03592
Training Data Eval:
  Num examples: 49920, Num correct: 49618, Precision @ 1: 0.9940
('Testing Data Eval: EPOCH->', 192)
  Num examples: 9984, Num correct: 7461, Precision @ 1: 0.7473
Step 74495: loss = 0.03016
Step 74500: loss = 0.03692
Step 74505: loss = 0.04371
Step 74510: loss = 0.03263
Step 74515: loss = 0.03480
Step 74520: loss = 0.04827
Step 74525: loss = 0.03820
Step 74530: loss = 0.02549
Step 74535: loss = 0.03702
Step 74540: loss = 0.04072
Step 74545: loss = 0.04433
Step 74550: loss = 0.04627
Step 74555: loss = 0.05804
Step 74560: loss = 0.07384
Step 74565: loss = 0.07978
Step 74570: loss = 0.03562
Step 74575: loss = 0.05386
Step 74580: loss = 0.05058
Step 74585: loss = 0.03698
Step 74590: loss = 0.03738
Step 74595: loss = 0.05185
Step 74600: loss = 0.04727
Step 74605: loss = 0.07706
Step 74610: loss = 0.12191
Step 74615: loss = 0.04851
Step 74620: loss = 0.09275
Step 74625: loss = 0.05898
Step 74630: loss = 0.04419
Step 74635: loss = 0.09823
Step 74640: loss = 0.07576
Step 74645: loss = 0.05910
Step 74650: loss = 0.04966
Step 74655: loss = 0.08489
Step 74660: loss = 0.04647
Step 74665: loss = 0.03971
Step 74670: loss = 0.04929
Step 74675: loss = 0.06078
Step 74680: loss = 0.06014
Step 74685: loss = 0.06018
Step 74690: loss = 0.05139
Step 74695: loss = 0.03921
Step 74700: loss = 0.03093
Step 74705: loss = 0.05081
Step 74710: loss = 0.03840
Step 74715: loss = 0.06936
Step 74720: loss = 0.04405
Step 74725: loss = 0.03710
Step 74730: loss = 0.05508
Step 74735: loss = 0.04191
Step 74740: loss = 0.03896
Step 74745: loss = 0.04986
Step 74750: loss = 0.05645
Step 74755: loss = 0.06535
Step 74760: loss = 0.05570
Step 74765: loss = 0.06049
Step 74770: loss = 0.04295
Step 74775: loss = 0.04287
Step 74780: loss = 0.06185
Step 74785: loss = 0.06838
Step 74790: loss = 0.05615
Step 74795: loss = 0.03514
Step 74800: loss = 0.13316
Step 74805: loss = 0.04720
Step 74810: loss = 0.06838
Step 74815: loss = 0.04093
Step 74820: loss = 0.04998
Step 74825: loss = 0.08128
Step 74830: loss = 0.07300
Step 74835: loss = 0.05272
Step 74840: loss = 0.09049
Step 74845: loss = 0.04159
Step 74850: loss = 0.03664
Step 74855: loss = 0.04261
Step 74860: loss = 0.05048
Step 74865: loss = 0.04300
Step 74870: loss = 0.06021
Step 74875: loss = 0.04832
Step 74880: loss = 0.03914
Training Data Eval:
  Num examples: 49920, Num correct: 49682, Precision @ 1: 0.9952
('Testing Data Eval: EPOCH->', 193)
  Num examples: 9984, Num correct: 7523, Precision @ 1: 0.7535
Step 74885: loss = 0.04073
Step 74890: loss = 0.04351
Step 74895: loss = 0.04744
Step 74900: loss = 0.06330
Step 74905: loss = 0.04914
Step 74910: loss = 0.08273
Step 74915: loss = 0.03539
Step 74920: loss = 0.04939
Step 74925: loss = 0.02849
Step 74930: loss = 0.03081
Step 74935: loss = 0.03329
Step 74940: loss = 0.04525
Step 74945: loss = 0.03190
Step 74950: loss = 0.03524
Step 74955: loss = 0.04116
Step 74960: loss = 0.04549
Step 74965: loss = 0.04207
Step 74970: loss = 0.04420
Step 74975: loss = 0.03783
Step 74980: loss = 0.09053
Step 74985: loss = 0.03250
Step 74990: loss = 0.04286
Step 74995: loss = 0.02811
Step 75000: loss = 0.06765
Step 75005: loss = 0.04297
Step 75010: loss = 0.03890
Step 75015: loss = 0.05062
Step 75020: loss = 0.06286
Step 75025: loss = 0.03627
Step 75030: loss = 0.04896
Step 75035: loss = 0.05469
Step 75040: loss = 0.07162
Step 75045: loss = 0.04975
Step 75050: loss = 0.03764
Step 75055: loss = 0.03756
Step 75060: loss = 0.09019
Step 75065: loss = 0.04724
Step 75070: loss = 0.03980
Step 75075: loss = 0.04227
Step 75080: loss = 0.06263
Step 75085: loss = 0.03626
Step 75090: loss = 0.06372
Step 75095: loss = 0.03731
Step 75100: loss = 0.05649
Step 75105: loss = 0.03705
Step 75110: loss = 0.06685
Step 75115: loss = 0.04158
Step 75120: loss = 0.05274
Step 75125: loss = 0.03081
Step 75130: loss = 0.04043
Step 75135: loss = 0.04142
Step 75140: loss = 0.02776
Step 75145: loss = 0.05635
Step 75150: loss = 0.03752
Step 75155: loss = 0.04150
Step 75160: loss = 0.05154
Step 75165: loss = 0.05902
Step 75170: loss = 0.03353
Step 75175: loss = 0.05447
Step 75180: loss = 0.03798
Step 75185: loss = 0.04184
Step 75190: loss = 0.03188
Step 75195: loss = 0.07290
Step 75200: loss = 0.04537
Step 75205: loss = 0.05919
Step 75210: loss = 0.05193
Step 75215: loss = 0.03952
Step 75220: loss = 0.06771
Step 75225: loss = 0.07003
Step 75230: loss = 0.06826
Step 75235: loss = 0.07018
Step 75240: loss = 0.07100
Step 75245: loss = 0.07184
Step 75250: loss = 0.05547
Step 75255: loss = 0.07658
Step 75260: loss = 0.06981
Step 75265: loss = 0.05325
Step 75270: loss = 0.03959
Training Data Eval:
  Num examples: 49920, Num correct: 49606, Precision @ 1: 0.9937
('Testing Data Eval: EPOCH->', 194)
  Num examples: 9984, Num correct: 7434, Precision @ 1: 0.7446
Step 75275: loss = 0.03977
Step 75280: loss = 0.03963
Step 75285: loss = 0.04602
Step 75290: loss = 0.05011
Step 75295: loss = 0.02966
Step 75300: loss = 0.09819
Step 75305: loss = 0.08666
Step 75310: loss = 0.05321
Step 75315: loss = 0.03606
Step 75320: loss = 0.05304
Step 75325: loss = 0.07253
Step 75330: loss = 0.03326
Step 75335: loss = 0.06321
Step 75340: loss = 0.07677
Step 75345: loss = 0.06360
Step 75350: loss = 0.03940
Step 75355: loss = 0.03466
Step 75360: loss = 0.05419
Step 75365: loss = 0.10157
Step 75370: loss = 0.11082
Step 75375: loss = 0.04105
Step 75380: loss = 0.07718
Step 75385: loss = 0.03540
Step 75390: loss = 0.03948
Step 75395: loss = 0.06454
Step 75400: loss = 0.03858
Step 75405: loss = 0.06750
Step 75410: loss = 0.09058
Step 75415: loss = 0.07509
Step 75420: loss = 0.03286
Step 75425: loss = 0.08102
Step 75430: loss = 0.07698
Step 75435: loss = 0.05402
Step 75440: loss = 0.05806
Step 75445: loss = 0.03396
Step 75450: loss = 0.03050
Step 75455: loss = 0.04694
Step 75460: loss = 0.04147
Step 75465: loss = 0.04639
Step 75470: loss = 0.03654
Step 75475: loss = 0.04086
Step 75480: loss = 0.04304
Step 75485: loss = 0.04172
Step 75490: loss = 0.07370
Step 75495: loss = 0.07023
Step 75500: loss = 0.04222
Step 75505: loss = 0.04050
Step 75510: loss = 0.05764
Step 75515: loss = 0.03952
Step 75520: loss = 0.04983
Step 75525: loss = 0.05820
Step 75530: loss = 0.04490
Step 75535: loss = 0.03105
Step 75540: loss = 0.10714
Step 75545: loss = 0.03065
Step 75550: loss = 0.05069
Step 75555: loss = 0.04474
Step 75560: loss = 0.04468
Step 75565: loss = 0.07127
Step 75570: loss = 0.04079
Step 75575: loss = 0.03433
Step 75580: loss = 0.04530
Step 75585: loss = 0.04861
Step 75590: loss = 0.04994
Step 75595: loss = 0.03945
Step 75600: loss = 0.09018
Step 75605: loss = 0.10239
Step 75610: loss = 0.04317
Step 75615: loss = 0.03661
Step 75620: loss = 0.08562
Step 75625: loss = 0.04918
Step 75630: loss = 0.03566
Step 75635: loss = 0.05254
Step 75640: loss = 0.06806
Step 75645: loss = 0.05836
Step 75650: loss = 0.06078
Step 75655: loss = 0.04347
Step 75660: loss = 0.04895
Training Data Eval:
  Num examples: 49920, Num correct: 49597, Precision @ 1: 0.9935
('Testing Data Eval: EPOCH->', 195)
  Num examples: 9984, Num correct: 7492, Precision @ 1: 0.7504
Step 75665: loss = 0.08757
Step 75670: loss = 0.05363
Step 75675: loss = 0.04279
Step 75680: loss = 0.09167
Step 75685: loss = 0.04043
Step 75690: loss = 0.04110
Step 75695: loss = 0.07294
Step 75700: loss = 0.04649
Step 75705: loss = 0.04565
Step 75710: loss = 0.05211
Step 75715: loss = 0.04162
Step 75720: loss = 0.04966
Step 75725: loss = 0.05120
Step 75730: loss = 0.03137
Step 75735: loss = 0.05546
Step 75740: loss = 0.03798
Step 75745: loss = 0.03885
Step 75750: loss = 0.07230
Step 75755: loss = 0.05931
Step 75760: loss = 0.04999
Step 75765: loss = 0.04009
Step 75770: loss = 0.04627
Step 75775: loss = 0.04600
Step 75780: loss = 0.06083
Step 75785: loss = 0.06333
Step 75790: loss = 0.04359
Step 75795: loss = 0.05741
Step 75800: loss = 0.03146
Step 75805: loss = 0.04955
Step 75810: loss = 0.05983
Step 75815: loss = 0.03519
Step 75820: loss = 0.04058
Step 75825: loss = 0.03330
Step 75830: loss = 0.05785
Step 75835: loss = 0.05316
Step 75840: loss = 0.06363
Step 75845: loss = 0.04860
Step 75850: loss = 0.05242
Step 75855: loss = 0.05030
Step 75860: loss = 0.03986
Step 75865: loss = 0.04365
Step 75870: loss = 0.04922
Step 75875: loss = 0.03176
Step 75880: loss = 0.03289
Step 75885: loss = 0.03944
Step 75890: loss = 0.05115
Step 75895: loss = 0.05118
Step 75900: loss = 0.03864
Step 75905: loss = 0.04281
Step 75910: loss = 0.04472
Step 75915: loss = 0.05469
Step 75920: loss = 0.04584
Step 75925: loss = 0.03748
Step 75930: loss = 0.04263
Step 75935: loss = 0.03002
Step 75940: loss = 0.03537
Step 75945: loss = 0.05995
Step 75950: loss = 0.06853
Step 75955: loss = 0.04690
Step 75960: loss = 0.03276
Step 75965: loss = 0.08727
Step 75970: loss = 0.03619
Step 75975: loss = 0.03669
Step 75980: loss = 0.07064
Step 75985: loss = 0.06440
Step 75990: loss = 0.03563
Step 75995: loss = 0.06237
Step 76000: loss = 0.04075
Step 76005: loss = 0.05146
Step 76010: loss = 0.05995
Step 76015: loss = 0.03260
Step 76020: loss = 0.03307
Step 76025: loss = 0.04867
Step 76030: loss = 0.03867
Step 76035: loss = 0.03157
Step 76040: loss = 0.04243
Step 76045: loss = 0.03455
Step 76050: loss = 0.05468
Training Data Eval:
  Num examples: 49920, Num correct: 49660, Precision @ 1: 0.9948
('Testing Data Eval: EPOCH->', 196)
  Num examples: 9984, Num correct: 7447, Precision @ 1: 0.7459
Step 76055: loss = 0.04456
Step 76060: loss = 0.03460
Step 76065: loss = 0.04204
Step 76070: loss = 0.05664
Step 76075: loss = 0.04884
Step 76080: loss = 0.04163
Step 76085: loss = 0.03722
Step 76090: loss = 0.05060
Step 76095: loss = 0.03791
Step 76100: loss = 0.06393
Step 76105: loss = 0.03194
Step 76110: loss = 0.03630
Step 76115: loss = 0.04959
Step 76120: loss = 0.02804
Step 76125: loss = 0.05614
Step 76130: loss = 0.04339
Step 76135: loss = 0.05338
Step 76140: loss = 0.07020
Step 76145: loss = 0.04409
Step 76150: loss = 0.04631
Step 76155: loss = 0.04273
Step 76160: loss = 0.03190
Step 76165: loss = 0.04293
Step 76170: loss = 0.10326
Step 76175: loss = 0.04135
Step 76180: loss = 0.03952
Step 76185: loss = 0.04653
Step 76190: loss = 0.06459
Step 76195: loss = 0.03701
Step 76200: loss = 0.05428
Step 76205: loss = 0.05802
Step 76210: loss = 0.04592
Step 76215: loss = 0.06471
Step 76220: loss = 0.05664
Step 76225: loss = 0.03447
Step 76230: loss = 0.04678
Step 76235: loss = 0.05543
Step 76240: loss = 0.06932
Step 76245: loss = 0.04780
Step 76250: loss = 0.03953
Step 76255: loss = 0.02980
Step 76260: loss = 0.03785
Step 76265: loss = 0.03906
Step 76270: loss = 0.04812
Step 76275: loss = 0.04926
Step 76280: loss = 0.04460
Step 76285: loss = 0.04593
Step 76290: loss = 0.03887
Step 76295: loss = 0.03603
Step 76300: loss = 0.05744
Step 76305: loss = 0.03565
Step 76310: loss = 0.05948
Step 76315: loss = 0.06929
Step 76320: loss = 0.09177
Step 76325: loss = 0.07414
Step 76330: loss = 0.07262
Step 76335: loss = 0.03200
Step 76340: loss = 0.07842
Step 76345: loss = 0.08049
Step 76350: loss = 0.05353
Step 76355: loss = 0.04114
Step 76360: loss = 0.08232
Step 76365: loss = 0.03371
Step 76370: loss = 0.07690
Step 76375: loss = 0.06495
Step 76380: loss = 0.04002
Step 76385: loss = 0.04556
Step 76390: loss = 0.03942
Step 76395: loss = 0.02912
Step 76400: loss = 0.04774
Step 76405: loss = 0.11021
Step 76410: loss = 0.04329
Step 76415: loss = 0.06412
Step 76420: loss = 0.07399
Step 76425: loss = 0.03811
Step 76430: loss = 0.05530
Step 76435: loss = 0.03551
Step 76440: loss = 0.05453
Training Data Eval:
  Num examples: 49920, Num correct: 49654, Precision @ 1: 0.9947
('Testing Data Eval: EPOCH->', 197)
  Num examples: 9984, Num correct: 7504, Precision @ 1: 0.7516
Step 76445: loss = 0.04807
Step 76450: loss = 0.03318
Step 76455: loss = 0.04399
Step 76460: loss = 0.03590
Step 76465: loss = 0.04630
Step 76470: loss = 0.04564
Step 76475: loss = 0.06266
Step 76480: loss = 0.03666
Step 76485: loss = 0.07614
Step 76490: loss = 0.03675
Step 76495: loss = 0.03644
Step 76500: loss = 0.08361
Step 76505: loss = 0.04360
Step 76510: loss = 0.06486
Step 76515: loss = 0.07705
Step 76520: loss = 0.06241
Step 76525: loss = 0.04519
Step 76530: loss = 0.03623
Step 76535: loss = 0.05377
Step 76540: loss = 0.03682
Step 76545: loss = 0.05289
Step 76550: loss = 0.09541
Step 76555: loss = 0.07196
Step 76560: loss = 0.04723
Step 76565: loss = 0.04633
Step 76570: loss = 0.04988
Step 76575: loss = 0.03909
Step 76580: loss = 0.04258
Step 76585: loss = 0.06580
Step 76590: loss = 0.04792
Step 76595: loss = 0.03679
Step 76600: loss = 0.04569
Step 76605: loss = 0.07218
Step 76610: loss = 0.04165
Step 76615: loss = 0.05704
Step 76620: loss = 0.04541
Step 76625: loss = 0.06945
Step 76630: loss = 0.06614
Step 76635: loss = 0.03726
Step 76640: loss = 0.07110
Step 76645: loss = 0.05892
Step 76650: loss = 0.04804
Step 76655: loss = 0.02955
Step 76660: loss = 0.03640
Step 76665: loss = 0.04268
Step 76670: loss = 0.05348
Step 76675: loss = 0.04884
Step 76680: loss = 0.04077
Step 76685: loss = 0.03763
Step 76690: loss = 0.03770
Step 76695: loss = 0.05836
Step 76700: loss = 0.05372
Step 76705: loss = 0.02734
Step 76710: loss = 0.03983
Step 76715: loss = 0.05967
Step 76720: loss = 0.08153
Step 76725: loss = 0.03356
Step 76730: loss = 0.04322
Step 76735: loss = 0.04692
Step 76740: loss = 0.07010
Step 76745: loss = 0.02942
Step 76750: loss = 0.04152
Step 76755: loss = 0.03881
Step 76760: loss = 0.04731
Step 76765: loss = 0.05371
Step 76770: loss = 0.03517
Step 76775: loss = 0.03784
Step 76780: loss = 0.04741
Step 76785: loss = 0.04181
Step 76790: loss = 0.05648
Step 76795: loss = 0.05716
Step 76800: loss = 0.03147
Step 76805: loss = 0.05588
Step 76810: loss = 0.05159
Step 76815: loss = 0.07207
Step 76820: loss = 0.05209
Step 76825: loss = 0.04178
Step 76830: loss = 0.03805
Training Data Eval:
  Num examples: 49920, Num correct: 49700, Precision @ 1: 0.9956
('Testing Data Eval: EPOCH->', 198)
  Num examples: 9984, Num correct: 7501, Precision @ 1: 0.7513
Step 76835: loss = 0.04971
Step 76840: loss = 0.03836
Step 76845: loss = 0.04769
Step 76850: loss = 0.02762
Step 76855: loss = 0.03475
Step 76860: loss = 0.05056
Step 76865: loss = 0.03665
Step 76870: loss = 0.05831
Step 76875: loss = 0.04867
Step 76880: loss = 0.03518
Step 76885: loss = 0.05094
Step 76890: loss = 0.03576
Step 76895: loss = 0.06221
Step 76900: loss = 0.03796
Step 76905: loss = 0.04340
Step 76910: loss = 0.04241
Step 76915: loss = 0.03463
Step 76920: loss = 0.02976
Step 76925: loss = 0.03959
Step 76930: loss = 0.03076
Step 76935: loss = 0.07990
Step 76940: loss = 0.05736
Step 76945: loss = 0.05463
Step 76950: loss = 0.03758
Step 76955: loss = 0.10892
Step 76960: loss = 0.04227
Step 76965: loss = 0.05943
Step 76970: loss = 0.03245
Step 76975: loss = 0.04925
Step 76980: loss = 0.04058
Step 76985: loss = 0.03447
Step 76990: loss = 0.03146
Step 76995: loss = 0.04272
Step 77000: loss = 0.03723
Step 77005: loss = 0.03138
Step 77010: loss = 0.03538
Step 77015: loss = 0.06405
Step 77020: loss = 0.03545
Step 77025: loss = 0.04180
Step 77030: loss = 0.10070
Step 77035: loss = 0.06537
Step 77040: loss = 0.03600
Step 77045: loss = 0.03028
Step 77050: loss = 0.03352
Step 77055: loss = 0.03861
Step 77060: loss = 0.03133
Step 77065: loss = 0.03498
Step 77070: loss = 0.03834
Step 77075: loss = 0.03930
Step 77080: loss = 0.04209
Step 77085: loss = 0.03075
Step 77090: loss = 0.03214
Step 77095: loss = 0.02952
Step 77100: loss = 0.04266
Step 77105: loss = 0.07066
Step 77110: loss = 0.02438
Step 77115: loss = 0.06943
Step 77120: loss = 0.03750
Step 77125: loss = 0.04057
Step 77130: loss = 0.05656
Step 77135: loss = 0.03511
Step 77140: loss = 0.05338
Step 77145: loss = 0.03668
Step 77150: loss = 0.03700
Step 77155: loss = 0.04998
Step 77160: loss = 0.05179
Step 77165: loss = 0.03816
Step 77170: loss = 0.03128
Step 77175: loss = 0.02911
Step 77180: loss = 0.04025
Step 77185: loss = 0.04091
Step 77190: loss = 0.06045
Step 77195: loss = 0.04120
Step 77200: loss = 0.06978
Step 77205: loss = 0.04699
Step 77210: loss = 0.04746
Step 77215: loss = 0.08185
Step 77220: loss = 0.07115
Training Data Eval:
  Num examples: 49920, Num correct: 49649, Precision @ 1: 0.9946
('Testing Data Eval: EPOCH->', 199)
  Num examples: 9984, Num correct: 7481, Precision @ 1: 0.7493
Step 77225: loss = 0.02680
Step 77230: loss = 0.04856
Step 77235: loss = 0.05040
Step 77240: loss = 0.06719
Step 77245: loss = 0.05346
Step 77250: loss = 0.04873
Step 77255: loss = 0.03583
Step 77260: loss = 0.08744
Step 77265: loss = 0.03198
Step 77270: loss = 0.03304
Step 77275: loss = 0.04256
Step 77280: loss = 0.03444
Step 77285: loss = 0.03117
Step 77290: loss = 0.04053
Step 77295: loss = 0.04192
Step 77300: loss = 0.03769
Step 77305: loss = 0.04956
Step 77310: loss = 0.04234
Step 77315: loss = 0.04518
Step 77320: loss = 0.04083
Step 77325: loss = 0.03405
Step 77330: loss = 0.03324
Step 77335: loss = 0.04375
Step 77340: loss = 0.03743
Step 77345: loss = 0.04751
Step 77350: loss = 0.03525
Step 77355: loss = 0.05607
Step 77360: loss = 0.07939
Step 77365: loss = 0.05108
Step 77370: loss = 0.03737
Step 77375: loss = 0.03855
Step 77380: loss = 0.08478
Step 77385: loss = 0.03248
Step 77390: loss = 0.10378
Step 77395: loss = 0.05670
Step 77400: loss = 0.03251
Step 77405: loss = 0.05934
Step 77410: loss = 0.04240
Step 77415: loss = 0.05839
Step 77420: loss = 0.04875
Step 77425: loss = 0.04689
Step 77430: loss = 0.05510
Step 77435: loss = 0.03617
Step 77440: loss = 0.05179
Step 77445: loss = 0.03148
Step 77450: loss = 0.03811
Step 77455: loss = 0.04211
Step 77460: loss = 0.05207
Step 77465: loss = 0.05234
Step 77470: loss = 0.04269
Step 77475: loss = 0.02924
Step 77480: loss = 0.05479
Step 77485: loss = 0.04334
Step 77490: loss = 0.05275
Step 77495: loss = 0.03573
Step 77500: loss = 0.02954
Step 77505: loss = 0.05228
Step 77510: loss = 0.03884
Step 77515: loss = 0.04883
Step 77520: loss = 0.04501
Step 77525: loss = 0.03788
Step 77530: loss = 0.04885
Step 77535: loss = 0.06914
Step 77540: loss = 0.04295
Step 77545: loss = 0.04323
Step 77550: loss = 0.04736
Step 77555: loss = 0.06353
Step 77560: loss = 0.08927
Step 77565: loss = 0.04573
Step 77570: loss = 0.03917
Step 77575: loss = 0.05406
Step 77580: loss = 0.07788
Step 77585: loss = 0.04924
Step 77590: loss = 0.02983
Step 77595: loss = 0.05296
Step 77600: loss = 0.06882
Step 77605: loss = 0.08128
Step 77610: loss = 0.05470
Training Data Eval:
  Num examples: 49920, Num correct: 49604, Precision @ 1: 0.9937
('Testing Data Eval: EPOCH->', 200)
  Num examples: 9984, Num correct: 7481, Precision @ 1: 0.7493
Step 77615: loss = 0.05236
Step 77620: loss = 0.03717
Step 77625: loss = 0.04237
Step 77630: loss = 0.04161
Step 77635: loss = 0.03016
Step 77640: loss = 0.03661
Step 77645: loss = 0.04476
Step 77650: loss = 0.04178
Step 77655: loss = 0.02806
Step 77660: loss = 0.03453
Step 77665: loss = 0.06170
Step 77670: loss = 0.05342
Step 77675: loss = 0.03374
Step 77680: loss = 0.03167
Step 77685: loss = 0.04142
Step 77690: loss = 0.03641
Step 77695: loss = 0.03350
Step 77700: loss = 0.03767
Step 77705: loss = 0.10152
Step 77710: loss = 0.06866
Step 77715: loss = 0.04235
Step 77720: loss = 0.07883
Step 77725: loss = 0.04654
Step 77730: loss = 0.03158
Step 77735: loss = 0.04773
Step 77740: loss = 0.05946
Step 77745: loss = 0.03428
Step 77750: loss = 0.03285
Step 77755: loss = 0.05165
Step 77760: loss = 0.06844
Step 77765: loss = 0.03339
Step 77770: loss = 0.04905
Step 77775: loss = 0.09442
Step 77780: loss = 0.04018
Step 77785: loss = 0.06337
Step 77790: loss = 0.06724
Step 77795: loss = 0.05673
Step 77800: loss = 0.04632
Step 77805: loss = 0.04001
Step 77810: loss = 0.02993
Step 77815: loss = 0.03594
Step 77820: loss = 0.05478
Step 77825: loss = 0.08572
Step 77830: loss = 0.05496
Step 77835: loss = 0.04619
Step 77840: loss = 0.03469
Step 77845: loss = 0.07654
Step 77850: loss = 0.04546
Step 77855: loss = 0.04605
Step 77860: loss = 0.04333
Step 77865: loss = 0.04244
Step 77870: loss = 0.04430
Step 77875: loss = 0.03552
Step 77880: loss = 0.04484
Step 77885: loss = 0.04215
Step 77890: loss = 0.06142
Step 77895: loss = 0.03114
Step 77900: loss = 0.03406
Step 77905: loss = 0.07038
Step 77910: loss = 0.05406
Step 77915: loss = 0.04976
Step 77920: loss = 0.03447
Step 77925: loss = 0.05900
Step 77930: loss = 0.04328
Step 77935: loss = 0.05332
Step 77940: loss = 0.04335
Step 77945: loss = 0.03872
Step 77950: loss = 0.09920
Step 77955: loss = 0.05276
Step 77960: loss = 0.04125
Step 77965: loss = 0.03049
Step 77970: loss = 0.04398
Step 77975: loss = 0.03065
Step 77980: loss = 0.03938
Step 77985: loss = 0.02997
Step 77990: loss = 0.03744
Step 77995: loss = 0.05537
Step 78000: loss = 0.05437
Training Data Eval:
  Num examples: 49920, Num correct: 49617, Precision @ 1: 0.9939
('Testing Data Eval: EPOCH->', 201)
  Num examples: 9984, Num correct: 7454, Precision @ 1: 0.7466
Step 78005: loss = 0.09234
Step 78010: loss = 0.16129
Step 78015: loss = 0.08374
Step 78020: loss = 0.04397
Step 78025: loss = 0.07073
Step 78030: loss = 0.05279
Step 78035: loss = 0.05635
Step 78040: loss = 0.04848
Step 78045: loss = 0.09753
Step 78050: loss = 0.04128
Step 78055: loss = 0.07412
Step 78060: loss = 0.04940
Step 78065: loss = 0.06153
Step 78070: loss = 0.03368
Step 78075: loss = 0.06133
Step 78080: loss = 0.03532
Step 78085: loss = 0.09041
Step 78090: loss = 0.03220
Step 78095: loss = 0.05606
Step 78100: loss = 0.06400
Step 78105: loss = 0.04743
Step 78110: loss = 0.07369
Step 78115: loss = 0.05783
Step 78120: loss = 0.04958
Step 78125: loss = 0.04315
Step 78130: loss = 0.03617
Step 78135: loss = 0.07517
Step 78140: loss = 0.05329
Step 78145: loss = 0.05352
Step 78150: loss = 0.04332
Step 78155: loss = 0.04020
Step 78160: loss = 0.04297
Step 78165: loss = 0.03929
Step 78170: loss = 0.03328
Step 78175: loss = 0.09458
Step 78180: loss = 0.04928
Step 78185: loss = 0.05608
Step 78190: loss = 0.04043
Step 78195: loss = 0.04391
Step 78200: loss = 0.07487
Step 78205: loss = 0.06894
Step 78210: loss = 0.04001
Step 78215: loss = 0.03189
Step 78220: loss = 0.06390
Step 78225: loss = 0.06792
Step 78230: loss = 0.03020
Step 78235: loss = 0.03914
Step 78240: loss = 0.05137
Step 78245: loss = 0.04538
Step 78250: loss = 0.04333
Step 78255: loss = 0.03982
Step 78260: loss = 0.04533
Step 78265: loss = 0.05981
Step 78270: loss = 0.06951
Step 78275: loss = 0.03264
Step 78280: loss = 0.06638
Step 78285: loss = 0.05867
Step 78290: loss = 0.03900
Step 78295: loss = 0.05941
Step 78300: loss = 0.03170
Step 78305: loss = 0.03416
Step 78310: loss = 0.03317
Step 78315: loss = 0.03193
Step 78320: loss = 0.05397
Step 78325: loss = 0.04643
Step 78330: loss = 0.06576
Step 78335: loss = 0.03687
Step 78340: loss = 0.04941
Step 78345: loss = 0.04097
Step 78350: loss = 0.03146
Step 78355: loss = 0.04426
Step 78360: loss = 0.03785
Step 78365: loss = 0.05079
Step 78370: loss = 0.02849
Step 78375: loss = 0.03824
Step 78380: loss = 0.03287
Step 78385: loss = 0.05625
Step 78390: loss = 0.03104
Training Data Eval:
  Num examples: 49920, Num correct: 49709, Precision @ 1: 0.9958
('Testing Data Eval: EPOCH->', 202)
  Num examples: 9984, Num correct: 7448, Precision @ 1: 0.7460
Step 78395: loss = 0.02541
Step 78400: loss = 0.04212
Step 78405: loss = 0.03462
Step 78410: loss = 0.04143
Step 78415: loss = 0.05815
Step 78420: loss = 0.07094
Step 78425: loss = 0.03912
Step 78430: loss = 0.04371
Step 78435: loss = 0.03817
Step 78440: loss = 0.03696
Step 78445: loss = 0.04963
Step 78450: loss = 0.04783
Step 78455: loss = 0.05041
Step 78460: loss = 0.03413
Step 78465: loss = 0.04903
Step 78470: loss = 0.03048
Step 78475: loss = 0.03690
Step 78480: loss = 0.03374
Step 78485: loss = 0.02850
Step 78490: loss = 0.04388
Step 78495: loss = 0.03598
Step 78500: loss = 0.05314
Step 78505: loss = 0.03498
Step 78510: loss = 0.04573
Step 78515: loss = 0.04001
Step 78520: loss = 0.03505
Step 78525: loss = 0.03598
Step 78530: loss = 0.03723
Step 78535: loss = 0.03800
Step 78540: loss = 0.03755
Step 78545: loss = 0.05607
Step 78550: loss = 0.05215
Step 78555: loss = 0.03247
Step 78560: loss = 0.04132
Step 78565: loss = 0.08601
Step 78570: loss = 0.03335
Step 78575: loss = 0.05168
Step 78580: loss = 0.02684
Step 78585: loss = 0.03950
Step 78590: loss = 0.03125
Step 78595: loss = 0.06431
Step 78600: loss = 0.04127
Step 78605: loss = 0.03321
Step 78610: loss = 0.05691
Step 78615: loss = 0.03324
Step 78620: loss = 0.04425
Step 78625: loss = 0.03967
Step 78630: loss = 0.06544
Step 78635: loss = 0.04649
Step 78640: loss = 0.05529
Step 78645: loss = 0.09196
Step 78650: loss = 0.05823
Step 78655: loss = 0.03817
Step 78660: loss = 0.05697
Step 78665: loss = 0.04832
Step 78670: loss = 0.06896
Step 78675: loss = 0.04632
Step 78680: loss = 0.05186
Step 78685: loss = 0.08214
Step 78690: loss = 0.04299
Step 78695: loss = 0.07096
Step 78700: loss = 0.03564
Step 78705: loss = 0.04281
Step 78710: loss = 0.07122
Step 78715: loss = 0.08096
Step 78720: loss = 0.04121
Step 78725: loss = 0.03818
Step 78730: loss = 0.04019
Step 78735: loss = 0.04097
Step 78740: loss = 0.03413
Step 78745: loss = 0.05916
Step 78750: loss = 0.03761
Step 78755: loss = 0.06902
Step 78760: loss = 0.08825
Step 78765: loss = 0.05973
Step 78770: loss = 0.05695
Step 78775: loss = 0.05433
Step 78780: loss = 0.03635
Training Data Eval:
  Num examples: 49920, Num correct: 49661, Precision @ 1: 0.9948
('Testing Data Eval: EPOCH->', 203)
  Num examples: 9984, Num correct: 7469, Precision @ 1: 0.7481
Step 78785: loss = 0.07303
Step 78790: loss = 0.03240
Step 78795: loss = 0.04178
Step 78800: loss = 0.03260
Step 78805: loss = 0.03263
Step 78810: loss = 0.03199
Step 78815: loss = 0.11422
Step 78820: loss = 0.05538
Step 78825: loss = 0.04786
Step 78830: loss = 0.08204
Step 78835: loss = 0.04378
Step 78840: loss = 0.03238
Step 78845: loss = 0.03480
Step 78850: loss = 0.04845
Step 78855: loss = 0.05949
Step 78860: loss = 0.04982
Step 78865: loss = 0.04107
Step 78870: loss = 0.04012
Step 78875: loss = 0.04457
Step 78880: loss = 0.10248
Step 78885: loss = 0.04807
Step 78890: loss = 0.07944
Step 78895: loss = 0.03415
Step 78900: loss = 0.04648
Step 78905: loss = 0.10375
Step 78910: loss = 0.03597
Step 78915: loss = 0.03667
Step 78920: loss = 0.04386
Step 78925: loss = 0.04098
Step 78930: loss = 0.03451
Step 78935: loss = 0.04280
Step 78940: loss = 0.10814
Step 78945: loss = 0.04155
Step 78950: loss = 0.04630
Step 78955: loss = 0.03619
Step 78960: loss = 0.05931
Step 78965: loss = 0.06119
Step 78970: loss = 0.03114
Step 78975: loss = 0.04356
Step 78980: loss = 0.03746
Step 78985: loss = 0.04808
Step 78990: loss = 0.03319
Step 78995: loss = 0.06537
Step 79000: loss = 0.03410
Step 79005: loss = 0.03431
Step 79010: loss = 0.07147
Step 79015: loss = 0.03626
Step 79020: loss = 0.03292
Step 79025: loss = 0.03379
Step 79030: loss = 0.03718
Step 79035: loss = 0.03592
Step 79040: loss = 0.03456
Step 79045: loss = 0.04782
Step 79050: loss = 0.05259
Step 79055: loss = 0.03171
Step 79060: loss = 0.03841
Step 79065: loss = 0.06508
Step 79070: loss = 0.05222
Step 79075: loss = 0.03219
Step 79080: loss = 0.04094
Step 79085: loss = 0.08593
Step 79090: loss = 0.04183
Step 79095: loss = 0.03185
Step 79100: loss = 0.05363
Step 79105: loss = 0.06860
Step 79110: loss = 0.04295
Step 79115: loss = 0.04711
Step 79120: loss = 0.07833
Step 79125: loss = 0.05062
Step 79130: loss = 0.04057
Step 79135: loss = 0.07171
Step 79140: loss = 0.03422
Step 79145: loss = 0.07612
Step 79150: loss = 0.03354
Step 79155: loss = 0.05112
Step 79160: loss = 0.03975
Step 79165: loss = 0.07905
Step 79170: loss = 0.12539
Training Data Eval:
  Num examples: 49920, Num correct: 49731, Precision @ 1: 0.9962
('Testing Data Eval: EPOCH->', 204)
  Num examples: 9984, Num correct: 7477, Precision @ 1: 0.7489
Step 79175: loss = 0.03087
Step 79180: loss = 0.04103
Step 79185: loss = 0.07189
Step 79190: loss = 0.04741
Step 79195: loss = 0.03653
Step 79200: loss = 0.05960
Step 79205: loss = 0.03423
Step 79210: loss = 0.03754
Step 79215: loss = 0.03487
Step 79220: loss = 0.03608
Step 79225: loss = 0.02844
Step 79230: loss = 0.04767
Step 79235: loss = 0.03209
Step 79240: loss = 0.03925
Step 79245: loss = 0.04282
Step 79250: loss = 0.07062
Step 79255: loss = 0.02709
Step 79260: loss = 0.03944
Step 79265: loss = 0.04517
Step 79270: loss = 0.05059
Step 79275: loss = 0.06113
Step 79280: loss = 0.06617
Step 79285: loss = 0.04565
Step 79290: loss = 0.07460
Step 79295: loss = 0.05715
Step 79300: loss = 0.05096
Step 79305: loss = 0.04068
Step 79310: loss = 0.04719
Step 79315: loss = 0.04167
Step 79320: loss = 0.03625
Step 79325: loss = 0.05430
Step 79330: loss = 0.05913
Step 79335: loss = 0.07577
Step 79340: loss = 0.05260
Step 79345: loss = 0.04800
Step 79350: loss = 0.04360
Step 79355: loss = 0.05321
Step 79360: loss = 0.02307
Step 79365: loss = 0.06540
Step 79370: loss = 0.08976
Step 79375: loss = 0.03177
Step 79380: loss = 0.06860
Step 79385: loss = 0.12294
Step 79390: loss = 0.05653
Step 79395: loss = 0.03587
Step 79400: loss = 0.04331
Step 79405: loss = 0.03933
Step 79410: loss = 0.06711
Step 79415: loss = 0.04447
Step 79420: loss = 0.05930
Step 79425: loss = 0.03609
Step 79430: loss = 0.05594
Step 79435: loss = 0.04450
Step 79440: loss = 0.04822
Step 79445: loss = 0.08028
Step 79450: loss = 0.03877
Step 79455: loss = 0.04069
Step 79460: loss = 0.05646
Step 79465: loss = 0.07637
Step 79470: loss = 0.08298
Step 79475: loss = 0.06392
Step 79480: loss = 0.08199
Step 79485: loss = 0.04549
Step 79490: loss = 0.05535
Step 79495: loss = 0.04564
Step 79500: loss = 0.08565
Step 79505: loss = 0.04127
Step 79510: loss = 0.03670
Step 79515: loss = 0.04490
Step 79520: loss = 0.05561
Step 79525: loss = 0.03738
Step 79530: loss = 0.04159
Step 79535: loss = 0.05229
Step 79540: loss = 0.04061
Step 79545: loss = 0.05231
Step 79550: loss = 0.04765
Step 79555: loss = 0.03246
Step 79560: loss = 0.04181
Training Data Eval:
  Num examples: 49920, Num correct: 49517, Precision @ 1: 0.9919
('Testing Data Eval: EPOCH->', 205)
  Num examples: 9984, Num correct: 7412, Precision @ 1: 0.7424
Step 79565: loss = 0.07339
Step 79570: loss = 0.06051
Step 79575: loss = 0.04617
Step 79580: loss = 0.03588
Step 79585: loss = 0.03768
Step 79590: loss = 0.03341
Step 79595: loss = 0.03927
Step 79600: loss = 0.03433
Step 79605: loss = 0.03992
Step 79610: loss = 0.03936
Step 79615: loss = 0.04815
Step 79620: loss = 0.05342
Step 79625: loss = 0.04167
Step 79630: loss = 0.08912
Step 79635: loss = 0.04406
Step 79640: loss = 0.03147
Step 79645: loss = 0.03593
Step 79650: loss = 0.03953
Step 79655: loss = 0.03318
Step 79660: loss = 0.02829
Step 79665: loss = 0.03501
Step 79670: loss = 0.06060
Step 79675: loss = 0.03030
Step 79680: loss = 0.07101
Step 79685: loss = 0.07201
Step 79690: loss = 0.03230
Step 79695: loss = 0.03789
Step 79700: loss = 0.03413
Step 79705: loss = 0.04231
Step 79710: loss = 0.03454
Step 79715: loss = 0.04082
Step 79720: loss = 0.03577
Step 79725: loss = 0.03353
Step 79730: loss = 0.05329
Step 79735: loss = 0.04073
Step 79740: loss = 0.07547
Step 79745: loss = 0.04421
Step 79750: loss = 0.04431
Step 79755: loss = 0.03712
Step 79760: loss = 0.06165
Step 79765: loss = 0.05641
Step 79770: loss = 0.05061
Step 79775: loss = 0.03709
Step 79780: loss = 0.03147
Step 79785: loss = 0.05178
Step 79790: loss = 0.05263
Step 79795: loss = 0.02801
Step 79800: loss = 0.04006
Step 79805: loss = 0.04432
Step 79810: loss = 0.05776
Step 79815: loss = 0.05256
Step 79820: loss = 0.03327
Step 79825: loss = 0.08692
Step 79830: loss = 0.04325
Step 79835: loss = 0.04345
Step 79840: loss = 0.03693
Step 79845: loss = 0.05790
Step 79850: loss = 0.05262
Step 79855: loss = 0.04392
Step 79860: loss = 0.03186
Step 79865: loss = 0.03945
Step 79870: loss = 0.04235
Step 79875: loss = 0.04313
Step 79880: loss = 0.06063
Step 79885: loss = 0.04560
Step 79890: loss = 0.04285
Step 79895: loss = 0.05619
Step 79900: loss = 0.02890
Step 79905: loss = 0.03041
Step 79910: loss = 0.03672
Step 79915: loss = 0.03749
Step 79920: loss = 0.03452
Step 79925: loss = 0.10421
Step 79930: loss = 0.06278
Step 79935: loss = 0.04028
Step 79940: loss = 0.04222
Step 79945: loss = 0.03836
Step 79950: loss = 0.07779
Training Data Eval:
  Num examples: 49920, Num correct: 49701, Precision @ 1: 0.9956
('Testing Data Eval: EPOCH->', 206)
  Num examples: 9984, Num correct: 7395, Precision @ 1: 0.7407
Step 79955: loss = 0.06623
Step 79960: loss = 0.04959
Step 79965: loss = 0.06680
Step 79970: loss = 0.08129
Step 79975: loss = 0.04765
Step 79980: loss = 0.03585
Step 79985: loss = 0.05577
Step 79990: loss = 0.03413
Step 79995: loss = 0.04585
Step 80000: loss = 0.03320
Step 80005: loss = 0.03156
Step 80010: loss = 0.03461
Step 80015: loss = 0.06533
Step 80020: loss = 0.04675
Step 80025: loss = 0.02862
Step 80030: loss = 0.03257
Step 80035: loss = 0.05121
Step 80040: loss = 0.05789
Step 80045: loss = 0.03544
Step 80050: loss = 0.03117
Step 80055: loss = 0.03881
Step 80060: loss = 0.03451
Step 80065: loss = 0.05900
Step 80070: loss = 0.04580
Step 80075: loss = 0.07410
Step 80080: loss = 0.04469
Step 80085: loss = 0.04338
Step 80090: loss = 0.04103
Step 80095: loss = 0.03927
Step 80100: loss = 0.03932
Step 80105: loss = 0.03804
Step 80110: loss = 0.03380
Step 80115: loss = 0.03834
Step 80120: loss = 0.04737
Step 80125: loss = 0.03836
Step 80130: loss = 0.03528
Step 80135: loss = 0.03120
Step 80140: loss = 0.05341
Step 80145: loss = 0.03609
Step 80150: loss = 0.03253
Step 80155: loss = 0.03516
Step 80160: loss = 0.04137
Step 80165: loss = 0.05046
Step 80170: loss = 0.03865
Step 80175: loss = 0.03748
Step 80180: loss = 0.03528
Step 80185: loss = 0.04621
Step 80190: loss = 0.03887
Step 80195: loss = 0.06361
Step 80200: loss = 0.04354
Step 80205: loss = 0.08660
Step 80210: loss = 0.04230
Step 80215: loss = 0.04096
Step 80220: loss = 0.05780
Step 80225: loss = 0.03449
Step 80230: loss = 0.06686
Step 80235: loss = 0.03181
Step 80240: loss = 0.04455
Step 80245: loss = 0.05890
Step 80250: loss = 0.03979
Step 80255: loss = 0.04806
Step 80260: loss = 0.05662
Step 80265: loss = 0.04070
Step 80270: loss = 0.05089
Step 80275: loss = 0.03723
Step 80280: loss = 0.04457
Step 80285: loss = 0.07173
Step 80290: loss = 0.05934
Step 80295: loss = 0.03006
Step 80300: loss = 0.03895
Step 80305: loss = 0.04103
Step 80310: loss = 0.06680
Step 80315: loss = 0.03178
Step 80320: loss = 0.04856
Step 80325: loss = 0.04219
Step 80330: loss = 0.04766
Step 80335: loss = 0.06767
Step 80340: loss = 0.07500
Training Data Eval:
  Num examples: 49920, Num correct: 49660, Precision @ 1: 0.9948
('Testing Data Eval: EPOCH->', 207)
  Num examples: 9984, Num correct: 7461, Precision @ 1: 0.7473
Step 80345: loss = 0.04150
Step 80350: loss = 0.06928
Step 80355: loss = 0.04189
Step 80360: loss = 0.05658
Step 80365: loss = 0.05707
Step 80370: loss = 0.03908
Step 80375: loss = 0.03630
Step 80380: loss = 0.04097
Step 80385: loss = 0.02836
Step 80390: loss = 0.05547
Step 80395: loss = 0.05347
Step 80400: loss = 0.04184
Step 80405: loss = 0.04121
Step 80410: loss = 0.04555
Step 80415: loss = 0.04113
Step 80420: loss = 0.06837
Step 80425: loss = 0.03831
Step 80430: loss = 0.04878
Step 80435: loss = 0.03441
Step 80440: loss = 0.04064
Step 80445: loss = 0.04973
Step 80450: loss = 0.03827
Step 80455: loss = 0.08578
Step 80460: loss = 0.04307
Step 80465: loss = 0.07677
Step 80470: loss = 0.07195
Step 80475: loss = 0.04578
Step 80480: loss = 0.04366
Step 80485: loss = 0.06277
Step 80490: loss = 0.03207
Step 80495: loss = 0.06009
Step 80500: loss = 0.05062
Step 80505: loss = 0.06381
Step 80510: loss = 0.03906
Step 80515: loss = 0.03439
Step 80520: loss = 0.08681
Step 80525: loss = 0.04979
Step 80530: loss = 0.07332
Step 80535: loss = 0.05333
Step 80540: loss = 0.04508
Step 80545: loss = 0.03649
Step 80550: loss = 0.04742
Step 80555: loss = 0.03675
Step 80560: loss = 0.03533
Step 80565: loss = 0.03875
Step 80570: loss = 0.03608
Step 80575: loss = 0.03750
Step 80580: loss = 0.04534
Step 80585: loss = 0.02822
Step 80590: loss = 0.03678
Step 80595: loss = 0.03556
Step 80600: loss = 0.06667
Step 80605: loss = 0.07641
Step 80610: loss = 0.06032
Step 80615: loss = 0.04296
Step 80620: loss = 0.05406
Step 80625: loss = 0.04236
Step 80630: loss = 0.04541
Step 80635: loss = 0.03358
Step 80640: loss = 0.04768
Step 80645: loss = 0.03507
Step 80650: loss = 0.04508
Step 80655: loss = 0.05349
Step 80660: loss = 0.03826
Step 80665: loss = 0.04147
Step 80670: loss = 0.02851
Step 80675: loss = 0.03903
Step 80680: loss = 0.04382
Step 80685: loss = 0.04063
Step 80690: loss = 0.06695
Step 80695: loss = 0.08303
Step 80700: loss = 0.03715
Step 80705: loss = 0.03023
Step 80710: loss = 0.03388
Step 80715: loss = 0.07597
Step 80720: loss = 0.03658
Step 80725: loss = 0.03863
Step 80730: loss = 0.03040
Training Data Eval:
  Num examples: 49920, Num correct: 49747, Precision @ 1: 0.9965
('Testing Data Eval: EPOCH->', 208)
  Num examples: 9984, Num correct: 7520, Precision @ 1: 0.7532
Step 80735: loss = 0.03271
Step 80740: loss = 0.03803
Step 80745: loss = 0.03711
Step 80750: loss = 0.07280
Step 80755: loss = 0.02984
Step 80760: loss = 0.05175
Step 80765: loss = 0.05446
Step 80770: loss = 0.03629
Step 80775: loss = 0.10991
Step 80780: loss = 0.09097
Step 80785: loss = 0.09290
Step 80790: loss = 0.04017
Step 80795: loss = 0.04824
Step 80800: loss = 0.08758
Step 80805: loss = 0.04788
Step 80810: loss = 0.06257
Step 80815: loss = 0.03136
Step 80820: loss = 0.03361
Step 80825: loss = 0.03726
Step 80830: loss = 0.04708
Step 80835: loss = 0.06918
Step 80840: loss = 0.05816
Step 80845: loss = 0.05778
Step 80850: loss = 0.03612
Step 80855: loss = 0.05278
Step 80860: loss = 0.05656
Step 80865: loss = 0.04262
Step 80870: loss = 0.05405
Step 80875: loss = 0.08293
Step 80880: loss = 0.04448
Step 80885: loss = 0.03794
Step 80890: loss = 0.04082
Step 80895: loss = 0.04344
Step 80900: loss = 0.05816
Step 80905: loss = 0.04230
Step 80910: loss = 0.07769
Step 80915: loss = 0.04978
Step 80920: loss = 0.03487
Step 80925: loss = 0.06609
Step 80930: loss = 0.03413
Step 80935: loss = 0.03117
Step 80940: loss = 0.03776
Step 80945: loss = 0.07638
Step 80950: loss = 0.05879
Step 80955: loss = 0.04444
Step 80960: loss = 0.06735
Step 80965: loss = 0.05786
Step 80970: loss = 0.06166
Step 80975: loss = 0.04032
Step 80980: loss = 0.03845
Step 80985: loss = 0.09211
Step 80990: loss = 0.05943
Step 80995: loss = 0.04203
Step 81000: loss = 0.06400
Step 81005: loss = 0.05878
Step 81010: loss = 0.09886
Step 81015: loss = 0.05880
Step 81020: loss = 0.05442
Step 81025: loss = 0.05789
Step 81030: loss = 0.04273
Step 81035: loss = 0.04126
Step 81040: loss = 0.04285
Step 81045: loss = 0.04296
Step 81050: loss = 0.04112
Step 81055: loss = 0.06789
Step 81060: loss = 0.03361
Step 81065: loss = 0.05205
Step 81070: loss = 0.03225
Step 81075: loss = 0.04029
Step 81080: loss = 0.06825
Step 81085: loss = 0.04975
Step 81090: loss = 0.09008
Step 81095: loss = 0.04348
Step 81100: loss = 0.03933
Step 81105: loss = 0.03755
Step 81110: loss = 0.03274
Step 81115: loss = 0.03593
Step 81120: loss = 0.05646
Training Data Eval:
  Num examples: 49920, Num correct: 49699, Precision @ 1: 0.9956
('Testing Data Eval: EPOCH->', 209)
  Num examples: 9984, Num correct: 7531, Precision @ 1: 0.7543
Step 81125: loss = 0.03978
Step 81130: loss = 0.03796
Step 81135: loss = 0.05537
Step 81140: loss = 0.05691
Step 81145: loss = 0.04143
Step 81150: loss = 0.06718
Step 81155: loss = 0.05651
Step 81160: loss = 0.04764
Step 81165: loss = 0.03524
Step 81170: loss = 0.08633
Step 81175: loss = 0.04484
Step 81180: loss = 0.11485
Step 81185: loss = 0.04704
Step 81190: loss = 0.03254
Step 81195: loss = 0.03810
Step 81200: loss = 0.03801
Step 81205: loss = 0.03173
Step 81210: loss = 0.03898
Step 81215: loss = 0.04895
Step 81220: loss = 0.04792
Step 81225: loss = 0.04896
Step 81230: loss = 0.03793
Step 81235: loss = 0.06659
Step 81240: loss = 0.07081
Step 81245: loss = 0.10719
Step 81250: loss = 0.03726
Step 81255: loss = 0.05278
Step 81260: loss = 0.05125
Step 81265: loss = 0.07804
Step 81270: loss = 0.03750
Step 81275: loss = 0.03806
Step 81280: loss = 0.03860
Step 81285: loss = 0.05699
Step 81290: loss = 0.04430
Step 81295: loss = 0.06896
Step 81300: loss = 0.03620
Step 81305: loss = 0.07263
Step 81310: loss = 0.06350
Step 81315: loss = 0.03704
Step 81320: loss = 0.04994
Step 81325: loss = 0.02876
Step 81330: loss = 0.02992
Step 81335: loss = 0.03870
Step 81340: loss = 0.05662
Step 81345: loss = 0.06434
Step 81350: loss = 0.03388
Step 81355: loss = 0.04204
Step 81360: loss = 0.02864
Step 81365: loss = 0.05750
Step 81370: loss = 0.03871
Step 81375: loss = 0.04406
Step 81380: loss = 0.03830
Step 81385: loss = 0.04489
Step 81390: loss = 0.03898
Step 81395: loss = 0.03579
Step 81400: loss = 0.05918
Step 81405: loss = 0.04809
Step 81410: loss = 0.05452
Step 81415: loss = 0.05918
Step 81420: loss = 0.03916
Step 81425: loss = 0.08303
Step 81430: loss = 0.07318
Step 81435: loss = 0.07483
Step 81440: loss = 0.03941
Step 81445: loss = 0.04836
Step 81450: loss = 0.03933
Step 81455: loss = 0.04696
Step 81460: loss = 0.04660
Step 81465: loss = 0.04287
Step 81470: loss = 0.03435
Step 81475: loss = 0.04893
Step 81480: loss = 0.04285
Step 81485: loss = 0.02448
Step 81490: loss = 0.06405
Step 81495: loss = 0.03127
Step 81500: loss = 0.03760
Step 81505: loss = 0.03994
Step 81510: loss = 0.05278
Training Data Eval:
  Num examples: 49920, Num correct: 49700, Precision @ 1: 0.9956
('Testing Data Eval: EPOCH->', 210)
  Num examples: 9984, Num correct: 7333, Precision @ 1: 0.7345
Step 81515: loss = 0.05071
Step 81520: loss = 0.07304
Step 81525: loss = 0.05917
Step 81530: loss = 0.05005
Step 81535: loss = 0.04470
Step 81540: loss = 0.02956
Step 81545: loss = 0.05097
Step 81550: loss = 0.03762
Step 81555: loss = 0.02990
Step 81560: loss = 0.03640
Step 81565: loss = 0.03946
Step 81570: loss = 0.05194
Step 81575: loss = 0.03625
Step 81580: loss = 0.10214
Step 81585: loss = 0.04579
Step 81590: loss = 0.04361
Step 81595: loss = 0.04490
Step 81600: loss = 0.07759
Step 81605: loss = 0.05385
Step 81610: loss = 0.03408
Step 81615: loss = 0.03782
Step 81620: loss = 0.05076
Step 81625: loss = 0.02983
Step 81630: loss = 0.04443
Step 81635: loss = 0.04572
Step 81640: loss = 0.03386
Step 81645: loss = 0.03387
Step 81650: loss = 0.04257
Step 81655: loss = 0.05659
Step 81660: loss = 0.03713
Step 81665: loss = 0.03311
Step 81670: loss = 0.03847
Step 81675: loss = 0.03988
Step 81680: loss = 0.03884
Step 81685: loss = 0.02566
Step 81690: loss = 0.03480
Step 81695: loss = 0.03926
Step 81700: loss = 0.03277
Step 81705: loss = 0.09227
Step 81710: loss = 0.04174
Step 81715: loss = 0.05288
Step 81720: loss = 0.03442
Step 81725: loss = 0.08083
Step 81730: loss = 0.08666
Step 81735: loss = 0.03525
Step 81740: loss = 0.05325
Step 81745: loss = 0.06572
Step 81750: loss = 0.05506
Step 81755: loss = 0.04767
Step 81760: loss = 0.03878
Step 81765: loss = 0.05416
Step 81770: loss = 0.03198
Step 81775: loss = 0.04216
Step 81780: loss = 0.05668
Step 81785: loss = 0.03819
Step 81790: loss = 0.03175
Step 81795: loss = 0.05957
Step 81800: loss = 0.08462
Step 81805: loss = 0.08179
Step 81810: loss = 0.03496
Step 81815: loss = 0.07196
Step 81820: loss = 0.08187
Step 81825: loss = 0.05690
Step 81830: loss = 0.06102
Step 81835: loss = 0.03404
Step 81840: loss = 0.05303
Step 81845: loss = 0.04629
Step 81850: loss = 0.04040
Step 81855: loss = 0.04222
Step 81860: loss = 0.03971
Step 81865: loss = 0.03758
Step 81870: loss = 0.03242
Step 81875: loss = 0.03449
Step 81880: loss = 0.03587
Step 81885: loss = 0.06015
Step 81890: loss = 0.06074
Step 81895: loss = 0.03274
Step 81900: loss = 0.05052
Training Data Eval:
  Num examples: 49920, Num correct: 49702, Precision @ 1: 0.9956
('Testing Data Eval: EPOCH->', 211)
  Num examples: 9984, Num correct: 7488, Precision @ 1: 0.7500
Step 81905: loss = 0.03413
Step 81910: loss = 0.03152
Step 81915: loss = 0.04844
Step 81920: loss = 0.03908
Step 81925: loss = 0.03515
Step 81930: loss = 0.04664
Step 81935: loss = 0.04342
Step 81940: loss = 0.03090
Step 81945: loss = 0.06049
Step 81950: loss = 0.03635
Step 81955: loss = 0.04100
Step 81960: loss = 0.03730
Step 81965: loss = 0.03114
Step 81970: loss = 0.04049
Step 81975: loss = 0.06702
Step 81980: loss = 0.03011
Step 81985: loss = 0.02964
Step 81990: loss = 0.03219
Step 81995: loss = 0.04369
Step 82000: loss = 0.08130
Step 82005: loss = 0.04068
Step 82010: loss = 0.03779
Step 82015: loss = 0.05913
Step 82020: loss = 0.04471
Step 82025: loss = 0.02919
Step 82030: loss = 0.03319
Step 82035: loss = 0.03287
Step 82040: loss = 0.03845
Step 82045: loss = 0.04395
Step 82050: loss = 0.04236
Step 82055: loss = 0.03808
Step 82060: loss = 0.03341
Step 82065: loss = 0.03860
Step 82070: loss = 0.05458
Step 82075: loss = 0.04074
Step 82080: loss = 0.03187
Step 82085: loss = 0.04751
Step 82090: loss = 0.03604
Step 82095: loss = 0.03327
Step 82100: loss = 0.04650
Step 82105: loss = 0.07942
Step 82110: loss = 0.03953
Step 82115: loss = 0.05935
Step 82120: loss = 0.04620
Step 82125: loss = 0.03389
Step 82130: loss = 0.03246
Step 82135: loss = 0.07644
Step 82140: loss = 0.03533
Step 82145: loss = 0.03763
Step 82150: loss = 0.09697
Step 82155: loss = 0.04322
Step 82160: loss = 0.03272
Step 82165: loss = 0.03193
Step 82170: loss = 0.04166
Step 82175: loss = 0.03069
Step 82180: loss = 0.04980
Step 82185: loss = 0.04722
Step 82190: loss = 0.05461
Step 82195: loss = 0.04154
Step 82200: loss = 0.04758
Step 82205: loss = 0.06485
Step 82210: loss = 0.05224
Step 82215: loss = 0.03817
Step 82220: loss = 0.04660
Step 82225: loss = 0.03727
Step 82230: loss = 0.03207
Step 82235: loss = 0.04484
Step 82240: loss = 0.03602
Step 82245: loss = 0.04740
Step 82250: loss = 0.03938
Step 82255: loss = 0.06567
Step 82260: loss = 0.05665
Step 82265: loss = 0.05155
Step 82270: loss = 0.05829
Step 82275: loss = 0.03215
Step 82280: loss = 0.03368
Step 82285: loss = 0.03132
Step 82290: loss = 0.04262
Training Data Eval:
  Num examples: 49920, Num correct: 49734, Precision @ 1: 0.9963
('Testing Data Eval: EPOCH->', 212)
  Num examples: 9984, Num correct: 7491, Precision @ 1: 0.7503
Step 82295: loss = 0.04927
Step 82300: loss = 0.02642
Step 82305: loss = 0.04598
Step 82310: loss = 0.02978
Step 82315: loss = 0.03456
Step 82320: loss = 0.04042
Step 82325: loss = 0.04169
Step 82330: loss = 0.06893
Step 82335: loss = 0.04029
Step 82340: loss = 0.02712
Step 82345: loss = 0.04704
Step 82350: loss = 0.02986
Step 82355: loss = 0.03392
Step 82360: loss = 0.03189
Step 82365: loss = 0.03274
Step 82370: loss = 0.04201
Step 82375: loss = 0.03791
Step 82380: loss = 0.03378
Step 82385: loss = 0.04960
Step 82390: loss = 0.04859
Step 82395: loss = 0.03273
Step 82400: loss = 0.04346
Step 82405: loss = 0.06521
Step 82410: loss = 0.05811
Step 82415: loss = 0.07542
Step 82420: loss = 0.04839
Step 82425: loss = 0.03925
Step 82430: loss = 0.05486
Step 82435: loss = 0.02792
Step 82440: loss = 0.03455
Step 82445: loss = 0.06160
Step 82450: loss = 0.04384
Step 82455: loss = 0.03569
Step 82460: loss = 0.04825
Step 82465: loss = 0.06959
Step 82470: loss = 0.05395
Step 82475: loss = 0.02934
Step 82480: loss = 0.02306
Step 82485: loss = 0.05529
Step 82490: loss = 0.04300
Step 82495: loss = 0.05670
Step 82500: loss = 0.04638
Step 82505: loss = 0.03856
Step 82510: loss = 0.05122
Step 82515: loss = 0.05008
Step 82520: loss = 0.03946
Step 82525: loss = 0.03537
Step 82530: loss = 0.04520
Step 82535: loss = 0.04053
Step 82540: loss = 0.09766
Step 82545: loss = 0.04665
Step 82550: loss = 0.04312
Step 82555: loss = 0.03279
Step 82560: loss = 0.03204
Step 82565: loss = 0.04071
Step 82570: loss = 0.04273
Step 82575: loss = 0.04229
Step 82580: loss = 0.03275
Step 82585: loss = 0.04192
Step 82590: loss = 0.05369
Step 82595: loss = 0.06829
Step 82600: loss = 0.06091
Step 82605: loss = 0.04800
Step 82610: loss = 0.05237
Step 82615: loss = 0.04836
Step 82620: loss = 0.05309
Step 82625: loss = 0.04683
Step 82630: loss = 0.06337
Step 82635: loss = 0.05733
Step 82640: loss = 0.05158
Step 82645: loss = 0.04269
Step 82650: loss = 0.02903
Step 82655: loss = 0.03750
Step 82660: loss = 0.08942
Step 82665: loss = 0.03966
Step 82670: loss = 0.06285
Step 82675: loss = 0.05612
Step 82680: loss = 0.03185
Training Data Eval:
  Num examples: 49920, Num correct: 49702, Precision @ 1: 0.9956
('Testing Data Eval: EPOCH->', 213)
  Num examples: 9984, Num correct: 7468, Precision @ 1: 0.7480
Step 82685: loss = 0.03206
Step 82690: loss = 0.07515
Step 82695: loss = 0.05453
Step 82700: loss = 0.03093
Step 82705: loss = 0.03311
Step 82710: loss = 0.02923
Step 82715: loss = 0.03948
Step 82720: loss = 0.04238
Step 82725: loss = 0.03936
Step 82730: loss = 0.03790
Step 82735: loss = 0.04865
Step 82740: loss = 0.03421
Step 82745: loss = 0.03721
Step 82750: loss = 0.05472
Step 82755: loss = 0.03380
Step 82760: loss = 0.03656
Step 82765: loss = 0.07236
Step 82770: loss = 0.05113
Step 82775: loss = 0.03509
Step 82780: loss = 0.03523
Step 82785: loss = 0.04045
Step 82790: loss = 0.03669
Step 82795: loss = 0.04238
Step 82800: loss = 0.02861
Step 82805: loss = 0.03776
Step 82810: loss = 0.03355
Step 82815: loss = 0.03155
Step 82820: loss = 0.05504
Step 82825: loss = 0.03196
Step 82830: loss = 0.04108
Step 82835: loss = 0.04463
Step 82840: loss = 0.02814
Step 82845: loss = 0.05820
Step 82850: loss = 0.04393
Step 82855: loss = 0.05584
Step 82860: loss = 0.10514
Step 82865: loss = 0.04086
Step 82870: loss = 0.03714
Step 82875: loss = 0.04199
Step 82880: loss = 0.05240
Step 82885: loss = 0.03421
Step 82890: loss = 0.05581
Step 82895: loss = 0.06058
Step 82900: loss = 0.03499
Step 82905: loss = 0.03688
Step 82910: loss = 0.04496
Step 82915: loss = 0.03714
Step 82920: loss = 0.04037
Step 82925: loss = 0.04455
Step 82930: loss = 0.06214
Step 82935: loss = 0.03183
Step 82940: loss = 0.03752
Step 82945: loss = 0.06422
Step 82950: loss = 0.03845
Step 82955: loss = 0.05936
Step 82960: loss = 0.03078
Step 82965: loss = 0.03832
Step 82970: loss = 0.03875
Step 82975: loss = 0.02925
Step 82980: loss = 0.04669
Step 82985: loss = 0.03549
