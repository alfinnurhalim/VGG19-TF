Student with Knowledge Distillation Approach
Step 0: loss = 51.20522
Training Data Eval:
  Num examples: 49920, Num correct: 5087, Precision @ 1: 0.1019
('Testing Data Eval: EPOCH->', 1)
  Num examples: 9984, Num correct: 1013, Precision @ 1: 0.1015
Step 5: loss = 33.18996
Step 10: loss = 28.65107
Step 15: loss = 18.65153
Step 20: loss = 18.04365
Step 25: loss = 13.09928
Step 30: loss = 9.48796
Step 35: loss = 7.83959
Step 40: loss = 6.61018
Step 45: loss = 5.07819
Step 50: loss = 6.35630
Step 55: loss = 4.78599
Step 60: loss = 7.06929
Step 65: loss = 4.76987
Step 70: loss = 5.38326
Step 75: loss = 5.31780
Step 80: loss = 5.02615
Step 85: loss = 5.53283
Step 90: loss = 6.13509
Step 95: loss = 6.29367
Step 100: loss = 5.32285
Step 105: loss = 6.80410
Step 110: loss = 4.24549
Step 115: loss = 3.84287
Step 120: loss = 5.06775
Step 125: loss = 2.83910
Step 130: loss = 2.85921
Step 135: loss = 4.64503
Step 140: loss = 3.63306
Step 145: loss = 3.60605
Step 150: loss = 3.73840
Step 155: loss = 2.59652
Step 160: loss = 2.91390
Step 165: loss = 3.92853
Step 170: loss = 5.62882
Step 175: loss = 2.68952
Step 180: loss = 5.12804
Step 185: loss = 3.28613
Step 190: loss = 2.50369
Step 195: loss = 3.77484
Step 200: loss = 3.16166
Step 205: loss = 5.29567
Step 210: loss = 4.26006
Step 215: loss = 2.60343
Step 220: loss = 5.56893
Step 225: loss = 2.65171
Step 230: loss = 4.68358
Step 235: loss = 3.29893
Step 240: loss = 3.18559
Step 245: loss = 4.68408
Step 250: loss = 2.68317
Step 255: loss = 3.60907
Step 260: loss = 2.95046
Step 265: loss = 4.43206
Step 270: loss = 4.09840
Step 275: loss = 2.92942
Step 280: loss = 3.07073
Step 285: loss = 2.48167
Step 290: loss = 3.48647
Step 295: loss = 2.72819
Step 300: loss = 2.51073
Step 305: loss = 5.37627
Step 310: loss = 4.43963
Step 315: loss = 2.24649
Step 320: loss = 2.39236
Step 325: loss = 4.59460
Step 330: loss = 2.48309
Step 335: loss = 3.15715
Step 340: loss = 3.51114
Step 345: loss = 4.60988
Step 350: loss = 3.73242
Step 355: loss = 2.49486
Step 360: loss = 3.82637
Step 365: loss = 2.12590
Step 370: loss = 3.09866
Step 375: loss = 2.58892
Step 380: loss = 3.22490
Step 385: loss = 2.43567
Step 390: loss = 3.96972
Training Data Eval:
  Num examples: 49920, Num correct: 9226, Precision @ 1: 0.1848
('Testing Data Eval: EPOCH->', 2)
  Num examples: 9984, Num correct: 1883, Precision @ 1: 0.1886
Step 395: loss = 3.67929
Step 400: loss = 4.74278
Step 405: loss = 3.19889
Step 410: loss = 2.81899
Step 415: loss = 2.24546
Step 420: loss = 3.58172
Step 425: loss = 3.24982
Step 430: loss = 2.55431
Step 435: loss = 3.28348
Step 440: loss = 2.05243
Step 445: loss = 2.13748
Step 450: loss = 3.52991
Step 455: loss = 3.18425
Step 460: loss = 3.61653
Step 465: loss = 2.52368
Step 470: loss = 2.47123
Step 475: loss = 3.64216
Step 480: loss = 2.83268
Step 485: loss = 2.78576
Step 490: loss = 2.15710
Step 495: loss = 1.98216
Step 500: loss = 2.85503
Step 505: loss = 2.03113
Step 510: loss = 1.89101
Step 515: loss = 1.89816
Step 520: loss = 2.57077
Step 525: loss = 1.97689
Step 530: loss = 1.91544
Step 535: loss = 1.87832
Step 540: loss = 2.54994
Step 545: loss = 3.00113
Step 550: loss = 2.24942
Step 555: loss = 2.44681
Step 560: loss = 2.98290
Step 565: loss = 2.26115
Step 570: loss = 2.50226
Step 575: loss = 2.02777
Step 580: loss = 2.01107
Step 585: loss = 3.76329
Step 590: loss = 2.01613
Step 595: loss = 2.33427
Step 600: loss = 2.14123
Step 605: loss = 2.11776
Step 610: loss = 1.96919
Step 615: loss = 2.32533
Step 620: loss = 1.92696
Step 625: loss = 2.10347
Step 630: loss = 4.36385
Step 635: loss = 2.15486
Step 640: loss = 2.55400
Step 645: loss = 2.20460
Step 650: loss = 2.02029
Step 655: loss = 1.91608
Step 660: loss = 2.49641
Step 665: loss = 1.91891
Step 670: loss = 2.00917
Step 675: loss = 2.85773
Step 680: loss = 2.12991
Step 685: loss = 2.26637
Step 690: loss = 1.83085
Step 695: loss = 2.31577
Step 700: loss = 1.78351
Step 705: loss = 2.40021
Step 710: loss = 1.86340
Step 715: loss = 1.99379
Step 720: loss = 1.91759
Step 725: loss = 2.89043
Step 730: loss = 1.78127
Step 735: loss = 2.75704
Step 740: loss = 2.21236
Step 745: loss = 2.34426
Step 750: loss = 1.78566
Step 755: loss = 2.30344
Step 760: loss = 1.94589
Step 765: loss = 2.24379
Step 770: loss = 1.92956
Step 775: loss = 1.79732
Step 780: loss = 2.18779
Training Data Eval:
  Num examples: 49920, Num correct: 15764, Precision @ 1: 0.3158
('Testing Data Eval: EPOCH->', 3)
  Num examples: 9984, Num correct: 3048, Precision @ 1: 0.3053
Step 785: loss = 1.71523
Step 790: loss = 2.34139
Step 795: loss = 2.42934
Step 800: loss = 2.19817
Step 805: loss = 1.86303
Step 810: loss = 2.33262
Step 815: loss = 1.76156
Step 820: loss = 1.65585
Step 825: loss = 1.98439
Step 830: loss = 2.00379
Step 835: loss = 1.90984
Step 840: loss = 1.82923
Step 845: loss = 1.74792
Step 850: loss = 1.81438
Step 855: loss = 2.43708
Step 860: loss = 2.15127
Step 865: loss = 1.72567
Step 870: loss = 3.11174
Step 875: loss = 2.35144
Step 880: loss = 1.91108
Step 885: loss = 2.06573
Step 890: loss = 2.76442
Step 895: loss = 2.49833
Step 900: loss = 1.71214
Step 905: loss = 1.85204
Step 910: loss = 1.85245
Step 915: loss = 2.20563
Step 920: loss = 1.81188
Step 925: loss = 1.98975
Step 930: loss = 2.05008
Step 935: loss = 1.69817
Step 940: loss = 1.82852
Step 945: loss = 1.68569
Step 950: loss = 1.94360
Step 955: loss = 1.80304
Step 960: loss = 2.13190
Step 965: loss = 1.71033
Step 970: loss = 1.79445
Step 975: loss = 1.87298
Step 980: loss = 1.89249
Step 985: loss = 2.16901
Step 990: loss = 1.81832
Step 995: loss = 1.71622
Step 1000: loss = 2.08342
Step 1005: loss = 2.14835
Step 1010: loss = 1.65347
Step 1015: loss = 1.52877
Step 1020: loss = 2.42739
Step 1025: loss = 1.86147
Step 1030: loss = 2.59024
Step 1035: loss = 1.66764
Step 1040: loss = 1.63469
Step 1045: loss = 1.89004
Step 1050: loss = 1.69402
Step 1055: loss = 1.73822
Step 1060: loss = 1.70246
Step 1065: loss = 1.75491
Step 1070: loss = 1.62761
Step 1075: loss = 2.34937
Step 1080: loss = 2.10333
Step 1085: loss = 1.65999
Step 1090: loss = 1.79265
Step 1095: loss = 1.68855
Step 1100: loss = 1.58044
Step 1105: loss = 1.71606
Step 1110: loss = 1.83498
Step 1115: loss = 1.84504
Step 1120: loss = 1.72121
Step 1125: loss = 2.90476
Step 1130: loss = 1.80174
Step 1135: loss = 1.86663
Step 1140: loss = 1.43669
Step 1145: loss = 1.45582
Step 1150: loss = 1.55340
Step 1155: loss = 1.64758
Step 1160: loss = 1.86785
Step 1165: loss = 2.40116
Step 1170: loss = 1.72358
Training Data Eval:
  Num examples: 49920, Num correct: 19238, Precision @ 1: 0.3854
('Testing Data Eval: EPOCH->', 4)
  Num examples: 9984, Num correct: 3670, Precision @ 1: 0.3676
Step 1175: loss = 1.91392
Step 1180: loss = 1.81586
Step 1185: loss = 1.75268
Step 1190: loss = 1.57704
Step 1195: loss = 1.75185
Step 1200: loss = 1.50171
Step 1205: loss = 1.51114
Step 1210: loss = 1.65106
Step 1215: loss = 2.50292
Step 1220: loss = 2.15097
Step 1225: loss = 2.12996
Step 1230: loss = 1.77292
Step 1235: loss = 2.02794
Step 1240: loss = 1.63988
Step 1245: loss = 1.67126
Step 1250: loss = 2.35255
Step 1255: loss = 1.81319
Step 1260: loss = 1.71131
Step 1265: loss = 1.47416
Step 1270: loss = 1.72129
Step 1275: loss = 1.60121
Step 1280: loss = 1.84648
Step 1285: loss = 1.68242
Step 1290: loss = 1.84817
Step 1295: loss = 1.52076
Step 1300: loss = 1.66303
Step 1305: loss = 2.77375
Step 1310: loss = 1.50501
Step 1315: loss = 1.63463
Step 1320: loss = 1.74959
Step 1325: loss = 1.65160
Step 1330: loss = 1.70278
Step 1335: loss = 1.76005
Step 1340: loss = 1.54146
Step 1345: loss = 2.20416
Step 1350: loss = 1.49265
Step 1355: loss = 1.55829
Step 1360: loss = 2.08227
Step 1365: loss = 1.76968
Step 1370: loss = 1.48114
Step 1375: loss = 1.78052
Step 1380: loss = 1.85131
Step 1385: loss = 1.70041
Step 1390: loss = 1.99110
Step 1395: loss = 1.78001
Step 1400: loss = 2.00669
Step 1405: loss = 1.81079
Step 1410: loss = 1.63644
Step 1415: loss = 1.68467
Step 1420: loss = 1.70677
Step 1425: loss = 1.44619
Step 1430: loss = 1.57594
Step 1435: loss = 1.49397
Step 1440: loss = 1.52550
Step 1445: loss = 1.52956
Step 1450: loss = 1.54961
Step 1455: loss = 1.45018
Step 1460: loss = 1.66612
Step 1465: loss = 1.58594
Step 1470: loss = 1.52729
Step 1475: loss = 1.70826
Step 1480: loss = 1.61986
Step 1485: loss = 2.15645
Step 1490: loss = 1.95336
Step 1495: loss = 1.76112
Step 1500: loss = 1.57418
Step 1505: loss = 1.61668
Step 1510: loss = 1.56169
Step 1515: loss = 1.57153
Step 1520: loss = 1.50624
Step 1525: loss = 1.44808
Step 1530: loss = 1.83935
Step 1535: loss = 1.89100
Step 1540: loss = 1.50334
Step 1545: loss = 1.46829
Step 1550: loss = 1.53245
Step 1555: loss = 1.49568
Step 1560: loss = 1.51926
Training Data Eval:
  Num examples: 49920, Num correct: 21961, Precision @ 1: 0.4399
('Testing Data Eval: EPOCH->', 5)
  Num examples: 9984, Num correct: 4092, Precision @ 1: 0.4099
Step 1565: loss = 1.50072
Step 1570: loss = 1.51291
Step 1575: loss = 1.38028
Step 1580: loss = 1.68620
Step 1585: loss = 1.46856
Step 1590: loss = 1.57697
Step 1595: loss = 1.53737
Step 1600: loss = 1.40681
Step 1605: loss = 1.56273
Step 1610: loss = 1.56717
Step 1615: loss = 1.63478
Step 1620: loss = 1.58645
Step 1625: loss = 1.55529
Step 1630: loss = 1.43091
Step 1635: loss = 1.46983
Step 1640: loss = 1.83461
Step 1645: loss = 1.75672
Step 1650: loss = 1.64845
Step 1655: loss = 1.43056
Step 1660: loss = 1.31557
Step 1665: loss = 1.49693
Step 1670: loss = 1.55537
Step 1675: loss = 1.54865
Step 1680: loss = 1.67605
Step 1685: loss = 1.42057
Step 1690: loss = 1.92618
Step 1695: loss = 1.87287
Step 1700: loss = 1.56558
Step 1705: loss = 1.42470
Step 1710: loss = 1.30027
Step 1715: loss = 1.40517
Step 1720: loss = 1.38337
Step 1725: loss = 1.37080
Step 1730: loss = 1.56906
Step 1735: loss = 1.45424
Step 1740: loss = 1.46944
Step 1745: loss = 1.38919
Step 1750: loss = 1.50639
Step 1755: loss = 1.41339
Step 1760: loss = 1.55844
Step 1765: loss = 1.42037
Step 1770: loss = 1.46277
Step 1775: loss = 1.68100
Step 1780: loss = 1.47437
Step 1785: loss = 1.43969
Step 1790: loss = 1.46908
Step 1795: loss = 1.66805
Step 1800: loss = 1.36711
Step 1805: loss = 1.44429
Step 1810: loss = 1.60906
Step 1815: loss = 1.27536
Step 1820: loss = 1.72238
Step 1825: loss = 1.45008
Step 1830: loss = 1.88926
Step 1835: loss = 1.59829
Step 1840: loss = 1.29779
Step 1845: loss = 1.48578
Step 1850: loss = 1.61210
Step 1855: loss = 1.34870
Step 1860: loss = 1.37982
Step 1865: loss = 1.66595
Step 1870: loss = 1.44917
Step 1875: loss = 1.33966
Step 1880: loss = 1.57126
Step 1885: loss = 1.51162
Step 1890: loss = 1.78176
Step 1895: loss = 1.39025
Step 1900: loss = 1.60911
Step 1905: loss = 1.52463
Step 1910: loss = 1.37411
Step 1915: loss = 1.38633
Step 1920: loss = 1.46992
Step 1925: loss = 1.46762
Step 1930: loss = 1.35054
Step 1935: loss = 1.34700
Step 1940: loss = 1.68331
Step 1945: loss = 1.50842
Step 1950: loss = 1.33289
Training Data Eval:
  Num examples: 49920, Num correct: 23850, Precision @ 1: 0.4778
('Testing Data Eval: EPOCH->', 6)
  Num examples: 9984, Num correct: 4369, Precision @ 1: 0.4376
Step 1955: loss = 1.73807
Step 1960: loss = 1.67800
Step 1965: loss = 1.44166
Step 1970: loss = 1.41489
Step 1975: loss = 1.33196
Step 1980: loss = 1.34283
Step 1985: loss = 1.36233
Step 1990: loss = 1.45512
Step 1995: loss = 1.38068
Step 2000: loss = 1.29384
Step 2005: loss = 1.62220
Step 2010: loss = 1.55297
Step 2015: loss = 1.26243
Step 2020: loss = 1.37488
Step 2025: loss = 1.54068
Step 2030: loss = 1.45792
Step 2035: loss = 1.18673
Step 2040: loss = 1.66302
Step 2045: loss = 1.57236
Step 2050: loss = 1.34176
Step 2055: loss = 1.27016
Step 2060: loss = 1.45518
Step 2065: loss = 1.26928
Step 2070: loss = 1.95616
Step 2075: loss = 1.40160
Step 2080: loss = 1.35406
Step 2085: loss = 1.50910
Step 2090: loss = 1.23847
Step 2095: loss = 1.38966
Step 2100: loss = 1.63910
Step 2105: loss = 1.43984
Step 2110: loss = 1.38921
Step 2115: loss = 1.39258
Step 2120: loss = 1.57533
Step 2125: loss = 1.48035
Step 2130: loss = 1.69176
Step 2135: loss = 1.45355
Step 2140: loss = 1.33171
Step 2145: loss = 1.32977
Step 2150: loss = 1.55109
Step 2155: loss = 1.51542
Step 2160: loss = 1.40441
Step 2165: loss = 1.48657
Step 2170: loss = 1.40200
Step 2175: loss = 1.40755
Step 2180: loss = 1.65774
Step 2185: loss = 1.27573
Step 2190: loss = 1.38000
Step 2195: loss = 1.49984
Step 2200: loss = 1.49065
Step 2205: loss = 1.55828
Step 2210: loss = 1.56350
Step 2215: loss = 1.50778
Step 2220: loss = 1.47962
Step 2225: loss = 1.27624
Step 2230: loss = 1.30223
Step 2235: loss = 1.41871
Step 2240: loss = 1.42383
Step 2245: loss = 1.51405
Step 2250: loss = 1.77283
Step 2255: loss = 1.40453
Step 2260: loss = 1.39856
Step 2265: loss = 1.51198
Step 2270: loss = 1.21886
Step 2275: loss = 1.29044
Step 2280: loss = 1.57197
Step 2285: loss = 1.44505
Step 2290: loss = 1.68581
Step 2295: loss = 1.17959
Step 2300: loss = 1.49148
Step 2305: loss = 1.74134
Step 2310: loss = 1.51593
Step 2315: loss = 1.47232
Step 2320: loss = 1.42592
Step 2325: loss = 1.50845
Step 2330: loss = 1.42224
Step 2335: loss = 1.49910
Step 2340: loss = 1.25920
Training Data Eval:
  Num examples: 49920, Num correct: 25549, Precision @ 1: 0.5118
('Testing Data Eval: EPOCH->', 7)
  Num examples: 9984, Num correct: 4741, Precision @ 1: 0.4749
Step 2345: loss = 1.37968
Step 2350: loss = 1.42049
Step 2355: loss = 1.45258
Step 2360: loss = 1.26943
Step 2365: loss = 1.43082
Step 2370: loss = 1.40704
Step 2375: loss = 1.49819
Step 2380: loss = 1.54202
Step 2385: loss = 1.20333
Step 2390: loss = 1.46776
Step 2395: loss = 1.37192
Step 2400: loss = 1.58470
Step 2405: loss = 1.33969
Step 2410: loss = 1.35957
Step 2415: loss = 1.72161
Step 2420: loss = 1.24356
Step 2425: loss = 1.32293
Step 2430: loss = 1.08375
Step 2435: loss = 1.62569
Step 2440: loss = 1.27695
Step 2445: loss = 1.34832
Step 2450: loss = 1.38515
Step 2455: loss = 1.28654
Step 2460: loss = 1.24674
Step 2465: loss = 1.26456
Step 2470: loss = 1.50979
Step 2475: loss = 1.52343
Step 2480: loss = 1.70324
Step 2485: loss = 1.34654
Step 2490: loss = 1.29590
Step 2495: loss = 1.26555
Step 2500: loss = 1.17838
Step 2505: loss = 1.30730
Step 2510: loss = 1.13123
Step 2515: loss = 1.36182
Step 2520: loss = 1.41679
Step 2525: loss = 1.23981
Step 2530: loss = 1.26806
Step 2535: loss = 1.30558
Step 2540: loss = 1.41310
Step 2545: loss = 1.20481
Step 2550: loss = 1.23084
Step 2555: loss = 1.51458
Step 2560: loss = 1.32009
Step 2565: loss = 1.44644
Step 2570: loss = 1.36209
Step 2575: loss = 1.67002
Step 2580: loss = 1.23144
Step 2585: loss = 1.22608
Step 2590: loss = 1.55836
Step 2595: loss = 1.27604
Step 2600: loss = 1.36296
Step 2605: loss = 1.38916
Step 2610: loss = 1.58129
Step 2615: loss = 1.43729
Step 2620: loss = 1.23972
Step 2625: loss = 1.47918
Step 2630: loss = 1.48949
Step 2635: loss = 1.29534
Step 2640: loss = 1.42648
Step 2645: loss = 1.47284
Step 2650: loss = 1.57305
Step 2655: loss = 1.34049
Step 2660: loss = 1.34093
Step 2665: loss = 1.57570
Step 2670: loss = 1.32405
Step 2675: loss = 1.24598
Step 2680: loss = 1.15320
Step 2685: loss = 1.32561
Step 2690: loss = 1.38860
Step 2695: loss = 1.11916
Step 2700: loss = 1.21893
Step 2705: loss = 1.37442
Step 2710: loss = 1.45938
Step 2715: loss = 1.18626
Step 2720: loss = 1.14013
Step 2725: loss = 1.41964
Step 2730: loss = 1.10266
Training Data Eval:
  Num examples: 49920, Num correct: 26999, Precision @ 1: 0.5408
('Testing Data Eval: EPOCH->', 8)
  Num examples: 9984, Num correct: 4996, Precision @ 1: 0.5004
Step 2735: loss = 1.28894
Step 2740: loss = 1.49337
Step 2745: loss = 1.35222
Step 2750: loss = 1.15731
Step 2755: loss = 1.31321
Step 2760: loss = 1.42694
Step 2765: loss = 1.49398
Step 2770: loss = 1.24448
Step 2775: loss = 1.42497
Step 2780: loss = 1.36163
Step 2785: loss = 1.33120
Step 2790: loss = 1.30191
Step 2795: loss = 1.11331
Step 2800: loss = 1.31519
Step 2805: loss = 1.38654
Step 2810: loss = 1.16155
Step 2815: loss = 1.26180
Step 2820: loss = 1.31380
Step 2825: loss = 1.24396
Step 2830: loss = 1.09418
Step 2835: loss = 1.21597
Step 2840: loss = 1.30422
Step 2845: loss = 1.26822
Step 2850: loss = 1.33314
Step 2855: loss = 1.32719
Step 2860: loss = 1.26308
Step 2865: loss = 1.30864
Step 2870: loss = 1.20427
Step 2875: loss = 1.38773
Step 2880: loss = 1.33677
Step 2885: loss = 1.28336
Step 2890: loss = 1.13151
Step 2895: loss = 1.19008
Step 2900: loss = 1.03901
Step 2905: loss = 1.09219
Step 2910: loss = 1.35519
Step 2915: loss = 1.08979
Step 2920: loss = 1.45489
Step 2925: loss = 1.29398
Step 2930: loss = 1.35120
Step 2935: loss = 1.25839
Step 2940: loss = 1.07412
Step 2945: loss = 1.12059
Step 2950: loss = 1.16211
Step 2955: loss = 1.15526
Step 2960: loss = 1.44435
Step 2965: loss = 1.12077
Step 2970: loss = 1.32922
Step 2975: loss = 1.21597
Step 2980: loss = 1.24096
Step 2985: loss = 1.18659
Step 2990: loss = 1.17032
Step 2995: loss = 1.30084
Step 3000: loss = 1.26401
Step 3005: loss = 1.11714
Step 3010: loss = 1.27872
Step 3015: loss = 1.33836
Step 3020: loss = 1.27137
Step 3025: loss = 1.20324
Step 3030: loss = 1.17782
Step 3035: loss = 1.15504
Step 3040: loss = 1.59791
Step 3045: loss = 1.34202
Step 3050: loss = 1.08989
Step 3055: loss = 1.35680
Step 3060: loss = 1.33506
Step 3065: loss = 1.27693
Step 3070: loss = 1.41995
Step 3075: loss = 1.16979
Step 3080: loss = 1.28957
Step 3085: loss = 1.16931
Step 3090: loss = 1.09852
Step 3095: loss = 1.17998
Step 3100: loss = 1.40987
Step 3105: loss = 1.24983
Step 3110: loss = 1.11482
Step 3115: loss = 1.26895
Step 3120: loss = 1.31361
Training Data Eval:
  Num examples: 49920, Num correct: 28539, Precision @ 1: 0.5717
('Testing Data Eval: EPOCH->', 9)
  Num examples: 9984, Num correct: 5117, Precision @ 1: 0.5125
Step 3125: loss = 1.20276
Step 3130: loss = 1.16335
Step 3135: loss = 1.20131
Step 3140: loss = 1.21532
Step 3145: loss = 1.24272
Step 3150: loss = 1.18303
Step 3155: loss = 1.33496
Step 3160: loss = 1.21321
Step 3165: loss = 1.25884
Step 3170: loss = 1.14389
Step 3175: loss = 1.43893
Step 3180: loss = 1.29612
Step 3185: loss = 1.45330
Step 3190: loss = 1.25803
Step 3195: loss = 1.32633
Step 3200: loss = 1.13970
Step 3205: loss = 1.20902
Step 3210: loss = 1.13387
Step 3215: loss = 1.33039
Step 3220: loss = 1.12263
Step 3225: loss = 1.33545
Step 3230: loss = 1.14448
Step 3235: loss = 1.50672
Step 3240: loss = 1.22371
Step 3245: loss = 1.15691
Step 3250: loss = 1.06269
Step 3255: loss = 1.13031
Step 3260: loss = 1.19739
Step 3265: loss = 0.96175
Step 3270: loss = 1.17799
Step 3275: loss = 1.31663
Step 3280: loss = 1.30713
Step 3285: loss = 1.09183
Step 3290: loss = 1.00475
Step 3295: loss = 1.24318
Step 3300: loss = 1.21958
Step 3305: loss = 1.14612
Step 3310: loss = 1.23788
Step 3315: loss = 1.21611
Step 3320: loss = 1.18752
Step 3325: loss = 1.14280
Step 3330: loss = 1.22259
Step 3335: loss = 1.06756
Step 3340: loss = 1.09207
Step 3345: loss = 1.27331
Step 3350: loss = 1.19077
Step 3355: loss = 1.14415
Step 3360: loss = 1.08932
Step 3365: loss = 1.61389
Step 3370: loss = 1.23722
Step 3375: loss = 1.15958
Step 3380: loss = 1.26264
Step 3385: loss = 1.26216
Step 3390: loss = 1.13621
Step 3395: loss = 1.21621
Step 3400: loss = 1.21008
Step 3405: loss = 1.02557
Step 3410: loss = 1.14843
Step 3415: loss = 0.84735
Step 3420: loss = 1.17248
Step 3425: loss = 0.98471
Step 3430: loss = 1.08981
Step 3435: loss = 1.19358
Step 3440: loss = 1.00632
Step 3445: loss = 1.32870
Step 3450: loss = 1.15936
Step 3455: loss = 1.25072
Step 3460: loss = 1.20588
Step 3465: loss = 1.16558
Step 3470: loss = 1.24306
Step 3475: loss = 1.31167
Step 3480: loss = 1.10762
Step 3485: loss = 1.15608
Step 3490: loss = 1.14314
Step 3495: loss = 1.31111
Step 3500: loss = 1.12145
Step 3505: loss = 1.22678
Step 3510: loss = 1.15346
Training Data Eval:
  Num examples: 49920, Num correct: 29588, Precision @ 1: 0.5927
('Testing Data Eval: EPOCH->', 10)
  Num examples: 9984, Num correct: 5303, Precision @ 1: 0.5311
Step 3515: loss = 1.17395
Step 3520: loss = 1.13046
Step 3525: loss = 1.10739
Step 3530: loss = 1.21963
Step 3535: loss = 1.06387
Step 3540: loss = 1.11286
Step 3545: loss = 1.20918
Step 3550: loss = 1.27127
Step 3555: loss = 1.12607
Step 3560: loss = 1.15960
Step 3565: loss = 1.30265
Step 3570: loss = 0.90840
Step 3575: loss = 1.02306
Step 3580: loss = 1.11969
Step 3585: loss = 1.22697
Step 3590: loss = 1.17956
Step 3595: loss = 1.26458
Step 3600: loss = 1.11938
Step 3605: loss = 1.19737
Step 3610: loss = 1.14333
Step 3615: loss = 1.21852
Step 3620: loss = 1.07152
Step 3625: loss = 1.16026
Step 3630: loss = 1.07756
Step 3635: loss = 1.16313
Step 3640: loss = 1.05274
Step 3645: loss = 1.14552
Step 3650: loss = 1.23515
Step 3655: loss = 1.26685
Step 3660: loss = 1.01279
Step 3665: loss = 1.15396
Step 3670: loss = 1.31622
Step 3675: loss = 1.22156
Step 3680: loss = 1.27438
Step 3685: loss = 1.12066
Step 3690: loss = 1.05043
Step 3695: loss = 1.13614
Step 3700: loss = 1.28327
Step 3705: loss = 1.14997
Step 3710: loss = 1.06875
Step 3715: loss = 1.13968
Step 3720: loss = 1.19199
Step 3725: loss = 1.23663
Step 3730: loss = 1.00238
Step 3735: loss = 1.20118
Step 3740: loss = 1.19642
Step 3745: loss = 1.23780
Step 3750: loss = 1.26769
Step 3755: loss = 1.06203
Step 3760: loss = 1.13313
Step 3765: loss = 1.23686
Step 3770: loss = 1.08280
Step 3775: loss = 0.99380
Step 3780: loss = 1.25339
Step 3785: loss = 1.01961
Step 3790: loss = 1.12158
Step 3795: loss = 1.07266
Step 3800: loss = 1.29068
Step 3805: loss = 1.21938
Step 3810: loss = 0.98667
Step 3815: loss = 1.10648
Step 3820: loss = 1.07707
Step 3825: loss = 1.09423
Step 3830: loss = 1.04986
Step 3835: loss = 1.24925
Step 3840: loss = 1.11738
Step 3845: loss = 1.07481
Step 3850: loss = 1.07414
Step 3855: loss = 1.30015
Step 3860: loss = 1.17593
Step 3865: loss = 1.09602
Step 3870: loss = 1.09103
Step 3875: loss = 1.07187
Step 3880: loss = 1.19675
Step 3885: loss = 1.13432
Step 3890: loss = 1.17045
Step 3895: loss = 1.10951
Step 3900: loss = 1.07852
Training Data Eval:
  Num examples: 49920, Num correct: 30931, Precision @ 1: 0.6196
('Testing Data Eval: EPOCH->', 11)
  Num examples: 9984, Num correct: 5548, Precision @ 1: 0.5557
Step 3905: loss = 0.99644
Step 3910: loss = 1.13154
Step 3915: loss = 0.88635
Step 3920: loss = 1.06121
Step 3925: loss = 1.03355
Step 3930: loss = 1.18803
Step 3935: loss = 1.07494
Step 3940: loss = 1.13772
Step 3945: loss = 1.00412
Step 3950: loss = 1.10549
Step 3955: loss = 0.94752
Step 3960: loss = 1.21330
Step 3965: loss = 0.99110
Step 3970: loss = 1.03313
Step 3975: loss = 0.94887
Step 3980: loss = 1.17057
Step 3985: loss = 1.19504
Step 3990: loss = 1.20590
Step 3995: loss = 1.00893
Step 4000: loss = 1.33042
Step 4005: loss = 1.10078
Step 4010: loss = 0.95240
Step 4015: loss = 1.07666
Step 4020: loss = 1.14097
Step 4025: loss = 1.06776
Step 4030: loss = 1.02345
Step 4035: loss = 1.14036
Step 4040: loss = 0.99048
Step 4045: loss = 1.14526
Step 4050: loss = 1.09617
Step 4055: loss = 0.99138
Step 4060: loss = 1.34083
Step 4065: loss = 0.98017
Step 4070: loss = 0.99586
Step 4075: loss = 0.95235
Step 4080: loss = 1.10074
Step 4085: loss = 1.12496
Step 4090: loss = 0.95266
Step 4095: loss = 0.94375
Step 4100: loss = 1.07955
Step 4105: loss = 1.19631
Step 4110: loss = 1.01547
Step 4115: loss = 0.82165
Step 4120: loss = 0.99247
Step 4125: loss = 1.11598
Step 4130: loss = 1.08324
Step 4135: loss = 1.00314
Step 4140: loss = 1.24198
Step 4145: loss = 1.19309
Step 4150: loss = 1.06497
Step 4155: loss = 0.96743
Step 4160: loss = 1.17507
Step 4165: loss = 1.24087
Step 4170: loss = 1.12212
Step 4175: loss = 1.22531
Step 4180: loss = 1.11670
Step 4185: loss = 1.11740
Step 4190: loss = 1.03732
Step 4195: loss = 1.19908
Step 4200: loss = 1.14952
Step 4205: loss = 1.21094
Step 4210: loss = 0.97221
Step 4215: loss = 1.06983
Step 4220: loss = 1.11256
Step 4225: loss = 1.03991
Step 4230: loss = 1.15591
Step 4235: loss = 1.06423
Step 4240: loss = 0.98179
Step 4245: loss = 1.00540
Step 4250: loss = 1.10811
Step 4255: loss = 1.02424
Step 4260: loss = 1.29023
Step 4265: loss = 1.03323
Step 4270: loss = 1.02468
Step 4275: loss = 1.15331
Step 4280: loss = 1.10495
Step 4285: loss = 1.01813
Step 4290: loss = 0.97635
Training Data Eval:
  Num examples: 49920, Num correct: 31992, Precision @ 1: 0.6409
('Testing Data Eval: EPOCH->', 12)
  Num examples: 9984, Num correct: 5561, Precision @ 1: 0.5570
Step 4295: loss = 1.01628
Step 4300: loss = 1.12843
Step 4305: loss = 0.93912
Step 4310: loss = 1.01373
Step 4315: loss = 1.00763
Step 4320: loss = 1.21583
Step 4325: loss = 0.80135
Step 4330: loss = 1.11507
Step 4335: loss = 0.93079
Step 4340: loss = 0.84976
Step 4345: loss = 0.99478
Step 4350: loss = 1.15246
Step 4355: loss = 0.98552
Step 4360: loss = 1.04756
Step 4365: loss = 1.01745
Step 4370: loss = 1.05387
Step 4375: loss = 1.07166
Step 4380: loss = 1.09770
Step 4385: loss = 1.14824
Step 4390: loss = 1.20108
Step 4395: loss = 1.20411
Step 4400: loss = 1.02745
Step 4405: loss = 1.17775
Step 4410: loss = 1.13119
Step 4415: loss = 1.11537
Step 4420: loss = 1.08927
Step 4425: loss = 1.09102
Step 4430: loss = 1.18855
Step 4435: loss = 0.97865
Step 4440: loss = 1.20687
Step 4445: loss = 1.01759
Step 4450: loss = 1.06516
Step 4455: loss = 1.19865
Step 4460: loss = 1.02985
Step 4465: loss = 1.07716
Step 4470: loss = 1.11995
Step 4475: loss = 1.06057
Step 4480: loss = 0.88928
Step 4485: loss = 0.87868
Step 4490: loss = 0.96294
Step 4495: loss = 1.21667
Step 4500: loss = 0.95862
Step 4505: loss = 1.04799
Step 4510: loss = 0.79104
Step 4515: loss = 1.03918
Step 4520: loss = 0.99226
Step 4525: loss = 1.20948
Step 4530: loss = 1.23593
Step 4535: loss = 0.96143
Step 4540: loss = 0.98806
Step 4545: loss = 1.04775
Step 4550: loss = 1.16778
Step 4555: loss = 1.04410
Step 4560: loss = 1.01620
Step 4565: loss = 1.18921
Step 4570: loss = 1.05944
Step 4575: loss = 0.92479
Step 4580: loss = 1.12906
Step 4585: loss = 1.03547
Step 4590: loss = 1.02509
Step 4595: loss = 1.04436
Step 4600: loss = 0.84117
Step 4605: loss = 1.00922
Step 4610: loss = 0.98513
Step 4615: loss = 0.83164
Step 4620: loss = 1.14051
Step 4625: loss = 1.13309
Step 4630: loss = 0.98824
Step 4635: loss = 1.06234
Step 4640: loss = 1.03091
Step 4645: loss = 0.98170
Step 4650: loss = 0.99498
Step 4655: loss = 1.14138
Step 4660: loss = 0.84377
Step 4665: loss = 1.12737
Step 4670: loss = 1.01049
Step 4675: loss = 0.99510
Step 4680: loss = 1.06830
Training Data Eval:
  Num examples: 49920, Num correct: 32876, Precision @ 1: 0.6586
('Testing Data Eval: EPOCH->', 13)
  Num examples: 9984, Num correct: 5886, Precision @ 1: 0.5895
Step 4685: loss = 0.75917
Step 4690: loss = 0.84130
Step 4695: loss = 0.87726
Step 4700: loss = 1.02856
Step 4705: loss = 0.80500
Step 4710: loss = 0.97829
Step 4715: loss = 0.91997
Step 4720: loss = 1.12001
Step 4725: loss = 1.13943
Step 4730: loss = 0.93878
Step 4735: loss = 1.18686
Step 4740: loss = 1.06675
Step 4745: loss = 1.01630
Step 4750: loss = 1.08758
Step 4755: loss = 1.13257
Step 4760: loss = 1.06878
Step 4765: loss = 1.08054
Step 4770: loss = 1.10907
Step 4775: loss = 1.14766
Step 4780: loss = 0.80911
Step 4785: loss = 0.97991
Step 4790: loss = 1.07561
Step 4795: loss = 1.15981
Step 4800: loss = 1.08051
Step 4805: loss = 1.19034
Step 4810: loss = 1.02874
Step 4815: loss = 0.86897
Step 4820: loss = 1.07639
Step 4825: loss = 1.19087
Step 4830: loss = 0.80434
Step 4835: loss = 0.86943
Step 4840: loss = 0.85578
Step 4845: loss = 0.78529
Step 4850: loss = 1.10411
Step 4855: loss = 0.71877
Step 4860: loss = 0.83524
Step 4865: loss = 1.00716
Step 4870: loss = 0.85461
Step 4875: loss = 0.95461
Step 4880: loss = 1.08176
Step 4885: loss = 1.11139
Step 4890: loss = 1.04875
Step 4895: loss = 0.87604
Step 4900: loss = 0.86150
Step 4905: loss = 0.99361
Step 4910: loss = 0.89397
Step 4915: loss = 0.91915
Step 4920: loss = 0.99152
Step 4925: loss = 0.95710
Step 4930: loss = 0.84897
Step 4935: loss = 0.91514
Step 4940: loss = 0.86415
Step 4945: loss = 0.89619
Step 4950: loss = 0.97446
Step 4955: loss = 0.91335
Step 4960: loss = 1.03414
Step 4965: loss = 1.00399
Step 4970: loss = 1.26895
Step 4975: loss = 0.87849
Step 4980: loss = 0.99212
Step 4985: loss = 0.83443
Step 4990: loss = 1.01258
Step 4995: loss = 0.86246
Step 5000: loss = 1.04050
Step 5005: loss = 1.05571
Step 5010: loss = 1.15797
Step 5015: loss = 1.22868
Step 5020: loss = 1.11984
Step 5025: loss = 0.95442
Step 5030: loss = 0.96454
Step 5035: loss = 0.92184
Step 5040: loss = 1.05987
Step 5045: loss = 0.83160
Step 5050: loss = 0.96344
Step 5055: loss = 0.97505
Step 5060: loss = 1.12447
Step 5065: loss = 0.94952
Step 5070: loss = 0.98808
Training Data Eval:
  Num examples: 49920, Num correct: 32942, Precision @ 1: 0.6599
('Testing Data Eval: EPOCH->', 14)
  Num examples: 9984, Num correct: 5824, Precision @ 1: 0.5833
Step 5075: loss = 0.88002
Step 5080: loss = 0.91461
Step 5085: loss = 0.90933
Step 5090: loss = 0.88004
Step 5095: loss = 0.85379
Step 5100: loss = 0.96523
Step 5105: loss = 0.93514
Step 5110: loss = 1.02383
Step 5115: loss = 0.81399
Step 5120: loss = 0.83117
Step 5125: loss = 0.89242
Step 5130: loss = 0.85480
Step 5135: loss = 0.97979
Step 5140: loss = 1.01340
Step 5145: loss = 0.91594
Step 5150: loss = 0.81020
Step 5155: loss = 0.96219
Step 5160: loss = 0.84464
Step 5165: loss = 0.98218
Step 5170: loss = 0.91638
Step 5175: loss = 0.89257
Step 5180: loss = 0.96422
Step 5185: loss = 0.85974
Step 5190: loss = 0.95529
Step 5195: loss = 1.04305
Step 5200: loss = 0.95858
Step 5205: loss = 0.76348
Step 5210: loss = 0.99640
Step 5215: loss = 0.99187
Step 5220: loss = 0.90819
Step 5225: loss = 0.87878
Step 5230: loss = 0.97208
Step 5235: loss = 0.85365
Step 5240: loss = 1.01686
Step 5245: loss = 0.86351
Step 5250: loss = 0.80954
Step 5255: loss = 0.82744
Step 5260: loss = 0.99961
Step 5265: loss = 1.01574
Step 5270: loss = 0.94430
Step 5275: loss = 0.88113
Step 5280: loss = 0.78536
Step 5285: loss = 0.80683
Step 5290: loss = 1.11009
Step 5295: loss = 0.98767
Step 5300: loss = 1.11002
Step 5305: loss = 0.87129
Step 5310: loss = 0.83622
Step 5315: loss = 0.77756
Step 5320: loss = 0.88272
Step 5325: loss = 0.89520
Step 5330: loss = 1.06428
Step 5335: loss = 0.88447
Step 5340: loss = 1.02442
Step 5345: loss = 1.04199
Step 5350: loss = 1.03829
Step 5355: loss = 1.09845
Step 5360: loss = 0.98430
Step 5365: loss = 1.01380
Step 5370: loss = 1.01922
Step 5375: loss = 0.86745
Step 5380: loss = 0.91476
Step 5385: loss = 1.04219
Step 5390: loss = 0.79499
Step 5395: loss = 1.23734
Step 5400: loss = 0.91963
Step 5405: loss = 0.97182
Step 5410: loss = 0.97170
Step 5415: loss = 1.03932
Step 5420: loss = 0.98507
Step 5425: loss = 0.88413
Step 5430: loss = 0.90594
Step 5435: loss = 0.84519
Step 5440: loss = 1.05474
Step 5445: loss = 0.95482
Step 5450: loss = 0.93865
Step 5455: loss = 0.93884
Step 5460: loss = 0.96107
Training Data Eval:
  Num examples: 49920, Num correct: 34709, Precision @ 1: 0.6953
('Testing Data Eval: EPOCH->', 15)
  Num examples: 9984, Num correct: 6151, Precision @ 1: 0.6161
Step 5465: loss = 0.93272
Step 5470: loss = 0.77095
Step 5475: loss = 0.77029
Step 5480: loss = 0.97665
Step 5485: loss = 0.76250
Step 5490: loss = 0.92159
Step 5495: loss = 0.94327
Step 5500: loss = 0.89021
Step 5505: loss = 0.80039
Step 5510: loss = 1.14591
Step 5515: loss = 0.95774
Step 5520: loss = 0.91403
Step 5525: loss = 0.73626
Step 5530: loss = 0.75507
Step 5535: loss = 0.84658
Step 5540: loss = 0.91460
Step 5545: loss = 0.99276
Step 5550: loss = 0.92651
Step 5555: loss = 1.04788
Step 5560: loss = 1.01972
Step 5565: loss = 0.94317
Step 5570: loss = 0.85621
Step 5575: loss = 0.97839
Step 5580: loss = 0.88810
Step 5585: loss = 0.92006
Step 5590: loss = 0.98078
Step 5595: loss = 0.85427
Step 5600: loss = 0.73157
Step 5605: loss = 0.97560
Step 5610: loss = 0.92242
Step 5615: loss = 0.82421
Step 5620: loss = 0.84504
Step 5625: loss = 0.74877
Step 5630: loss = 0.80222
Step 5635: loss = 0.88162
Step 5640: loss = 0.89482
Step 5645: loss = 0.80730
Step 5650: loss = 0.74494
Step 5655: loss = 0.98861
Step 5660: loss = 1.00139
Step 5665: loss = 0.79293
Step 5670: loss = 0.81161
Step 5675: loss = 0.95624
Step 5680: loss = 0.80924
Step 5685: loss = 0.83763
Step 5690: loss = 0.93246
Step 5695: loss = 0.83542
Step 5700: loss = 1.02761
Step 5705: loss = 0.79591
Step 5710: loss = 0.84064
Step 5715: loss = 0.90314
Step 5720: loss = 0.94070
Step 5725: loss = 0.89884
Step 5730: loss = 0.77582
Step 5735: loss = 0.96003
Step 5740: loss = 0.80605
Step 5745: loss = 1.08440
Step 5750: loss = 0.95094
Step 5755: loss = 1.05662
Step 5760: loss = 1.00916
Step 5765: loss = 0.87891
Step 5770: loss = 0.89456
Step 5775: loss = 0.95495
Step 5780: loss = 0.77541
Step 5785: loss = 1.24702
Step 5790: loss = 0.84377
Step 5795: loss = 0.91581
Step 5800: loss = 0.93138
Step 5805: loss = 0.78395
Step 5810: loss = 0.84332
Step 5815: loss = 0.83680
Step 5820: loss = 1.11561
Step 5825: loss = 0.84621
Step 5830: loss = 0.79096
Step 5835: loss = 0.82553
Step 5840: loss = 0.92361
Step 5845: loss = 0.88272
Step 5850: loss = 0.92872
Training Data Eval:
  Num examples: 49920, Num correct: 34993, Precision @ 1: 0.7010
('Testing Data Eval: EPOCH->', 16)
  Num examples: 9984, Num correct: 6333, Precision @ 1: 0.6343
Step 5855: loss = 0.84419
Step 5860: loss = 0.97204
Step 5865: loss = 0.76544
Step 5870: loss = 0.92764
Step 5875: loss = 0.82806
Step 5880: loss = 0.85095
Step 5885: loss = 0.79169
Step 5890: loss = 0.93187
Step 5895: loss = 0.70002
Step 5900: loss = 0.83243
Step 5905: loss = 0.72042
Step 5910: loss = 0.97555
Step 5915: loss = 0.71958
Step 5920: loss = 0.68124
Step 5925: loss = 0.76193
Step 5930: loss = 0.89914
Step 5935: loss = 0.94448
Step 5940: loss = 0.86149
Step 5945: loss = 0.79903
Step 5950: loss = 0.72518
Step 5955: loss = 0.82063
Step 5960: loss = 0.90496
Step 5965: loss = 0.98436
Step 5970: loss = 0.80455
Step 5975: loss = 1.05370
Step 5980: loss = 0.94710
Step 5985: loss = 0.78805
Step 5990: loss = 0.70505
Step 5995: loss = 0.65928
Step 6000: loss = 0.81027
Step 6005: loss = 0.92851
Step 6010: loss = 0.90091
Step 6015: loss = 0.89261
Step 6020: loss = 0.88261
Step 6025: loss = 0.79890
Step 6030: loss = 0.80074
Step 6035: loss = 1.02599
Step 6040: loss = 0.78338
Step 6045: loss = 0.66070
Step 6050: loss = 0.96249
Step 6055: loss = 1.05604
Step 6060: loss = 0.79834
Step 6065: loss = 0.86479
Step 6070: loss = 0.79260
Step 6075: loss = 0.97891
Step 6080: loss = 0.82864
Step 6085: loss = 0.70923
Step 6090: loss = 0.76965
Step 6095: loss = 0.75604
Step 6100: loss = 0.68575
Step 6105: loss = 0.77735
Step 6110: loss = 0.98922
Step 6115: loss = 1.03602
Step 6120: loss = 0.91239
Step 6125: loss = 0.90394
Step 6130: loss = 0.83307
Step 6135: loss = 0.79945
Step 6140: loss = 0.96606
Step 6145: loss = 1.20283
Step 6150: loss = 0.71031
Step 6155: loss = 0.79746
Step 6160: loss = 0.73073
Step 6165: loss = 0.97922
Step 6170: loss = 0.90679
Step 6175: loss = 0.62303
Step 6180: loss = 0.65083
Step 6185: loss = 0.73807
Step 6190: loss = 0.84167
Step 6195: loss = 0.81880
Step 6200: loss = 0.89565
Step 6205: loss = 1.05076
Step 6210: loss = 0.66104
Step 6215: loss = 0.75184
Step 6220: loss = 0.89522
Step 6225: loss = 1.06441
Step 6230: loss = 0.77927
Step 6235: loss = 0.84753
Step 6240: loss = 0.67272
Training Data Eval:
  Num examples: 49920, Num correct: 35892, Precision @ 1: 0.7190
('Testing Data Eval: EPOCH->', 17)
  Num examples: 9984, Num correct: 6450, Precision @ 1: 0.6460
Step 6245: loss = 0.63734
Step 6250: loss = 0.98050
Step 6255: loss = 0.87144
Step 6260: loss = 0.91522
Step 6265: loss = 0.83033
Step 6270: loss = 0.85928
Step 6275: loss = 0.72609
Step 6280: loss = 0.97488
Step 6285: loss = 0.93020
Step 6290: loss = 0.77867
Step 6295: loss = 0.85086
Step 6300: loss = 0.79083
Step 6305: loss = 0.87170
Step 6310: loss = 0.81145
Step 6315: loss = 0.87797
Step 6320: loss = 0.72773
Step 6325: loss = 0.72856
Step 6330: loss = 0.66644
Step 6335: loss = 0.87377
Step 6340: loss = 0.87433
Step 6345: loss = 0.89236
Step 6350: loss = 0.89943
Step 6355: loss = 0.81614
Step 6360: loss = 0.72316
Step 6365: loss = 0.80761
Step 6370: loss = 0.76814
Step 6375: loss = 0.86126
Step 6380: loss = 0.71006
Step 6385: loss = 0.73586
Step 6390: loss = 0.90271
Step 6395: loss = 1.01968
Step 6400: loss = 0.77939
Step 6405: loss = 0.96901
Step 6410: loss = 0.71402
Step 6415: loss = 0.77967
Step 6420: loss = 0.77206
Step 6425: loss = 0.74745
Step 6430: loss = 0.74797
Step 6435: loss = 0.71595
Step 6440: loss = 0.86803
Step 6445: loss = 0.66497
Step 6450: loss = 0.91532
Step 6455: loss = 0.75469
Step 6460: loss = 0.72514
Step 6465: loss = 0.79507
Step 6470: loss = 1.10975
Step 6475: loss = 0.85368
Step 6480: loss = 0.78164
Step 6485: loss = 1.00985
Step 6490: loss = 0.87291
Step 6495: loss = 0.73103
Step 6500: loss = 0.77935
Step 6505: loss = 0.82400
Step 6510: loss = 0.71463
Step 6515: loss = 0.93278
Step 6520: loss = 0.70491
Step 6525: loss = 0.98593
Step 6530: loss = 0.88877
Step 6535: loss = 0.93943
Step 6540: loss = 0.89187
Step 6545: loss = 0.76348
Step 6550: loss = 0.92811
Step 6555: loss = 0.72172
Step 6560: loss = 0.76260
Step 6565: loss = 0.82989
Step 6570: loss = 0.83310
Step 6575: loss = 0.76794
Step 6580: loss = 0.92871
Step 6585: loss = 0.76312
Step 6590: loss = 0.59514
Step 6595: loss = 0.50897
Step 6600: loss = 0.71781
Step 6605: loss = 0.69672
Step 6610: loss = 0.59655
Step 6615: loss = 0.76507
Step 6620: loss = 0.73192
Step 6625: loss = 0.66333
Step 6630: loss = 0.78923
Training Data Eval:
  Num examples: 49920, Num correct: 36369, Precision @ 1: 0.7285
('Testing Data Eval: EPOCH->', 18)
  Num examples: 9984, Num correct: 6388, Precision @ 1: 0.6398
Step 6635: loss = 0.65124
Step 6640: loss = 0.74566
Step 6645: loss = 0.63060
Step 6650: loss = 0.78028
Step 6655: loss = 0.81629
Step 6660: loss = 0.78268
Step 6665: loss = 0.74293
Step 6670: loss = 0.82303
Step 6675: loss = 0.80891
Step 6680: loss = 0.83070
Step 6685: loss = 0.68888
Step 6690: loss = 0.88974
Step 6695: loss = 0.80789
Step 6700: loss = 0.82640
Step 6705: loss = 0.69819
Step 6710: loss = 0.82100
Step 6715: loss = 0.80752
Step 6720: loss = 0.89050
Step 6725: loss = 0.79165
Step 6730: loss = 0.73266
Step 6735: loss = 0.62735
Step 6740: loss = 0.88565
Step 6745: loss = 0.87546
Step 6750: loss = 0.68301
Step 6755: loss = 0.88550
Step 6760: loss = 0.97233
Step 6765: loss = 0.84794
Step 6770: loss = 0.74256
Step 6775: loss = 0.89198
Step 6780: loss = 0.79847
Step 6785: loss = 0.81218
Step 6790: loss = 0.71764
Step 6795: loss = 0.82374
Step 6800: loss = 0.95307
Step 6805: loss = 0.66725
Step 6810: loss = 0.84001
Step 6815: loss = 0.83379
Step 6820: loss = 0.77465
Step 6825: loss = 0.88810
Step 6830: loss = 0.87597
Step 6835: loss = 0.86192
Step 6840: loss = 0.82057
Step 6845: loss = 0.69031
Step 6850: loss = 0.74442
Step 6855: loss = 0.79612
Step 6860: loss = 0.85337
Step 6865: loss = 0.74875
Step 6870: loss = 0.76643
Step 6875: loss = 0.75501
Step 6880: loss = 0.68681
Step 6885: loss = 0.67437
Step 6890: loss = 0.73451
Step 6895: loss = 0.82050
Step 6900: loss = 0.80093
Step 6905: loss = 0.84706
Step 6910: loss = 0.81217
Step 6915: loss = 0.80891
Step 6920: loss = 0.68119
Step 6925: loss = 0.74745
Step 6930: loss = 0.87295
Step 6935: loss = 0.69209
Step 6940: loss = 0.75576
Step 6945: loss = 0.78686
Step 6950: loss = 0.73454
Step 6955: loss = 0.76195
Step 6960: loss = 0.84156
Step 6965: loss = 0.72568
Step 6970: loss = 0.79259
Step 6975: loss = 0.67828
Step 6980: loss = 0.84196
Step 6985: loss = 0.61228
Step 6990: loss = 0.72532
Step 6995: loss = 0.84565
Step 7000: loss = 0.79636
Step 7005: loss = 0.75424
Step 7010: loss = 0.78993
Step 7015: loss = 1.00630
Step 7020: loss = 0.70621
Training Data Eval:
  Num examples: 49920, Num correct: 36917, Precision @ 1: 0.7395
('Testing Data Eval: EPOCH->', 19)
  Num examples: 9984, Num correct: 6566, Precision @ 1: 0.6577
Step 7025: loss = 0.73902
Step 7030: loss = 0.79185
Step 7035: loss = 0.83177
Step 7040: loss = 0.77615
Step 7045: loss = 0.80043
Step 7050: loss = 0.65098
Step 7055: loss = 0.69220
Step 7060: loss = 0.79786
Step 7065: loss = 0.76966
Step 7070: loss = 0.86278
Step 7075: loss = 0.68798
Step 7080: loss = 0.85833
Step 7085: loss = 0.85438
Step 7090: loss = 0.70367
Step 7095: loss = 0.74369
Step 7100: loss = 0.75625
Step 7105: loss = 0.76853
Step 7110: loss = 0.75982
Step 7115: loss = 0.79866
Step 7120: loss = 0.81798
Step 7125: loss = 0.90670
Step 7130: loss = 0.83181
Step 7135: loss = 0.75545
Step 7140: loss = 0.59864
Step 7145: loss = 0.60809
Step 7150: loss = 0.82318
Step 7155: loss = 0.77721
Step 7160: loss = 0.74894
Step 7165: loss = 0.81170
Step 7170: loss = 0.84269
Step 7175: loss = 0.72383
Step 7180: loss = 0.72048
Step 7185: loss = 0.87525
Step 7190: loss = 0.64621
Step 7195: loss = 0.71661
Step 7200: loss = 0.74921
Step 7205: loss = 0.79464
Step 7210: loss = 0.68039
Step 7215: loss = 0.91208
Step 7220: loss = 0.83764
Step 7225: loss = 0.75926
Step 7230: loss = 0.54694
Step 7235: loss = 0.62829
Step 7240: loss = 0.57328
Step 7245: loss = 0.88274
Step 7250: loss = 0.74630
Step 7255: loss = 0.63261
Step 7260: loss = 0.81430
Step 7265: loss = 0.81845
Step 7270: loss = 0.69930
Step 7275: loss = 0.82061
Step 7280: loss = 0.56860
Step 7285: loss = 0.71983
Step 7290: loss = 0.75985
Step 7295: loss = 0.68345
Step 7300: loss = 0.66921
Step 7305: loss = 0.82031
Step 7310: loss = 0.81308
Step 7315: loss = 0.71799
Step 7320: loss = 0.75504
Step 7325: loss = 0.63762
Step 7330: loss = 0.72605
Step 7335: loss = 0.54595
Step 7340: loss = 0.79797
Step 7345: loss = 0.63849
Step 7350: loss = 0.61410
Step 7355: loss = 0.80948
Step 7360: loss = 0.64800
Step 7365: loss = 0.66398
Step 7370: loss = 0.70369
Step 7375: loss = 0.72375
Step 7380: loss = 0.82266
Step 7385: loss = 0.65492
Step 7390: loss = 0.59450
Step 7395: loss = 0.78288
Step 7400: loss = 0.52117
Step 7405: loss = 0.86321
Step 7410: loss = 0.79737
Training Data Eval:
  Num examples: 49920, Num correct: 38578, Precision @ 1: 0.7728
('Testing Data Eval: EPOCH->', 20)
  Num examples: 9984, Num correct: 6791, Precision @ 1: 0.6802
Step 7415: loss = 0.69316
Step 7420: loss = 0.69662
Step 7425: loss = 0.79206
Step 7430: loss = 0.61636
Step 7435: loss = 0.74941
Step 7440: loss = 0.61936
Step 7445: loss = 0.78710
Step 7450: loss = 0.71533
Step 7455: loss = 0.65882
Step 7460: loss = 0.57417
Step 7465: loss = 0.63567
Step 7470: loss = 0.62220
Step 7475: loss = 0.69160
Step 7480: loss = 0.70873
Step 7485: loss = 0.59152
Step 7490: loss = 0.62269
Step 7495: loss = 0.61878
Step 7500: loss = 0.72237
Step 7505: loss = 0.80060
Step 7510: loss = 0.64614
Step 7515: loss = 0.63441
Step 7520: loss = 0.62201
Step 7525: loss = 0.82817
Step 7530: loss = 0.81515
Step 7535: loss = 0.76625
Step 7540: loss = 0.96310
Step 7545: loss = 0.53015
Step 7550: loss = 0.68540
Step 7555: loss = 0.81779
Step 7560: loss = 0.67791
Step 7565: loss = 0.66089
Step 7570: loss = 0.62537
Step 7575: loss = 0.83616
Step 7580: loss = 0.57505
Step 7585: loss = 0.64851
Step 7590: loss = 0.60877
Step 7595: loss = 0.78361
Step 7600: loss = 0.87535
Step 7605: loss = 0.71038
Step 7610: loss = 0.71604
Step 7615: loss = 0.67168
Step 7620: loss = 0.59508
Step 7625: loss = 0.57199
Step 7630: loss = 0.82639
Step 7635: loss = 0.53815
Step 7640: loss = 0.80832
Step 7645: loss = 0.59645
Step 7650: loss = 0.70378
Step 7655: loss = 0.67696
Step 7660: loss = 0.94490
Step 7665: loss = 0.64175
Step 7670: loss = 0.73018
Step 7675: loss = 0.84080
Step 7680: loss = 0.77848
Step 7685: loss = 0.63444
Step 7690: loss = 0.69847
Step 7695: loss = 0.73856
Step 7700: loss = 0.66861
Step 7705: loss = 0.58034
Step 7710: loss = 0.81200
Step 7715: loss = 0.67995
Step 7720: loss = 0.63136
Step 7725: loss = 0.78381
Step 7730: loss = 0.73622
Step 7735: loss = 0.78448
Step 7740: loss = 0.77622
Step 7745: loss = 0.55118
Step 7750: loss = 0.80191
Step 7755: loss = 0.61635
Step 7760: loss = 0.69533
Step 7765: loss = 0.55786
Step 7770: loss = 0.66306
Step 7775: loss = 0.66105
Step 7780: loss = 0.52703
Step 7785: loss = 0.58095
Step 7790: loss = 0.51158
Step 7795: loss = 0.75619
Step 7800: loss = 0.76095
Training Data Eval:
  Num examples: 49920, Num correct: 38522, Precision @ 1: 0.7717
('Testing Data Eval: EPOCH->', 21)
  Num examples: 9984, Num correct: 6717, Precision @ 1: 0.6728
Step 7805: loss = 0.68206
Step 7810: loss = 0.76220
Step 7815: loss = 0.49414
Step 7820: loss = 0.71093
Step 7825: loss = 0.61947
Step 7830: loss = 0.44322
Step 7835: loss = 0.69681
Step 7840: loss = 0.55163
Step 7845: loss = 0.58302
Step 7850: loss = 0.56612
Step 7855: loss = 0.56673
Step 7860: loss = 0.73553
Step 7865: loss = 0.49761
Step 7870: loss = 0.54327
Step 7875: loss = 0.63314
Step 7880: loss = 0.64252
Step 7885: loss = 0.47284
Step 7890: loss = 0.73704
Step 7895: loss = 0.79638
Step 7900: loss = 0.70248
Step 7905: loss = 0.81320
Step 7910: loss = 0.79435
Step 7915: loss = 0.97111
Step 7920: loss = 0.78821
Step 7925: loss = 0.73272
Step 7930: loss = 0.62042
Step 7935: loss = 0.68375
Step 7940: loss = 0.66493
Step 7945: loss = 0.57840
Step 7950: loss = 0.74098
Step 7955: loss = 0.67665
Step 7960: loss = 0.72726
Step 7965: loss = 0.63178
Step 7970: loss = 0.62902
Step 7975: loss = 0.70682
Step 7980: loss = 0.76435
Step 7985: loss = 0.72463
Step 7990: loss = 0.68647
Step 7995: loss = 0.71014
Step 8000: loss = 0.72545
Step 8005: loss = 0.62567
Step 8010: loss = 0.54112
Step 8015: loss = 0.58028
Step 8020: loss = 0.63996
Step 8025: loss = 0.77885
Step 8030: loss = 0.58334
Step 8035: loss = 0.46299
Step 8040: loss = 0.78982
Step 8045: loss = 0.74489
Step 8050: loss = 0.64865
Step 8055: loss = 0.65676
Step 8060: loss = 0.77177
Step 8065: loss = 0.73075
Step 8070: loss = 0.70000
Step 8075: loss = 0.63675
Step 8080: loss = 0.50018
Step 8085: loss = 0.69080
Step 8090: loss = 0.72217
Step 8095: loss = 0.68732
Step 8100: loss = 0.57720
Step 8105: loss = 0.82118
Step 8110: loss = 0.72871
Step 8115: loss = 0.61106
Step 8120: loss = 0.74286
Step 8125: loss = 0.80475
Step 8130: loss = 0.56001
Step 8135: loss = 0.69953
Step 8140: loss = 0.64998
Step 8145: loss = 0.73337
Step 8150: loss = 0.72469
Step 8155: loss = 0.49270
Step 8160: loss = 0.75969
Step 8165: loss = 0.76553
Step 8170: loss = 0.59592
Step 8175: loss = 0.62443
Step 8180: loss = 0.55290
Step 8185: loss = 0.58522
Step 8190: loss = 0.64287
Training Data Eval:
  Num examples: 49920, Num correct: 39447, Precision @ 1: 0.7902
('Testing Data Eval: EPOCH->', 22)
  Num examples: 9984, Num correct: 6957, Precision @ 1: 0.6968
Step 8195: loss = 0.54784
Step 8200: loss = 0.62479
Step 8205: loss = 0.58311
Step 8210: loss = 0.60872
Step 8215: loss = 0.76372
Step 8220: loss = 0.63874
Step 8225: loss = 0.59043
Step 8230: loss = 0.58275
Step 8235: loss = 0.61877
Step 8240: loss = 0.64203
Step 8245: loss = 0.75276
Step 8250: loss = 0.60469
Step 8255: loss = 0.65387
Step 8260: loss = 1.02748
Step 8265: loss = 0.74914
Step 8270: loss = 0.60244
Step 8275: loss = 0.71441
Step 8280: loss = 0.79087
Step 8285: loss = 0.51105
Step 8290: loss = 0.64325
Step 8295: loss = 0.80797
Step 8300: loss = 0.48504
Step 8305: loss = 0.57583
Step 8310: loss = 0.65948
Step 8315: loss = 0.77713
Step 8320: loss = 0.80292
Step 8325: loss = 0.55072
Step 8330: loss = 0.65152
Step 8335: loss = 0.72979
Step 8340: loss = 0.73035
Step 8345: loss = 0.66999
Step 8350: loss = 0.65443
Step 8355: loss = 0.54636
Step 8360: loss = 0.51345
Step 8365: loss = 0.50784
Step 8370: loss = 0.63666
Step 8375: loss = 0.76194
Step 8380: loss = 0.60453
Step 8385: loss = 0.62080
Step 8390: loss = 0.59428
Step 8395: loss = 0.53504
Step 8400: loss = 0.51876
Step 8405: loss = 0.42394
Step 8410: loss = 0.58492
Step 8415: loss = 0.61810
Step 8420: loss = 0.64975
Step 8425: loss = 0.59384
Step 8430: loss = 0.60832
Step 8435: loss = 0.76993
Step 8440: loss = 0.57926
Step 8445: loss = 0.61205
Step 8450: loss = 0.40939
Step 8455: loss = 0.59964
Step 8460: loss = 0.80586
Step 8465: loss = 0.65593
Step 8470: loss = 0.65978
Step 8475: loss = 0.67385
Step 8480: loss = 0.69214
Step 8485: loss = 0.55331
Step 8490: loss = 0.53239
Step 8495: loss = 0.68987
Step 8500: loss = 0.50581
Step 8505: loss = 0.74189
Step 8510: loss = 0.63060
Step 8515: loss = 0.91541
Step 8520: loss = 0.58130
Step 8525: loss = 0.56603
Step 8530: loss = 0.57212
Step 8535: loss = 0.60129
Step 8540: loss = 0.51064
Step 8545: loss = 0.67268
Step 8550: loss = 0.77327
Step 8555: loss = 0.59195
Step 8560: loss = 0.62387
Step 8565: loss = 0.61003
Step 8570: loss = 0.48897
Step 8575: loss = 0.69054
Step 8580: loss = 0.59161
Training Data Eval:
  Num examples: 49920, Num correct: 39335, Precision @ 1: 0.7880
('Testing Data Eval: EPOCH->', 23)
  Num examples: 9984, Num correct: 6885, Precision @ 1: 0.6896
Step 8585: loss = 0.64438
Step 8590: loss = 0.56008
Step 8595: loss = 0.67848
Step 8600: loss = 0.51894
Step 8605: loss = 0.50529
Step 8610: loss = 0.54442
Step 8615: loss = 0.70398
Step 8620: loss = 0.41614
Step 8625: loss = 0.54831
Step 8630: loss = 0.53163
Step 8635: loss = 0.61181
Step 8640: loss = 0.44662
Step 8645: loss = 0.58394
Step 8650: loss = 0.51221
Step 8655: loss = 0.43970
Step 8660: loss = 0.46914
Step 8665: loss = 0.59337
Step 8670: loss = 0.53871
Step 8675: loss = 0.52490
Step 8680: loss = 0.38805
Step 8685: loss = 0.72650
Step 8690: loss = 0.66971
Step 8695: loss = 0.67049
Step 8700: loss = 0.63549
Step 8705: loss = 0.61437
Step 8710: loss = 0.60591
Step 8715: loss = 0.69367
Step 8720: loss = 0.73608
Step 8725: loss = 0.67243
Step 8730: loss = 0.68094
Step 8735: loss = 0.61222
Step 8740: loss = 0.75146
Step 8745: loss = 0.56799
Step 8750: loss = 0.71787
Step 8755: loss = 0.53088
Step 8760: loss = 0.51950
Step 8765: loss = 0.59228
Step 8770: loss = 0.56256
Step 8775: loss = 0.58218
Step 8780: loss = 0.61255
Step 8785: loss = 0.61757
Step 8790: loss = 0.60589
Step 8795: loss = 0.61215
Step 8800: loss = 0.58122
Step 8805: loss = 0.64479
Step 8810: loss = 0.58751
Step 8815: loss = 0.63981
Step 8820: loss = 0.60470
Step 8825: loss = 0.62220
Step 8830: loss = 0.89154
Step 8835: loss = 0.88220
Step 8840: loss = 0.65853
Step 8845: loss = 0.64854
Step 8850: loss = 0.51849
Step 8855: loss = 0.49029
Step 8860: loss = 0.74239
Step 8865: loss = 0.79610
Step 8870: loss = 0.57128
Step 8875: loss = 0.50418
Step 8880: loss = 0.72832
Step 8885: loss = 0.64855
Step 8890: loss = 0.65760
Step 8895: loss = 0.54912
Step 8900: loss = 0.53160
Step 8905: loss = 0.44852
Step 8910: loss = 0.55992
Step 8915: loss = 0.48653
Step 8920: loss = 0.80334
Step 8925: loss = 0.61578
Step 8930: loss = 0.72299
Step 8935: loss = 0.57807
Step 8940: loss = 0.71350
Step 8945: loss = 0.48858
Step 8950: loss = 0.60491
Step 8955: loss = 0.49233
Step 8960: loss = 0.45856
Step 8965: loss = 0.67818
Step 8970: loss = 0.59220
Training Data Eval:
  Num examples: 49920, Num correct: 40243, Precision @ 1: 0.8061
('Testing Data Eval: EPOCH->', 24)
  Num examples: 9984, Num correct: 6934, Precision @ 1: 0.6945
Step 8975: loss = 0.54269
Step 8980: loss = 0.57150
Step 8985: loss = 0.40825
Step 8990: loss = 0.53584
Step 8995: loss = 0.44994
Step 9000: loss = 0.53763
Step 9005: loss = 0.56113
Step 9010: loss = 0.58546
Step 9015: loss = 0.58316
Step 9020: loss = 0.56933
Step 9025: loss = 0.58698
Step 9030: loss = 0.63517
Step 9035: loss = 0.63404
Step 9040: loss = 0.66741
Step 9045: loss = 0.49664
Step 9050: loss = 0.64139
Step 9055: loss = 0.46538
Step 9060: loss = 0.51348
Step 9065: loss = 0.50611
Step 9070: loss = 0.71678
Step 9075: loss = 0.55733
Step 9080: loss = 0.47464
Step 9085: loss = 0.53754
Step 9090: loss = 0.54493
Step 9095: loss = 0.53722
Step 9100: loss = 0.44543
Step 9105: loss = 0.66960
Step 9110: loss = 0.52044
Step 9115: loss = 0.55068
Step 9120: loss = 0.51132
Step 9125: loss = 0.53929
Step 9130: loss = 0.51093
Step 9135: loss = 0.49916
Step 9140: loss = 0.46347
Step 9145: loss = 0.55116
Step 9150: loss = 0.40166
Step 9155: loss = 0.59945
Step 9160: loss = 0.55864
Step 9165: loss = 0.56845
Step 9170: loss = 0.61808
Step 9175: loss = 0.58598
Step 9180: loss = 0.72429
Step 9185: loss = 0.54704
Step 9190: loss = 0.68803
Step 9195: loss = 0.62795
Step 9200: loss = 0.59275
Step 9205: loss = 0.51209
Step 9210: loss = 0.58611
Step 9215: loss = 0.51572
Step 9220: loss = 0.64247
Step 9225: loss = 0.57255
Step 9230: loss = 0.59590
Step 9235: loss = 0.55810
Step 9240: loss = 0.56283
Step 9245: loss = 0.58460
Step 9250: loss = 0.60065
Step 9255: loss = 0.58011
Step 9260: loss = 0.59895
Step 9265: loss = 0.46311
Step 9270: loss = 0.72784
Step 9275: loss = 0.63305
Step 9280: loss = 0.58369
Step 9285: loss = 0.50752
Step 9290: loss = 0.59038
Step 9295: loss = 0.48677
Step 9300: loss = 0.50268
Step 9305: loss = 0.51303
Step 9310: loss = 0.58782
Step 9315: loss = 0.65235
Step 9320: loss = 0.50849
Step 9325: loss = 0.63650
Step 9330: loss = 0.49106
Step 9335: loss = 0.58809
Step 9340: loss = 0.58680
Step 9345: loss = 0.69864
Step 9350: loss = 0.74430
Step 9355: loss = 0.66511
Step 9360: loss = 0.76318
Training Data Eval:
  Num examples: 49920, Num correct: 40391, Precision @ 1: 0.8091
('Testing Data Eval: EPOCH->', 25)
  Num examples: 9984, Num correct: 7039, Precision @ 1: 0.7050
Step 9365: loss = 0.50520
Step 9370: loss = 0.41340
Step 9375: loss = 0.59088
Step 9380: loss = 0.57113
Step 9385: loss = 0.59601
Step 9390: loss = 0.53122
Step 9395: loss = 0.54289
Step 9400: loss = 0.62153
Step 9405: loss = 0.53819
Step 9410: loss = 0.58455
Step 9415: loss = 0.60909
Step 9420: loss = 0.86739
Step 9425: loss = 0.59245
Step 9430: loss = 0.58895
Step 9435: loss = 0.66574
Step 9440: loss = 0.53123
Step 9445: loss = 0.62745
Step 9450: loss = 0.69799
Step 9455: loss = 0.62542
Step 9460: loss = 0.51505
Step 9465: loss = 0.57513
Step 9470: loss = 0.57384
Step 9475: loss = 0.50302
Step 9480: loss = 0.50922
Step 9485: loss = 0.44386
Step 9490: loss = 0.57862
Step 9495: loss = 0.58391
Step 9500: loss = 0.48725
Step 9505: loss = 0.43451
Step 9510: loss = 0.56251
Step 9515: loss = 0.50753
Step 9520: loss = 0.54489
Step 9525: loss = 0.58731
Step 9530: loss = 0.50161
Step 9535: loss = 0.51378
Step 9540: loss = 0.54882
Step 9545: loss = 0.47074
Step 9550: loss = 0.55739
Step 9555: loss = 0.56873
Step 9560: loss = 0.52188
Step 9565: loss = 0.70570
Step 9570: loss = 0.46991
Step 9575: loss = 0.67858
Step 9580: loss = 0.57686
Step 9585: loss = 0.57870
Step 9590: loss = 0.59427
Step 9595: loss = 0.43795
Step 9600: loss = 0.50890
Step 9605: loss = 0.56634
Step 9610: loss = 0.42705
Step 9615: loss = 0.62647
Step 9620: loss = 0.63377
Step 9625: loss = 0.47847
Step 9630: loss = 0.45867
Step 9635: loss = 0.51256
Step 9640: loss = 0.48670
Step 9645: loss = 0.51893
Step 9650: loss = 0.67091
Step 9655: loss = 0.56708
Step 9660: loss = 0.61469
Step 9665: loss = 0.60787
Step 9670: loss = 0.74987
Step 9675: loss = 0.55771
Step 9680: loss = 0.50973
Step 9685: loss = 0.60711
Step 9690: loss = 0.65785
Step 9695: loss = 0.59571
Step 9700: loss = 0.59170
Step 9705: loss = 0.46406
Step 9710: loss = 0.59885
Step 9715: loss = 0.57031
Step 9720: loss = 0.79857
Step 9725: loss = 0.61693
Step 9730: loss = 0.49772
Step 9735: loss = 0.53291
Step 9740: loss = 0.66331
Step 9745: loss = 0.63421
Step 9750: loss = 0.56948
Training Data Eval:
  Num examples: 49920, Num correct: 41415, Precision @ 1: 0.8296
('Testing Data Eval: EPOCH->', 26)
  Num examples: 9984, Num correct: 7046, Precision @ 1: 0.7057
Step 9755: loss = 0.48077
Step 9760: loss = 0.36708
Step 9765: loss = 0.48094
Step 9770: loss = 0.56668
Step 9775: loss = 0.48775
Step 9780: loss = 0.50575
Step 9785: loss = 0.40467
Step 9790: loss = 0.50143
Step 9795: loss = 0.42349
Step 9800: loss = 0.45529
Step 9805: loss = 0.39661
Step 9810: loss = 0.45457
Step 9815: loss = 0.42876
Step 9820: loss = 0.47068
Step 9825: loss = 0.51599
Step 9830: loss = 0.52082
Step 9835: loss = 0.44548
Step 9840: loss = 0.60388
Step 9845: loss = 0.63491
Step 9850: loss = 0.55953
Step 9855: loss = 0.45993
Step 9860: loss = 0.49200
Step 9865: loss = 0.53329
Step 9870: loss = 0.52312
Step 9875: loss = 0.76758
Step 9880: loss = 0.43864
Step 9885: loss = 0.51312
Step 9890: loss = 0.53058
Step 9895: loss = 0.47445
Step 9900: loss = 0.52032
Step 9905: loss = 0.57918
Step 9910: loss = 0.59447
Step 9915: loss = 0.49112
Step 9920: loss = 0.59647
Step 9925: loss = 0.47044
Step 9930: loss = 0.58138
Step 9935: loss = 0.53670
Step 9940: loss = 0.56970
Step 9945: loss = 0.73795
Step 9950: loss = 0.40311
Step 9955: loss = 0.57432
Step 9960: loss = 0.55638
Step 9965: loss = 0.59464
Step 9970: loss = 0.65427
Step 9975: loss = 0.53449
Step 9980: loss = 0.55248
Step 9985: loss = 0.59015
Step 9990: loss = 0.44503
Step 9995: loss = 0.64215
Step 10000: loss = 0.54425
Step 10005: loss = 0.57431
Step 10010: loss = 0.40551
Step 10015: loss = 0.51732
Step 10020: loss = 0.49102
Step 10025: loss = 0.51802
Step 10030: loss = 0.59313
Step 10035: loss = 0.54893
Step 10040: loss = 0.64367
Step 10045: loss = 0.74433
Step 10050: loss = 0.56855
Step 10055: loss = 0.43597
Step 10060: loss = 0.52541
Step 10065: loss = 0.53662
Step 10070: loss = 0.49023
Step 10075: loss = 0.45396
Step 10080: loss = 0.39321
Step 10085: loss = 0.39788
Step 10090: loss = 0.41357
Step 10095: loss = 0.54743
Step 10100: loss = 0.42756
Step 10105: loss = 0.55025
Step 10110: loss = 0.51315
Step 10115: loss = 0.33026
Step 10120: loss = 0.58815
Step 10125: loss = 0.64633
Step 10130: loss = 0.31393
Step 10135: loss = 0.63617
Step 10140: loss = 0.54331
Training Data Eval:
  Num examples: 49920, Num correct: 41874, Precision @ 1: 0.8388
('Testing Data Eval: EPOCH->', 27)
  Num examples: 9984, Num correct: 7122, Precision @ 1: 0.7133
Step 10145: loss = 0.39748
Step 10150: loss = 0.49863
Step 10155: loss = 0.44461
Step 10160: loss = 0.39027
Step 10165: loss = 0.48574
Step 10170: loss = 0.47364
Step 10175: loss = 0.52022
Step 10180: loss = 0.44162
Step 10185: loss = 0.47763
Step 10190: loss = 0.33571
Step 10195: loss = 0.56559
Step 10200: loss = 0.35594
Step 10205: loss = 0.39018
Step 10210: loss = 0.50392
Step 10215: loss = 0.64355
Step 10220: loss = 0.48119
Step 10225: loss = 0.53875
Step 10230: loss = 0.36474
Step 10235: loss = 0.38956
Step 10240: loss = 0.68178
Step 10245: loss = 0.53326
Step 10250: loss = 0.68555
Step 10255: loss = 0.62730
Step 10260: loss = 0.44923
Step 10265: loss = 0.69851
Step 10270: loss = 0.70818
Step 10275: loss = 0.57318
Step 10280: loss = 0.43998
Step 10285: loss = 0.49235
Step 10290: loss = 0.46376
Step 10295: loss = 0.44758
Step 10300: loss = 0.52651
Step 10305: loss = 0.54395
Step 10310: loss = 0.49424
Step 10315: loss = 0.52436
Step 10320: loss = 0.64904
Step 10325: loss = 0.51698
Step 10330: loss = 0.43751
Step 10335: loss = 0.44197
Step 10340: loss = 0.55519
Step 10345: loss = 0.46206
Step 10350: loss = 0.51609
Step 10355: loss = 0.57975
Step 10360: loss = 0.52594
Step 10365: loss = 0.42737
Step 10370: loss = 0.46970
Step 10375: loss = 0.66436
Step 10380: loss = 0.61365
Step 10385: loss = 0.52327
Step 10390: loss = 0.36758
Step 10395: loss = 0.40749
Step 10400: loss = 0.53778
Step 10405: loss = 0.57474
Step 10410: loss = 0.49355
Step 10415: loss = 0.63402
Step 10420: loss = 0.36420
Step 10425: loss = 0.55118
Step 10430: loss = 0.53579
Step 10435: loss = 0.54735
Step 10440: loss = 0.62656
Step 10445: loss = 0.61306
Step 10450: loss = 0.61310
Step 10455: loss = 0.51526
Step 10460: loss = 0.55613
Step 10465: loss = 0.55181
Step 10470: loss = 0.39384
Step 10475: loss = 0.46094
Step 10480: loss = 0.52806
Step 10485: loss = 0.50994
Step 10490: loss = 0.51756
Step 10495: loss = 0.39881
Step 10500: loss = 0.60140
Step 10505: loss = 0.65598
Step 10510: loss = 0.50361
Step 10515: loss = 0.46570
Step 10520: loss = 0.48583
Step 10525: loss = 0.55443
Step 10530: loss = 0.57033
Training Data Eval:
  Num examples: 49920, Num correct: 42087, Precision @ 1: 0.8431
('Testing Data Eval: EPOCH->', 28)
  Num examples: 9984, Num correct: 7172, Precision @ 1: 0.7183
Step 10535: loss = 0.41203
Step 10540: loss = 0.54960
Step 10545: loss = 0.36797
Step 10550: loss = 0.48254
Step 10555: loss = 0.57068
Step 10560: loss = 0.53735
Step 10565: loss = 0.40661
Step 10570: loss = 0.42619
Step 10575: loss = 0.43304
Step 10580: loss = 0.39780
Step 10585: loss = 0.45287
Step 10590: loss = 0.47469
Step 10595: loss = 0.36919
Step 10600: loss = 0.49115
Step 10605: loss = 0.52633
Step 10610: loss = 0.46132
Step 10615: loss = 0.47657
Step 10620: loss = 0.57613
Step 10625: loss = 0.54824
Step 10630: loss = 0.49420
Step 10635: loss = 0.60180
Step 10640: loss = 0.43980
Step 10645: loss = 0.61519
Step 10650: loss = 0.59267
Step 10655: loss = 0.46361
Step 10660: loss = 0.51955
Step 10665: loss = 0.33247
Step 10670: loss = 0.58233
Step 10675: loss = 0.51441
Step 10680: loss = 0.45315
Step 10685: loss = 0.47693
Step 10690: loss = 0.33664
Step 10695: loss = 0.43631
Step 10700: loss = 0.63601
Step 10705: loss = 0.42653
Step 10710: loss = 0.47367
Step 10715: loss = 0.55950
Step 10720: loss = 0.55612
Step 10725: loss = 0.34736
Step 10730: loss = 0.41957
Step 10735: loss = 0.46626
Step 10740: loss = 0.52871
Step 10745: loss = 0.54396
Step 10750: loss = 0.39573
Step 10755: loss = 0.51780
Step 10760: loss = 0.42292
Step 10765: loss = 0.36491
Step 10770: loss = 0.62960
Step 10775: loss = 0.51907
Step 10780: loss = 0.37619
Step 10785: loss = 0.36077
Step 10790: loss = 0.46103
Step 10795: loss = 0.34779
Step 10800: loss = 0.35775
Step 10805: loss = 0.46956
Step 10810: loss = 0.50994
Step 10815: loss = 0.62847
Step 10820: loss = 0.48898
Step 10825: loss = 0.68097
Step 10830: loss = 0.53156
Step 10835: loss = 0.58914
Step 10840: loss = 0.60048
Step 10845: loss = 0.58784
Step 10850: loss = 0.53517
Step 10855: loss = 0.54753
Step 10860: loss = 0.41533
Step 10865: loss = 0.50690
Step 10870: loss = 0.43529
Step 10875: loss = 0.59115
Step 10880: loss = 0.43590
Step 10885: loss = 0.50793
Step 10890: loss = 0.48949
Step 10895: loss = 0.43712
Step 10900: loss = 0.59613
Step 10905: loss = 0.55024
Step 10910: loss = 0.60831
Step 10915: loss = 0.45410
Step 10920: loss = 0.57793
Training Data Eval:
  Num examples: 49920, Num correct: 41571, Precision @ 1: 0.8328
('Testing Data Eval: EPOCH->', 29)
  Num examples: 9984, Num correct: 7074, Precision @ 1: 0.7085
Step 10925: loss = 0.42302
Step 10930: loss = 0.30111
Step 10935: loss = 0.41765
Step 10940: loss = 0.32538
Step 10945: loss = 0.54546
Step 10950: loss = 0.41157
Step 10955: loss = 0.40880
Step 10960: loss = 0.50182
Step 10965: loss = 0.52255
Step 10970: loss = 0.30358
Step 10975: loss = 0.55414
Step 10980: loss = 0.56439
Step 10985: loss = 0.48865
Step 10990: loss = 0.56599
Step 10995: loss = 0.43846
Step 11000: loss = 0.37800
Step 11005: loss = 0.36244
Step 11010: loss = 0.35929
Step 11015: loss = 0.39807
Step 11020: loss = 0.59520
Step 11025: loss = 0.53212
Step 11030: loss = 0.37245
Step 11035: loss = 0.53553
Step 11040: loss = 0.49524
Step 11045: loss = 0.44938
Step 11050: loss = 0.42832
Step 11055: loss = 0.38598
Step 11060: loss = 0.45261
Step 11065: loss = 0.62260
Step 11070: loss = 0.47418
Step 11075: loss = 0.53683
Step 11080: loss = 0.47840
Step 11085: loss = 0.43229
Step 11090: loss = 0.40836
Step 11095: loss = 0.47158
Step 11100: loss = 0.44712
Step 11105: loss = 0.49228
Step 11110: loss = 0.52019
Step 11115: loss = 0.41727
Step 11120: loss = 0.41884
Step 11125: loss = 0.32099
Step 11130: loss = 0.54019
Step 11135: loss = 0.50396
Step 11140: loss = 0.56044
Step 11145: loss = 0.42917
Step 11150: loss = 0.38041
Step 11155: loss = 0.53804
Step 11160: loss = 0.29271
Step 11165: loss = 0.46075
Step 11170: loss = 0.62952
Step 11175: loss = 0.46912
Step 11180: loss = 0.27942
Step 11185: loss = 0.43153
Step 11190: loss = 0.42633
Step 11195: loss = 0.34394
Step 11200: loss = 0.45366
Step 11205: loss = 0.48261
Step 11210: loss = 0.42436
Step 11215: loss = 0.33755
Step 11220: loss = 0.48041
Step 11225: loss = 0.49247
Step 11230: loss = 0.41755
Step 11235: loss = 0.34601
Step 11240: loss = 0.51397
Step 11245: loss = 0.29582
Step 11250: loss = 0.54935
Step 11255: loss = 0.71102
Step 11260: loss = 0.40485
Step 11265: loss = 0.52426
Step 11270: loss = 0.51497
Step 11275: loss = 0.34732
Step 11280: loss = 0.49093
Step 11285: loss = 0.38352
Step 11290: loss = 0.51273
Step 11295: loss = 0.37149
Step 11300: loss = 0.50247
Step 11305: loss = 0.56172
Step 11310: loss = 0.49010
Training Data Eval:
  Num examples: 49920, Num correct: 42252, Precision @ 1: 0.8464
('Testing Data Eval: EPOCH->', 30)
  Num examples: 9984, Num correct: 7033, Precision @ 1: 0.7044
Step 11315: loss = 0.39255
Step 11320: loss = 0.44642
Step 11325: loss = 0.25002
Step 11330: loss = 0.33739
Step 11335: loss = 0.41486
Step 11340: loss = 0.37122
Step 11345: loss = 0.37236
Step 11350: loss = 0.43641
Step 11355: loss = 0.41570
Step 11360: loss = 0.47086
Step 11365: loss = 0.48933
Step 11370: loss = 0.40009
Step 11375: loss = 0.49109
Step 11380: loss = 0.42139
Step 11385: loss = 0.28881
Step 11390: loss = 0.57716
Step 11395: loss = 0.53111
Step 11400: loss = 0.45889
Step 11405: loss = 0.42166
Step 11410: loss = 0.45227
Step 11415: loss = 0.39077
Step 11420: loss = 0.44841
Step 11425: loss = 0.49740
Step 11430: loss = 0.39349
Step 11435: loss = 0.53943
Step 11440: loss = 0.57579
Step 11445: loss = 0.37485
Step 11450: loss = 0.42842
Step 11455: loss = 0.36686
Step 11460: loss = 0.69218
Step 11465: loss = 0.62690
Step 11470: loss = 0.48136
Step 11475: loss = 0.59658
Step 11480: loss = 0.50459
Step 11485: loss = 0.44111
Step 11490: loss = 0.48912
Step 11495: loss = 0.35125
Step 11500: loss = 0.33094
Step 11505: loss = 0.39412
Step 11510: loss = 0.35339
Step 11515: loss = 0.44261
Step 11520: loss = 0.50003
Step 11525: loss = 0.30396
Step 11530: loss = 0.52021
Step 11535: loss = 0.51119
Step 11540: loss = 0.36577
Step 11545: loss = 0.48083
Step 11550: loss = 0.43542
Step 11555: loss = 0.35307
Step 11560: loss = 0.49860
Step 11565: loss = 0.47661
Step 11570: loss = 0.53702
Step 11575: loss = 0.44169
Step 11580: loss = 0.53161
Step 11585: loss = 0.56654
Step 11590: loss = 0.64727
Step 11595: loss = 0.44470
Step 11600: loss = 0.40811
Step 11605: loss = 0.36521
Step 11610: loss = 0.37977
Step 11615: loss = 0.40760
Step 11620: loss = 0.56276
Step 11625: loss = 0.35627
Step 11630: loss = 0.49482
Step 11635: loss = 0.43281
Step 11640: loss = 0.31074
Step 11645: loss = 0.24866
Step 11650: loss = 0.41526
Step 11655: loss = 0.40472
Step 11660: loss = 0.39374
Step 11665: loss = 0.40573
Step 11670: loss = 0.33093
Step 11675: loss = 0.50288
Step 11680: loss = 0.40048
Step 11685: loss = 0.43869
Step 11690: loss = 0.41405
Step 11695: loss = 0.42350
Step 11700: loss = 0.36072
Training Data Eval:
  Num examples: 49920, Num correct: 43149, Precision @ 1: 0.8644
('Testing Data Eval: EPOCH->', 31)
  Num examples: 9984, Num correct: 7167, Precision @ 1: 0.7178
Step 11705: loss = 0.34541
Step 11710: loss = 0.47515
Step 11715: loss = 0.45669
Step 11720: loss = 0.39916
Step 11725: loss = 0.40194
Step 11730: loss = 0.26593
Step 11735: loss = 0.22058
Step 11740: loss = 0.29537
Step 11745: loss = 0.41894
Step 11750: loss = 0.33525
Step 11755: loss = 0.29580
Step 11760: loss = 0.53162
Step 11765: loss = 0.59165
Step 11770: loss = 0.53430
Step 11775: loss = 0.48330
Step 11780: loss = 0.44088
Step 11785: loss = 0.52885
Step 11790: loss = 0.35932
Step 11795: loss = 0.37893
Step 11800: loss = 0.41566
Step 11805: loss = 0.40009
Step 11810: loss = 0.50017
Step 11815: loss = 0.39330
Step 11820: loss = 0.50770
Step 11825: loss = 0.38018
Step 11830: loss = 0.57367
Step 11835: loss = 0.39369
Step 11840: loss = 0.35641
Step 11845: loss = 0.46667
Step 11850: loss = 0.42354
Step 11855: loss = 0.40425
Step 11860: loss = 0.58334
Step 11865: loss = 0.40170
Step 11870: loss = 0.41065
Step 11875: loss = 0.35270
Step 11880: loss = 0.35944
Step 11885: loss = 0.40731
Step 11890: loss = 0.42846
Step 11895: loss = 0.38672
Step 11900: loss = 0.38497
Step 11905: loss = 0.37183
Step 11910: loss = 0.39814
Step 11915: loss = 0.49462
Step 11920: loss = 0.41799
Step 11925: loss = 0.39525
Step 11930: loss = 0.33235
Step 11935: loss = 0.28020
Step 11940: loss = 0.52975
Step 11945: loss = 0.38802
Step 11950: loss = 0.33141
Step 11955: loss = 0.54623
Step 11960: loss = 0.53633
Step 11965: loss = 0.38493
Step 11970: loss = 0.36654
Step 11975: loss = 0.35224
Step 11980: loss = 0.33320
Step 11985: loss = 0.57246
Step 11990: loss = 0.46525
Step 11995: loss = 0.44273
Step 12000: loss = 0.39583
Step 12005: loss = 0.41739
Step 12010: loss = 0.34791
Step 12015: loss = 0.42996
Step 12020: loss = 0.36760
Step 12025: loss = 0.44565
Step 12030: loss = 0.36199
Step 12035: loss = 0.32912
Step 12040: loss = 0.35432
Step 12045: loss = 0.33647
Step 12050: loss = 0.36522
Step 12055: loss = 0.46611
Step 12060: loss = 0.30836
Step 12065: loss = 0.44471
Step 12070: loss = 0.43128
Step 12075: loss = 0.43827
Step 12080: loss = 0.39863
Step 12085: loss = 0.40284
Step 12090: loss = 0.44215
Training Data Eval:
  Num examples: 49920, Num correct: 43729, Precision @ 1: 0.8760
('Testing Data Eval: EPOCH->', 32)
  Num examples: 9984, Num correct: 7223, Precision @ 1: 0.7235
Step 12095: loss = 0.41522
Step 12100: loss = 0.37495
Step 12105: loss = 0.38277
Step 12110: loss = 0.41569
Step 12115: loss = 0.26690
Step 12120: loss = 0.35531
Step 12125: loss = 0.35490
Step 12130: loss = 0.35863
Step 12135: loss = 0.38895
Step 12140: loss = 0.24360
Step 12145: loss = 0.39756
Step 12150: loss = 0.30947
Step 12155: loss = 0.35589
Step 12160: loss = 0.35356
Step 12165: loss = 0.42730
Step 12170: loss = 0.24469
Step 12175: loss = 0.44298
Step 12180: loss = 0.42550
Step 12185: loss = 0.42989
Step 12190: loss = 0.46221
Step 12195: loss = 0.30895
Step 12200: loss = 0.48896
Step 12205: loss = 0.45563
Step 12210: loss = 0.40354
Step 12215: loss = 0.56160
Step 12220: loss = 0.43138
Step 12225: loss = 0.34784
Step 12230: loss = 0.33642
Step 12235: loss = 0.47885
Step 12240: loss = 0.34364
Step 12245: loss = 0.39960
Step 12250: loss = 0.32406
Step 12255: loss = 0.40855
Step 12260: loss = 0.30424
Step 12265: loss = 0.30920
Step 12270: loss = 0.33649
Step 12275: loss = 0.44368
Step 12280: loss = 0.41538
Step 12285: loss = 0.49904
Step 12290: loss = 0.42284
Step 12295: loss = 0.25660
Step 12300: loss = 0.33536
Step 12305: loss = 0.36375
Step 12310: loss = 0.45013
Step 12315: loss = 0.39387
Step 12320: loss = 0.39579
Step 12325: loss = 0.31851
Step 12330: loss = 0.31369
Step 12335: loss = 0.35802
Step 12340: loss = 0.45844
Step 12345: loss = 0.19962
Step 12350: loss = 0.29102
Step 12355: loss = 0.43101
Step 12360: loss = 0.35570
Step 12365: loss = 0.33536
Step 12370: loss = 0.57884
Step 12375: loss = 0.46288
Step 12380: loss = 0.25361
Step 12385: loss = 0.49046
Step 12390: loss = 0.55351
Step 12395: loss = 0.40448
Step 12400: loss = 0.51873
Step 12405: loss = 0.46024
Step 12410: loss = 0.44114
Step 12415: loss = 0.37036
Step 12420: loss = 0.42078
Step 12425: loss = 0.32402
Step 12430: loss = 0.36241
Step 12435: loss = 0.27053
Step 12440: loss = 0.32730
Step 12445: loss = 0.40786
Step 12450: loss = 0.50744
Step 12455: loss = 0.44798
Step 12460: loss = 0.51711
Step 12465: loss = 0.48005
Step 12470: loss = 0.34319
Step 12475: loss = 0.43410
Step 12480: loss = 0.35000
Training Data Eval:
  Num examples: 49920, Num correct: 43654, Precision @ 1: 0.8745
('Testing Data Eval: EPOCH->', 33)
  Num examples: 9984, Num correct: 7232, Precision @ 1: 0.7244
Step 12485: loss = 0.30601
Step 12490: loss = 0.54329
Step 12495: loss = 0.35228
Step 12500: loss = 0.35396
Step 12505: loss = 0.36698
Step 12510: loss = 0.28847
Step 12515: loss = 0.59705
Step 12520: loss = 0.27917
Step 12525: loss = 0.37067
Step 12530: loss = 0.40667
Step 12535: loss = 0.41351
Step 12540: loss = 0.24248
Step 12545: loss = 0.40024
Step 12550: loss = 0.39295
Step 12555: loss = 0.37395
Step 12560: loss = 0.40308
Step 12565: loss = 0.37394
Step 12570: loss = 0.43088
Step 12575: loss = 0.45052
Step 12580: loss = 0.42708
Step 12585: loss = 0.38663
Step 12590: loss = 0.38511
Step 12595: loss = 0.36888
Step 12600: loss = 0.47927
Step 12605: loss = 0.43144
Step 12610: loss = 0.43878
Step 12615: loss = 0.54569
Step 12620: loss = 0.39911
Step 12625: loss = 0.43246
Step 12630: loss = 0.35345
Step 12635: loss = 0.44554
Step 12640: loss = 0.32350
Step 12645: loss = 0.36357
Step 12650: loss = 0.49125
Step 12655: loss = 0.35755
Step 12660: loss = 0.35658
Step 12665: loss = 0.32702
Step 12670: loss = 0.47953
Step 12675: loss = 0.30037
Step 12680: loss = 0.29473
Step 12685: loss = 0.37170
Step 12690: loss = 0.34888
Step 12695: loss = 0.39134
Step 12700: loss = 0.44369
Step 12705: loss = 0.38961
Step 12710: loss = 0.45074
Step 12715: loss = 0.41066
Step 12720: loss = 0.40412
Step 12725: loss = 0.45152
Step 12730: loss = 0.39429
Step 12735: loss = 0.36772
Step 12740: loss = 0.41020
Step 12745: loss = 0.51882
Step 12750: loss = 0.34538
Step 12755: loss = 0.43557
Step 12760: loss = 0.31770
Step 12765: loss = 0.40707
Step 12770: loss = 0.33890
Step 12775: loss = 0.35364
Step 12780: loss = 0.33798
Step 12785: loss = 0.58598
Step 12790: loss = 0.26142
Step 12795: loss = 0.32746
Step 12800: loss = 0.31431
Step 12805: loss = 0.35957
Step 12810: loss = 0.30835
Step 12815: loss = 0.42445
Step 12820: loss = 0.59423
Step 12825: loss = 0.31137
Step 12830: loss = 0.37398
Step 12835: loss = 0.41384
Step 12840: loss = 0.42378
Step 12845: loss = 0.29720
Step 12850: loss = 0.37316
Step 12855: loss = 0.52885
Step 12860: loss = 0.27781
Step 12865: loss = 0.33223
Step 12870: loss = 0.55370
Training Data Eval:
  Num examples: 49920, Num correct: 43976, Precision @ 1: 0.8809
('Testing Data Eval: EPOCH->', 34)
  Num examples: 9984, Num correct: 7215, Precision @ 1: 0.7227
Step 12875: loss = 0.32594
Step 12880: loss = 0.37977
Step 12885: loss = 0.32330
Step 12890: loss = 0.35865
Step 12895: loss = 0.35040
Step 12900: loss = 0.19139
Step 12905: loss = 0.28913
Step 12910: loss = 0.32931
Step 12915: loss = 0.38592
Step 12920: loss = 0.25946
Step 12925: loss = 0.31471
Step 12930: loss = 0.43775
Step 12935: loss = 0.29977
Step 12940: loss = 0.33996
Step 12945: loss = 0.33801
Step 12950: loss = 0.36749
Step 12955: loss = 0.41690
Step 12960: loss = 0.41542
Step 12965: loss = 0.35849
Step 12970: loss = 0.44466
Step 12975: loss = 0.36206
Step 12980: loss = 0.33893
Step 12985: loss = 0.38157
Step 12990: loss = 0.35762
Step 12995: loss = 0.40174
Step 13000: loss = 0.31450
Step 13005: loss = 0.28486
Step 13010: loss = 0.50431
Step 13015: loss = 0.35988
Step 13020: loss = 0.34348
Step 13025: loss = 0.36941
Step 13030: loss = 0.28682
Step 13035: loss = 0.44684
Step 13040: loss = 0.24815
Step 13045: loss = 0.34152
Step 13050: loss = 0.40719
Step 13055: loss = 0.29893
Step 13060: loss = 0.29717
Step 13065: loss = 0.39417
Step 13070: loss = 0.39892
Step 13075: loss = 0.46533
Step 13080: loss = 0.26256
Step 13085: loss = 0.29283
Step 13090: loss = 0.56439
Step 13095: loss = 0.31881
Step 13100: loss = 0.39552
Step 13105: loss = 0.36886
Step 13110: loss = 0.46187
Step 13115: loss = 0.33366
Step 13120: loss = 0.36066
Step 13125: loss = 0.35119
Step 13130: loss = 0.28036
Step 13135: loss = 0.37640
Step 13140: loss = 0.42314
Step 13145: loss = 0.42175
Step 13150: loss = 0.44477
Step 13155: loss = 0.43384
Step 13160: loss = 0.41900
Step 13165: loss = 0.53729
Step 13170: loss = 0.39581
Step 13175: loss = 0.34511
Step 13180: loss = 0.43799
Step 13185: loss = 0.37415
Step 13190: loss = 0.42441
Step 13195: loss = 0.36191
Step 13200: loss = 0.41351
Step 13205: loss = 0.59567
Step 13210: loss = 0.40494
Step 13215: loss = 0.36048
Step 13220: loss = 0.33394
Step 13225: loss = 0.34179
Step 13230: loss = 0.36556
Step 13235: loss = 0.49003
Step 13240: loss = 0.40840
Step 13245: loss = 0.30144
Step 13250: loss = 0.28632
Step 13255: loss = 0.31957
Step 13260: loss = 0.36429
Training Data Eval:
  Num examples: 49920, Num correct: 44332, Precision @ 1: 0.8881
('Testing Data Eval: EPOCH->', 35)
  Num examples: 9984, Num correct: 7216, Precision @ 1: 0.7228
Step 13265: loss = 0.26760
Step 13270: loss = 0.28670
Step 13275: loss = 0.35909
Step 13280: loss = 0.31283
Step 13285: loss = 0.34492
Step 13290: loss = 0.42385
Step 13295: loss = 0.34800
Step 13300: loss = 0.31212
Step 13305: loss = 0.32492
Step 13310: loss = 0.45893
Step 13315: loss = 0.33168
Step 13320: loss = 0.38788
Step 13325: loss = 0.39858
Step 13330: loss = 0.27962
Step 13335: loss = 0.29579
Step 13340: loss = 0.36649
Step 13345: loss = 0.49782
Step 13350: loss = 0.46662
Step 13355: loss = 0.37752
Step 13360: loss = 0.38524
Step 13365: loss = 0.27421
Step 13370: loss = 0.21611
Step 13375: loss = 0.42575
Step 13380: loss = 0.36228
Step 13385: loss = 0.44358
Step 13390: loss = 0.44603
Step 13395: loss = 0.40185
Step 13400: loss = 0.32315
Step 13405: loss = 0.34438
Step 13410: loss = 0.34232
Step 13415: loss = 0.38453
Step 13420: loss = 0.41582
Step 13425: loss = 0.30846
Step 13430: loss = 0.45736
Step 13435: loss = 0.29688
Step 13440: loss = 0.25150
Step 13445: loss = 0.42384
Step 13450: loss = 0.28293
Step 13455: loss = 0.21872
Step 13460: loss = 0.41338
Step 13465: loss = 0.36389
Step 13470: loss = 0.39769
Step 13475: loss = 0.38207
Step 13480: loss = 0.52262
Step 13485: loss = 0.39191
Step 13490: loss = 0.33863
Step 13495: loss = 0.37847
Step 13500: loss = 0.51740
Step 13505: loss = 0.36346
Step 13510: loss = 0.43092
Step 13515: loss = 0.28037
Step 13520: loss = 0.33338
Step 13525: loss = 0.54116
Step 13530: loss = 0.36757
Step 13535: loss = 0.38884
Step 13540: loss = 0.36265
Step 13545: loss = 0.48925
Step 13550: loss = 0.38988
Step 13555: loss = 0.34992
Step 13560: loss = 0.49612
Step 13565: loss = 0.42573
Step 13570: loss = 0.58765
Step 13575: loss = 0.28578
Step 13580: loss = 0.45878
Step 13585: loss = 0.23628
Step 13590: loss = 0.36030
Step 13595: loss = 0.39166
Step 13600: loss = 0.47936
Step 13605: loss = 0.48401
Step 13610: loss = 0.30095
Step 13615: loss = 0.34835
Step 13620: loss = 0.37796
Step 13625: loss = 0.30459
Step 13630: loss = 0.50914
Step 13635: loss = 0.34624
Step 13640: loss = 0.32471
Step 13645: loss = 0.29638
Step 13650: loss = 0.40990
Training Data Eval:
  Num examples: 49920, Num correct: 44295, Precision @ 1: 0.8873
('Testing Data Eval: EPOCH->', 36)
  Num examples: 9984, Num correct: 7222, Precision @ 1: 0.7234
Step 13655: loss = 0.41278
Step 13660: loss = 0.27026
Step 13665: loss = 0.32890
Step 13670: loss = 0.27917
Step 13675: loss = 0.33345
Step 13680: loss = 0.28160
Step 13685: loss = 0.42358
Step 13690: loss = 0.43410
Step 13695: loss = 0.54501
Step 13700: loss = 0.31086
Step 13705: loss = 0.28269
Step 13710: loss = 0.32974
Step 13715: loss = 0.32106
Step 13720: loss = 0.29509
Step 13725: loss = 0.31320
Step 13730: loss = 0.26997
Step 13735: loss = 0.43437
Step 13740: loss = 0.26147
Step 13745: loss = 0.26189
Step 13750: loss = 0.35269
Step 13755: loss = 0.23002
Step 13760: loss = 0.32617
Step 13765: loss = 0.25180
Step 13770: loss = 0.43137
Step 13775: loss = 0.31706
Step 13780: loss = 0.45909
Step 13785: loss = 0.34396
Step 13790: loss = 0.28516
Step 13795: loss = 0.34564
Step 13800: loss = 0.35811
Step 13805: loss = 0.47865
Step 13810: loss = 0.33760
Step 13815: loss = 0.45160
Step 13820: loss = 0.47918
Step 13825: loss = 0.31480
Step 13830: loss = 0.50720
Step 13835: loss = 0.38944
Step 13840: loss = 0.26287
Step 13845: loss = 0.40144
Step 13850: loss = 0.45162
Step 13855: loss = 0.24039
Step 13860: loss = 0.34746
Step 13865: loss = 0.37210
Step 13870: loss = 0.30835
Step 13875: loss = 0.23608
Step 13880: loss = 0.32924
Step 13885: loss = 0.30788
Step 13890: loss = 0.44295
Step 13895: loss = 0.38738
Step 13900: loss = 0.28616
Step 13905: loss = 0.31044
Step 13910: loss = 0.40642
Step 13915: loss = 0.29267
Step 13920: loss = 0.31872
Step 13925: loss = 0.40976
Step 13930: loss = 0.24406
Step 13935: loss = 0.35548
Step 13940: loss = 0.42803
Step 13945: loss = 0.21468
Step 13950: loss = 0.30670
Step 13955: loss = 0.32151
Step 13960: loss = 0.29820
Step 13965: loss = 0.46396
Step 13970: loss = 0.33749
Step 13975: loss = 0.28422
Step 13980: loss = 0.45783
Step 13985: loss = 0.35318
Step 13990: loss = 0.28337
Step 13995: loss = 0.40063
Step 14000: loss = 0.33092
Step 14005: loss = 0.29197
Step 14010: loss = 0.38590
Step 14015: loss = 0.40076
Step 14020: loss = 0.28701
Step 14025: loss = 0.32116
Step 14030: loss = 0.34870
Step 14035: loss = 0.23579
Step 14040: loss = 0.27143
Training Data Eval:
  Num examples: 49920, Num correct: 44950, Precision @ 1: 0.9004
('Testing Data Eval: EPOCH->', 37)
  Num examples: 9984, Num correct: 7239, Precision @ 1: 0.7251
Step 14045: loss = 0.28330
Step 14050: loss = 0.27697
Step 14055: loss = 0.36278
Step 14060: loss = 0.36516
Step 14065: loss = 0.27368
Step 14070: loss = 0.24009
Step 14075: loss = 0.28698
Step 14080: loss = 0.28792
Step 14085: loss = 0.30680
Step 14090: loss = 0.20039
Step 14095: loss = 0.32594
Step 14100: loss = 0.17936
Step 14105: loss = 0.23406
Step 14110: loss = 0.52271
Step 14115: loss = 0.26769
Step 14120: loss = 0.44338
Step 14125: loss = 0.27320
Step 14130: loss = 0.20772
Step 14135: loss = 0.22989
Step 14140: loss = 0.36790
Step 14145: loss = 0.37646
Step 14150: loss = 0.44625
Step 14155: loss = 0.19548
Step 14160: loss = 0.26312
Step 14165: loss = 0.43732
Step 14170: loss = 0.27179
Step 14175: loss = 0.36859
Step 14180: loss = 0.38849
Step 14185: loss = 0.36762
Step 14190: loss = 0.44029
Step 14195: loss = 0.30640
Step 14200: loss = 0.30955
Step 14205: loss = 0.28173
Step 14210: loss = 0.29521
Step 14215: loss = 0.29036
Step 14220: loss = 0.43268
Step 14225: loss = 0.33189
Step 14230: loss = 0.26354
Step 14235: loss = 0.22781
Step 14240: loss = 0.34032
Step 14245: loss = 0.38723
Step 14250: loss = 0.25634
Step 14255: loss = 0.38478
Step 14260: loss = 0.30689
Step 14265: loss = 0.29554
Step 14270: loss = 0.39240
Step 14275: loss = 0.41323
Step 14280: loss = 0.33934
Step 14285: loss = 0.36401
Step 14290: loss = 0.29620
Step 14295: loss = 0.36342
Step 14300: loss = 0.38269
Step 14305: loss = 0.24706
Step 14310: loss = 0.35880
Step 14315: loss = 0.24607
Step 14320: loss = 0.30577
Step 14325: loss = 0.52505
Step 14330: loss = 0.19932
Step 14335: loss = 0.41531
Step 14340: loss = 0.31606
Step 14345: loss = 0.32477
Step 14350: loss = 0.32726
Step 14355: loss = 0.31586
Step 14360: loss = 0.30673
Step 14365: loss = 0.25001
Step 14370: loss = 0.39944
Step 14375: loss = 0.42350
Step 14380: loss = 0.40802
Step 14385: loss = 0.39688
Step 14390: loss = 0.30517
Step 14395: loss = 0.26043
Step 14400: loss = 0.42282
Step 14405: loss = 0.36536
Step 14410: loss = 0.32284
Step 14415: loss = 0.16665
Step 14420: loss = 0.39763
Step 14425: loss = 0.37100
Step 14430: loss = 0.31075
Training Data Eval:
  Num examples: 49920, Num correct: 44855, Precision @ 1: 0.8985
('Testing Data Eval: EPOCH->', 38)
  Num examples: 9984, Num correct: 7198, Precision @ 1: 0.7210
Step 14435: loss = 0.17343
Step 14440: loss = 0.31283
Step 14445: loss = 0.24459
Step 14450: loss = 0.26402
Step 14455: loss = 0.35867
Step 14460: loss = 0.24137
Step 14465: loss = 0.22702
Step 14470: loss = 0.25635
Step 14475: loss = 0.25057
Step 14480: loss = 0.27063
Step 14485: loss = 0.38294
Step 14490: loss = 0.28099
Step 14495: loss = 0.25263
Step 14500: loss = 0.36093
Step 14505: loss = 0.34380
Step 14510: loss = 0.28237
Step 14515: loss = 0.39873
Step 14520: loss = 0.34372
Step 14525: loss = 0.31574
Step 14530: loss = 0.33011
Step 14535: loss = 0.32243
Step 14540: loss = 0.36093
Step 14545: loss = 0.32585
Step 14550: loss = 0.22918
Step 14555: loss = 0.21024
Step 14560: loss = 0.25663
Step 14565: loss = 0.28701
Step 14570: loss = 0.30792
Step 14575: loss = 0.16147
Step 14580: loss = 0.28118
Step 14585: loss = 0.50190
Step 14590: loss = 0.24081
Step 14595: loss = 0.27688
Step 14600: loss = 0.27677
Step 14605: loss = 0.32635
Step 14610: loss = 0.21467
Step 14615: loss = 0.33334
Step 14620: loss = 0.21109
Step 14625: loss = 0.25130
Step 14630: loss = 0.31059
Step 14635: loss = 0.26463
Step 14640: loss = 0.24798
Step 14645: loss = 0.30536
Step 14650: loss = 0.45786
Step 14655: loss = 0.25485
Step 14660: loss = 0.40917
Step 14665: loss = 0.32386
Step 14670: loss = 0.25273
Step 14675: loss = 0.38107
Step 14680: loss = 0.37487
Step 14685: loss = 0.26044
Step 14690: loss = 0.20307
Step 14695: loss = 0.37218
Step 14700: loss = 0.25665
Step 14705: loss = 0.27891
Step 14710: loss = 0.46803
Step 14715: loss = 0.31731
Step 14720: loss = 0.24918
Step 14725: loss = 0.39986
Step 14730: loss = 0.26896
Step 14735: loss = 0.27875
Step 14740: loss = 0.36000
Step 14745: loss = 0.27242
Step 14750: loss = 0.27573
Step 14755: loss = 0.33325
Step 14760: loss = 0.38004
Step 14765: loss = 0.22662
Step 14770: loss = 0.25763
Step 14775: loss = 0.33334
Step 14780: loss = 0.44311
Step 14785: loss = 0.25974
Step 14790: loss = 0.20795
Step 14795: loss = 0.37828
Step 14800: loss = 0.27406
Step 14805: loss = 0.31696
Step 14810: loss = 0.26072
Step 14815: loss = 0.44272
Step 14820: loss = 0.28106
Training Data Eval:
  Num examples: 49920, Num correct: 45350, Precision @ 1: 0.9085
('Testing Data Eval: EPOCH->', 39)
  Num examples: 9984, Num correct: 7365, Precision @ 1: 0.7377
Step 14825: loss = 0.21922
Step 14830: loss = 0.31949
Step 14835: loss = 0.27270
Step 14840: loss = 0.23470
Step 14845: loss = 0.27289
Step 14850: loss = 0.20903
Step 14855: loss = 0.23428
Step 14860: loss = 0.27366
Step 14865: loss = 0.17735
Step 14870: loss = 0.41641
Step 14875: loss = 0.18989
Step 14880: loss = 0.33343
Step 14885: loss = 0.16780
Step 14890: loss = 0.27340
Step 14895: loss = 0.34541
Step 14900: loss = 0.29123
Step 14905: loss = 0.29642
Step 14910: loss = 0.40082
Step 14915: loss = 0.27622
Step 14920: loss = 0.25726
Step 14925: loss = 0.28323
Step 14930: loss = 0.23251
Step 14935: loss = 0.26234
Step 14940: loss = 0.34582
Step 14945: loss = 0.23662
Step 14950: loss = 0.27129
Step 14955: loss = 0.24719
Step 14960: loss = 0.31475
Step 14965: loss = 0.36703
Step 14970: loss = 0.27748
Step 14975: loss = 0.28722
Step 14980: loss = 0.33576
Step 14985: loss = 0.38260
Step 14990: loss = 0.31641
Step 14995: loss = 0.40492
Step 15000: loss = 0.32105
Step 15005: loss = 0.13247
Step 15010: loss = 0.27071
Step 15015: loss = 0.34550
Step 15020: loss = 0.37223
Step 15025: loss = 0.19270
Step 15030: loss = 0.23319
Step 15035: loss = 0.23978
Step 15040: loss = 0.22709
Step 15045: loss = 0.30408
Step 15050: loss = 0.27149
Step 15055: loss = 0.19347
Step 15060: loss = 0.21637
Step 15065: loss = 0.17820
Step 15070: loss = 0.21589
Step 15075: loss = 0.38804
Step 15080: loss = 0.39413
Step 15085: loss = 0.37326
Step 15090: loss = 0.22705
Step 15095: loss = 0.31909
Step 15100: loss = 0.23692
Step 15105: loss = 0.19393
Step 15110: loss = 0.19936
Step 15115: loss = 0.28546
Step 15120: loss = 0.24504
Step 15125: loss = 0.25063
Step 15130: loss = 0.22528
Step 15135: loss = 0.28819
Step 15140: loss = 0.35431
Step 15145: loss = 0.25497
Step 15150: loss = 0.42155
Step 15155: loss = 0.29585
Step 15160: loss = 0.30457
Step 15165: loss = 0.46809
Step 15170: loss = 0.17217
Step 15175: loss = 0.29365
Step 15180: loss = 0.22072
Step 15185: loss = 0.27657
Step 15190: loss = 0.30442
Step 15195: loss = 0.53577
Step 15200: loss = 0.22431
Step 15205: loss = 0.24379
Step 15210: loss = 0.22971
Training Data Eval:
  Num examples: 49920, Num correct: 45411, Precision @ 1: 0.9097
('Testing Data Eval: EPOCH->', 40)
  Num examples: 9984, Num correct: 7183, Precision @ 1: 0.7195
Step 15215: loss = 0.29953
Step 15220: loss = 0.24910
Step 15225: loss = 0.18443
Step 15230: loss = 0.13137
Step 15235: loss = 0.15195
Step 15240: loss = 0.16853
Step 15245: loss = 0.23978
Step 15250: loss = 0.24955
Step 15255: loss = 0.22198
Step 15260: loss = 0.16181
Step 15265: loss = 0.30256
Step 15270: loss = 0.30944
Step 15275: loss = 0.21099
Step 15280: loss = 0.13173
Step 15285: loss = 0.21489
Step 15290: loss = 0.18241
Step 15295: loss = 0.26793
Step 15300: loss = 0.20504
Step 15305: loss = 0.22415
Step 15310: loss = 0.24746
Step 15315: loss = 0.31303
Step 15320: loss = 0.20501
Step 15325: loss = 0.22461
Step 15330: loss = 0.41133
Step 15335: loss = 0.16733
Step 15340: loss = 0.29758
Step 15345: loss = 0.27210
Step 15350: loss = 0.21795
Step 15355: loss = 0.24762
Step 15360: loss = 0.33335
Step 15365: loss = 0.23769
Step 15370: loss = 0.24602
Step 15375: loss = 0.23211
Step 15380: loss = 0.51124
Step 15385: loss = 0.21171
Step 15390: loss = 0.20699
Step 15395: loss = 0.33214
Step 15400: loss = 0.26679
Step 15405: loss = 0.30142
Step 15410: loss = 0.27064
Step 15415: loss = 0.21912
Step 15420: loss = 0.44308
Step 15425: loss = 0.29676
Step 15430: loss = 0.38368
Step 15435: loss = 0.28204
Step 15440: loss = 0.30114
Step 15445: loss = 0.36678
Step 15450: loss = 0.17148
Step 15455: loss = 0.25653
Step 15460: loss = 0.32989
Step 15465: loss = 0.28874
Step 15470: loss = 0.20889
Step 15475: loss = 0.27047
Step 15480: loss = 0.26175
Step 15485: loss = 0.23595
Step 15490: loss = 0.31747
Step 15495: loss = 0.16984
Step 15500: loss = 0.28444
Step 15505: loss = 0.25576
Step 15510: loss = 0.26353
Step 15515: loss = 0.20278
Step 15520: loss = 0.33815
Step 15525: loss = 0.20807
Step 15530: loss = 0.15837
Step 15535: loss = 0.28071
Step 15540: loss = 0.13928
Step 15545: loss = 0.32083
Step 15550: loss = 0.20100
Step 15555: loss = 0.26369
Step 15560: loss = 0.16841
Step 15565: loss = 0.27500
Step 15570: loss = 0.36790
Step 15575: loss = 0.22412
Step 15580: loss = 0.18153
Step 15585: loss = 0.22745
Step 15590: loss = 0.25120
Step 15595: loss = 0.25711
Step 15600: loss = 0.24997
Training Data Eval:
  Num examples: 49920, Num correct: 46064, Precision @ 1: 0.9228
('Testing Data Eval: EPOCH->', 41)
  Num examples: 9984, Num correct: 7201, Precision @ 1: 0.7213
Step 15605: loss = 0.15332
Step 15610: loss = 0.23759
Step 15615: loss = 0.20690
Step 15620: loss = 0.26211
Step 15625: loss = 0.24609
Step 15630: loss = 0.32973
Step 15635: loss = 0.22667
Step 15640: loss = 0.18502
Step 15645: loss = 0.38803
Step 15650: loss = 0.23796
Step 15655: loss = 0.24452
Step 15660: loss = 0.24621
Step 15665: loss = 0.31904
Step 15670: loss = 0.33024
Step 15675: loss = 0.23317
Step 15680: loss = 0.32192
Step 15685: loss = 0.34998
Step 15690: loss = 0.40081
Step 15695: loss = 0.25763
Step 15700: loss = 0.14912
Step 15705: loss = 0.38673
Step 15710: loss = 0.11039
Step 15715: loss = 0.36788
Step 15720: loss = 0.24921
Step 15725: loss = 0.22548
Step 15730: loss = 0.34384
Step 15735: loss = 0.31338
Step 15740: loss = 0.17304
Step 15745: loss = 0.23514
Step 15750: loss = 0.17406
Step 15755: loss = 0.21928
Step 15760: loss = 0.21421
Step 15765: loss = 0.18807
Step 15770: loss = 0.24777
Step 15775: loss = 0.25931
Step 15780: loss = 0.29438
Step 15785: loss = 0.15530
Step 15790: loss = 0.30411
Step 15795: loss = 0.29182
Step 15800: loss = 0.29351
Step 15805: loss = 0.24091
Step 15810: loss = 0.28160
Step 15815: loss = 0.35034
Step 15820: loss = 0.45353
Step 15825: loss = 0.18167
Step 15830: loss = 0.38482
Step 15835: loss = 0.24930
Step 15840: loss = 0.37820
Step 15845: loss = 0.20110
Step 15850: loss = 0.46264
Step 15855: loss = 0.27811
Step 15860: loss = 0.14029
Step 15865: loss = 0.30217
Step 15870: loss = 0.25625
Step 15875: loss = 0.25889
Step 15880: loss = 0.27192
Step 15885: loss = 0.19032
Step 15890: loss = 0.25480
Step 15895: loss = 0.25889
Step 15900: loss = 0.33952
Step 15905: loss = 0.24890
Step 15910: loss = 0.22405
Step 15915: loss = 0.28947
Step 15920: loss = 0.19461
Step 15925: loss = 0.31029
Step 15930: loss = 0.23710
Step 15935: loss = 0.27549
Step 15940: loss = 0.24235
Step 15945: loss = 0.19276
Step 15950: loss = 0.18629
Step 15955: loss = 0.19533
Step 15960: loss = 0.29023
Step 15965: loss = 0.22858
Step 15970: loss = 0.28865
Step 15975: loss = 0.18319
Step 15980: loss = 0.19435
Step 15985: loss = 0.23421
Step 15990: loss = 0.14154
Training Data Eval:
  Num examples: 49920, Num correct: 46311, Precision @ 1: 0.9277
('Testing Data Eval: EPOCH->', 42)
  Num examples: 9984, Num correct: 7288, Precision @ 1: 0.7300
Step 15995: loss = 0.14854
Step 16000: loss = 0.27305
Step 16005: loss = 0.21116
Step 16010: loss = 0.18878
Step 16015: loss = 0.25744
Step 16020: loss = 0.11910
Step 16025: loss = 0.22977
Step 16030: loss = 0.19464
Step 16035: loss = 0.21212
Step 16040: loss = 0.13075
Step 16045: loss = 0.14751
Step 16050: loss = 0.21667
Step 16055: loss = 0.19291
Step 16060: loss = 0.21948
Step 16065: loss = 0.24870
Step 16070: loss = 0.19394
Step 16075: loss = 0.19195
Step 16080: loss = 0.22209
Step 16085: loss = 0.27715
Step 16090: loss = 0.22479
Step 16095: loss = 0.23334
Step 16100: loss = 0.28173
Step 16105: loss = 0.26472
Step 16110: loss = 0.16841
Step 16115: loss = 0.35065
Step 16120: loss = 0.18937
Step 16125: loss = 0.31164
Step 16130: loss = 0.29366
Step 16135: loss = 0.30736
Step 16140: loss = 0.22531
Step 16145: loss = 0.30903
Step 16150: loss = 0.23220
Step 16155: loss = 0.13102
Step 16160: loss = 0.18434
Step 16165: loss = 0.17207
Step 16170: loss = 0.23571
Step 16175: loss = 0.19345
Step 16180: loss = 0.40831
Step 16185: loss = 0.36535
Step 16190: loss = 0.22353
Step 16195: loss = 0.13772
Step 16200: loss = 0.17919
Step 16205: loss = 0.33343
Step 16210: loss = 0.19617
Step 16215: loss = 0.24814
Step 16220: loss = 0.22913
Step 16225: loss = 0.37114
Step 16230: loss = 0.17850
Step 16235: loss = 0.23793
Step 16240: loss = 0.19902
Step 16245: loss = 0.16408
Step 16250: loss = 0.25136
Step 16255: loss = 0.28662
Step 16260: loss = 0.29849
Step 16265: loss = 0.18391
Step 16270: loss = 0.29968
Step 16275: loss = 0.20128
Step 16280: loss = 0.27836
Step 16285: loss = 0.20833
Step 16290: loss = 0.23659
Step 16295: loss = 0.28096
Step 16300: loss = 0.27084
Step 16305: loss = 0.17008
Step 16310: loss = 0.19058
Step 16315: loss = 0.17114
Step 16320: loss = 0.27077
Step 16325: loss = 0.18881
Step 16330: loss = 0.19054
Step 16335: loss = 0.14882
Step 16340: loss = 0.31075
Step 16345: loss = 0.38157
Step 16350: loss = 0.22199
Step 16355: loss = 0.40807
Step 16360: loss = 0.15282
Step 16365: loss = 0.24777
Step 16370: loss = 0.24595
Step 16375: loss = 0.30325
Step 16380: loss = 0.21195
Training Data Eval:
  Num examples: 49920, Num correct: 46190, Precision @ 1: 0.9253
('Testing Data Eval: EPOCH->', 43)
  Num examples: 9984, Num correct: 7218, Precision @ 1: 0.7230
Step 16385: loss = 0.28663
Step 16390: loss = 0.12622
Step 16395: loss = 0.26771
Step 16400: loss = 0.19916
Step 16405: loss = 0.18499
Step 16410: loss = 0.16405
Step 16415: loss = 0.12329
Step 16420: loss = 0.20715
Step 16425: loss = 0.26043
Step 16430: loss = 0.26064
Step 16435: loss = 0.13446
Step 16440: loss = 0.39396
Step 16445: loss = 0.09136
Step 16450: loss = 0.20182
Step 16455: loss = 0.26466
Step 16460: loss = 0.21880
Step 16465: loss = 0.28077
Step 16470: loss = 0.25219
Step 16475: loss = 0.19329
Step 16480: loss = 0.22987
Step 16485: loss = 0.29774
Step 16490: loss = 0.23029
Step 16495: loss = 0.19424
Step 16500: loss = 0.21611
Step 16505: loss = 0.19974
Step 16510: loss = 0.30113
Step 16515: loss = 0.31445
Step 16520: loss = 0.28878
Step 16525: loss = 0.20388
Step 16530: loss = 0.20513
Step 16535: loss = 0.27904
Step 16540: loss = 0.16309
Step 16545: loss = 0.31701
Step 16550: loss = 0.27121
Step 16555: loss = 0.30734
Step 16560: loss = 0.23634
Step 16565: loss = 0.20718
Step 16570: loss = 0.20580
Step 16575: loss = 0.17267
Step 16580: loss = 0.24106
Step 16585: loss = 0.22772
Step 16590: loss = 0.36306
Step 16595: loss = 0.23963
Step 16600: loss = 0.28169
Step 16605: loss = 0.24600
Step 16610: loss = 0.27827
Step 16615: loss = 0.29241
Step 16620: loss = 0.23371
Step 16625: loss = 0.14728
Step 16630: loss = 0.25186
Step 16635: loss = 0.27511
Step 16640: loss = 0.22568
Step 16645: loss = 0.15223
Step 16650: loss = 0.26256
Step 16655: loss = 0.24382
Step 16660: loss = 0.27676
Step 16665: loss = 0.29448
Step 16670: loss = 0.16278
Step 16675: loss = 0.32091
Step 16680: loss = 0.24647
Step 16685: loss = 0.18020
Step 16690: loss = 0.19561
Step 16695: loss = 0.22122
Step 16700: loss = 0.12415
Step 16705: loss = 0.42954
Step 16710: loss = 0.13569
Step 16715: loss = 0.13413
Step 16720: loss = 0.19465
Step 16725: loss = 0.32009
Step 16730: loss = 0.22257
Step 16735: loss = 0.25092
Step 16740: loss = 0.27278
Step 16745: loss = 0.28669
Step 16750: loss = 0.32906
Step 16755: loss = 0.16400
Step 16760: loss = 0.28084
Step 16765: loss = 0.40975
Step 16770: loss = 0.50690
Training Data Eval:
  Num examples: 49920, Num correct: 46379, Precision @ 1: 0.9291
('Testing Data Eval: EPOCH->', 44)
  Num examples: 9984, Num correct: 7317, Precision @ 1: 0.7329
Step 16775: loss = 0.18446
Step 16780: loss = 0.19655
Step 16785: loss = 0.08856
Step 16790: loss = 0.21477
Step 16795: loss = 0.27129
Step 16800: loss = 0.35873
Step 16805: loss = 0.15472
Step 16810: loss = 0.15633
Step 16815: loss = 0.24413
Step 16820: loss = 0.24176
Step 16825: loss = 0.18324
Step 16830: loss = 0.16039
Step 16835: loss = 0.30830
Step 16840: loss = 0.17634
Step 16845: loss = 0.24144
Step 16850: loss = 0.17801
Step 16855: loss = 0.13333
Step 16860: loss = 0.18437
Step 16865: loss = 0.16558
Step 16870: loss = 0.16190
Step 16875: loss = 0.44910
Step 16880: loss = 0.31477
Step 16885: loss = 0.29196
Step 16890: loss = 0.20327
Step 16895: loss = 0.25838
Step 16900: loss = 0.26893
Step 16905: loss = 0.16056
Step 16910: loss = 0.21956
Step 16915: loss = 0.30309
Step 16920: loss = 0.23316
Step 16925: loss = 0.27800
Step 16930: loss = 0.15168
Step 16935: loss = 0.22410
Step 16940: loss = 0.31820
Step 16945: loss = 0.26263
Step 16950: loss = 0.19320
Step 16955: loss = 0.28674
Step 16960: loss = 0.25970
Step 16965: loss = 0.18865
Step 16970: loss = 0.31593
Step 16975: loss = 0.28903
Step 16980: loss = 0.24144
Step 16985: loss = 0.31072
Step 16990: loss = 0.18232
Step 16995: loss = 0.28985
Step 17000: loss = 0.34003
Step 17005: loss = 0.14356
Step 17010: loss = 0.15062
Step 17015: loss = 0.18824
Step 17020: loss = 0.31131
Step 17025: loss = 0.19835
Step 17030: loss = 0.35979
Step 17035: loss = 0.34400
Step 17040: loss = 0.27186
Step 17045: loss = 0.24221
Step 17050: loss = 0.29570
Step 17055: loss = 0.32985
Step 17060: loss = 0.22314
Step 17065: loss = 0.20580
Step 17070: loss = 0.16737
Step 17075: loss = 0.15367
Step 17080: loss = 0.25629
Step 17085: loss = 0.25651
Step 17090: loss = 0.16301
Step 17095: loss = 0.23187
Step 17100: loss = 0.24492
Step 17105: loss = 0.24693
Step 17110: loss = 0.32855
Step 17115: loss = 0.22555
Step 17120: loss = 0.36321
Step 17125: loss = 0.32647
Step 17130: loss = 0.28335
Step 17135: loss = 0.26028
Step 17140: loss = 0.31820
Step 17145: loss = 0.26143
Step 17150: loss = 0.21640
Step 17155: loss = 0.22295
Step 17160: loss = 0.29177
Training Data Eval:
  Num examples: 49920, Num correct: 46555, Precision @ 1: 0.9326
('Testing Data Eval: EPOCH->', 45)
  Num examples: 9984, Num correct: 7250, Precision @ 1: 0.7262
Step 17165: loss = 0.16912
Step 17170: loss = 0.29063
Step 17175: loss = 0.09798
Step 17180: loss = 0.16140
Step 17185: loss = 0.21008
Step 17190: loss = 0.13336
Step 17195: loss = 0.29421
Step 17200: loss = 0.20789
Step 17205: loss = 0.18248
Step 17210: loss = 0.18010
Step 17215: loss = 0.23635
Step 17220: loss = 0.25921
Step 17225: loss = 0.26872
Step 17230: loss = 0.28038
Step 17235: loss = 0.17935
Step 17240: loss = 0.26553
Step 17245: loss = 0.26225
Step 17250: loss = 0.32696
Step 17255: loss = 0.26436
Step 17260: loss = 0.25236
Step 17265: loss = 0.19029
Step 17270: loss = 0.29325
Step 17275: loss = 0.27115
Step 17280: loss = 0.34686
Step 17285: loss = 0.25393
Step 17290: loss = 0.15819
Step 17295: loss = 0.20970
Step 17300: loss = 0.30714
Step 17305: loss = 0.34885
Step 17310: loss = 0.21474
Step 17315: loss = 0.20737
Step 17320: loss = 0.14849
Step 17325: loss = 0.25517
Step 17330: loss = 0.24060
Step 17335: loss = 0.15553
Step 17340: loss = 0.28728
Step 17345: loss = 0.27443
Step 17350: loss = 0.13429
Step 17355: loss = 0.13724
Step 17360: loss = 0.24591
Step 17365: loss = 0.22218
Step 17370: loss = 0.19813
Step 17375: loss = 0.19041
Step 17380: loss = 0.21975
Step 17385: loss = 0.16325
Step 17390: loss = 0.19362
Step 17395: loss = 0.32301
Step 17400: loss = 0.11210
Step 17405: loss = 0.11152
Step 17410: loss = 0.19652
Step 17415: loss = 0.20074
Step 17420: loss = 0.12517
Step 17425: loss = 0.20975
Step 17430: loss = 0.23346
Step 17435: loss = 0.33688
Step 17440: loss = 0.25054
Step 17445: loss = 0.14571
Step 17450: loss = 0.29341
Step 17455: loss = 0.14641
Step 17460: loss = 0.15663
Step 17465: loss = 0.47827
Step 17470: loss = 0.20979
Step 17475: loss = 0.20480
Step 17480: loss = 0.20701
Step 17485: loss = 0.22254
Step 17490: loss = 0.27896
Step 17495: loss = 0.21476
Step 17500: loss = 0.16751
Step 17505: loss = 0.32131
Step 17510: loss = 0.34737
Step 17515: loss = 0.23579
Step 17520: loss = 0.26345
Step 17525: loss = 0.25876
Step 17530: loss = 0.21435
Step 17535: loss = 0.25898
Step 17540: loss = 0.20970
Step 17545: loss = 0.35997
Step 17550: loss = 0.13172
Training Data Eval:
  Num examples: 49920, Num correct: 46577, Precision @ 1: 0.9330
('Testing Data Eval: EPOCH->', 46)
  Num examples: 9984, Num correct: 7261, Precision @ 1: 0.7273
Step 17555: loss = 0.17987
Step 17560: loss = 0.24531
Step 17565: loss = 0.16938
Step 17570: loss = 0.16382
Step 17575: loss = 0.10048
Step 17580: loss = 0.25537
Step 17585: loss = 0.10381
Step 17590: loss = 0.26123
Step 17595: loss = 0.17186
Step 17600: loss = 0.19045
Step 17605: loss = 0.12743
Step 17610: loss = 0.26246
Step 17615: loss = 0.23666
Step 17620: loss = 0.12744
Step 17625: loss = 0.20366
Step 17630: loss = 0.21549
Step 17635: loss = 0.32512
Step 17640: loss = 0.20788
Step 17645: loss = 0.27258
Step 17650: loss = 0.25896
Step 17655: loss = 0.13793
Step 17660: loss = 0.28670
Step 17665: loss = 0.20150
Step 17670: loss = 0.19401
Step 17675: loss = 0.09838
Step 17680: loss = 0.15113
Step 17685: loss = 0.35759
Step 17690: loss = 0.15141
Step 17695: loss = 0.23519
Step 17700: loss = 0.32591
Step 17705: loss = 0.18391
Step 17710: loss = 0.17507
Step 17715: loss = 0.26678
Step 17720: loss = 0.24680
Step 17725: loss = 0.28326
Step 17730: loss = 0.27199
Step 17735: loss = 0.32477
Step 17740: loss = 0.14511
Step 17745: loss = 0.20799
Step 17750: loss = 0.20396
Step 17755: loss = 0.14650
Step 17760: loss = 0.17241
Step 17765: loss = 0.18324
Step 17770: loss = 0.20901
Step 17775: loss = 0.35540
Step 17780: loss = 0.20946
Step 17785: loss = 0.41666
Step 17790: loss = 0.28918
Step 17795: loss = 0.18898
Step 17800: loss = 0.41314
Step 17805: loss = 0.20157
Step 17810: loss = 0.12185
Step 17815: loss = 0.23143
Step 17820: loss = 0.12034
Step 17825: loss = 0.27787
Step 17830: loss = 0.19154
Step 17835: loss = 0.34161
Step 17840: loss = 0.24247
Step 17845: loss = 0.20197
Step 17850: loss = 0.16998
Step 17855: loss = 0.30075
Step 17860: loss = 0.18306
Step 17865: loss = 0.32200
Step 17870: loss = 0.11645
Step 17875: loss = 0.23628
Step 17880: loss = 0.17909
Step 17885: loss = 0.09922
Step 17890: loss = 0.20299
Step 17895: loss = 0.15927
Step 17900: loss = 0.22509
Step 17905: loss = 0.08431
Step 17910: loss = 0.27361
Step 17915: loss = 0.28039
Step 17920: loss = 0.26299
Step 17925: loss = 0.26443
Step 17930: loss = 0.24274
Step 17935: loss = 0.31553
Step 17940: loss = 0.29470
Training Data Eval:
  Num examples: 49920, Num correct: 46466, Precision @ 1: 0.9308
('Testing Data Eval: EPOCH->', 47)
  Num examples: 9984, Num correct: 7253, Precision @ 1: 0.7265
Step 17945: loss = 0.19987
Step 17950: loss = 0.17351
Step 17955: loss = 0.30307
Step 17960: loss = 0.11405
Step 17965: loss = 0.23428
Step 17970: loss = 0.18864
Step 17975: loss = 0.11302
Step 17980: loss = 0.16973
Step 17985: loss = 0.20856
Step 17990: loss = 0.26674
Step 17995: loss = 0.20148
Step 18000: loss = 0.15520
Step 18005: loss = 0.15683
Step 18010: loss = 0.26016
Step 18015: loss = 0.30854
Step 18020: loss = 0.16264
Step 18025: loss = 0.17177
Step 18030: loss = 0.20080
Step 18035: loss = 0.17857
Step 18040: loss = 0.12659
Step 18045: loss = 0.16490
Step 18050: loss = 0.31226
Step 18055: loss = 0.10764
Step 18060: loss = 0.19046
Step 18065: loss = 0.10452
Step 18070: loss = 0.16608
Step 18075: loss = 0.14279
Step 18080: loss = 0.12730
Step 18085: loss = 0.12949
Step 18090: loss = 0.22824
Step 18095: loss = 0.20257
Step 18100: loss = 0.19124
Step 18105: loss = 0.19382
Step 18110: loss = 0.21052
Step 18115: loss = 0.19385
Step 18120: loss = 0.20394
Step 18125: loss = 0.12384
Step 18130: loss = 0.12174
Step 18135: loss = 0.25190
Step 18140: loss = 0.32465
Step 18145: loss = 0.19218
Step 18150: loss = 0.16054
Step 18155: loss = 0.17125
Step 18160: loss = 0.25566
Step 18165: loss = 0.18644
Step 18170: loss = 0.12082
Step 18175: loss = 0.27590
Step 18180: loss = 0.25416
Step 18185: loss = 0.27426
Step 18190: loss = 0.14767
Step 18195: loss = 0.22870
Step 18200: loss = 0.18853
Step 18205: loss = 0.30189
Step 18210: loss = 0.14230
Step 18215: loss = 0.16805
Step 18220: loss = 0.08874
Step 18225: loss = 0.15781
Step 18230: loss = 0.12646
Step 18235: loss = 0.19886
Step 18240: loss = 0.22547
Step 18245: loss = 0.13921
Step 18250: loss = 0.18234
Step 18255: loss = 0.13728
Step 18260: loss = 0.17059
Step 18265: loss = 0.13264
Step 18270: loss = 0.18669
Step 18275: loss = 0.16976
Step 18280: loss = 0.22907
Step 18285: loss = 0.14770
Step 18290: loss = 0.26167
Step 18295: loss = 0.27022
Step 18300: loss = 0.13884
Step 18305: loss = 0.36317
Step 18310: loss = 0.29331
Step 18315: loss = 0.21981
Step 18320: loss = 0.20433
Step 18325: loss = 0.30021
Step 18330: loss = 0.26955
Training Data Eval:
  Num examples: 49920, Num correct: 46689, Precision @ 1: 0.9353
('Testing Data Eval: EPOCH->', 48)
  Num examples: 9984, Num correct: 7274, Precision @ 1: 0.7286
Step 18335: loss = 0.17020
Step 18340: loss = 0.12358
Step 18345: loss = 0.24867
Step 18350: loss = 0.18250
Step 18355: loss = 0.22176
Step 18360: loss = 0.10070
Step 18365: loss = 0.21745
Step 18370: loss = 0.16303
Step 18375: loss = 0.18744
Step 18380: loss = 0.19350
Step 18385: loss = 0.14556
Step 18390: loss = 0.19822
Step 18395: loss = 0.12086
Step 18400: loss = 0.22645
Step 18405: loss = 0.31733
Step 18410: loss = 0.17912
Step 18415: loss = 0.31200
Step 18420: loss = 0.13258
Step 18425: loss = 0.16571
Step 18430: loss = 0.15522
Step 18435: loss = 0.20857
Step 18440: loss = 0.18869
Step 18445: loss = 0.24076
Step 18450: loss = 0.11683
Step 18455: loss = 0.19498
Step 18460: loss = 0.17853
Step 18465: loss = 0.17461
Step 18470: loss = 0.13019
Step 18475: loss = 0.19595
Step 18480: loss = 0.16491
Step 18485: loss = 0.22249
Step 18490: loss = 0.18387
Step 18495: loss = 0.14747
Step 18500: loss = 0.16889
Step 18505: loss = 0.16441
Step 18510: loss = 0.35574
Step 18515: loss = 0.22540
Step 18520: loss = 0.18472
Step 18525: loss = 0.27859
Step 18530: loss = 0.17781
Step 18535: loss = 0.12751
Step 18540: loss = 0.18781
Step 18545: loss = 0.12425
Step 18550: loss = 0.19497
Step 18555: loss = 0.22821
Step 18560: loss = 0.13584
Step 18565: loss = 0.25558
Step 18570: loss = 0.24998
Step 18575: loss = 0.26131
Step 18580: loss = 0.14109
Step 18585: loss = 0.21871
Step 18590: loss = 0.13730
Step 18595: loss = 0.29757
Step 18600: loss = 0.11493
Step 18605: loss = 0.23860
Step 18610: loss = 0.37518
Step 18615: loss = 0.15557
Step 18620: loss = 0.16687
Step 18625: loss = 0.18455
Step 18630: loss = 0.19757
Step 18635: loss = 0.22892
Step 18640: loss = 0.19003
Step 18645: loss = 0.18953
Step 18650: loss = 0.18071
Step 18655: loss = 0.14419
Step 18660: loss = 0.17556
Step 18665: loss = 0.13658
Step 18670: loss = 0.18974
Step 18675: loss = 0.19927
Step 18680: loss = 0.11271
Step 18685: loss = 0.13347
Step 18690: loss = 0.25498
Step 18695: loss = 0.16954
Step 18700: loss = 0.25791
Step 18705: loss = 0.25065
Step 18710: loss = 0.23695
Step 18715: loss = 0.19866
Step 18720: loss = 0.24866
Training Data Eval:
  Num examples: 49920, Num correct: 46772, Precision @ 1: 0.9369
('Testing Data Eval: EPOCH->', 49)
  Num examples: 9984, Num correct: 7291, Precision @ 1: 0.7303
Step 18725: loss = 0.14975
Step 18730: loss = 0.09922
Step 18735: loss = 0.21582
Step 18740: loss = 0.17978
Step 18745: loss = 0.18744
Step 18750: loss = 0.15890
Step 18755: loss = 0.17650
Step 18760: loss = 0.26573
Step 18765: loss = 0.09659
Step 18770: loss = 0.23746
Step 18775: loss = 0.21470
Step 18780: loss = 0.10319
Step 18785: loss = 0.20861
Step 18790: loss = 0.18483
Step 18795: loss = 0.26351
Step 18800: loss = 0.24340
Step 18805: loss = 0.19716
Step 18810: loss = 0.27956
Step 18815: loss = 0.13509
Step 18820: loss = 0.21290
Step 18825: loss = 0.25941
Step 18830: loss = 0.13153
Step 18835: loss = 0.27000
Step 18840: loss = 0.13107
Step 18845: loss = 0.31619
Step 18850: loss = 0.20229
Step 18855: loss = 0.20634
Step 18860: loss = 0.14638
Step 18865: loss = 0.26377
Step 18870: loss = 0.24878
Step 18875: loss = 0.27705
Step 18880: loss = 0.16330
Step 18885: loss = 0.25352
Step 18890: loss = 0.20893
Step 18895: loss = 0.17062
Step 18900: loss = 0.18913
Step 18905: loss = 0.11874
Step 18910: loss = 0.20481
Step 18915: loss = 0.19562
Step 18920: loss = 0.12127
Step 18925: loss = 0.21075
Step 18930: loss = 0.24331
Step 18935: loss = 0.19862
Step 18940: loss = 0.18713
Step 18945: loss = 0.21921
Step 18950: loss = 0.13921
Step 18955: loss = 0.15731
Step 18960: loss = 0.17706
Step 18965: loss = 0.23545
Step 18970: loss = 0.23953
Step 18975: loss = 0.18735
Step 18980: loss = 0.25286
Step 18985: loss = 0.17840
Step 18990: loss = 0.40375
Step 18995: loss = 0.20988
Step 19000: loss = 0.11145
Step 19005: loss = 0.16074
Step 19010: loss = 0.21085
Step 19015: loss = 0.17491
Step 19020: loss = 0.12870
Step 19025: loss = 0.14428
Step 19030: loss = 0.21236
Step 19035: loss = 0.21041
Step 19040: loss = 0.17320
Step 19045: loss = 0.14213
Step 19050: loss = 0.21053
Step 19055: loss = 0.13180
Step 19060: loss = 0.25805
Step 19065: loss = 0.15908
Step 19070: loss = 0.13792
Step 19075: loss = 0.17080
Step 19080: loss = 0.18182
Step 19085: loss = 0.13356
Step 19090: loss = 0.14940
Step 19095: loss = 0.19124
Step 19100: loss = 0.48096
Step 19105: loss = 0.24497
Step 19110: loss = 0.21706
Training Data Eval:
  Num examples: 49920, Num correct: 47233, Precision @ 1: 0.9462
('Testing Data Eval: EPOCH->', 50)
  Num examples: 9984, Num correct: 7352, Precision @ 1: 0.7364
Step 19115: loss = 0.10814
Step 19120: loss = 0.13253
Step 19125: loss = 0.29155
Step 19130: loss = 0.23223
Step 19135: loss = 0.16355
Step 19140: loss = 0.22042
Step 19145: loss = 0.15680
Step 19150: loss = 0.18529
Step 19155: loss = 0.28765
Step 19160: loss = 0.22793
Step 19165: loss = 0.15231
Step 19170: loss = 0.10681
Step 19175: loss = 0.18181
Step 19180: loss = 0.15881
Step 19185: loss = 0.12506
Step 19190: loss = 0.11957
Step 19195: loss = 0.14453
Step 19200: loss = 0.11724
Step 19205: loss = 0.26549
Step 19210: loss = 0.18659
Step 19215: loss = 0.13558
Step 19220: loss = 0.18453
Step 19225: loss = 0.17908
Step 19230: loss = 0.17372
Step 19235: loss = 0.22291
Step 19240: loss = 0.15615
Step 19245: loss = 0.10742
Step 19250: loss = 0.13528
Step 19255: loss = 0.25703
Step 19260: loss = 0.14278
Step 19265: loss = 0.16403
Step 19270: loss = 0.22162
Step 19275: loss = 0.14105
Step 19280: loss = 0.18452
Step 19285: loss = 0.26293
Step 19290: loss = 0.12214
Step 19295: loss = 0.05806
Step 19300: loss = 0.17518
Step 19305: loss = 0.15002
Step 19310: loss = 0.13825
Step 19315: loss = 0.20573
Step 19320: loss = 0.13816
Step 19325: loss = 0.22199
Step 19330: loss = 0.13896
Step 19335: loss = 0.13773
Step 19340: loss = 0.12618
Step 19345: loss = 0.17953
Step 19350: loss = 0.28173
Step 19355: loss = 0.22250
Step 19360: loss = 0.15090
Step 19365: loss = 0.20025
Step 19370: loss = 0.28830
Step 19375: loss = 0.23629
Step 19380: loss = 0.22251
Step 19385: loss = 0.19909
Step 19390: loss = 0.12950
Step 19395: loss = 0.26669
Step 19400: loss = 0.20750
Step 19405: loss = 0.17229
Step 19410: loss = 0.23179
Step 19415: loss = 0.12136
Step 19420: loss = 0.21761
Step 19425: loss = 0.14570
Step 19430: loss = 0.21534
Step 19435: loss = 0.23483
Step 19440: loss = 0.29275
Step 19445: loss = 0.17085
Step 19450: loss = 0.21301
Step 19455: loss = 0.08598
Step 19460: loss = 0.17825
Step 19465: loss = 0.16750
Step 19470: loss = 0.21288
Step 19475: loss = 0.22500
Step 19480: loss = 0.22885
Step 19485: loss = 0.27330
Step 19490: loss = 0.17919
Step 19495: loss = 0.11814
Step 19500: loss = 0.14152
Training Data Eval:
  Num examples: 49920, Num correct: 47033, Precision @ 1: 0.9422
('Testing Data Eval: EPOCH->', 51)
  Num examples: 9984, Num correct: 7261, Precision @ 1: 0.7273
Step 19505: loss = 0.22028
Step 19510: loss = 0.21060
Step 19515: loss = 0.09314
Step 19520: loss = 0.20395
Step 19525: loss = 0.17865
Step 19530: loss = 0.34536
Step 19535: loss = 0.16074
Step 19540: loss = 0.17406
Step 19545: loss = 0.11862
Step 19550: loss = 0.17282
Step 19555: loss = 0.23881
Step 19560: loss = 0.12191
Step 19565: loss = 0.19151
Step 19570: loss = 0.11709
Step 19575: loss = 0.18335
Step 19580: loss = 0.17135
Step 19585: loss = 0.15722
Step 19590: loss = 0.17781
Step 19595: loss = 0.14687
Step 19600: loss = 0.19380
Step 19605: loss = 0.26703
Step 19610: loss = 0.27289
Step 19615: loss = 0.21927
Step 19620: loss = 0.09035
Step 19625: loss = 0.23627
Step 19630: loss = 0.15555
Step 19635: loss = 0.26708
Step 19640: loss = 0.18815
Step 19645: loss = 0.12937
Step 19650: loss = 0.11703
Step 19655: loss = 0.08338
Step 19660: loss = 0.29622
Step 19665: loss = 0.19620
Step 19670: loss = 0.18743
Step 19675: loss = 0.13208
Step 19680: loss = 0.22644
Step 19685: loss = 0.16646
Step 19690: loss = 0.21185
Step 19695: loss = 0.19449
Step 19700: loss = 0.09491
Step 19705: loss = 0.16379
Step 19710: loss = 0.15844
Step 19715: loss = 0.19623
Step 19720: loss = 0.11986
Step 19725: loss = 0.10461
Step 19730: loss = 0.12826
Step 19735: loss = 0.13642
Step 19740: loss = 0.24528
Step 19745: loss = 0.19455
Step 19750: loss = 0.22720
Step 19755: loss = 0.20980
Step 19760: loss = 0.19271
Step 19765: loss = 0.25504
Step 19770: loss = 0.21929
Step 19775: loss = 0.08825
Step 19780: loss = 0.14486
Step 19785: loss = 0.13382
Step 19790: loss = 0.19923
Step 19795: loss = 0.25431
Step 19800: loss = 0.10694
Step 19805: loss = 0.18986
Step 19810: loss = 0.12912
Step 19815: loss = 0.16654
Step 19820: loss = 0.14098
Step 19825: loss = 0.14969
Step 19830: loss = 0.15195
Step 19835: loss = 0.17271
Step 19840: loss = 0.27856
Step 19845: loss = 0.17785
Step 19850: loss = 0.17659
Step 19855: loss = 0.22977
Step 19860: loss = 0.17585
Step 19865: loss = 0.20704
Step 19870: loss = 0.20097
Step 19875: loss = 0.12418
Step 19880: loss = 0.15166
Step 19885: loss = 0.27318
Step 19890: loss = 0.12461
Training Data Eval:
  Num examples: 49920, Num correct: 47414, Precision @ 1: 0.9498
('Testing Data Eval: EPOCH->', 52)
  Num examples: 9984, Num correct: 7114, Precision @ 1: 0.7125
Step 19895: loss = 0.15230
Step 19900: loss = 0.14025
Step 19905: loss = 0.10132
Step 19910: loss = 0.13872
Step 19915: loss = 0.18603
Step 19920: loss = 0.17360
Step 19925: loss = 0.10515
Step 19930: loss = 0.18486
Step 19935: loss = 0.14777
Step 19940: loss = 0.12277
Step 19945: loss = 0.14748
Step 19950: loss = 0.17010
Step 19955: loss = 0.12505
Step 19960: loss = 0.13396
Step 19965: loss = 0.18668
Step 19970: loss = 0.07940
Step 19975: loss = 0.24846
Step 19980: loss = 0.22126
Step 19985: loss = 0.14232
Step 19990: loss = 0.19195
Step 19995: loss = 0.18728
Step 20000: loss = 0.08600
Step 20005: loss = 0.17382
Step 20010: loss = 0.19356
Step 20015: loss = 0.19402
Step 20020: loss = 0.16824
Step 20025: loss = 0.15711
Step 20030: loss = 0.15540
Step 20035: loss = 0.19029
Step 20040: loss = 0.25571
Step 20045: loss = 0.20741
Step 20050: loss = 0.20685
Step 20055: loss = 0.13939
Step 20060: loss = 0.26647
Step 20065: loss = 0.24223
Step 20070: loss = 0.14953
Step 20075: loss = 0.15936
Step 20080: loss = 0.14328
Step 20085: loss = 0.11399
Step 20090: loss = 0.13656
Step 20095: loss = 0.20414
Step 20100: loss = 0.18546
Step 20105: loss = 0.15878
Step 20110: loss = 0.10769
Step 20115: loss = 0.11050
Step 20120: loss = 0.14318
Step 20125: loss = 0.19002
Step 20130: loss = 0.14202
Step 20135: loss = 0.11931
Step 20140: loss = 0.19038
Step 20145: loss = 0.20747
Step 20150: loss = 0.19536
Step 20155: loss = 0.24225
Step 20160: loss = 0.17539
Step 20165: loss = 0.19981
Step 20170: loss = 0.22618
Step 20175: loss = 0.13140
Step 20180: loss = 0.14367
Step 20185: loss = 0.14594
Step 20190: loss = 0.22922
Step 20195: loss = 0.27575
Step 20200: loss = 0.30648
Step 20205: loss = 0.13147
Step 20210: loss = 0.28439
Step 20215: loss = 0.14752
Step 20220: loss = 0.23354
Step 20225: loss = 0.25782
Step 20230: loss = 0.34728
Step 20235: loss = 0.09116
Step 20240: loss = 0.16473
Step 20245: loss = 0.25785
Step 20250: loss = 0.15886
Step 20255: loss = 0.22420
Step 20260: loss = 0.11149
Step 20265: loss = 0.16566
Step 20270: loss = 0.16891
Step 20275: loss = 0.12768
Step 20280: loss = 0.18363
Training Data Eval:
  Num examples: 49920, Num correct: 47321, Precision @ 1: 0.9479
('Testing Data Eval: EPOCH->', 53)
  Num examples: 9984, Num correct: 7271, Precision @ 1: 0.7283
Step 20285: loss = 0.09302
Step 20290: loss = 0.13388
Step 20295: loss = 0.17873
Step 20300: loss = 0.15589
Step 20305: loss = 0.22456
Step 20310: loss = 0.21014
Step 20315: loss = 0.13582
Step 20320: loss = 0.14782
Step 20325: loss = 0.09050
Step 20330: loss = 0.16056
Step 20335: loss = 0.12970
Step 20340: loss = 0.12361
Step 20345: loss = 0.31077
Step 20350: loss = 0.24019
Step 20355: loss = 0.26188
Step 20360: loss = 0.19425
Step 20365: loss = 0.20377
Step 20370: loss = 0.24114
Step 20375: loss = 0.11055
Step 20380: loss = 0.19306
Step 20385: loss = 0.12832
Step 20390: loss = 0.21925
Step 20395: loss = 0.20500
Step 20400: loss = 0.11281
Step 20405: loss = 0.15503
Step 20410: loss = 0.20824
Step 20415: loss = 0.10110
Step 20420: loss = 0.21364
Step 20425: loss = 0.16938
Step 20430: loss = 0.20431
Step 20435: loss = 0.17541
Step 20440: loss = 0.21783
Step 20445: loss = 0.12721
Step 20450: loss = 0.16392
Step 20455: loss = 0.21388
Step 20460: loss = 0.16265
Step 20465: loss = 0.11833
Step 20470: loss = 0.17394
Step 20475: loss = 0.14977
Step 20480: loss = 0.13655
Step 20485: loss = 0.08262
Step 20490: loss = 0.18259
Step 20495: loss = 0.21165
Step 20500: loss = 0.13018
Step 20505: loss = 0.12688
Step 20510: loss = 0.19526
Step 20515: loss = 0.09769
Step 20520: loss = 0.20058
Step 20525: loss = 0.24144
Step 20530: loss = 0.15028
Step 20535: loss = 0.20433
Step 20540: loss = 0.14850
Step 20545: loss = 0.18381
Step 20550: loss = 0.14249
Step 20555: loss = 0.19843
Step 20560: loss = 0.26432
Step 20565: loss = 0.20899
Step 20570: loss = 0.12181
Step 20575: loss = 0.17680
Step 20580: loss = 0.31807
Step 20585: loss = 0.26356
Step 20590: loss = 0.16484
Step 20595: loss = 0.10682
Step 20600: loss = 0.15385
Step 20605: loss = 0.14155
Step 20610: loss = 0.14179
Step 20615: loss = 0.10169
Step 20620: loss = 0.09969
Step 20625: loss = 0.22917
Step 20630: loss = 0.13228
Step 20635: loss = 0.18890
Step 20640: loss = 0.11930
Step 20645: loss = 0.16461
Step 20650: loss = 0.23366
Step 20655: loss = 0.26189
Step 20660: loss = 0.23663
Step 20665: loss = 0.16939
Step 20670: loss = 0.17231
Training Data Eval:
  Num examples: 49920, Num correct: 47331, Precision @ 1: 0.9481
('Testing Data Eval: EPOCH->', 54)
  Num examples: 9984, Num correct: 7277, Precision @ 1: 0.7289
Step 20675: loss = 0.14422
Step 20680: loss = 0.16595
Step 20685: loss = 0.08651
Step 20690: loss = 0.15399
Step 20695: loss = 0.07996
Step 20700: loss = 0.07709
Step 20705: loss = 0.23601
Step 20710: loss = 0.09290
Step 20715: loss = 0.13379
Step 20720: loss = 0.25510
Step 20725: loss = 0.14978
Step 20730: loss = 0.14327
Step 20735: loss = 0.12521
Step 20740: loss = 0.09484
Step 20745: loss = 0.10498
Step 20750: loss = 0.09141
Step 20755: loss = 0.22743
Step 20760: loss = 0.17535
Step 20765: loss = 0.10761
Step 20770: loss = 0.08219
Step 20775: loss = 0.17828
Step 20780: loss = 0.07058
Step 20785: loss = 0.19545
Step 20790: loss = 0.13312
Step 20795: loss = 0.14300
Step 20800: loss = 0.18209
Step 20805: loss = 0.15864
Step 20810: loss = 0.16680
Step 20815: loss = 0.17902
Step 20820: loss = 0.08048
Step 20825: loss = 0.22905
Step 20830: loss = 0.16033
Step 20835: loss = 0.11327
Step 20840: loss = 0.16347
Step 20845: loss = 0.14312
Step 20850: loss = 0.09117
Step 20855: loss = 0.17448
Step 20860: loss = 0.17391
Step 20865: loss = 0.46577
Step 20870: loss = 0.21300
Step 20875: loss = 0.26593
Step 20880: loss = 0.17569
Step 20885: loss = 0.08376
Step 20890: loss = 0.16479
Step 20895: loss = 0.20724
Step 20900: loss = 0.15313
Step 20905: loss = 0.13339
Step 20910: loss = 0.14954
Step 20915: loss = 0.10283
Step 20920: loss = 0.14031
Step 20925: loss = 0.13948
Step 20930: loss = 0.06808
Step 20935: loss = 0.15420
Step 20940: loss = 0.08444
Step 20945: loss = 0.12964
Step 20950: loss = 0.16780
Step 20955: loss = 0.29440
Step 20960: loss = 0.07525
Step 20965: loss = 0.14765
Step 20970: loss = 0.16678
Step 20975: loss = 0.10239
Step 20980: loss = 0.21697
Step 20985: loss = 0.23045
Step 20990: loss = 0.07186
Step 20995: loss = 0.14702
Step 21000: loss = 0.29999
Step 21005: loss = 0.14385
Step 21010: loss = 0.22313
Step 21015: loss = 0.18313
Step 21020: loss = 0.19708
Step 21025: loss = 0.10292
Step 21030: loss = 0.16355
Step 21035: loss = 0.13651
Step 21040: loss = 0.07862
Step 21045: loss = 0.13937
Step 21050: loss = 0.13029
Step 21055: loss = 0.16551
Step 21060: loss = 0.26187
Training Data Eval:
  Num examples: 49920, Num correct: 47202, Precision @ 1: 0.9456
('Testing Data Eval: EPOCH->', 55)
  Num examples: 9984, Num correct: 7201, Precision @ 1: 0.7213
Step 21065: loss = 0.10938
Step 21070: loss = 0.10699
Step 21075: loss = 0.25938
Step 21080: loss = 0.18697
Step 21085: loss = 0.10694
Step 21090: loss = 0.22023
Step 21095: loss = 0.10790
Step 21100: loss = 0.13806
Step 21105: loss = 0.13341
Step 21110: loss = 0.15135
Step 21115: loss = 0.13444
Step 21120: loss = 0.18877
Step 21125: loss = 0.08890
Step 21130: loss = 0.09277
Step 21135: loss = 0.10110
Step 21140: loss = 0.10048
Step 21145: loss = 0.19872
Step 21150: loss = 0.18254
Step 21155: loss = 0.12726
Step 21160: loss = 0.10881
Step 21165: loss = 0.10219
Step 21170: loss = 0.18159
Step 21175: loss = 0.20951
Step 21180: loss = 0.13484
Step 21185: loss = 0.10549
Step 21190: loss = 0.12394
Step 21195: loss = 0.07346
Step 21200: loss = 0.19753
Step 21205: loss = 0.12928
Step 21210: loss = 0.16483
Step 21215: loss = 0.31059
Step 21220: loss = 0.11963
Step 21225: loss = 0.05405
Step 21230: loss = 0.11828
Step 21235: loss = 0.09637
Step 21240: loss = 0.10049
Step 21245: loss = 0.33633
Step 21250: loss = 0.12779
Step 21255: loss = 0.12310
Step 21260: loss = 0.13095
Step 21265: loss = 0.16292
Step 21270: loss = 0.11806
Step 21275: loss = 0.16647
Step 21280: loss = 0.15677
Step 21285: loss = 0.07113
Step 21290: loss = 0.17680
Step 21295: loss = 0.22762
Step 21300: loss = 0.18081
Step 21305: loss = 0.25351
Step 21310: loss = 0.15739
Step 21315: loss = 0.20565
Step 21320: loss = 0.23966
Step 21325: loss = 0.30227
Step 21330: loss = 0.18592
Step 21335: loss = 0.11742
Step 21340: loss = 0.14880
Step 21345: loss = 0.08739
Step 21350: loss = 0.15952
Step 21355: loss = 0.11766
Step 21360: loss = 0.19593
Step 21365: loss = 0.16856
Step 21370: loss = 0.12532
Step 21375: loss = 0.16174
Step 21380: loss = 0.12315
Step 21385: loss = 0.08260
Step 21390: loss = 0.12985
Step 21395: loss = 0.12200
Step 21400: loss = 0.23380
Step 21405: loss = 0.16301
Step 21410: loss = 0.20921
Step 21415: loss = 0.11077
Step 21420: loss = 0.14064
Step 21425: loss = 0.09511
Step 21430: loss = 0.15661
Step 21435: loss = 0.16110
Step 21440: loss = 0.16458
Step 21445: loss = 0.21329
Step 21450: loss = 0.13073
Training Data Eval:
  Num examples: 49920, Num correct: 47564, Precision @ 1: 0.9528
('Testing Data Eval: EPOCH->', 56)
  Num examples: 9984, Num correct: 7227, Precision @ 1: 0.7239
Step 21455: loss = 0.09572
Step 21460: loss = 0.12720
Step 21465: loss = 0.10053
Step 21470: loss = 0.08871
Step 21475: loss = 0.13019
Step 21480: loss = 0.12478
Step 21485: loss = 0.20395
Step 21490: loss = 0.10827
Step 21495: loss = 0.12311
Step 21500: loss = 0.14600
Step 21505: loss = 0.20737
Step 21510: loss = 0.06505
Step 21515: loss = 0.11255
Step 21520: loss = 0.05428
Step 21525: loss = 0.08404
Step 21530: loss = 0.14762
Step 21535: loss = 0.19611
Step 21540: loss = 0.13599
Step 21545: loss = 0.10307
Step 21550: loss = 0.11386
Step 21555: loss = 0.13871
Step 21560: loss = 0.13325
Step 21565: loss = 0.14736
Step 21570: loss = 0.21579
Step 21575: loss = 0.10025
Step 21580: loss = 0.12725
Step 21585: loss = 0.10316
Step 21590: loss = 0.09237
Step 21595: loss = 0.10687
Step 21600: loss = 0.21868
Step 21605: loss = 0.17647
Step 21610: loss = 0.17858
Step 21615: loss = 0.18423
Step 21620: loss = 0.16193
Step 21625: loss = 0.13881
Step 21630: loss = 0.18909
Step 21635: loss = 0.18571
Step 21640: loss = 0.21197
Step 21645: loss = 0.19237
Step 21650: loss = 0.13910
Step 21655: loss = 0.15946
Step 21660: loss = 0.12318
Step 21665: loss = 0.21179
Step 21670: loss = 0.19583
Step 21675: loss = 0.06854
Step 21680: loss = 0.16123
Step 21685: loss = 0.12418
Step 21690: loss = 0.04777
Step 21695: loss = 0.15581
Step 21700: loss = 0.08930
Step 21705: loss = 0.05197
Step 21710: loss = 0.19942
Step 21715: loss = 0.18199
Step 21720: loss = 0.12162
Step 21725: loss = 0.10707
Step 21730: loss = 0.11169
Step 21735: loss = 0.19549
Step 21740: loss = 0.11796
Step 21745: loss = 0.09356
Step 21750: loss = 0.08127
Step 21755: loss = 0.10624
Step 21760: loss = 0.10878
Step 21765: loss = 0.14151
Step 21770: loss = 0.16738
Step 21775: loss = 0.08224
Step 21780: loss = 0.16125
Step 21785: loss = 0.21016
Step 21790: loss = 0.17815
Step 21795: loss = 0.20463
Step 21800: loss = 0.12456
Step 21805: loss = 0.18830
Step 21810: loss = 0.13228
Step 21815: loss = 0.12234
Step 21820: loss = 0.18255
Step 21825: loss = 0.11827
Step 21830: loss = 0.22312
Step 21835: loss = 0.13652
Step 21840: loss = 0.13339
Training Data Eval:
  Num examples: 49920, Num correct: 47842, Precision @ 1: 0.9584
('Testing Data Eval: EPOCH->', 57)
  Num examples: 9984, Num correct: 7342, Precision @ 1: 0.7354
Step 21845: loss = 0.13962
Step 21850: loss = 0.09690
Step 21855: loss = 0.08642
Step 21860: loss = 0.09871
Step 21865: loss = 0.13780
Step 21870: loss = 0.12241
Step 21875: loss = 0.05398
Step 21880: loss = 0.12185
Step 21885: loss = 0.06267
Step 21890: loss = 0.16446
Step 21895: loss = 0.08632
Step 21900: loss = 0.09711
Step 21905: loss = 0.22755
Step 21910: loss = 0.13482
Step 21915: loss = 0.05975
Step 21920: loss = 0.09188
Step 21925: loss = 0.11475
Step 21930: loss = 0.05301
Step 21935: loss = 0.26697
Step 21940: loss = 0.05872
Step 21945: loss = 0.11001
Step 21950: loss = 0.09414
Step 21955: loss = 0.10765
Step 21960: loss = 0.20980
Step 21965: loss = 0.13417
Step 21970: loss = 0.11776
Step 21975: loss = 0.11800
Step 21980: loss = 0.09560
Step 21985: loss = 0.12594
Step 21990: loss = 0.06606
Step 21995: loss = 0.18394
Step 22000: loss = 0.14634
Step 22005: loss = 0.13335
Step 22010: loss = 0.08138
Step 22015: loss = 0.09020
Step 22020: loss = 0.14703
Step 22025: loss = 0.10183
Step 22030: loss = 0.14014
Step 22035: loss = 0.18125
Step 22040: loss = 0.08933
Step 22045: loss = 0.26193
Step 22050: loss = 0.10372
Step 22055: loss = 0.05523
Step 22060: loss = 0.12515
Step 22065: loss = 0.17413
Step 22070: loss = 0.19307
Step 22075: loss = 0.16307
Step 22080: loss = 0.07914
Step 22085: loss = 0.17916
Step 22090: loss = 0.10560
Step 22095: loss = 0.14447
Step 22100: loss = 0.14174
Step 22105: loss = 0.18334
Step 22110: loss = 0.03237
Step 22115: loss = 0.17611
Step 22120: loss = 0.08051
Step 22125: loss = 0.19303
Step 22130: loss = 0.16530
Step 22135: loss = 0.21438
Step 22140: loss = 0.14990
Step 22145: loss = 0.11269
Step 22150: loss = 0.20389
Step 22155: loss = 0.13956
Step 22160: loss = 0.18885
Step 22165: loss = 0.11375
Step 22170: loss = 0.19280
Step 22175: loss = 0.14355
Step 22180: loss = 0.15864
Step 22185: loss = 0.11508
Step 22190: loss = 0.13976
Step 22195: loss = 0.10132
Step 22200: loss = 0.23604
Step 22205: loss = 0.15918
Step 22210: loss = 0.17406
Step 22215: loss = 0.11422
Step 22220: loss = 0.17052
Step 22225: loss = 0.15064
Step 22230: loss = 0.11048
Training Data Eval:
  Num examples: 49920, Num correct: 47597, Precision @ 1: 0.9535
('Testing Data Eval: EPOCH->', 58)
  Num examples: 9984, Num correct: 7339, Precision @ 1: 0.7351
Step 22235: loss = 0.05653
Step 22240: loss = 0.07630
Step 22245: loss = 0.15632
Step 22250: loss = 0.26730
Step 22255: loss = 0.03813
Step 22260: loss = 0.14802
Step 22265: loss = 0.09351
Step 22270: loss = 0.11067
Step 22275: loss = 0.21022
Step 22280: loss = 0.24126
Step 22285: loss = 0.10330
Step 22290: loss = 0.10000
Step 22295: loss = 0.23884
Step 22300: loss = 0.21517
Step 22305: loss = 0.14621
Step 22310: loss = 0.11959
Step 22315: loss = 0.18255
Step 22320: loss = 0.10596
Step 22325: loss = 0.11288
Step 22330: loss = 0.21399
Step 22335: loss = 0.09972
Step 22340: loss = 0.14174
Step 22345: loss = 0.11988
Step 22350: loss = 0.12959
Step 22355: loss = 0.17294
Step 22360: loss = 0.16288
Step 22365: loss = 0.12839
Step 22370: loss = 0.17024
Step 22375: loss = 0.14045
Step 22380: loss = 0.15562
Step 22385: loss = 0.16981
Step 22390: loss = 0.09400
Step 22395: loss = 0.24350
Step 22400: loss = 0.14948
Step 22405: loss = 0.09059
Step 22410: loss = 0.15936
Step 22415: loss = 0.09555
Step 22420: loss = 0.14151
Step 22425: loss = 0.09894
Step 22430: loss = 0.12532
Step 22435: loss = 0.13740
Step 22440: loss = 0.10433
Step 22445: loss = 0.13153
Step 22450: loss = 0.12411
Step 22455: loss = 0.28293
Step 22460: loss = 0.14729
Step 22465: loss = 0.09147
Step 22470: loss = 0.10900
Step 22475: loss = 0.12398
Step 22480: loss = 0.26857
Step 22485: loss = 0.10930
Step 22490: loss = 0.11976
Step 22495: loss = 0.12593
Step 22500: loss = 0.16068
Step 22505: loss = 0.17110
Step 22510: loss = 0.14992
Step 22515: loss = 0.09366
Step 22520: loss = 0.14049
Step 22525: loss = 0.21668
Step 22530: loss = 0.17290
Step 22535: loss = 0.09471
Step 22540: loss = 0.11408
Step 22545: loss = 0.16533
Step 22550: loss = 0.22842
Step 22555: loss = 0.10972
Step 22560: loss = 0.04409
Step 22565: loss = 0.12778
Step 22570: loss = 0.12442
Step 22575: loss = 0.16424
Step 22580: loss = 0.18862
Step 22585: loss = 0.15943
Step 22590: loss = 0.26427
Step 22595: loss = 0.18518
Step 22600: loss = 0.12717
Step 22605: loss = 0.17221
Step 22610: loss = 0.12679
Step 22615: loss = 0.12725
Step 22620: loss = 0.09884
Training Data Eval:
  Num examples: 49920, Num correct: 47815, Precision @ 1: 0.9578
('Testing Data Eval: EPOCH->', 59)
  Num examples: 9984, Num correct: 7361, Precision @ 1: 0.7373
Step 22625: loss = 0.21432
Step 22630: loss = 0.13493
Step 22635: loss = 0.13051
Step 22640: loss = 0.15892
Step 22645: loss = 0.06008
Step 22650: loss = 0.08131
Step 22655: loss = 0.05901
Step 22660: loss = 0.13287
Step 22665: loss = 0.21645
Step 22670: loss = 0.13592
Step 22675: loss = 0.23156
Step 22680: loss = 0.11668
Step 22685: loss = 0.09675
Step 22690: loss = 0.08487
Step 22695: loss = 0.17396
Step 22700: loss = 0.09793
Step 22705: loss = 0.05583
Step 22710: loss = 0.12453
Step 22715: loss = 0.15486
Step 22720: loss = 0.10655
Step 22725: loss = 0.09635
Step 22730: loss = 0.10064
Step 22735: loss = 0.10656
Step 22740: loss = 0.06658
Step 22745: loss = 0.08612
Step 22750: loss = 0.18649
Step 22755: loss = 0.20885
Step 22760: loss = 0.10210
Step 22765: loss = 0.25962
Step 22770: loss = 0.27982
Step 22775: loss = 0.08970
Step 22780: loss = 0.13207
Step 22785: loss = 0.16284
Step 22790: loss = 0.08179
Step 22795: loss = 0.12672
Step 22800: loss = 0.15303
Step 22805: loss = 0.09088
Step 22810: loss = 0.18399
Step 22815: loss = 0.13023
Step 22820: loss = 0.08231
Step 22825: loss = 0.08231
Step 22830: loss = 0.12110
Step 22835: loss = 0.20396
Step 22840: loss = 0.05667
Step 22845: loss = 0.10226
Step 22850: loss = 0.19755
Step 22855: loss = 0.06367
Step 22860: loss = 0.09613
Step 22865: loss = 0.11215
Step 22870: loss = 0.11576
Step 22875: loss = 0.09594
Step 22880: loss = 0.05329
Step 22885: loss = 0.08727
Step 22890: loss = 0.10846
Step 22895: loss = 0.14735
Step 22900: loss = 0.07236
Step 22905: loss = 0.05470
Step 22910: loss = 0.14477
Step 22915: loss = 0.04844
Step 22920: loss = 0.19784
Step 22925: loss = 0.16565
Step 22930: loss = 0.13804
Step 22935: loss = 0.07988
Step 22940: loss = 0.14343
Step 22945: loss = 0.27483
Step 22950: loss = 0.16598
Step 22955: loss = 0.25567
Step 22960: loss = 0.15138
Step 22965: loss = 0.17818
Step 22970: loss = 0.24491
Step 22975: loss = 0.13477
Step 22980: loss = 0.11562
Step 22985: loss = 0.13316
Step 22990: loss = 0.27599
Step 22995: loss = 0.10138
Step 23000: loss = 0.18835
Step 23005: loss = 0.19454
Step 23010: loss = 0.08202
Training Data Eval:
  Num examples: 49920, Num correct: 47817, Precision @ 1: 0.9579
('Testing Data Eval: EPOCH->', 60)
  Num examples: 9984, Num correct: 7304, Precision @ 1: 0.7316
Step 23015: loss = 0.14462
Step 23020: loss = 0.21867
Step 23025: loss = 0.05749
Step 23030: loss = 0.21772
Step 23035: loss = 0.15128
Step 23040: loss = 0.21022
Step 23045: loss = 0.14341
Step 23050: loss = 0.06131
Step 23055: loss = 0.06022
Step 23060: loss = 0.07829
Step 23065: loss = 0.09903
Step 23070: loss = 0.11566
Step 23075: loss = 0.12478
Step 23080: loss = 0.11087
Step 23085: loss = 0.16627
Step 23090: loss = 0.11292
Step 23095: loss = 0.08670
Step 23100: loss = 0.09698
Step 23105: loss = 0.08782
Step 23110: loss = 0.05488
Step 23115: loss = 0.04601
Step 23120: loss = 0.18721
Step 23125: loss = 0.09510
Step 23130: loss = 0.03759
Step 23135: loss = 0.25161
Step 23140: loss = 0.03571
Step 23145: loss = 0.11836
Step 23150: loss = 0.12697
Step 23155: loss = 0.13340
Step 23160: loss = 0.08973
Step 23165: loss = 0.05853
Step 23170: loss = 0.19101
Step 23175: loss = 0.12107
Step 23180: loss = 0.13640
Step 23185: loss = 0.13433
Step 23190: loss = 0.11504
Step 23195: loss = 0.20220
Step 23200: loss = 0.11392
Step 23205: loss = 0.10620
Step 23210: loss = 0.16403
Step 23215: loss = 0.11298
Step 23220: loss = 0.14659
Step 23225: loss = 0.12353
Step 23230: loss = 0.17559
Step 23235: loss = 0.17335
Step 23240: loss = 0.12753
Step 23245: loss = 0.18501
Step 23250: loss = 0.19119
Step 23255: loss = 0.14823
Step 23260: loss = 0.10884
Step 23265: loss = 0.07186
Step 23270: loss = 0.12762
Step 23275: loss = 0.10437
Step 23280: loss = 0.12675
Step 23285: loss = 0.16488
Step 23290: loss = 0.11369
Step 23295: loss = 0.09594
Step 23300: loss = 0.18545
Step 23305: loss = 0.16967
Step 23310: loss = 0.12084
Step 23315: loss = 0.07679
Step 23320: loss = 0.15192
Step 23325: loss = 0.07837
Step 23330: loss = 0.10911
Step 23335: loss = 0.06364
Step 23340: loss = 0.16113
Step 23345: loss = 0.12735
Step 23350: loss = 0.07287
Step 23355: loss = 0.08009
Step 23360: loss = 0.05276
Step 23365: loss = 0.08076
Step 23370: loss = 0.08486
Step 23375: loss = 0.12189
Step 23380: loss = 0.14259
Step 23385: loss = 0.12787
Step 23390: loss = 0.10337
Step 23395: loss = 0.06591
Step 23400: loss = 0.14121
Training Data Eval:
  Num examples: 49920, Num correct: 47875, Precision @ 1: 0.9590
('Testing Data Eval: EPOCH->', 61)
  Num examples: 9984, Num correct: 7257, Precision @ 1: 0.7269
Step 23405: loss = 0.07748
Step 23410: loss = 0.09176
Step 23415: loss = 0.05910
Step 23420: loss = 0.12029
Step 23425: loss = 0.11288
Step 23430: loss = 0.11396
Step 23435: loss = 0.07853
Step 23440: loss = 0.13824
Step 23445: loss = 0.12850
Step 23450: loss = 0.09135
Step 23455: loss = 0.10481
Step 23460: loss = 0.21925
Step 23465: loss = 0.13123
Step 23470: loss = 0.15508
Step 23475: loss = 0.19490
Step 23480: loss = 0.20229
Step 23485: loss = 0.12365
Step 23490: loss = 0.15308
Step 23495: loss = 0.09313
Step 23500: loss = 0.22149
Step 23505: loss = 0.06133
Step 23510: loss = 0.06412
Step 23515: loss = 0.14911
Step 23520: loss = 0.09707
Step 23525: loss = 0.18643
Step 23530: loss = 0.10647
Step 23535: loss = 0.12235
Step 23540: loss = 0.17261
Step 23545: loss = 0.03392
Step 23550: loss = 0.34950
Step 23555: loss = 0.15710
Step 23560: loss = 0.22427
Step 23565: loss = 0.11000
Step 23570: loss = 0.04508
Step 23575: loss = 0.09470
Step 23580: loss = 0.17290
Step 23585: loss = 0.13404
Step 23590: loss = 0.12418
Step 23595: loss = 0.05645
Step 23600: loss = 0.20461
Step 23605: loss = 0.09598
Step 23610: loss = 0.21325
Step 23615: loss = 0.13461
Step 23620: loss = 0.21501
Step 23625: loss = 0.11417
Step 23630: loss = 0.07288
Step 23635: loss = 0.18654
Step 23640: loss = 0.07625
Step 23645: loss = 0.12848
Step 23650: loss = 0.19552
Step 23655: loss = 0.18186
Step 23660: loss = 0.07624
Step 23665: loss = 0.10279
Step 23670: loss = 0.13179
Step 23675: loss = 0.06032
Step 23680: loss = 0.08239
Step 23685: loss = 0.08367
Step 23690: loss = 0.07913
Step 23695: loss = 0.05327
Step 23700: loss = 0.05209
Step 23705: loss = 0.12700
Step 23710: loss = 0.09533
Step 23715: loss = 0.08250
Step 23720: loss = 0.09427
Step 23725: loss = 0.02089
Step 23730: loss = 0.15429
Step 23735: loss = 0.12110
Step 23740: loss = 0.14920
Step 23745: loss = 0.16782
Step 23750: loss = 0.24160
Step 23755: loss = 0.16526
Step 23760: loss = 0.11117
Step 23765: loss = 0.16392
Step 23770: loss = 0.21362
Step 23775: loss = 0.08308
Step 23780: loss = 0.17160
Step 23785: loss = 0.13854
Step 23790: loss = 0.13718
Training Data Eval:
  Num examples: 49920, Num correct: 47782, Precision @ 1: 0.9572
('Testing Data Eval: EPOCH->', 62)
  Num examples: 9984, Num correct: 7173, Precision @ 1: 0.7184
Step 23795: loss = 0.17373
Step 23800: loss = 0.15956
Step 23805: loss = 0.11094
Step 23810: loss = 0.16955
Step 23815: loss = 0.11534
Step 23820: loss = 0.10118
Step 23825: loss = 0.23282
Step 23830: loss = 0.09600
Step 23835: loss = 0.05468
Step 23840: loss = 0.15998
Step 23845: loss = 0.12231
Step 23850: loss = 0.04708
Step 23855: loss = 0.12444
Step 23860: loss = 0.09185
Step 23865: loss = 0.10449
Step 23870: loss = 0.18697
Step 23875: loss = 0.08892
Step 23880: loss = 0.15239
Step 23885: loss = 0.19260
Step 23890: loss = 0.17941
Step 23895: loss = 0.12341
Step 23900: loss = 0.07944
Step 23905: loss = 0.06720
Step 23910: loss = 0.17636
Step 23915: loss = 0.13124
Step 23920: loss = 0.10761
Step 23925: loss = 0.11447
Step 23930: loss = 0.09186
Step 23935: loss = 0.10780
Step 23940: loss = 0.18392
Step 23945: loss = 0.12330
Step 23950: loss = 0.08767
Step 23955: loss = 0.14393
Step 23960: loss = 0.05238
Step 23965: loss = 0.17058
Step 23970: loss = 0.12935
Step 23975: loss = 0.11907
Step 23980: loss = 0.22039
Step 23985: loss = 0.12520
Step 23990: loss = 0.11356
Step 23995: loss = 0.07210
Step 24000: loss = 0.09585
Step 24005: loss = 0.07788
Step 24010: loss = 0.11367
Step 24015: loss = 0.09003
Step 24020: loss = 0.07878
Step 24025: loss = 0.07467
Step 24030: loss = 0.14640
Step 24035: loss = 0.10027
Step 24040: loss = 0.07589
Step 24045: loss = 0.13054
Step 24050: loss = 0.24416
Step 24055: loss = 0.06787
Step 24060: loss = 0.13023
Step 24065: loss = 0.19092
Step 24070: loss = 0.18880
Step 24075: loss = 0.24372
Step 24080: loss = 0.10985
Step 24085: loss = 0.06118
Step 24090: loss = 0.12508
Step 24095: loss = 0.10235
Step 24100: loss = 0.10445
Step 24105: loss = 0.13244
Step 24110: loss = 0.14447
Step 24115: loss = 0.11892
Step 24120: loss = 0.19722
Step 24125: loss = 0.09988
Step 24130: loss = 0.09640
Step 24135: loss = 0.21649
Step 24140: loss = 0.16572
Step 24145: loss = 0.11668
Step 24150: loss = 0.13678
Step 24155: loss = 0.08940
Step 24160: loss = 0.14975
Step 24165: loss = 0.11857
Step 24170: loss = 0.06500
Step 24175: loss = 0.07677
Step 24180: loss = 0.09326
Training Data Eval:
  Num examples: 49920, Num correct: 47665, Precision @ 1: 0.9548
('Testing Data Eval: EPOCH->', 63)
  Num examples: 9984, Num correct: 7235, Precision @ 1: 0.7247
Step 24185: loss = 0.10116
Step 24190: loss = 0.04753
Step 24195: loss = 0.06492
Step 24200: loss = 0.07715
Step 24205: loss = 0.12034
Step 24210: loss = 0.09447
Step 24215: loss = 0.13722
Step 24220: loss = 0.06769
Step 24225: loss = 0.17405
Step 24230: loss = 0.11990
Step 24235: loss = 0.11595
Step 24240: loss = 0.11053
Step 24245: loss = 0.14718
Step 24250: loss = 0.14628
Step 24255: loss = 0.20422
Step 24260: loss = 0.14407
Step 24265: loss = 0.18946
Step 24270: loss = 0.22831
Step 24275: loss = 0.14691
Step 24280: loss = 0.22386
Step 24285: loss = 0.06612
Step 24290: loss = 0.09500
Step 24295: loss = 0.20485
Step 24300: loss = 0.22381
Step 24305: loss = 0.13372
Step 24310: loss = 0.14032
Step 24315: loss = 0.11636
Step 24320: loss = 0.09329
Step 24325: loss = 0.20751
Step 24330: loss = 0.09561
Step 24335: loss = 0.11992
Step 24340: loss = 0.08823
Step 24345: loss = 0.19661
Step 24350: loss = 0.09422
Step 24355: loss = 0.21045
Step 24360: loss = 0.18199
Step 24365: loss = 0.09171
Step 24370: loss = 0.05775
Step 24375: loss = 0.20990
Step 24380: loss = 0.25583
Step 24385: loss = 0.17788
Step 24390: loss = 0.29503
Step 24395: loss = 0.10536
Step 24400: loss = 0.09089
Step 24405: loss = 0.13579
Step 24410: loss = 0.14431
Step 24415: loss = 0.17651
Step 24420: loss = 0.15775
Step 24425: loss = 0.15664
Step 24430: loss = 0.13705
Step 24435: loss = 0.21107
Step 24440: loss = 0.10708
Step 24445: loss = 0.13833
Step 24450: loss = 0.16182
Step 24455: loss = 0.08067
Step 24460: loss = 0.10047
Step 24465: loss = 0.12327
Step 24470: loss = 0.19294
Step 24475: loss = 0.15597
Step 24480: loss = 0.16463
Step 24485: loss = 0.13937
Step 24490: loss = 0.21947
Step 24495: loss = 0.11771
Step 24500: loss = 0.15821
Step 24505: loss = 0.21997
Step 24510: loss = 0.12899
Step 24515: loss = 0.16962
Step 24520: loss = 0.08386
Step 24525: loss = 0.15448
Step 24530: loss = 0.13469
Step 24535: loss = 0.20734
Step 24540: loss = 0.16101
Step 24545: loss = 0.12103
Step 24550: loss = 0.15545
Step 24555: loss = 0.08751
Step 24560: loss = 0.16487
Step 24565: loss = 0.14909
Step 24570: loss = 0.30286
Training Data Eval:
  Num examples: 49920, Num correct: 47883, Precision @ 1: 0.9592
('Testing Data Eval: EPOCH->', 64)
  Num examples: 9984, Num correct: 7270, Precision @ 1: 0.7282
Step 24575: loss = 0.05352
Step 24580: loss = 0.16962
Step 24585: loss = 0.09378
Step 24590: loss = 0.10040
Step 24595: loss = 0.12276
Step 24600: loss = 0.17774
Step 24605: loss = 0.18825
Step 24610: loss = 0.10715
Step 24615: loss = 0.04489
Step 24620: loss = 0.08018
Step 24625: loss = 0.13335
Step 24630: loss = 0.15327
Step 24635: loss = 0.10799
Step 24640: loss = 0.19129
Step 24645: loss = 0.13791
Step 24650: loss = 0.09699
Step 24655: loss = 0.10359
Step 24660: loss = 0.13897
Step 24665: loss = 0.06766
Step 24670: loss = 0.10803
Step 24675: loss = 0.09070
Step 24680: loss = 0.08345
Step 24685: loss = 0.14004
Step 24690: loss = 0.17440
Step 24695: loss = 0.14662
Step 24700: loss = 0.09837
Step 24705: loss = 0.10459
Step 24710: loss = 0.10431
Step 24715: loss = 0.06767
Step 24720: loss = 0.18031
Step 24725: loss = 0.16480
Step 24730: loss = 0.14111
Step 24735: loss = 0.13377
Step 24740: loss = 0.17192
Step 24745: loss = 0.17108
Step 24750: loss = 0.15191
Step 24755: loss = 0.20177
Step 24760: loss = 0.13485
Step 24765: loss = 0.06979
Step 24770: loss = 0.11541
Step 24775: loss = 0.09846
Step 24780: loss = 0.09305
Step 24785: loss = 0.08462
Step 24790: loss = 0.20630
Step 24795: loss = 0.24004
Step 24800: loss = 0.11299
Step 24805: loss = 0.16013
Step 24810: loss = 0.07249
Step 24815: loss = 0.15783
Step 24820: loss = 0.17857
Step 24825: loss = 0.27150
Step 24830: loss = 0.11454
Step 24835: loss = 0.14959
Step 24840: loss = 0.12503
Step 24845: loss = 0.07650
Step 24850: loss = 0.10291
Step 24855: loss = 0.13514
Step 24860: loss = 0.17826
Step 24865: loss = 0.13963
Step 24870: loss = 0.14989
Step 24875: loss = 0.05958
Step 24880: loss = 0.12459
Step 24885: loss = 0.15837
Step 24890: loss = 0.16982
Step 24895: loss = 0.19231
Step 24900: loss = 0.16418
Step 24905: loss = 0.13062
Step 24910: loss = 0.11987
Step 24915: loss = 0.11111
Step 24920: loss = 0.14471
Step 24925: loss = 0.05631
Step 24930: loss = 0.09014
Step 24935: loss = 0.12483
Step 24940: loss = 0.12452
Step 24945: loss = 0.10831
Step 24950: loss = 0.12187
Step 24955: loss = 0.19265
Step 24960: loss = 0.05848
Training Data Eval:
  Num examples: 49920, Num correct: 47971, Precision @ 1: 0.9610
('Testing Data Eval: EPOCH->', 65)
  Num examples: 9984, Num correct: 7206, Precision @ 1: 0.7218
Step 24965: loss = 0.10464
Step 24970: loss = 0.05067
Step 24975: loss = 0.10876
Step 24980: loss = 0.07860
Step 24985: loss = 0.13109
Step 24990: loss = 0.08328
Step 24995: loss = 0.08834
Step 25000: loss = 0.09764
Step 25005: loss = 0.06779
Step 25010: loss = 0.07504
Step 25015: loss = 0.05490
Step 25020: loss = 0.10433
Step 25025: loss = 0.05939
Step 25030: loss = 0.09000
Step 25035: loss = 0.13969
Step 25040: loss = 0.09757
Step 25045: loss = 0.10795
Step 25050: loss = 0.19572
Step 25055: loss = 0.14842
Step 25060: loss = 0.12936
Step 25065: loss = 0.22764
Step 25070: loss = 0.11768
Step 25075: loss = 0.10305
Step 25080: loss = 0.10079
Step 25085: loss = 0.07949
Step 25090: loss = 0.11380
Step 25095: loss = 0.17429
Step 25100: loss = 0.09191
Step 25105: loss = 0.15573
Step 25110: loss = 0.13181
Step 25115: loss = 0.07430
Step 25120: loss = 0.10400
Step 25125: loss = 0.13222
Step 25130: loss = 0.14402
Step 25135: loss = 0.15075
Step 25140: loss = 0.13391
Step 25145: loss = 0.08062
Step 25150: loss = 0.05068
Step 25155: loss = 0.10990
Step 25160: loss = 0.35143
Step 25165: loss = 0.11682
Step 25170: loss = 0.23010
Step 25175: loss = 0.11558
Step 25180: loss = 0.14715
Step 25185: loss = 0.12153
Step 25190: loss = 0.08329
Step 25195: loss = 0.08329
Step 25200: loss = 0.06911
Step 25205: loss = 0.07597
Step 25210: loss = 0.06707
Step 25215: loss = 0.07735
Step 25220: loss = 0.13201
Step 25225: loss = 0.12963
Step 25230: loss = 0.16277
Step 25235: loss = 0.10348
Step 25240: loss = 0.13964
Step 25245: loss = 0.16288
Step 25250: loss = 0.18587
Step 25255: loss = 0.09162
Step 25260: loss = 0.08322
Step 25265: loss = 0.12084
Step 25270: loss = 0.07307
Step 25275: loss = 0.13139
Step 25280: loss = 0.11446
Step 25285: loss = 0.09520
Step 25290: loss = 0.14050
Step 25295: loss = 0.12046
Step 25300: loss = 0.12416
Step 25305: loss = 0.12090
Step 25310: loss = 0.08602
Step 25315: loss = 0.07026
Step 25320: loss = 0.08226
Step 25325: loss = 0.14768
Step 25330: loss = 0.12670
Step 25335: loss = 0.06045
Step 25340: loss = 0.08692
Step 25345: loss = 0.08660
Step 25350: loss = 0.09319
Training Data Eval:
  Num examples: 49920, Num correct: 48091, Precision @ 1: 0.9634
('Testing Data Eval: EPOCH->', 66)
  Num examples: 9984, Num correct: 7279, Precision @ 1: 0.7291
Step 25355: loss = 0.18289
Step 25360: loss = 0.11301
Step 25365: loss = 0.08256
Step 25370: loss = 0.08777
Step 25375: loss = 0.06510
Step 25380: loss = 0.05353
Step 25385: loss = 0.14399
Step 25390: loss = 0.05437
Step 25395: loss = 0.08400
Step 25400: loss = 0.06887
Step 25405: loss = 0.10373
Step 25410: loss = 0.06394
Step 25415: loss = 0.10800
Step 25420: loss = 0.08208
Step 25425: loss = 0.11987
Step 25430: loss = 0.05680
Step 25435: loss = 0.24786
Step 25440: loss = 0.06852
Step 25445: loss = 0.10514
Step 25450: loss = 0.13777
Step 25455: loss = 0.13551
Step 25460: loss = 0.24101
Step 25465: loss = 0.08453
Step 25470: loss = 0.07523
Step 25475: loss = 0.10921
Step 25480: loss = 0.11706
Step 25485: loss = 0.15370
Step 25490: loss = 0.13248
Step 25495: loss = 0.14497
Step 25500: loss = 0.12085
Step 25505: loss = 0.12765
Step 25510: loss = 0.12234
Step 25515: loss = 0.16938
Step 25520: loss = 0.10747
Step 25525: loss = 0.04718
Step 25530: loss = 0.12705
Step 25535: loss = 0.11192
Step 25540: loss = 0.09926
Step 25545: loss = 0.10298
Step 25550: loss = 0.13819
Step 25555: loss = 0.11756
Step 25560: loss = 0.13062
Step 25565: loss = 0.11766
Step 25570: loss = 0.11864
Step 25575: loss = 0.13915
Step 25580: loss = 0.14272
Step 25585: loss = 0.12441
Step 25590: loss = 0.17233
Step 25595: loss = 0.14725
Step 25600: loss = 0.08876
Step 25605: loss = 0.19182
Step 25610: loss = 0.04787
Step 25615: loss = 0.06420
Step 25620: loss = 0.13525
Step 25625: loss = 0.08778
Step 25630: loss = 0.11936
Step 25635: loss = 0.11599
Step 25640: loss = 0.11345
Step 25645: loss = 0.08777
Step 25650: loss = 0.03776
Step 25655: loss = 0.11717
Step 25660: loss = 0.14697
Step 25665: loss = 0.11563
Step 25670: loss = 0.20444
Step 25675: loss = 0.13176
Step 25680: loss = 0.04564
Step 25685: loss = 0.09736
Step 25690: loss = 0.14252
Step 25695: loss = 0.10314
Step 25700: loss = 0.13665
Step 25705: loss = 0.15972
Step 25710: loss = 0.14123
Step 25715: loss = 0.19500
Step 25720: loss = 0.10808
Step 25725: loss = 0.13241
Step 25730: loss = 0.21535
Step 25735: loss = 0.12477
Step 25740: loss = 0.08707
Training Data Eval:
  Num examples: 49920, Num correct: 48197, Precision @ 1: 0.9655
('Testing Data Eval: EPOCH->', 67)
  Num examples: 9984, Num correct: 7279, Precision @ 1: 0.7291
Step 25745: loss = 0.08203
Step 25750: loss = 0.09088
Step 25755: loss = 0.12162
Step 25760: loss = 0.11267
Step 25765: loss = 0.12945
Step 25770: loss = 0.06168
Step 25775: loss = 0.04726
Step 25780: loss = 0.05307
Step 25785: loss = 0.07801
Step 25790: loss = 0.05151
Step 25795: loss = 0.16307
Step 25800: loss = 0.08796
Step 25805: loss = 0.13378
Step 25810: loss = 0.16930
Step 25815: loss = 0.04222
Step 25820: loss = 0.08549
Step 25825: loss = 0.12909
Step 25830: loss = 0.15979
Step 25835: loss = 0.18156
Step 25840: loss = 0.18681
Step 25845: loss = 0.10021
Step 25850: loss = 0.14370
Step 25855: loss = 0.05511
Step 25860: loss = 0.11767
Step 25865: loss = 0.15180
Step 25870: loss = 0.10182
Step 25875: loss = 0.19009
Step 25880: loss = 0.14608
Step 25885: loss = 0.11902
Step 25890: loss = 0.09167
Step 25895: loss = 0.09129
Step 25900: loss = 0.18610
Step 25905: loss = 0.16026
Step 25910: loss = 0.10622
Step 25915: loss = 0.11801
Step 25920: loss = 0.06447
Step 25925: loss = 0.10956
Step 25930: loss = 0.07687
Step 25935: loss = 0.06719
Step 25940: loss = 0.13886
Step 25945: loss = 0.13148
Step 25950: loss = 0.08253
Step 25955: loss = 0.10530
Step 25960: loss = 0.13933
Step 25965: loss = 0.08496
Step 25970: loss = 0.15228
Step 25975: loss = 0.22424
Step 25980: loss = 0.04340
Step 25985: loss = 0.14219
Step 25990: loss = 0.11462
Step 25995: loss = 0.15777
Step 26000: loss = 0.16398
Step 26005: loss = 0.11263
Step 26010: loss = 0.08431
Step 26015: loss = 0.11735
Step 26020: loss = 0.20335
Step 26025: loss = 0.04606
Step 26030: loss = 0.15042
Step 26035: loss = 0.06432
Step 26040: loss = 0.08832
Step 26045: loss = 0.10832
Step 26050: loss = 0.05773
Step 26055: loss = 0.12100
Step 26060: loss = 0.07747
Step 26065: loss = 0.16078
Step 26070: loss = 0.07959
Step 26075: loss = 0.12136
Step 26080: loss = 0.28668
Step 26085: loss = 0.08349
Step 26090: loss = 0.12176
Step 26095: loss = 0.07703
Step 26100: loss = 0.15044
Step 26105: loss = 0.34003
Step 26110: loss = 0.14612
Step 26115: loss = 0.14662
Step 26120: loss = 0.10935
Step 26125: loss = 0.14310
Step 26130: loss = 0.12789
Training Data Eval:
  Num examples: 49920, Num correct: 48026, Precision @ 1: 0.9621
('Testing Data Eval: EPOCH->', 68)
  Num examples: 9984, Num correct: 7366, Precision @ 1: 0.7378
Step 26135: loss = 0.11087
Step 26140: loss = 0.05622
Step 26145: loss = 0.06202
Step 26150: loss = 0.04824
Step 26155: loss = 0.07486
Step 26160: loss = 0.06562
Step 26165: loss = 0.04135
Step 26170: loss = 0.08150
Step 26175: loss = 0.07572
Step 26180: loss = 0.06883
Step 26185: loss = 0.06286
Step 26190: loss = 0.13007
Step 26195: loss = 0.09960
Step 26200: loss = 0.20659
Step 26205: loss = 0.09186
Step 26210: loss = 0.10578
Step 26215: loss = 0.22519
Step 26220: loss = 0.13663
Step 26225: loss = 0.04683
Step 26230: loss = 0.02333
Step 26235: loss = 0.10165
Step 26240: loss = 0.12229
Step 26245: loss = 0.13230
Step 26250: loss = 0.08781
Step 26255: loss = 0.08252
Step 26260: loss = 0.06959
Step 26265: loss = 0.08611
Step 26270: loss = 0.14291
Step 26275: loss = 0.06394
Step 26280: loss = 0.13700
Step 26285: loss = 0.16956
Step 26290: loss = 0.08988
Step 26295: loss = 0.17135
Step 26300: loss = 0.06551
Step 26305: loss = 0.05353
Step 26310: loss = 0.18061
Step 26315: loss = 0.12668
Step 26320: loss = 0.12645
Step 26325: loss = 0.08448
Step 26330: loss = 0.09482
Step 26335: loss = 0.18561
Step 26340: loss = 0.06810
Step 26345: loss = 0.03134
Step 26350: loss = 0.06846
Step 26355: loss = 0.09794
Step 26360: loss = 0.09561
Step 26365: loss = 0.06438
Step 26370: loss = 0.14191
Step 26375: loss = 0.10550
Step 26380: loss = 0.17814
Step 26385: loss = 0.04807
Step 26390: loss = 0.13110
Step 26395: loss = 0.05237
Step 26400: loss = 0.07081
Step 26405: loss = 0.04470
Step 26410: loss = 0.12188
Step 26415: loss = 0.10298
Step 26420: loss = 0.11896
Step 26425: loss = 0.10076
Step 26430: loss = 0.14188
Step 26435: loss = 0.17766
Step 26440: loss = 0.06304
Step 26445: loss = 0.12035
Step 26450: loss = 0.07516
Step 26455: loss = 0.09815
Step 26460: loss = 0.08185
Step 26465: loss = 0.11732
Step 26470: loss = 0.07589
Step 26475: loss = 0.09139
Step 26480: loss = 0.13729
Step 26485: loss = 0.14728
Step 26490: loss = 0.25187
Step 26495: loss = 0.10589
Step 26500: loss = 0.12740
Step 26505: loss = 0.15343
Step 26510: loss = 0.16822
Step 26515: loss = 0.16595
Step 26520: loss = 0.12190
Training Data Eval:
  Num examples: 49920, Num correct: 48141, Precision @ 1: 0.9644
('Testing Data Eval: EPOCH->', 69)
  Num examples: 9984, Num correct: 7312, Precision @ 1: 0.7324
Step 26525: loss = 0.10672
Step 26530: loss = 0.05428
Step 26535: loss = 0.05660
Step 26540: loss = 0.17894
Step 26545: loss = 0.22239
Step 26550: loss = 0.08172
Step 26555: loss = 0.09589
Step 26560: loss = 0.10451
Step 26565: loss = 0.06829
Step 26570: loss = 0.14353
Step 26575: loss = 0.13512
Step 26580: loss = 0.04322
Step 26585: loss = 0.17995
Step 26590: loss = 0.11877
Step 26595: loss = 0.08279
Step 26600: loss = 0.13163
Step 26605: loss = 0.11371
Step 26610: loss = 0.19021
Step 26615: loss = 0.07489
Step 26620: loss = 0.12783
Step 26625: loss = 0.21055
Step 26630: loss = 0.09878
Step 26635: loss = 0.06338
Step 26640: loss = 0.07638
Step 26645: loss = 0.14309
Step 26650: loss = 0.09779
Step 26655: loss = 0.12742
Step 26660: loss = 0.14392
Step 26665: loss = 0.15635
Step 26670: loss = 0.07431
Step 26675: loss = 0.15888
Step 26680: loss = 0.04984
Step 26685: loss = 0.14808
Step 26690: loss = 0.12022
Step 26695: loss = 0.19761
Step 26700: loss = 0.05878
Step 26705: loss = 0.11453
Step 26710: loss = 0.13230
Step 26715: loss = 0.17762
Step 26720: loss = 0.05150
Step 26725: loss = 0.12187
Step 26730: loss = 0.14859
Step 26735: loss = 0.08156
Step 26740: loss = 0.05718
Step 26745: loss = 0.10425
Step 26750: loss = 0.04709
Step 26755: loss = 0.05772
Step 26760: loss = 0.12217
Step 26765: loss = 0.15661
Step 26770: loss = 0.10117
Step 26775: loss = 0.10230
Step 26780: loss = 0.10435
Step 26785: loss = 0.06149
Step 26790: loss = 0.14389
Step 26795: loss = 0.17326
Step 26800: loss = 0.14707
Step 26805: loss = 0.06867
Step 26810: loss = 0.06736
Step 26815: loss = 0.12559
Step 26820: loss = 0.06614
Step 26825: loss = 0.08749
Step 26830: loss = 0.21080
Step 26835: loss = 0.06534
Step 26840: loss = 0.13202
Step 26845: loss = 0.17643
Step 26850: loss = 0.08720
Step 26855: loss = 0.14472
Step 26860: loss = 0.21268
Step 26865: loss = 0.08550
Step 26870: loss = 0.14173
Step 26875: loss = 0.08667
Step 26880: loss = 0.12678
Step 26885: loss = 0.14690
Step 26890: loss = 0.16823
Step 26895: loss = 0.06680
Step 26900: loss = 0.13135
Step 26905: loss = 0.06963
Step 26910: loss = 0.12906
Training Data Eval:
  Num examples: 49920, Num correct: 48179, Precision @ 1: 0.9651
('Testing Data Eval: EPOCH->', 70)
  Num examples: 9984, Num correct: 7275, Precision @ 1: 0.7287
Step 26915: loss = 0.07627
Step 26920: loss = 0.12907
Step 26925: loss = 0.04510
Step 26930: loss = 0.07991
Step 26935: loss = 0.22382
Step 26940: loss = 0.16050
Step 26945: loss = 0.13420
Step 26950: loss = 0.12724
Step 26955: loss = 0.10450
Step 26960: loss = 0.08941
Step 26965: loss = 0.03388
Step 26970: loss = 0.07051
Step 26975: loss = 0.04528
Step 26980: loss = 0.06396
Step 26985: loss = 0.13755
Step 26990: loss = 0.14888
Step 26995: loss = 0.05668
Step 27000: loss = 0.07437
Step 27005: loss = 0.04907
Step 27010: loss = 0.04490
Step 27015: loss = 0.13407
Step 27020: loss = 0.07215
Step 27025: loss = 0.01930
Step 27030: loss = 0.11675
Step 27035: loss = 0.06720
Step 27040: loss = 0.08130
Step 27045: loss = 0.14873
Step 27050: loss = 0.06852
Step 27055: loss = 0.05987
Step 27060: loss = 0.08028
Step 27065: loss = 0.07104
Step 27070: loss = 0.07845
Step 27075: loss = 0.07146
Step 27080: loss = 0.27453
Step 27085: loss = 0.07666
Step 27090: loss = 0.08485
Step 27095: loss = 0.14449
Step 27100: loss = 0.03615
Step 27105: loss = 0.11940
Step 27110: loss = 0.07147
Step 27115: loss = 0.10554
Step 27120: loss = 0.17574
Step 27125: loss = 0.12655
Step 27130: loss = 0.15869
Step 27135: loss = 0.11254
Step 27140: loss = 0.06861
Step 27145: loss = 0.03996
Step 27150: loss = 0.13633
Step 27155: loss = 0.16229
Step 27160: loss = 0.25957
Step 27165: loss = 0.13044
Step 27170: loss = 0.12744
Step 27175: loss = 0.15499
Step 27180: loss = 0.19999
Step 27185: loss = 0.29351
Step 27190: loss = 0.07943
Step 27195: loss = 0.12240
Step 27200: loss = 0.14466
Step 27205: loss = 0.13032
Step 27210: loss = 0.12986
Step 27215: loss = 0.03885
Step 27220: loss = 0.08135
Step 27225: loss = 0.04964
Step 27230: loss = 0.08645
Step 27235: loss = 0.12449
Step 27240: loss = 0.12030
Step 27245: loss = 0.14507
Step 27250: loss = 0.06103
Step 27255: loss = 0.05087
Step 27260: loss = 0.15792
Step 27265: loss = 0.15247
Step 27270: loss = 0.12978
Step 27275: loss = 0.05147
Step 27280: loss = 0.12627
Step 27285: loss = 0.08272
Step 27290: loss = 0.13139
Step 27295: loss = 0.16970
Step 27300: loss = 0.06081
Training Data Eval:
  Num examples: 49920, Num correct: 48342, Precision @ 1: 0.9684
('Testing Data Eval: EPOCH->', 71)
  Num examples: 9984, Num correct: 7333, Precision @ 1: 0.7345
Step 27305: loss = 0.09498
Step 27310: loss = 0.06039
Step 27315: loss = 0.06538
Step 27320: loss = 0.11781
Step 27325: loss = 0.10181
Step 27330: loss = 0.11558
Step 27335: loss = 0.12817
Step 27340: loss = 0.12345
Step 27345: loss = 0.03016
Step 27350: loss = 0.08576
Step 27355: loss = 0.09439
Step 27360: loss = 0.08982
Step 27365: loss = 0.04103
Step 27370: loss = 0.09619
Step 27375: loss = 0.12347
Step 27380: loss = 0.07621
Step 27385: loss = 0.06722
Step 27390: loss = 0.10143
Step 27395: loss = 0.08249
Step 27400: loss = 0.08628
Step 27405: loss = 0.05261
Step 27410: loss = 0.18107
Step 27415: loss = 0.03812
Step 27420: loss = 0.08517
Step 27425: loss = 0.05625
Step 27430: loss = 0.09749
Step 27435: loss = 0.07252
Step 27440: loss = 0.12301
Step 27445: loss = 0.11781
Step 27450: loss = 0.12887
Step 27455: loss = 0.06901
Step 27460: loss = 0.14942
Step 27465: loss = 0.12270
Step 27470: loss = 0.05252
Step 27475: loss = 0.14158
Step 27480: loss = 0.05987
Step 27485: loss = 0.12831
Step 27490: loss = 0.07333
Step 27495: loss = 0.09331
Step 27500: loss = 0.09115
Step 27505: loss = 0.13199
Step 27510: loss = 0.09913
Step 27515: loss = 0.07074
Step 27520: loss = 0.07831
Step 27525: loss = 0.13592
Step 27530: loss = 0.08293
Step 27535: loss = 0.10691
Step 27540: loss = 0.10613
Step 27545: loss = 0.12019
Step 27550: loss = 0.13576
Step 27555: loss = 0.08827
Step 27560: loss = 0.12919
Step 27565: loss = 0.09740
Step 27570: loss = 0.10668
Step 27575: loss = 0.09401
Step 27580: loss = 0.17440
Step 27585: loss = 0.09423
Step 27590: loss = 0.09169
Step 27595: loss = 0.09747
Step 27600: loss = 0.12211
Step 27605: loss = 0.04105
Step 27610: loss = 0.13709
Step 27615: loss = 0.09753
Step 27620: loss = 0.09298
Step 27625: loss = 0.03932
Step 27630: loss = 0.09965
Step 27635: loss = 0.18007
Step 27640: loss = 0.15428
Step 27645: loss = 0.03239
Step 27650: loss = 0.14398
Step 27655: loss = 0.18930
Step 27660: loss = 0.08807
Step 27665: loss = 0.11092
Step 27670: loss = 0.08990
Step 27675: loss = 0.22314
Step 27680: loss = 0.16953
Step 27685: loss = 0.06852
Step 27690: loss = 0.18259
Training Data Eval:
  Num examples: 49920, Num correct: 48211, Precision @ 1: 0.9658
('Testing Data Eval: EPOCH->', 72)
  Num examples: 9984, Num correct: 7334, Precision @ 1: 0.7346
Step 27695: loss = 0.17753
Step 27700: loss = 0.12005
Step 27705: loss = 0.15505
Step 27710: loss = 0.11495
Step 27715: loss = 0.08786
Step 27720: loss = 0.10095
Step 27725: loss = 0.06401
Step 27730: loss = 0.05090
Step 27735: loss = 0.12592
Step 27740: loss = 0.13749
Step 27745: loss = 0.22255
Step 27750: loss = 0.13807
Step 27755: loss = 0.15090
Step 27760: loss = 0.08815
Step 27765: loss = 0.03936
Step 27770: loss = 0.18457
Step 27775: loss = 0.10595
Step 27780: loss = 0.05132
Step 27785: loss = 0.15691
Step 27790: loss = 0.13952
Step 27795: loss = 0.06962
Step 27800: loss = 0.13326
Step 27805: loss = 0.09088
Step 27810: loss = 0.04035
Step 27815: loss = 0.07646
Step 27820: loss = 0.04650
Step 27825: loss = 0.23682
Step 27830: loss = 0.09007
Step 27835: loss = 0.03277
Step 27840: loss = 0.08748
Step 27845: loss = 0.16748
Step 27850: loss = 0.06659
Step 27855: loss = 0.06363
Step 27860: loss = 0.04473
Step 27865: loss = 0.07831
Step 27870: loss = 0.17625
Step 27875: loss = 0.09696
Step 27880: loss = 0.07929
Step 27885: loss = 0.08145
Step 27890: loss = 0.04402
Step 27895: loss = 0.08549
Step 27900: loss = 0.07180
Step 27905: loss = 0.14211
Step 27910: loss = 0.09689
Step 27915: loss = 0.11028
Step 27920: loss = 0.19657
Step 27925: loss = 0.04903
Step 27930: loss = 0.06940
Step 27935: loss = 0.13174
Step 27940: loss = 0.05785
Step 27945: loss = 0.09211
Step 27950: loss = 0.11283
Step 27955: loss = 0.03529
Step 27960: loss = 0.11411
Step 27965: loss = 0.16013
Step 27970: loss = 0.08226
Step 27975: loss = 0.09138
Step 27980: loss = 0.06775
Step 27985: loss = 0.09376
Step 27990: loss = 0.11002
Step 27995: loss = 0.10246
Step 28000: loss = 0.07576
Step 28005: loss = 0.11783
Step 28010: loss = 0.06834
Step 28015: loss = 0.09766
Step 28020: loss = 0.12429
Step 28025: loss = 0.07043
Step 28030: loss = 0.16461
Step 28035: loss = 0.13990
Step 28040: loss = 0.10109
Step 28045: loss = 0.12735
Step 28050: loss = 0.13223
Step 28055: loss = 0.14304
Step 28060: loss = 0.11912
Step 28065: loss = 0.06386
Step 28070: loss = 0.11330
Step 28075: loss = 0.07217
Step 28080: loss = 0.04579
Training Data Eval:
  Num examples: 49920, Num correct: 48348, Precision @ 1: 0.9685
('Testing Data Eval: EPOCH->', 73)
  Num examples: 9984, Num correct: 7408, Precision @ 1: 0.7420
Step 28085: loss = 0.04265
Step 28090: loss = 0.06911
Step 28095: loss = 0.05970
Step 28100: loss = 0.03652
Step 28105: loss = 0.08280
Step 28110: loss = 0.13417
Step 28115: loss = 0.05227
Step 28120: loss = 0.05281
Step 28125: loss = 0.09548
Step 28130: loss = 0.07893
Step 28135: loss = 0.14680
Step 28140: loss = 0.09600
Step 28145: loss = 0.08843
Step 28150: loss = 0.10390
Step 28155: loss = 0.05877
Step 28160: loss = 0.07618
Step 28165: loss = 0.06811
Step 28170: loss = 0.12451
Step 28175: loss = 0.03781
Step 28180: loss = 0.17786
Step 28185: loss = 0.09914
Step 28190: loss = 0.12550
Step 28195: loss = 0.07383
Step 28200: loss = 0.09431
Step 28205: loss = 0.12542
Step 28210: loss = 0.05426
Step 28215: loss = 0.12904
Step 28220: loss = 0.09419
Step 28225: loss = 0.09489
Step 28230: loss = 0.06814
Step 28235: loss = 0.08895
Step 28240: loss = 0.12331
Step 28245: loss = 0.24502
Step 28250: loss = 0.08381
Step 28255: loss = 0.25324
Step 28260: loss = 0.19662
Step 28265: loss = 0.11751
Step 28270: loss = 0.09209
Step 28275: loss = 0.19091
Step 28280: loss = 0.03890
Step 28285: loss = 0.20843
Step 28290: loss = 0.07395
Step 28295: loss = 0.14949
Step 28300: loss = 0.10087
Step 28305: loss = 0.08660
Step 28310: loss = 0.16408
Step 28315: loss = 0.17768
Step 28320: loss = 0.05535
Step 28325: loss = 0.10761
Step 28330: loss = 0.05546
Step 28335: loss = 0.08722
Step 28340: loss = 0.19144
Step 28345: loss = 0.13203
Step 28350: loss = 0.14840
Step 28355: loss = 0.10125
Step 28360: loss = 0.08613
Step 28365: loss = 0.16645
Step 28370: loss = 0.05382
Step 28375: loss = 0.15319
Step 28380: loss = 0.09218
Step 28385: loss = 0.06023
Step 28390: loss = 0.07176
Step 28395: loss = 0.11479
Step 28400: loss = 0.06229
Step 28405: loss = 0.11720
Step 28410: loss = 0.06846
Step 28415: loss = 0.07661
Step 28420: loss = 0.05607
Step 28425: loss = 0.05275
Step 28430: loss = 0.07931
Step 28435: loss = 0.11352
Step 28440: loss = 0.06760
Step 28445: loss = 0.08804
Step 28450: loss = 0.08775
Step 28455: loss = 0.10904
Step 28460: loss = 0.02742
Step 28465: loss = 0.10422
Step 28470: loss = 0.10522
Training Data Eval:
  Num examples: 49920, Num correct: 48480, Precision @ 1: 0.9712
('Testing Data Eval: EPOCH->', 74)
  Num examples: 9984, Num correct: 7374, Precision @ 1: 0.7386
Step 28475: loss = 0.08218
Step 28480: loss = 0.07229
Step 28485: loss = 0.10827
Step 28490: loss = 0.05433
Step 28495: loss = 0.12104
Step 28500: loss = 0.16670
Step 28505: loss = 0.11421
Step 28510: loss = 0.08481
Step 28515: loss = 0.12725
Step 28520: loss = 0.11718
Step 28525: loss = 0.09204
Step 28530: loss = 0.04851
Step 28535: loss = 0.04281
Step 28540: loss = 0.10296
Step 28545: loss = 0.15623
Step 28550: loss = 0.07564
Step 28555: loss = 0.09958
Step 28560: loss = 0.08357
Step 28565: loss = 0.16295
Step 28570: loss = 0.11513
Step 28575: loss = 0.09161
Step 28580: loss = 0.05346
Step 28585: loss = 0.12125
Step 28590: loss = 0.08612
Step 28595: loss = 0.12714
Step 28600: loss = 0.06439
Step 28605: loss = 0.02274
Step 28610: loss = 0.15754
Step 28615: loss = 0.08224
Step 28620: loss = 0.17633
Step 28625: loss = 0.09768
Step 28630: loss = 0.02495
Step 28635: loss = 0.03821
Step 28640: loss = 0.05442
Step 28645: loss = 0.11681
Step 28650: loss = 0.19007
Step 28655: loss = 0.15152
Step 28660: loss = 0.07629
Step 28665: loss = 0.15577
Step 28670: loss = 0.10745
Step 28675: loss = 0.12020
Step 28680: loss = 0.05327
Step 28685: loss = 0.11349
Step 28690: loss = 0.05146
Step 28695: loss = 0.17867
Step 28700: loss = 0.18514
Step 28705: loss = 0.06164
Step 28710: loss = 0.10760
Step 28715: loss = 0.04856
Step 28720: loss = 0.04130
Step 28725: loss = 0.08959
Step 28730: loss = 0.06037
Step 28735: loss = 0.17696
Step 28740: loss = 0.12697
Step 28745: loss = 0.18532
Step 28750: loss = 0.09091
Step 28755: loss = 0.04574
Step 28760: loss = 0.16403
Step 28765: loss = 0.11649
Step 28770: loss = 0.11346
Step 28775: loss = 0.07968
Step 28780: loss = 0.12569
Step 28785: loss = 0.10700
Step 28790: loss = 0.10382
Step 28795: loss = 0.12065
Step 28800: loss = 0.10549
Step 28805: loss = 0.11574
Step 28810: loss = 0.03150
Step 28815: loss = 0.08526
Step 28820: loss = 0.11189
Step 28825: loss = 0.08128
Step 28830: loss = 0.04682
Step 28835: loss = 0.09470
Step 28840: loss = 0.13201
Step 28845: loss = 0.04422
Step 28850: loss = 0.06395
Step 28855: loss = 0.09455
Step 28860: loss = 0.04405
Training Data Eval:
  Num examples: 49920, Num correct: 48195, Precision @ 1: 0.9654
('Testing Data Eval: EPOCH->', 75)
  Num examples: 9984, Num correct: 7306, Precision @ 1: 0.7318
Step 28865: loss = 0.08609
Step 28870: loss = 0.07887
Step 28875: loss = 0.12472
Step 28880: loss = 0.09830
Step 28885: loss = 0.04060
Step 28890: loss = 0.10217
Step 28895: loss = 0.09305
Step 28900: loss = 0.05640
Step 28905: loss = 0.03924
Step 28910: loss = 0.13061
Step 28915: loss = 0.11773
Step 28920: loss = 0.18964
Step 28925: loss = 0.06045
Step 28930: loss = 0.07898
Step 28935: loss = 0.10601
Step 28940: loss = 0.08585
Step 28945: loss = 0.17427
Step 28950: loss = 0.09743
Step 28955: loss = 0.12299
Step 28960: loss = 0.10244
Step 28965: loss = 0.02909
Step 28970: loss = 0.03985
Step 28975: loss = 0.08363
Step 28980: loss = 0.05064
Step 28985: loss = 0.05397
Step 28990: loss = 0.19560
Step 28995: loss = 0.14876
Step 29000: loss = 0.12050
Step 29005: loss = 0.14303
Step 29010: loss = 0.09648
Step 29015: loss = 0.11763
Step 29020: loss = 0.16849
Step 29025: loss = 0.10867
Step 29030: loss = 0.13224
Step 29035: loss = 0.12292
Step 29040: loss = 0.15599
Step 29045: loss = 0.12182
Step 29050: loss = 0.11405
Step 29055: loss = 0.07951
Step 29060: loss = 0.10223
Step 29065: loss = 0.13158
Step 29070: loss = 0.15354
Step 29075: loss = 0.13928
Step 29080: loss = 0.11910
Step 29085: loss = 0.06858
Step 29090: loss = 0.08163
Step 29095: loss = 0.05205
Step 29100: loss = 0.09218
Step 29105: loss = 0.15392
Step 29110: loss = 0.05682
Step 29115: loss = 0.07398
Step 29120: loss = 0.09140
Step 29125: loss = 0.09660
Step 29130: loss = 0.19345
Step 29135: loss = 0.04381
Step 29140: loss = 0.03817
Step 29145: loss = 0.07541
Step 29150: loss = 0.05184
Step 29155: loss = 0.10882
Step 29160: loss = 0.09715
Step 29165: loss = 0.06194
Step 29170: loss = 0.10534
Step 29175: loss = 0.06692
Step 29180: loss = 0.05544
Step 29185: loss = 0.03711
Step 29190: loss = 0.10083
Step 29195: loss = 0.13232
Step 29200: loss = 0.13295
Step 29205: loss = 0.05573
Step 29210: loss = 0.11261
Step 29215: loss = 0.03193
Step 29220: loss = 0.09895
Step 29225: loss = 0.04384
Step 29230: loss = 0.07895
Step 29235: loss = 0.11246
Step 29240: loss = 0.04820
Step 29245: loss = 0.07140
Step 29250: loss = 0.06931
Training Data Eval:
  Num examples: 49920, Num correct: 48474, Precision @ 1: 0.9710
('Testing Data Eval: EPOCH->', 76)
  Num examples: 9984, Num correct: 7357, Precision @ 1: 0.7369
Step 29255: loss = 0.14805
Step 29260: loss = 0.05105
Step 29265: loss = 0.10472
Step 29270: loss = 0.08544
Step 29275: loss = 0.06469
Step 29280: loss = 0.09111
Step 29285: loss = 0.10378
Step 29290: loss = 0.17880
Step 29295: loss = 0.09423
Step 29300: loss = 0.11710
Step 29305: loss = 0.16735
Step 29310: loss = 0.11416
Step 29315: loss = 0.04984
Step 29320: loss = 0.15192
Step 29325: loss = 0.10033
Step 29330: loss = 0.14235
Step 29335: loss = 0.06190
Step 29340: loss = 0.11801
Step 29345: loss = 0.08310
Step 29350: loss = 0.14242
Step 29355: loss = 0.09411
Step 29360: loss = 0.13557
Step 29365: loss = 0.06959
Step 29370: loss = 0.07451
Step 29375: loss = 0.14482
Step 29380: loss = 0.09508
Step 29385: loss = 0.12170
Step 29390: loss = 0.11989
Step 29395: loss = 0.05483
Step 29400: loss = 0.11732
Step 29405: loss = 0.25046
Step 29410: loss = 0.15180
Step 29415: loss = 0.08576
Step 29420: loss = 0.03759
Step 29425: loss = 0.15103
Step 29430: loss = 0.09822
Step 29435: loss = 0.09638
Step 29440: loss = 0.16318
Step 29445: loss = 0.16228
Step 29450: loss = 0.10981
Step 29455: loss = 0.15037
Step 29460: loss = 0.07649
Step 29465: loss = 0.08362
Step 29470: loss = 0.18364
Step 29475: loss = 0.08623
Step 29480: loss = 0.07870
Step 29485: loss = 0.09134
Step 29490: loss = 0.12587
Step 29495: loss = 0.10693
Step 29500: loss = 0.11862
Step 29505: loss = 0.08926
Step 29510: loss = 0.25873
Step 29515: loss = 0.06222
Step 29520: loss = 0.08496
Step 29525: loss = 0.10392
Step 29530: loss = 0.13597
Step 29535: loss = 0.13172
Step 29540: loss = 0.08943
Step 29545: loss = 0.07434
Step 29550: loss = 0.22471
Step 29555: loss = 0.35303
Step 29560: loss = 0.19426
Step 29565: loss = 0.09946
Step 29570: loss = 0.13689
Step 29575: loss = 0.13169
Step 29580: loss = 0.14662
Step 29585: loss = 0.16718
Step 29590: loss = 0.12997
Step 29595: loss = 0.11031
Step 29600: loss = 0.08418
Step 29605: loss = 0.08308
Step 29610: loss = 0.13556
Step 29615: loss = 0.13626
Step 29620: loss = 0.08949
Step 29625: loss = 0.10904
Step 29630: loss = 0.06114
Step 29635: loss = 0.07536
Step 29640: loss = 0.09062
Training Data Eval:
  Num examples: 49920, Num correct: 48402, Precision @ 1: 0.9696
('Testing Data Eval: EPOCH->', 77)
  Num examples: 9984, Num correct: 7308, Precision @ 1: 0.7320
Step 29645: loss = 0.14125
Step 29650: loss = 0.09952
Step 29655: loss = 0.03025
Step 29660: loss = 0.07016
Step 29665: loss = 0.06030
Step 29670: loss = 0.19709
Step 29675: loss = 0.10658
Step 29680: loss = 0.08644
Step 29685: loss = 0.09031
Step 29690: loss = 0.07548
Step 29695: loss = 0.08577
Step 29700: loss = 0.11044
Step 29705: loss = 0.09510
Step 29710: loss = 0.09086
Step 29715: loss = 0.06208
Step 29720: loss = 0.13153
Step 29725: loss = 0.04039
Step 29730: loss = 0.08706
Step 29735: loss = 0.05301
Step 29740: loss = 0.05288
Step 29745: loss = 0.06259
Step 29750: loss = 0.14026
Step 29755: loss = 0.05609
Step 29760: loss = 0.08566
Step 29765: loss = 0.10858
Step 29770: loss = 0.11231
Step 29775: loss = 0.10287
Step 29780: loss = 0.17051
Step 29785: loss = 0.04533
Step 29790: loss = 0.09936
Step 29795: loss = 0.11517
Step 29800: loss = 0.08345
Step 29805: loss = 0.14698
Step 29810: loss = 0.06166
Step 29815: loss = 0.06370
Step 29820: loss = 0.05970
Step 29825: loss = 0.11693
Step 29830: loss = 0.06065
Step 29835: loss = 0.03564
Step 29840: loss = 0.03336
Step 29845: loss = 0.06818
Step 29850: loss = 0.02673
Step 29855: loss = 0.04037
Step 29860: loss = 0.05284
Step 29865: loss = 0.18204
Step 29870: loss = 0.06515
Step 29875: loss = 0.07838
Step 29880: loss = 0.23802
Step 29885: loss = 0.17114
Step 29890: loss = 0.11826
Step 29895: loss = 0.09806
Step 29900: loss = 0.09769
Step 29905: loss = 0.06931
Step 29910: loss = 0.09427
Step 29915: loss = 0.09949
Step 29920: loss = 0.09598
Step 29925: loss = 0.06242
Step 29930: loss = 0.12896
Step 29935: loss = 0.08756
Step 29940: loss = 0.10312
Step 29945: loss = 0.06584
Step 29950: loss = 0.07745
Step 29955: loss = 0.01300
Step 29960: loss = 0.13324
Step 29965: loss = 0.07910
Step 29970: loss = 0.13064
Step 29975: loss = 0.08075
Step 29980: loss = 0.09561
Step 29985: loss = 0.07591
Step 29990: loss = 0.13603
Step 29995: loss = 0.02163
Step 30000: loss = 0.04038
Step 30005: loss = 0.11618
Step 30010: loss = 0.09583
Step 30015: loss = 0.07749
Step 30020: loss = 0.14799
Step 30025: loss = 0.07586
Step 30030: loss = 0.14210
Training Data Eval:
  Num examples: 49920, Num correct: 48198, Precision @ 1: 0.9655
('Testing Data Eval: EPOCH->', 78)
  Num examples: 9984, Num correct: 7281, Precision @ 1: 0.7293
Step 30035: loss = 0.07841
Step 30040: loss = 0.04569
Step 30045: loss = 0.04916
Step 30050: loss = 0.03491
Step 30055: loss = 0.07524
Step 30060: loss = 0.06382
Step 30065: loss = 0.06661
Step 30070: loss = 0.12449
Step 30075: loss = 0.04105
Step 30080: loss = 0.02480
Step 30085: loss = 0.10968
Step 30090: loss = 0.14051
Step 30095: loss = 0.04231
Step 30100: loss = 0.04186
Step 30105: loss = 0.04707
Step 30110: loss = 0.09028
Step 30115: loss = 0.20380
Step 30120: loss = 0.07458
Step 30125: loss = 0.07724
Step 30130: loss = 0.09271
Step 30135: loss = 0.06936
Step 30140: loss = 0.06679
Step 30145: loss = 0.07363
Step 30150: loss = 0.14492
Step 30155: loss = 0.10132
Step 30160: loss = 0.15873
Step 30165: loss = 0.13452
Step 30170: loss = 0.10877
Step 30175: loss = 0.15464
Step 30180: loss = 0.11605
Step 30185: loss = 0.05529
Step 30190: loss = 0.09760
Step 30195: loss = 0.14141
Step 30200: loss = 0.10824
Step 30205: loss = 0.12231
Step 30210: loss = 0.06638
Step 30215: loss = 0.09352
Step 30220: loss = 0.03994
Step 30225: loss = 0.07844
Step 30230: loss = 0.07391
Step 30235: loss = 0.05004
Step 30240: loss = 0.16410
Step 30245: loss = 0.04110
Step 30250: loss = 0.11369
Step 30255: loss = 0.12026
Step 30260: loss = 0.16292
Step 30265: loss = 0.18277
Step 30270: loss = 0.08610
Step 30275: loss = 0.12039
Step 30280: loss = 0.05257
Step 30285: loss = 0.06344
Step 30290: loss = 0.08464
Step 30295: loss = 0.14644
Step 30300: loss = 0.10446
Step 30305: loss = 0.15647
Step 30310: loss = 0.08540
Step 30315: loss = 0.08333
Step 30320: loss = 0.10141
Step 30325: loss = 0.06961
Step 30330: loss = 0.04624
Step 30335: loss = 0.05318
Step 30340: loss = 0.05741
Step 30345: loss = 0.09003
Step 30350: loss = 0.09946
Step 30355: loss = 0.06490
Step 30360: loss = 0.10591
Step 30365: loss = 0.17684
Step 30370: loss = 0.07807
Step 30375: loss = 0.12934
Step 30380: loss = 0.11566
Step 30385: loss = 0.10141
Step 30390: loss = 0.14083
Step 30395: loss = 0.09861
Step 30400: loss = 0.12019
Step 30405: loss = 0.06780
Step 30410: loss = 0.15467
Step 30415: loss = 0.10703
Step 30420: loss = 0.07403
Training Data Eval:
  Num examples: 49920, Num correct: 48118, Precision @ 1: 0.9639
('Testing Data Eval: EPOCH->', 79)
  Num examples: 9984, Num correct: 7287, Precision @ 1: 0.7299
Step 30425: loss = 0.11750
Step 30430: loss = 0.08476
Step 30435: loss = 0.13346
Step 30440: loss = 0.07497
Step 30445: loss = 0.04944
Step 30450: loss = 0.07332
Step 30455: loss = 0.09186
Step 30460: loss = 0.23764
Step 30465: loss = 0.05317
Step 30470: loss = 0.04542
Step 30475: loss = 0.12606
Step 30480: loss = 0.06477
Step 30485: loss = 0.14143
Step 30490: loss = 0.14688
Step 30495: loss = 0.06875
Step 30500: loss = 0.04931
Step 30505: loss = 0.19551
Step 30510: loss = 0.06749
Step 30515: loss = 0.08513
Step 30520: loss = 0.05059
Step 30525: loss = 0.06249
Step 30530: loss = 0.09713
Step 30535: loss = 0.07090
Step 30540: loss = 0.14195
Step 30545: loss = 0.09310
Step 30550: loss = 0.07169
Step 30555: loss = 0.11805
Step 30560: loss = 0.24812
Step 30565: loss = 0.14267
Step 30570: loss = 0.16105
Step 30575: loss = 0.11048
Step 30580: loss = 0.12000
Step 30585: loss = 0.17845
Step 30590: loss = 0.16521
Step 30595: loss = 0.10968
Step 30600: loss = 0.05919
Step 30605: loss = 0.15486
Step 30610: loss = 0.07142
Step 30615: loss = 0.17093
Step 30620: loss = 0.04617
Step 30625: loss = 0.09585
Step 30630: loss = 0.06993
Step 30635: loss = 0.06485
Step 30640: loss = 0.05092
Step 30645: loss = 0.06810
Step 30650: loss = 0.05699
Step 30655: loss = 0.18991
Step 30660: loss = 0.13081
Step 30665: loss = 0.08303
Step 30670: loss = 0.13097
Step 30675: loss = 0.15610
Step 30680: loss = 0.08955
Step 30685: loss = 0.03041
Step 30690: loss = 0.08392
Step 30695: loss = 0.14101
Step 30700: loss = 0.11893
Step 30705: loss = 0.11609
Step 30710: loss = 0.07197
Step 30715: loss = 0.12424
Step 30720: loss = 0.06765
Step 30725: loss = 0.04697
Step 30730: loss = 0.13967
Step 30735: loss = 0.04553
Step 30740: loss = 0.10861
Step 30745: loss = 0.08023
Step 30750: loss = 0.26688
Step 30755: loss = 0.10540
Step 30760: loss = 0.12206
Step 30765: loss = 0.06143
Step 30770: loss = 0.04022
Step 30775: loss = 0.14363
Step 30780: loss = 0.15378
Step 30785: loss = 0.07783
Step 30790: loss = 0.06059
Step 30795: loss = 0.07287
Step 30800: loss = 0.13387
Step 30805: loss = 0.10494
Step 30810: loss = 0.08944
Training Data Eval:
  Num examples: 49920, Num correct: 48402, Precision @ 1: 0.9696
('Testing Data Eval: EPOCH->', 80)
  Num examples: 9984, Num correct: 7202, Precision @ 1: 0.7214
Step 30815: loss = 0.08392
Step 30820: loss = 0.11138
Step 30825: loss = 0.09597
Step 30830: loss = 0.02694
Step 30835: loss = 0.13186
Step 30840: loss = 0.11388
Step 30845: loss = 0.07650
Step 30850: loss = 0.05431
Step 30855: loss = 0.04912
Step 30860: loss = 0.07023
Step 30865: loss = 0.04909
Step 30870: loss = 0.10287
Step 30875: loss = 0.09391
Step 30880: loss = 0.05755
Step 30885: loss = 0.07111
Step 30890: loss = 0.12244
Step 30895: loss = 0.01873
Step 30900: loss = 0.09872
Step 30905: loss = 0.06156
Step 30910: loss = 0.03912
Step 30915: loss = 0.09082
Step 30920: loss = 0.09005
Step 30925: loss = 0.06788
Step 30930: loss = 0.01715
Step 30935: loss = 0.10335
Step 30940: loss = 0.06333
Step 30945: loss = 0.03355
Step 30950: loss = 0.03990
Step 30955: loss = 0.06888
Step 30960: loss = 0.08240
Step 30965: loss = 0.22723
Step 30970: loss = 0.06135
Step 30975: loss = 0.07076
Step 30980: loss = 0.08289
Step 30985: loss = 0.10136
Step 30990: loss = 0.22219
Step 30995: loss = 0.05730
Step 31000: loss = 0.03596
Step 31005: loss = 0.08885
Step 31010: loss = 0.09522
Step 31015: loss = 0.18099
Step 31020: loss = 0.06097
Step 31025: loss = 0.12952
Step 31030: loss = 0.18256
Step 31035: loss = 0.08869
Step 31040: loss = 0.07476
Step 31045: loss = 0.04549
Step 31050: loss = 0.14337
Step 31055: loss = 0.14737
Step 31060: loss = 0.13994
Step 31065: loss = 0.17337
Step 31070: loss = 0.09450
Step 31075: loss = 0.06259
Step 31080: loss = 0.13264
Step 31085: loss = 0.12173
Step 31090: loss = 0.13739
Step 31095: loss = 0.12167
Step 31100: loss = 0.06040
Step 31105: loss = 0.12372
Step 31110: loss = 0.14971
Step 31115: loss = 0.19502
Step 31120: loss = 0.06449
Step 31125: loss = 0.12913
Step 31130: loss = 0.10197
Step 31135: loss = 0.13784
Step 31140: loss = 0.07899
Step 31145: loss = 0.16198
Step 31150: loss = 0.19029
Step 31155: loss = 0.07718
Step 31160: loss = 0.15948
Step 31165: loss = 0.07753
Step 31170: loss = 0.18172
Step 31175: loss = 0.05198
Step 31180: loss = 0.13434
Step 31185: loss = 0.09978
Step 31190: loss = 0.06055
Step 31195: loss = 0.06972
Step 31200: loss = 0.06381
Training Data Eval:
  Num examples: 49920, Num correct: 48425, Precision @ 1: 0.9701
('Testing Data Eval: EPOCH->', 81)
  Num examples: 9984, Num correct: 7210, Precision @ 1: 0.7222
Step 31205: loss = 0.03444
Step 31210: loss = 0.09387
Step 31215: loss = 0.07524
Step 31220: loss = 0.09623
Step 31225: loss = 0.13935
Step 31230: loss = 0.07737
Step 31235: loss = 0.03314
Step 31240: loss = 0.07308
Step 31245: loss = 0.08770
Step 31250: loss = 0.10624
Step 31255: loss = 0.09289
Step 31260: loss = 0.08264
Step 31265: loss = 0.10314
Step 31270: loss = 0.11958
Step 31275: loss = 0.14581
Step 31280: loss = 0.08987
Step 31285: loss = 0.10809
Step 31290: loss = 0.07524
Step 31295: loss = 0.10059
Step 31300: loss = 0.06534
Step 31305: loss = 0.16293
Step 31310: loss = 0.13013
Step 31315: loss = 0.03110
Step 31320: loss = 0.06300
Step 31325: loss = 0.05863
Step 31330: loss = 0.15117
Step 31335: loss = 0.13522
Step 31340: loss = 0.09882
Step 31345: loss = 0.17910
Step 31350: loss = 0.04411
Step 31355: loss = 0.03633
Step 31360: loss = 0.10328
Step 31365: loss = 0.05471
Step 31370: loss = 0.14038
Step 31375: loss = 0.11862
Step 31380: loss = 0.08433
Step 31385: loss = 0.07915
Step 31390: loss = 0.19270
Step 31395: loss = 0.05551
Step 31400: loss = 0.11074
Step 31405: loss = 0.08491
Step 31410: loss = 0.08752
Step 31415: loss = 0.08385
Step 31420: loss = 0.11548
Step 31425: loss = 0.18811
Step 31430: loss = 0.04959
Step 31435: loss = 0.15216
Step 31440: loss = 0.15689
Step 31445: loss = 0.11047
Step 31450: loss = 0.06938
Step 31455: loss = 0.06042
Step 31460: loss = 0.19205
Step 31465: loss = 0.08873
Step 31470: loss = 0.03404
Step 31475: loss = 0.14209
Step 31480: loss = 0.11235
Step 31485: loss = 0.09583
Step 31490: loss = 0.03722
Step 31495: loss = 0.08446
Step 31500: loss = 0.14007
Step 31505: loss = 0.05650
Step 31510: loss = 0.10072
Step 31515: loss = 0.12001
Step 31520: loss = 0.16880
Step 31525: loss = 0.07596
Step 31530: loss = 0.06919
Step 31535: loss = 0.04872
Step 31540: loss = 0.09521
Step 31545: loss = 0.18993
Step 31550: loss = 0.06293
Step 31555: loss = 0.08780
Step 31560: loss = 0.05478
Step 31565: loss = 0.17333
Step 31570: loss = 0.02580
Step 31575: loss = 0.13410
Step 31580: loss = 0.04804
Step 31585: loss = 0.07726
Step 31590: loss = 0.24152
Training Data Eval:
  Num examples: 49920, Num correct: 48421, Precision @ 1: 0.9700
('Testing Data Eval: EPOCH->', 82)
  Num examples: 9984, Num correct: 7357, Precision @ 1: 0.7369
Step 31595: loss = 0.07552
Step 31600: loss = 0.02961
Step 31605: loss = 0.06698
Step 31610: loss = 0.09176
Step 31615: loss = 0.07641
Step 31620: loss = 0.04278
Step 31625: loss = 0.08308
Step 31630: loss = 0.04782
Step 31635: loss = 0.08631
Step 31640: loss = 0.08167
Step 31645: loss = 0.10658
Step 31650: loss = 0.07885
Step 31655: loss = 0.09656
Step 31660: loss = 0.10177
Step 31665: loss = 0.12886
Step 31670: loss = 0.08055
Step 31675: loss = 0.05216
Step 31680: loss = 0.10689
Step 31685: loss = 0.12695
Step 31690: loss = 0.09020
Step 31695: loss = 0.05290
Step 31700: loss = 0.14956
Step 31705: loss = 0.08082
Step 31710: loss = 0.24668
Step 31715: loss = 0.09167
Step 31720: loss = 0.08659
Step 31725: loss = 0.12750
Step 31730: loss = 0.03219
Step 31735: loss = 0.08119
Step 31740: loss = 0.13045
Step 31745: loss = 0.10322
Step 31750: loss = 0.07572
Step 31755: loss = 0.02805
Step 31760: loss = 0.10014
Step 31765: loss = 0.11832
Step 31770: loss = 0.15518
Step 31775: loss = 0.09387
Step 31780: loss = 0.11062
Step 31785: loss = 0.06604
Step 31790: loss = 0.12229
Step 31795: loss = 0.07302
Step 31800: loss = 0.11334
Step 31805: loss = 0.05629
Step 31810: loss = 0.07267
Step 31815: loss = 0.11529
Step 31820: loss = 0.04814
Step 31825: loss = 0.06996
Step 31830: loss = 0.14307
Step 31835: loss = 0.12486
Step 31840: loss = 0.06844
Step 31845: loss = 0.08451
Step 31850: loss = 0.08217
Step 31855: loss = 0.13534
Step 31860: loss = 0.13164
Step 31865: loss = 0.05320
Step 31870: loss = 0.05416
Step 31875: loss = 0.15223
Step 31880: loss = 0.10055
Step 31885: loss = 0.10666
Step 31890: loss = 0.08552
Step 31895: loss = 0.30283
Step 31900: loss = 0.10398
Step 31905: loss = 0.06963
Step 31910: loss = 0.05812
Step 31915: loss = 0.17433
Step 31920: loss = 0.09112
Step 31925: loss = 0.03572
Step 31930: loss = 0.03110
Step 31935: loss = 0.12395
Step 31940: loss = 0.05669
Step 31945: loss = 0.06593
Step 31950: loss = 0.09661
Step 31955: loss = 0.08486
Step 31960: loss = 0.04752
Step 31965: loss = 0.11555
Step 31970: loss = 0.08584
Step 31975: loss = 0.05067
Step 31980: loss = 0.10239
Training Data Eval:
  Num examples: 49920, Num correct: 48662, Precision @ 1: 0.9748
('Testing Data Eval: EPOCH->', 83)
  Num examples: 9984, Num correct: 7381, Precision @ 1: 0.7393
Step 31985: loss = 0.05664
Step 31990: loss = 0.06196
Step 31995: loss = 0.14018
Step 32000: loss = 0.06208
Step 32005: loss = 0.10393
Step 32010: loss = 0.07122
Step 32015: loss = 0.05069
Step 32020: loss = 0.08743
Step 32025: loss = 0.07602
Step 32030: loss = 0.05865
Step 32035: loss = 0.05168
Step 32040: loss = 0.08113
Step 32045: loss = 0.06704
Step 32050: loss = 0.06034
Step 32055: loss = 0.06980
Step 32060: loss = 0.06973
Step 32065: loss = 0.14345
Step 32070: loss = 0.14396
Step 32075: loss = 0.06408
Step 32080: loss = 0.06229
Step 32085: loss = 0.06708
Step 32090: loss = 0.08578
Step 32095: loss = 0.09085
Step 32100: loss = 0.08963
Step 32105: loss = 0.06665
Step 32110: loss = 0.13960
Step 32115: loss = 0.06991
Step 32120: loss = 0.10357
Step 32125: loss = 0.11695
Step 32130: loss = 0.13841
Step 32135: loss = 0.08351
Step 32140: loss = 0.14427
Step 32145: loss = 0.10560
Step 32150: loss = 0.10723
Step 32155: loss = 0.11776
Step 32160: loss = 0.10784
Step 32165: loss = 0.15550
Step 32170: loss = 0.09365
Step 32175: loss = 0.06872
Step 32180: loss = 0.05086
Step 32185: loss = 0.08594
Step 32190: loss = 0.13860
Step 32195: loss = 0.03651
Step 32200: loss = 0.05980
Step 32205: loss = 0.12134
Step 32210: loss = 0.14098
Step 32215: loss = 0.17782
Step 32220: loss = 0.14991
Step 32225: loss = 0.08486
Step 32230: loss = 0.03488
Step 32235: loss = 0.08420
Step 32240: loss = 0.10611
Step 32245: loss = 0.02006
Step 32250: loss = 0.06972
Step 32255: loss = 0.07439
Step 32260: loss = 0.10545
Step 32265: loss = 0.18324
Step 32270: loss = 0.18145
Step 32275: loss = 0.09051
Step 32280: loss = 0.05412
Step 32285: loss = 0.11545
Step 32290: loss = 0.11888
Step 32295: loss = 0.04912
Step 32300: loss = 0.08487
Step 32305: loss = 0.12362
Step 32310: loss = 0.15477
Step 32315: loss = 0.06097
Step 32320: loss = 0.04285
Step 32325: loss = 0.05296
Step 32330: loss = 0.06024
Step 32335: loss = 0.14249
Step 32340: loss = 0.09062
Step 32345: loss = 0.15342
Step 32350: loss = 0.07396
Step 32355: loss = 0.15790
Step 32360: loss = 0.08592
Step 32365: loss = 0.11140
Step 32370: loss = 0.07999
Training Data Eval:
  Num examples: 49920, Num correct: 48565, Precision @ 1: 0.9729
('Testing Data Eval: EPOCH->', 84)
  Num examples: 9984, Num correct: 7323, Precision @ 1: 0.7335
Step 32375: loss = 0.04699
Step 32380: loss = 0.07111
Step 32385: loss = 0.04554
Step 32390: loss = 0.10684
Step 32395: loss = 0.02878
Step 32400: loss = 0.07736
Step 32405: loss = 0.02776
Step 32410: loss = 0.04616
Step 32415: loss = 0.02575
Step 32420: loss = 0.06315
Step 32425: loss = 0.08097
Step 32430: loss = 0.04426
Step 32435: loss = 0.05697
Step 32440: loss = 0.12665
Step 32445: loss = 0.05463
Step 32450: loss = 0.20991
Step 32455: loss = 0.09481
Step 32460: loss = 0.22272
Step 32465: loss = 0.04598
Step 32470: loss = 0.05929
Step 32475: loss = 0.11340
Step 32480: loss = 0.08638
Step 32485: loss = 0.13458
Step 32490: loss = 0.07403
Step 32495: loss = 0.14937
Step 32500: loss = 0.06852
Step 32505: loss = 0.09031
Step 32510: loss = 0.08134
Step 32515: loss = 0.07677
Step 32520: loss = 0.13749
Step 32525: loss = 0.12732
Step 32530: loss = 0.07512
Step 32535: loss = 0.12092
Step 32540: loss = 0.19223
Step 32545: loss = 0.14820
Step 32550: loss = 0.09938
Step 32555: loss = 0.10852
Step 32560: loss = 0.12929
Step 32565: loss = 0.10703
Step 32570: loss = 0.06814
Step 32575: loss = 0.11537
Step 32580: loss = 0.15769
Step 32585: loss = 0.01883
Step 32590: loss = 0.08668
Step 32595: loss = 0.12806
Step 32600: loss = 0.02598
Step 32605: loss = 0.15448
Step 32610: loss = 0.19316
Step 32615: loss = 0.03010
Step 32620: loss = 0.17416
Step 32625: loss = 0.13391
Step 32630: loss = 0.08774
Step 32635: loss = 0.11172
Step 32640: loss = 0.08709
Step 32645: loss = 0.15313
Step 32650: loss = 0.03038
Step 32655: loss = 0.06178
Step 32660: loss = 0.12846
Step 32665: loss = 0.10041
Step 32670: loss = 0.06919
Step 32675: loss = 0.10925
Step 32680: loss = 0.10228
Step 32685: loss = 0.08709
Step 32690: loss = 0.10922
Step 32695: loss = 0.04099
Step 32700: loss = 0.13537
Step 32705: loss = 0.09714
Step 32710: loss = 0.14404
Step 32715: loss = 0.07433
Step 32720: loss = 0.17708
Step 32725: loss = 0.11996
Step 32730: loss = 0.04027
Step 32735: loss = 0.13227
Step 32740: loss = 0.09843
Step 32745: loss = 0.11587
Step 32750: loss = 0.12322
Step 32755: loss = 0.07494
Step 32760: loss = 0.08979
Training Data Eval:
  Num examples: 49920, Num correct: 48428, Precision @ 1: 0.9701
('Testing Data Eval: EPOCH->', 85)
  Num examples: 9984, Num correct: 7439, Precision @ 1: 0.7451
Step 32765: loss = 0.03049
Step 32770: loss = 0.11485
Step 32775: loss = 0.05188
Step 32780: loss = 0.08676
Step 32785: loss = 0.02647
Step 32790: loss = 0.12169
Step 32795: loss = 0.18540
Step 32800: loss = 0.07424
Step 32805: loss = 0.09737
Step 32810: loss = 0.05939
Step 32815: loss = 0.07227
Step 32820: loss = 0.03566
Step 32825: loss = 0.09029
Step 32830: loss = 0.14894
Step 32835: loss = 0.07173
Step 32840: loss = 0.17079
Step 32845: loss = 0.13258
Step 32850: loss = 0.09044
Step 32855: loss = 0.10541
Step 32860: loss = 0.09275
Step 32865: loss = 0.10974
Step 32870: loss = 0.10063
Step 32875: loss = 0.04779
Step 32880: loss = 0.07425
Step 32885: loss = 0.07142
Step 32890: loss = 0.15746
Step 32895: loss = 0.05089
Step 32900: loss = 0.24262
Step 32905: loss = 0.20017
Step 32910: loss = 0.11474
Step 32915: loss = 0.10172
Step 32920: loss = 0.10157
Step 32925: loss = 0.06595
Step 32930: loss = 0.05354
Step 32935: loss = 0.16319
Step 32940: loss = 0.06633
Step 32945: loss = 0.09296
Step 32950: loss = 0.09604
Step 32955: loss = 0.06096
Step 32960: loss = 0.05654
Step 32965: loss = 0.12601
Step 32970: loss = 0.07084
Step 32975: loss = 0.08245
Step 32980: loss = 0.13185
Step 32985: loss = 0.04258
Step 32990: loss = 0.15733
Step 32995: loss = 0.10510
Step 33000: loss = 0.09051
Step 33005: loss = 0.28947
Step 33010: loss = 0.05858
Step 33015: loss = 0.13295
Step 33020: loss = 0.08014
Step 33025: loss = 0.15258
Step 33030: loss = 0.15636
Step 33035: loss = 0.06308
Step 33040: loss = 0.07298
Step 33045: loss = 0.15492
Step 33050: loss = 0.08438
Step 33055: loss = 0.04376
Step 33060: loss = 0.06557
Step 33065: loss = 0.05103
Step 33070: loss = 0.10270
Step 33075: loss = 0.05565
Step 33080: loss = 0.07471
Step 33085: loss = 0.10623
Step 33090: loss = 0.12596
Step 33095: loss = 0.04793
Step 33100: loss = 0.07471
Step 33105: loss = 0.15407
Step 33110: loss = 0.06208
Step 33115: loss = 0.03619
Step 33120: loss = 0.03958
Step 33125: loss = 0.24408
Step 33130: loss = 0.04586
Step 33135: loss = 0.14251
Step 33140: loss = 0.06424
Step 33145: loss = 0.22632
Step 33150: loss = 0.07374
Training Data Eval:
  Num examples: 49920, Num correct: 48594, Precision @ 1: 0.9734
('Testing Data Eval: EPOCH->', 86)
  Num examples: 9984, Num correct: 7339, Precision @ 1: 0.7351
Step 33155: loss = 0.07699
Step 33160: loss = 0.13461
Step 33165: loss = 0.06185
Step 33170: loss = 0.03191
Step 33175: loss = 0.09251
Step 33180: loss = 0.09736
Step 33185: loss = 0.03614
Step 33190: loss = 0.15127
Step 33195: loss = 0.09148
Step 33200: loss = 0.05680
Step 33205: loss = 0.12137
Step 33210: loss = 0.04967
Step 33215: loss = 0.13265
Step 33220: loss = 0.10713
Step 33225: loss = 0.07716
Step 33230: loss = 0.06112
Step 33235: loss = 0.10690
Step 33240: loss = 0.09473
Step 33245: loss = 0.09982
Step 33250: loss = 0.09670
Step 33255: loss = 0.10891
Step 33260: loss = 0.14085
Step 33265: loss = 0.04754
Step 33270: loss = 0.07464
Step 33275: loss = 0.09145
Step 33280: loss = 0.08510
Step 33285: loss = 0.03553
Step 33290: loss = 0.07155
Step 33295: loss = 0.05210
Step 33300: loss = 0.04845
Step 33305: loss = 0.09361
Step 33310: loss = 0.05142
Step 33315: loss = 0.08969
Step 33320: loss = 0.10908
Step 33325: loss = 0.10549
Step 33330: loss = 0.11352
Step 33335: loss = 0.04905
Step 33340: loss = 0.06626
Step 33345: loss = 0.05560
Step 33350: loss = 0.10342
Step 33355: loss = 0.07889
Step 33360: loss = 0.08091
Step 33365: loss = 0.17992
Step 33370: loss = 0.07867
Step 33375: loss = 0.18729
Step 33380: loss = 0.04846
Step 33385: loss = 0.05655
Step 33390: loss = 0.12111
Step 33395: loss = 0.06433
Step 33400: loss = 0.11031
Step 33405: loss = 0.09555
Step 33410: loss = 0.12181
Step 33415: loss = 0.10191
Step 33420: loss = 0.05523
Step 33425: loss = 0.13075
Step 33430: loss = 0.10107
Step 33435: loss = 0.10073
Step 33440: loss = 0.02815
Step 33445: loss = 0.04326
Step 33450: loss = 0.17106
Step 33455: loss = 0.06315
Step 33460: loss = 0.14067
Step 33465: loss = 0.08999
Step 33470: loss = 0.13089
Step 33475: loss = 0.12064
Step 33480: loss = 0.07319
Step 33485: loss = 0.10483
Step 33490: loss = 0.04014
Step 33495: loss = 0.10987
Step 33500: loss = 0.09109
Step 33505: loss = 0.10368
Step 33510: loss = 0.15173
Step 33515: loss = 0.18068
Step 33520: loss = 0.07118
Step 33525: loss = 0.10044
Step 33530: loss = 0.06860
Step 33535: loss = 0.12524
Step 33540: loss = 0.07729
Training Data Eval:
  Num examples: 49920, Num correct: 48431, Precision @ 1: 0.9702
('Testing Data Eval: EPOCH->', 87)
  Num examples: 9984, Num correct: 7263, Precision @ 1: 0.7275
Step 33545: loss = 0.04883
Step 33550: loss = 0.07808
Step 33555: loss = 0.18192
Step 33560: loss = 0.06740
Step 33565: loss = 0.08859
Step 33570: loss = 0.12871
Step 33575: loss = 0.07552
Step 33580: loss = 0.13566
Step 33585: loss = 0.09480
Step 33590: loss = 0.08403
Step 33595: loss = 0.09307
Step 33600: loss = 0.05877
Step 33605: loss = 0.04546
Step 33610: loss = 0.08699
Step 33615: loss = 0.06991
Step 33620: loss = 0.07786
Step 33625: loss = 0.09583
Step 33630: loss = 0.07113
Step 33635: loss = 0.06860
Step 33640: loss = 0.06579
Step 33645: loss = 0.11663
Step 33650: loss = 0.02235
Step 33655: loss = 0.04813
Step 33660: loss = 0.03635
Step 33665: loss = 0.08658
Step 33670: loss = 0.08081
Step 33675: loss = 0.07925
Step 33680: loss = 0.09411
Step 33685: loss = 0.08989
Step 33690: loss = 0.03761
Step 33695: loss = 0.07202
Step 33700: loss = 0.14163
Step 33705: loss = 0.08052
Step 33710: loss = 0.14003
Step 33715: loss = 0.06273
Step 33720: loss = 0.05929
Step 33725: loss = 0.14383
Step 33730: loss = 0.09989
Step 33735: loss = 0.09116
Step 33740: loss = 0.04832
Step 33745: loss = 0.09100
Step 33750: loss = 0.08210
Step 33755: loss = 0.13572
Step 33760: loss = 0.07256
Step 33765: loss = 0.23989
Step 33770: loss = 0.07069
Step 33775: loss = 0.11395
Step 33780: loss = 0.09611
Step 33785: loss = 0.09125
Step 33790: loss = 0.04122
Step 33795: loss = 0.14974
Step 33800: loss = 0.16625
Step 33805: loss = 0.03451
Step 33810: loss = 0.10484
Step 33815: loss = 0.09569
Step 33820: loss = 0.12699
Step 33825: loss = 0.12157
Step 33830: loss = 0.06973
Step 33835: loss = 0.08657
Step 33840: loss = 0.16422
Step 33845: loss = 0.14397
Step 33850: loss = 0.04152
Step 33855: loss = 0.06664
Step 33860: loss = 0.04860
Step 33865: loss = 0.09374
Step 33870: loss = 0.06835
Step 33875: loss = 0.04710
Step 33880: loss = 0.10991
Step 33885: loss = 0.09470
Step 33890: loss = 0.06656
Step 33895: loss = 0.03755
Step 33900: loss = 0.08302
Step 33905: loss = 0.10643
Step 33910: loss = 0.11180
Step 33915: loss = 0.04909
Step 33920: loss = 0.07821
Step 33925: loss = 0.10257
Step 33930: loss = 0.07492
Training Data Eval:
  Num examples: 49920, Num correct: 48745, Precision @ 1: 0.9765
('Testing Data Eval: EPOCH->', 88)
  Num examples: 9984, Num correct: 7455, Precision @ 1: 0.7467
Step 33935: loss = 0.10144
Step 33940: loss = 0.05553
Step 33945: loss = 0.04567
Step 33950: loss = 0.07011
Step 33955: loss = 0.10797
Step 33960: loss = 0.08455
Step 33965: loss = 0.02807
Step 33970: loss = 0.02774
Step 33975: loss = 0.11316
Step 33980: loss = 0.09004
Step 33985: loss = 0.07499
Step 33990: loss = 0.03839
Step 33995: loss = 0.06730
Step 34000: loss = 0.05910
Step 34005: loss = 0.08910
Step 34010: loss = 0.10289
Step 34015: loss = 0.05856
Step 34020: loss = 0.06411
Step 34025: loss = 0.06348
Step 34030: loss = 0.04584
Step 34035: loss = 0.07546
Step 34040: loss = 0.05141
Step 34045: loss = 0.11116
Step 34050: loss = 0.04707
Step 34055: loss = 0.11243
Step 34060: loss = 0.17970
Step 34065: loss = 0.04831
Step 34070: loss = 0.10066
Step 34075: loss = 0.14075
Step 34080: loss = 0.04199
Step 34085: loss = 0.04642
Step 34090: loss = 0.14221
Step 34095: loss = 0.17159
Step 34100: loss = 0.11560
Step 34105: loss = 0.02262
Step 34110: loss = 0.04595
Step 34115: loss = 0.12866
Step 34120: loss = 0.06428
Step 34125: loss = 0.05497
Step 34130: loss = 0.07054
Step 34135: loss = 0.04807
Step 34140: loss = 0.03457
Step 34145: loss = 0.05541
Step 34150: loss = 0.11970
Step 34155: loss = 0.12813
Step 34160: loss = 0.12982
Step 34165: loss = 0.03913
Step 34170: loss = 0.10290
Step 34175: loss = 0.04277
Step 34180: loss = 0.13854
Step 34185: loss = 0.09872
Step 34190: loss = 0.07430
Step 34195: loss = 0.05645
Step 34200: loss = 0.11951
Step 34205: loss = 0.12498
Step 34210: loss = 0.01733
Step 34215: loss = 0.06418
Step 34220: loss = 0.05972
Step 34225: loss = 0.03620
Step 34230: loss = 0.03350
Step 34235: loss = 0.04596
Step 34240: loss = 0.01811
Step 34245: loss = 0.05929
Step 34250: loss = 0.10688
Step 34255: loss = 0.11746
Step 34260: loss = 0.07235
Step 34265: loss = 0.18810
Step 34270: loss = 0.16103
Step 34275: loss = 0.06498
Step 34280: loss = 0.07702
Step 34285: loss = 0.15878
Step 34290: loss = 0.07461
Step 34295: loss = 0.13857
Step 34300: loss = 0.11959
Step 34305: loss = 0.08047
Step 34310: loss = 0.05961
Step 34315: loss = 0.07508
Step 34320: loss = 0.13227
Training Data Eval:
  Num examples: 49920, Num correct: 48530, Precision @ 1: 0.9722
('Testing Data Eval: EPOCH->', 89)
  Num examples: 9984, Num correct: 7306, Precision @ 1: 0.7318
Step 34325: loss = 0.13007
Step 34330: loss = 0.04362
Step 34335: loss = 0.03623
Step 34340: loss = 0.19444
Step 34345: loss = 0.08400
Step 34350: loss = 0.20085
Step 34355: loss = 0.20711
Step 34360: loss = 0.04208
Step 34365: loss = 0.05862
Step 34370: loss = 0.04373
Step 34375: loss = 0.02319
Step 34380: loss = 0.14051
Step 34385: loss = 0.07132
Step 34390: loss = 0.07676
Step 34395: loss = 0.17950
Step 34400: loss = 0.11501
Step 34405: loss = 0.07777
Step 34410: loss = 0.16849
Step 34415: loss = 0.08186
Step 34420: loss = 0.05018
Step 34425: loss = 0.11636
Step 34430: loss = 0.06620
Step 34435: loss = 0.10732
Step 34440: loss = 0.07328
Step 34445: loss = 0.03539
Step 34450: loss = 0.04929
Step 34455: loss = 0.04962
Step 34460: loss = 0.11758
Step 34465: loss = 0.07026
Step 34470: loss = 0.06120
Step 34475: loss = 0.08485
Step 34480: loss = 0.02578
Step 34485: loss = 0.06794
Step 34490: loss = 0.07981
Step 34495: loss = 0.12784
Step 34500: loss = 0.23432
Step 34505: loss = 0.04361
Step 34510: loss = 0.18682
Step 34515: loss = 0.08109
Step 34520: loss = 0.05916
Step 34525: loss = 0.07476
Step 34530: loss = 0.10128
Step 34535: loss = 0.12466
Step 34540: loss = 0.03311
Step 34545: loss = 0.12724
Step 34550: loss = 0.05546
Step 34555: loss = 0.09133
Step 34560: loss = 0.10646
Step 34565: loss = 0.10625
Step 34570: loss = 0.06545
Step 34575: loss = 0.07476
Step 34580: loss = 0.08056
Step 34585: loss = 0.05695
Step 34590: loss = 0.07515
Step 34595: loss = 0.03007
Step 34600: loss = 0.02333
Step 34605: loss = 0.13878
Step 34610: loss = 0.05592
Step 34615: loss = 0.12798
Step 34620: loss = 0.08854
Step 34625: loss = 0.06021
Step 34630: loss = 0.05097
Step 34635: loss = 0.03465
Step 34640: loss = 0.03323
Step 34645: loss = 0.09176
Step 34650: loss = 0.07982
Step 34655: loss = 0.02954
Step 34660: loss = 0.09339
Step 34665: loss = 0.07436
Step 34670: loss = 0.10529
Step 34675: loss = 0.06037
Step 34680: loss = 0.02757
Step 34685: loss = 0.10731
Step 34690: loss = 0.09421
Step 34695: loss = 0.06960
Step 34700: loss = 0.08258
Step 34705: loss = 0.05440
Step 34710: loss = 0.06976
Training Data Eval:
  Num examples: 49920, Num correct: 48528, Precision @ 1: 0.9721
('Testing Data Eval: EPOCH->', 90)
  Num examples: 9984, Num correct: 7335, Precision @ 1: 0.7347
Step 34715: loss = 0.07078
Step 34720: loss = 0.06233
Step 34725: loss = 0.07560
Step 34730: loss = 0.06341
Step 34735: loss = 0.13404
Step 34740: loss = 0.17365
Step 34745: loss = 0.03232
Step 34750: loss = 0.06389
Step 34755: loss = 0.05673
Step 34760: loss = 0.08976
Step 34765: loss = 0.07371
Step 34770: loss = 0.09099
Step 34775: loss = 0.05915
Step 34780: loss = 0.09985
Step 34785: loss = 0.08696
Step 34790: loss = 0.02648
Step 34795: loss = 0.05428
Step 34800: loss = 0.05032
Step 34805: loss = 0.11877
Step 34810: loss = 0.11213
Step 34815: loss = 0.02426
Step 34820: loss = 0.05568
Step 34825: loss = 0.10648
Step 34830: loss = 0.05875
Step 34835: loss = 0.05408
Step 34840: loss = 0.03388
Step 34845: loss = 0.08292
Step 34850: loss = 0.06925
Step 34855: loss = 0.14715
Step 34860: loss = 0.04530
Step 34865: loss = 0.07463
Step 34870: loss = 0.06209
Step 34875: loss = 0.07313
Step 34880: loss = 0.07051
Step 34885: loss = 0.18936
Step 34890: loss = 0.03800
Step 34895: loss = 0.02518
Step 34900: loss = 0.03941
Step 34905: loss = 0.05712
Step 34910: loss = 0.07093
Step 34915: loss = 0.07198
Step 34920: loss = 0.02745
Step 34925: loss = 0.10835
Step 34930: loss = 0.12681
Step 34935: loss = 0.08545
Step 34940: loss = 0.06512
Step 34945: loss = 0.13958
Step 34950: loss = 0.03736
Step 34955: loss = 0.10978
Step 34960: loss = 0.03208
Step 34965: loss = 0.04682
Step 34970: loss = 0.04598
Step 34975: loss = 0.08095
Step 34980: loss = 0.09781
Step 34985: loss = 0.14011
Step 34990: loss = 0.10888
Step 34995: loss = 0.07855
Step 35000: loss = 0.02324
Step 35005: loss = 0.07661
Step 35010: loss = 0.08303
Step 35015: loss = 0.08629
Step 35020: loss = 0.08598
Step 35025: loss = 0.10928
Step 35030: loss = 0.09463
Step 35035: loss = 0.09058
Step 35040: loss = 0.09226
Step 35045: loss = 0.08190
Step 35050: loss = 0.09679
Step 35055: loss = 0.06815
Step 35060: loss = 0.19193
Step 35065: loss = 0.02959
Step 35070: loss = 0.04532
Step 35075: loss = 0.11266
Step 35080: loss = 0.11197
Step 35085: loss = 0.12284
Step 35090: loss = 0.03563
Step 35095: loss = 0.09602
Step 35100: loss = 0.04979
Training Data Eval:
  Num examples: 49920, Num correct: 48686, Precision @ 1: 0.9753
('Testing Data Eval: EPOCH->', 91)
  Num examples: 9984, Num correct: 7275, Precision @ 1: 0.7287
Step 35105: loss = 0.04259
Step 35110: loss = 0.10346
Step 35115: loss = 0.15883
Step 35120: loss = 0.03756
Step 35125: loss = 0.11068
Step 35130: loss = 0.05092
Step 35135: loss = 0.08374
Step 35140: loss = 0.06302
Step 35145: loss = 0.08474
Step 35150: loss = 0.07335
Step 35155: loss = 0.09417
Step 35160: loss = 0.04884
Step 35165: loss = 0.13653
Step 35170: loss = 0.09351
Step 35175: loss = 0.06468
Step 35180: loss = 0.09725
Step 35185: loss = 0.07492
Step 35190: loss = 0.04477
Step 35195: loss = 0.01531
Step 35200: loss = 0.12098
Step 35205: loss = 0.11564
Step 35210: loss = 0.09548
Step 35215: loss = 0.23946
Step 35220: loss = 0.11032
Step 35225: loss = 0.03064
Step 35230: loss = 0.14179
Step 35235: loss = 0.04563
Step 35240: loss = 0.03476
Step 35245: loss = 0.11696
Step 35250: loss = 0.05314
Step 35255: loss = 0.02817
Step 35260: loss = 0.24710
Step 35265: loss = 0.11969
Step 35270: loss = 0.03255
Step 35275: loss = 0.06030
Step 35280: loss = 0.02455
Step 35285: loss = 0.05123
Step 35290: loss = 0.13552
Step 35295: loss = 0.13862
Step 35300: loss = 0.06099
Step 35305: loss = 0.06074
Step 35310: loss = 0.05089
Step 35315: loss = 0.08215
Step 35320: loss = 0.02246
Step 35325: loss = 0.02957
Step 35330: loss = 0.12942
Step 35335: loss = 0.07771
Step 35340: loss = 0.10398
Step 35345: loss = 0.05394
Step 35350: loss = 0.07610
Step 35355: loss = 0.06583
Step 35360: loss = 0.08251
Step 35365: loss = 0.10975
Step 35370: loss = 0.07295
Step 35375: loss = 0.05718
Step 35380: loss = 0.09102
Step 35385: loss = 0.03252
Step 35390: loss = 0.04106
Step 35395: loss = 0.11504
Step 35400: loss = 0.06954
Step 35405: loss = 0.03682
Step 35410: loss = 0.17332
Step 35415: loss = 0.04597
Step 35420: loss = 0.06982
Step 35425: loss = 0.03498
Step 35430: loss = 0.12305
Step 35435: loss = 0.15060
Step 35440: loss = 0.11430
Step 35445: loss = 0.07089
Step 35450: loss = 0.03975
Step 35455: loss = 0.11471
Step 35460: loss = 0.05469
Step 35465: loss = 0.06250
Step 35470: loss = 0.10713
Step 35475: loss = 0.13547
Step 35480: loss = 0.10756
Step 35485: loss = 0.11099
Step 35490: loss = 0.13251
Training Data Eval:
  Num examples: 49920, Num correct: 48539, Precision @ 1: 0.9723
('Testing Data Eval: EPOCH->', 92)
  Num examples: 9984, Num correct: 7410, Precision @ 1: 0.7422
Step 35495: loss = 0.04865
Step 35500: loss = 0.05686
Step 35505: loss = 0.14977
Step 35510: loss = 0.05195
Step 35515: loss = 0.17952
Step 35520: loss = 0.06018
Step 35525: loss = 0.04933
Step 35530: loss = 0.07785
Step 35535: loss = 0.07173
Step 35540: loss = 0.05276
Step 35545: loss = 0.13632
Step 35550: loss = 0.07606
Step 35555: loss = 0.02737
Step 35560: loss = 0.01584
Step 35565: loss = 0.03581
Step 35570: loss = 0.05212
Step 35575: loss = 0.02375
Step 35580: loss = 0.10080
Step 35585: loss = 0.12985
Step 35590: loss = 0.08723
Step 35595: loss = 0.05288
Step 35600: loss = 0.00925
Step 35605: loss = 0.11012
Step 35610: loss = 0.04609
Step 35615: loss = 0.03377
Step 35620: loss = 0.08718
Step 35625: loss = 0.08642
Step 35630: loss = 0.07431
Step 35635: loss = 0.06690
Step 35640: loss = 0.16820
Step 35645: loss = 0.14267
Step 35650: loss = 0.07163
Step 35655: loss = 0.06965
Step 35660: loss = 0.06563
Step 35665: loss = 0.12976
Step 35670: loss = 0.04084
Step 35675: loss = 0.20403
Step 35680: loss = 0.07320
Step 35685: loss = 0.05937
Step 35690: loss = 0.06478
Step 35695: loss = 0.17029
Step 35700: loss = 0.11871
Step 35705: loss = 0.07306
Step 35710: loss = 0.12647
Step 35715: loss = 0.05567
Step 35720: loss = 0.16939
Step 35725: loss = 0.05669
Step 35730: loss = 0.09296
Step 35735: loss = 0.07351
Step 35740: loss = 0.09159
Step 35745: loss = 0.12938
Step 35750: loss = 0.07574
Step 35755: loss = 0.06403
Step 35760: loss = 0.05803
Step 35765: loss = 0.09322
Step 35770: loss = 0.05833
Step 35775: loss = 0.07570
Step 35780: loss = 0.10829
Step 35785: loss = 0.04659
Step 35790: loss = 0.03573
Step 35795: loss = 0.11804
Step 35800: loss = 0.13968
Step 35805: loss = 0.05656
Step 35810: loss = 0.06802
Step 35815: loss = 0.10220
Step 35820: loss = 0.08534
Step 35825: loss = 0.04641
Step 35830: loss = 0.14500
Step 35835: loss = 0.03034
Step 35840: loss = 0.10094
Step 35845: loss = 0.04612
Step 35850: loss = 0.09652
Step 35855: loss = 0.05044
Step 35860: loss = 0.05013
Step 35865: loss = 0.05208
Step 35870: loss = 0.06157
Step 35875: loss = 0.13391
Step 35880: loss = 0.08748
Training Data Eval:
  Num examples: 49920, Num correct: 48774, Precision @ 1: 0.9770
('Testing Data Eval: EPOCH->', 93)
  Num examples: 9984, Num correct: 7374, Precision @ 1: 0.7386
Step 35885: loss = 0.03367
Step 35890: loss = 0.14943
Step 35895: loss = 0.07926
Step 35900: loss = 0.14112
Step 35905: loss = 0.06295
Step 35910: loss = 0.10129
Step 35915: loss = 0.03523
Step 35920: loss = 0.08237
Step 35925: loss = 0.07180
Step 35930: loss = 0.05947
Step 35935: loss = 0.17708
Step 35940: loss = 0.12512
Step 35945: loss = 0.13429
Step 35950: loss = 0.14546
Step 35955: loss = 0.08171
Step 35960: loss = 0.07478
Step 35965: loss = 0.01713
Step 35970: loss = 0.07082
Step 35975: loss = 0.02033
Step 35980: loss = 0.08879
Step 35985: loss = 0.05396
Step 35990: loss = 0.07930
Step 35995: loss = 0.11740
Step 36000: loss = 0.02925
Step 36005: loss = 0.04719
Step 36010: loss = 0.12409
Step 36015: loss = 0.04162
Step 36020: loss = 0.04074
Step 36025: loss = 0.08389
Step 36030: loss = 0.09592
Step 36035: loss = 0.06504
Step 36040: loss = 0.07536
Step 36045: loss = 0.09394
Step 36050: loss = 0.03928
Step 36055: loss = 0.02839
Step 36060: loss = 0.13546
Step 36065: loss = 0.09699
Step 36070: loss = 0.04424
Step 36075: loss = 0.06032
Step 36080: loss = 0.06668
Step 36085: loss = 0.07783
Step 36090: loss = 0.06279
Step 36095: loss = 0.02148
Step 36100: loss = 0.07518
Step 36105: loss = 0.04422
Step 36110: loss = 0.10685
Step 36115: loss = 0.08408
Step 36120: loss = 0.04887
Step 36125: loss = 0.10899
Step 36130: loss = 0.05664
Step 36135: loss = 0.09328
Step 36140: loss = 0.13591
Step 36145: loss = 0.09221
Step 36150: loss = 0.07497
Step 36155: loss = 0.05156
Step 36160: loss = 0.06887
Step 36165: loss = 0.04563
Step 36170: loss = 0.05276
Step 36175: loss = 0.06934
Step 36180: loss = 0.04579
Step 36185: loss = 0.08479
Step 36190: loss = 0.07180
Step 36195: loss = 0.02189
Step 36200: loss = 0.09183
Step 36205: loss = 0.07898
Step 36210: loss = 0.07254
Step 36215: loss = 0.05271
Step 36220: loss = 0.12925
Step 36225: loss = 0.17108
Step 36230: loss = 0.11619
Step 36235: loss = 0.06148
Step 36240: loss = 0.05252
Step 36245: loss = 0.15598
Step 36250: loss = 0.02327
Step 36255: loss = 0.03100
Step 36260: loss = 0.12180
Step 36265: loss = 0.11591
Step 36270: loss = 0.05642
Training Data Eval:
  Num examples: 49920, Num correct: 48669, Precision @ 1: 0.9749
('Testing Data Eval: EPOCH->', 94)
  Num examples: 9984, Num correct: 7287, Precision @ 1: 0.7299
Step 36275: loss = 0.05885
Step 36280: loss = 0.06660
Step 36285: loss = 0.08184
Step 36290: loss = 0.06299
Step 36295: loss = 0.09886
Step 36300: loss = 0.05910
Step 36305: loss = 0.02920
Step 36310: loss = 0.07143
Step 36315: loss = 0.05936
Step 36320: loss = 0.14223
Step 36325: loss = 0.05596
Step 36330: loss = 0.02980
Step 36335: loss = 0.09375
Step 36340: loss = 0.07555
Step 36345: loss = 0.07862
Step 36350: loss = 0.12336
Step 36355: loss = 0.02718
Step 36360: loss = 0.15748
Step 36365: loss = 0.02319
Step 36370: loss = 0.02179
Step 36375: loss = 0.05598
Step 36380: loss = 0.13423
Step 36385: loss = 0.08612
Step 36390: loss = 0.05939
Step 36395: loss = 0.13574
Step 36400: loss = 0.05802
Step 36405: loss = 0.11234
Step 36410: loss = 0.06118
Step 36415: loss = 0.08977
Step 36420: loss = 0.04926
Step 36425: loss = 0.09084
Step 36430: loss = 0.01400
Step 36435: loss = 0.14855
Step 36440: loss = 0.09175
Step 36445: loss = 0.09259
Step 36450: loss = 0.04080
Step 36455: loss = 0.07996
Step 36460: loss = 0.04233
Step 36465: loss = 0.07094
Step 36470: loss = 0.13854
Step 36475: loss = 0.04530
Step 36480: loss = 0.04793
Step 36485: loss = 0.10800
Step 36490: loss = 0.07492
Step 36495: loss = 0.08311
Step 36500: loss = 0.08546
Step 36505: loss = 0.10488
Step 36510: loss = 0.11412
Step 36515: loss = 0.07921
Step 36520: loss = 0.04682
Step 36525: loss = 0.13328
Step 36530: loss = 0.03182
Step 36535: loss = 0.11441
Step 36540: loss = 0.05507
Step 36545: loss = 0.10252
Step 36550: loss = 0.16166
Step 36555: loss = 0.04449
Step 36560: loss = 0.05589
Step 36565: loss = 0.07574
Step 36570: loss = 0.03416
Step 36575: loss = 0.03489
Step 36580: loss = 0.02721
Step 36585: loss = 0.08193
Step 36590: loss = 0.06915
Step 36595: loss = 0.07731
Step 36600: loss = 0.10694
Step 36605: loss = 0.05988
Step 36610: loss = 0.06712
Step 36615: loss = 0.17358
Step 36620: loss = 0.04339
Step 36625: loss = 0.04006
Step 36630: loss = 0.05217
Step 36635: loss = 0.07700
Step 36640: loss = 0.04571
Step 36645: loss = 0.05356
Step 36650: loss = 0.08053
Step 36655: loss = 0.05309
Step 36660: loss = 0.07635
Training Data Eval:
  Num examples: 49920, Num correct: 48698, Precision @ 1: 0.9755
('Testing Data Eval: EPOCH->', 95)
  Num examples: 9984, Num correct: 7382, Precision @ 1: 0.7394
Step 36665: loss = 0.07377
Step 36670: loss = 0.04376
Step 36675: loss = 0.03548
Step 36680: loss = 0.06796
Step 36685: loss = 0.07887
Step 36690: loss = 0.02459
Step 36695: loss = 0.04760
Step 36700: loss = 0.07418
Step 36705: loss = 0.08031
Step 36710: loss = 0.12424
Step 36715: loss = 0.03586
Step 36720: loss = 0.04202
Step 36725: loss = 0.13497
Step 36730: loss = 0.03863
Step 36735: loss = 0.06015
Step 36740: loss = 0.16284
Step 36745: loss = 0.06880
Step 36750: loss = 0.03141
Step 36755: loss = 0.19166
Step 36760: loss = 0.11896
Step 36765: loss = 0.04560
Step 36770: loss = 0.06874
Step 36775: loss = 0.02278
Step 36780: loss = 0.11118
Step 36785: loss = 0.03744
Step 36790: loss = 0.10008
Step 36795: loss = 0.10540
Step 36800: loss = 0.03558
Step 36805: loss = 0.04942
Step 36810: loss = 0.10838
Step 36815: loss = 0.03389
Step 36820: loss = 0.03886
Step 36825: loss = 0.05230
Step 36830: loss = 0.06512
Step 36835: loss = 0.06629
Step 36840: loss = 0.14812
Step 36845: loss = 0.04384
Step 36850: loss = 0.07066
Step 36855: loss = 0.05998
Step 36860: loss = 0.06192
Step 36865: loss = 0.06377
Step 36870: loss = 0.17702
Step 36875: loss = 0.06299
Step 36880: loss = 0.14943
Step 36885: loss = 0.12989
Step 36890: loss = 0.16106
Step 36895: loss = 0.03499
Step 36900: loss = 0.05072
Step 36905: loss = 0.01881
Step 36910: loss = 0.03555
Step 36915: loss = 0.03727
Step 36920: loss = 0.16347
Step 36925: loss = 0.03456
Step 36930: loss = 0.07189
Step 36935: loss = 0.17381
Step 36940: loss = 0.07286
Step 36945: loss = 0.09159
Step 36950: loss = 0.05335
Step 36955: loss = 0.02671
Step 36960: loss = 0.09063
Step 36965: loss = 0.04255
Step 36970: loss = 0.03978
Step 36975: loss = 0.05547
Step 36980: loss = 0.06693
Step 36985: loss = 0.16198
Step 36990: loss = 0.07220
Step 36995: loss = 0.14066
Step 37000: loss = 0.02060
Step 37005: loss = 0.05916
Step 37010: loss = 0.10240
Step 37015: loss = 0.05315
Step 37020: loss = 0.07560
Step 37025: loss = 0.04682
Step 37030: loss = 0.09583
Step 37035: loss = 0.08710
Step 37040: loss = 0.06453
Step 37045: loss = 0.06141
Step 37050: loss = 0.05783
Training Data Eval:
  Num examples: 49920, Num correct: 48708, Precision @ 1: 0.9757
('Testing Data Eval: EPOCH->', 96)
  Num examples: 9984, Num correct: 7436, Precision @ 1: 0.7448
Step 37055: loss = 0.05561
Step 37060: loss = 0.09974
Step 37065: loss = 0.08966
Step 37070: loss = 0.01562
Step 37075: loss = 0.02031
Step 37080: loss = 0.12721
Step 37085: loss = 0.05136
Step 37090: loss = 0.02911
Step 37095: loss = 0.04368
Step 37100: loss = 0.08759
Step 37105: loss = 0.10303
Step 37110: loss = 0.06525
Step 37115: loss = 0.03191
Step 37120: loss = 0.04220
Step 37125: loss = 0.06348
Step 37130: loss = 0.04809
Step 37135: loss = 0.02689
Step 37140: loss = 0.09971
Step 37145: loss = 0.05265
Step 37150: loss = 0.06339
Step 37155: loss = 0.03313
Step 37160: loss = 0.02143
Step 37165: loss = 0.06102
Step 37170: loss = 0.11510
Step 37175: loss = 0.06780
Step 37180: loss = 0.05702
Step 37185: loss = 0.08261
Step 37190: loss = 0.05896
Step 37195: loss = 0.08520
Step 37200: loss = 0.06249
Step 37205: loss = 0.06870
Step 37210: loss = 0.03426
Step 37215: loss = 0.05421
Step 37220: loss = 0.10534
Step 37225: loss = 0.06422
Step 37230: loss = 0.06662
Step 37235: loss = 0.03371
Step 37240: loss = 0.06862
Step 37245: loss = 0.05028
Step 37250: loss = 0.02526
Step 37255: loss = 0.07608
Step 37260: loss = 0.12859
Step 37265: loss = 0.02241
Step 37270: loss = 0.03474
Step 37275: loss = 0.04719
Step 37280: loss = 0.09298
Step 37285: loss = 0.12168
Step 37290: loss = 0.10952
Step 37295: loss = 0.03460
Step 37300: loss = 0.12803
Step 37305: loss = 0.07611
Step 37310: loss = 0.04882
Step 37315: loss = 0.13214
Step 37320: loss = 0.11953
Step 37325: loss = 0.08466
Step 37330: loss = 0.04825
Step 37335: loss = 0.03833
Step 37340: loss = 0.22040
Step 37345: loss = 0.08300
Step 37350: loss = 0.05234
Step 37355: loss = 0.03276
Step 37360: loss = 0.04656
Step 37365: loss = 0.02437
Step 37370: loss = 0.04613
Step 37375: loss = 0.08190
Step 37380: loss = 0.10989
Step 37385: loss = 0.03467
Step 37390: loss = 0.05059
Step 37395: loss = 0.04504
Step 37400: loss = 0.05926
Step 37405: loss = 0.09216
Step 37410: loss = 0.10282
Step 37415: loss = 0.02110
Step 37420: loss = 0.06806
Step 37425: loss = 0.03623
Step 37430: loss = 0.12625
Step 37435: loss = 0.09130
Step 37440: loss = 0.05718
Training Data Eval:
  Num examples: 49920, Num correct: 48875, Precision @ 1: 0.9791
('Testing Data Eval: EPOCH->', 97)
  Num examples: 9984, Num correct: 7375, Precision @ 1: 0.7387
Step 37445: loss = 0.03290
Step 37450: loss = 0.03286
Step 37455: loss = 0.08996
Step 37460: loss = 0.06615
Step 37465: loss = 0.09371
Step 37470: loss = 0.04313
Step 37475: loss = 0.08250
Step 37480: loss = 0.13504
Step 37485: loss = 0.01838
Step 37490: loss = 0.08815
Step 37495: loss = 0.08120
Step 37500: loss = 0.03707
Step 37505: loss = 0.05468
Step 37510: loss = 0.08597
Step 37515: loss = 0.06116
Step 37520: loss = 0.02391
Step 37525: loss = 0.04926
Step 37530: loss = 0.02865
Step 37535: loss = 0.03873
Step 37540: loss = 0.03668
Step 37545: loss = 0.03362
Step 37550: loss = 0.06145
Step 37555: loss = 0.06683
Step 37560: loss = 0.02638
Step 37565: loss = 0.11501
Step 37570: loss = 0.08800
Step 37575: loss = 0.05317
Step 37580: loss = 0.04677
Step 37585: loss = 0.03515
Step 37590: loss = 0.04683
Step 37595: loss = 0.09648
Step 37600: loss = 0.05200
Step 37605: loss = 0.02969
Step 37610: loss = 0.05319
Step 37615: loss = 0.05816
Step 37620: loss = 0.11131
Step 37625: loss = 0.09088
Step 37630: loss = 0.05670
Step 37635: loss = 0.10808
Step 37640: loss = 0.03140
Step 37645: loss = 0.07232
Step 37650: loss = 0.02764
Step 37655: loss = 0.09202
Step 37660: loss = 0.03194
Step 37665: loss = 0.24694
Step 37670: loss = 0.12362
Step 37675: loss = 0.09149
Step 37680: loss = 0.13978
Step 37685: loss = 0.03870
Step 37690: loss = 0.17998
Step 37695: loss = 0.19267
Step 37700: loss = 0.08675
Step 37705: loss = 0.09068
Step 37710: loss = 0.05584
Step 37715: loss = 0.04820
Step 37720: loss = 0.03199
Step 37725: loss = 0.17474
Step 37730: loss = 0.03074
Step 37735: loss = 0.07401
Step 37740: loss = 0.07834
Step 37745: loss = 0.08713
Step 37750: loss = 0.09293
Step 37755: loss = 0.05285
Step 37760: loss = 0.09394
Step 37765: loss = 0.11821
Step 37770: loss = 0.03530
Step 37775: loss = 0.01703
Step 37780: loss = 0.11289
Step 37785: loss = 0.03743
Step 37790: loss = 0.09336
Step 37795: loss = 0.05881
Step 37800: loss = 0.17398
Step 37805: loss = 0.17439
Step 37810: loss = 0.03741
Step 37815: loss = 0.06382
Step 37820: loss = 0.04857
Step 37825: loss = 0.09640
Step 37830: loss = 0.01563
Training Data Eval:
  Num examples: 49920, Num correct: 48813, Precision @ 1: 0.9778
('Testing Data Eval: EPOCH->', 98)
  Num examples: 9984, Num correct: 7458, Precision @ 1: 0.7470
Step 37835: loss = 0.08408
Step 37840: loss = 0.03893
Step 37845: loss = 0.03544
Step 37850: loss = 0.03370
Step 37855: loss = 0.05378
Step 37860: loss = 0.06553
Step 37865: loss = 0.10473
Step 37870: loss = 0.06981
Step 37875: loss = 0.12320
Step 37880: loss = 0.04153
Step 37885: loss = 0.06777
Step 37890: loss = 0.08517
Step 37895: loss = 0.03644
Step 37900: loss = 0.04385
Step 37905: loss = 0.06601
Step 37910: loss = 0.03868
Step 37915: loss = 0.07215
Step 37920: loss = 0.01765
Step 37925: loss = 0.04980
Step 37930: loss = 0.05701
Step 37935: loss = 0.13396
Step 37940: loss = 0.03881
Step 37945: loss = 0.05402
Step 37950: loss = 0.03152
Step 37955: loss = 0.08036
Step 37960: loss = 0.06318
Step 37965: loss = 0.03130
Step 37970: loss = 0.04153
Step 37975: loss = 0.03584
Step 37980: loss = 0.04501
Step 37985: loss = 0.02696
Step 37990: loss = 0.07020
Step 37995: loss = 0.09655
Step 38000: loss = 0.07972
Step 38005: loss = 0.01387
Step 38010: loss = 0.09857
Step 38015: loss = 0.08979
Step 38020: loss = 0.07863
Step 38025: loss = 0.08929
Step 38030: loss = 0.03451
Step 38035: loss = 0.07061
Step 38040: loss = 0.06208
Step 38045: loss = 0.04254
Step 38050: loss = 0.07301
Step 38055: loss = 0.15372
Step 38060: loss = 0.04432
Step 38065: loss = 0.09067
Step 38070: loss = 0.09417
Step 38075: loss = 0.09009
Step 38080: loss = 0.11498
Step 38085: loss = 0.05527
Step 38090: loss = 0.03823
Step 38095: loss = 0.04343
Step 38100: loss = 0.09012
Step 38105: loss = 0.07335
Step 38110: loss = 0.13079
Step 38115: loss = 0.09225
Step 38120: loss = 0.05008
Step 38125: loss = 0.03802
Step 38130: loss = 0.07447
Step 38135: loss = 0.19231
Step 38140: loss = 0.10948
Step 38145: loss = 0.07888
Step 38150: loss = 0.05307
Step 38155: loss = 0.04164
Step 38160: loss = 0.05343
Step 38165: loss = 0.08824
Step 38170: loss = 0.12779
Step 38175: loss = 0.02935
Step 38180: loss = 0.03602
Step 38185: loss = 0.08052
Step 38190: loss = 0.10435
Step 38195: loss = 0.05026
Step 38200: loss = 0.02882
Step 38205: loss = 0.08754
Step 38210: loss = 0.10855
Step 38215: loss = 0.03551
Step 38220: loss = 0.07919
Training Data Eval:
  Num examples: 49920, Num correct: 48751, Precision @ 1: 0.9766
('Testing Data Eval: EPOCH->', 99)
  Num examples: 9984, Num correct: 7362, Precision @ 1: 0.7374
Step 38225: loss = 0.03247
Step 38230: loss = 0.03648
Step 38235: loss = 0.06635
Step 38240: loss = 0.05834
Step 38245: loss = 0.06777
Step 38250: loss = 0.03180
Step 38255: loss = 0.12823
Step 38260: loss = 0.02487
Step 38265: loss = 0.09123
Step 38270: loss = 0.03457
Step 38275: loss = 0.08076
Step 38280: loss = 0.06127
Step 38285: loss = 0.08343
Step 38290: loss = 0.09183
Step 38295: loss = 0.07609
Step 38300: loss = 0.09017
Step 38305: loss = 0.05958
Step 38310: loss = 0.11961
Step 38315: loss = 0.07342
Step 38320: loss = 0.05003
Step 38325: loss = 0.05619
Step 38330: loss = 0.10649
Step 38335: loss = 0.10235
Step 38340: loss = 0.04390
Step 38345: loss = 0.04112
Step 38350: loss = 0.06214
Step 38355: loss = 0.06766
Step 38360: loss = 0.12676
Step 38365: loss = 0.04738
Step 38370: loss = 0.15339
Step 38375: loss = 0.11501
Step 38380: loss = 0.07294
Step 38385: loss = 0.03227
Step 38390: loss = 0.04565
Step 38395: loss = 0.05076
Step 38400: loss = 0.11390
Step 38405: loss = 0.06249
Step 38410: loss = 0.24173
Step 38415: loss = 0.08371
Step 38420: loss = 0.08761
Step 38425: loss = 0.14095
Step 38430: loss = 0.04493
Step 38435: loss = 0.07849
Step 38440: loss = 0.19456
Step 38445: loss = 0.09728
Step 38450: loss = 0.10914
Step 38455: loss = 0.05935
Step 38460: loss = 0.14391
Step 38465: loss = 0.03019
Step 38470: loss = 0.05422
Step 38475: loss = 0.10622
Step 38480: loss = 0.11577
Step 38485: loss = 0.04300
Step 38490: loss = 0.09826
Step 38495: loss = 0.07622
Step 38500: loss = 0.05938
Step 38505: loss = 0.15148
Step 38510: loss = 0.04592
Step 38515: loss = 0.12355
Step 38520: loss = 0.02310
Step 38525: loss = 0.05497
Step 38530: loss = 0.06454
Step 38535: loss = 0.07462
Step 38540: loss = 0.04941
Step 38545: loss = 0.13111
Step 38550: loss = 0.05615
Step 38555: loss = 0.08792
Step 38560: loss = 0.09150
Step 38565: loss = 0.02136
Step 38570: loss = 0.11051
Step 38575: loss = 0.06212
Step 38580: loss = 0.15293
Step 38585: loss = 0.10760
Step 38590: loss = 0.03149
Step 38595: loss = 0.08159
Step 38600: loss = 0.02850
Step 38605: loss = 0.06126
Step 38610: loss = 0.02185
Training Data Eval:
  Num examples: 49920, Num correct: 48897, Precision @ 1: 0.9795
('Testing Data Eval: EPOCH->', 100)
  Num examples: 9984, Num correct: 7349, Precision @ 1: 0.7361
Step 38615: loss = 0.01954
Step 38620: loss = 0.02199
Step 38625: loss = 0.04119
Step 38630: loss = 0.04862
Step 38635: loss = 0.13468
Step 38640: loss = 0.07127
Step 38645: loss = 0.06022
Step 38650: loss = 0.07798
Step 38655: loss = 0.01963
Step 38660: loss = 0.04209
Step 38665: loss = 0.10162
Step 38670: loss = 0.07326
Step 38675: loss = 0.08073
Step 38680: loss = 0.07463
Step 38685: loss = 0.10327
Step 38690: loss = 0.02836
Step 38695: loss = 0.04332
Step 38700: loss = 0.13752
Step 38705: loss = 0.07784
Step 38710: loss = 0.13286
Step 38715: loss = 0.03809
Step 38720: loss = 0.09388
Step 38725: loss = 0.21257
Step 38730: loss = 0.12587
Step 38735: loss = 0.05367
Step 38740: loss = 0.06794
Step 38745: loss = 0.06475
Step 38750: loss = 0.04842
Step 38755: loss = 0.02241
Step 38760: loss = 0.05545
Step 38765: loss = 0.12576
Step 38770: loss = 0.02659
Step 38775: loss = 0.06101
Step 38780: loss = 0.07881
Step 38785: loss = 0.13308
Step 38790: loss = 0.02196
Step 38795: loss = 0.04689
Step 38800: loss = 0.14775
Step 38805: loss = 0.08510
Step 38810: loss = 0.05570
Step 38815: loss = 0.05927
Step 38820: loss = 0.18624
Step 38825: loss = 0.03600
Step 38830: loss = 0.09245
Step 38835: loss = 0.09658
Step 38840: loss = 0.02975
Step 38845: loss = 0.12691
Step 38850: loss = 0.07174
Step 38855: loss = 0.06861
Step 38860: loss = 0.05196
Step 38865: loss = 0.04786
Step 38870: loss = 0.10035
Step 38875: loss = 0.05140
Step 38880: loss = 0.09157
Step 38885: loss = 0.18113
Step 38890: loss = 0.06479
Step 38895: loss = 0.03558
Step 38900: loss = 0.04968
Step 38905: loss = 0.13318
Step 38910: loss = 0.01724
Step 38915: loss = 0.05851
Step 38920: loss = 0.05187
Step 38925: loss = 0.09262
Step 38930: loss = 0.07975
Step 38935: loss = 0.04417
Step 38940: loss = 0.05547
Step 38945: loss = 0.06853
Step 38950: loss = 0.08173
Step 38955: loss = 0.01809
Step 38960: loss = 0.09975
Step 38965: loss = 0.12461
Step 38970: loss = 0.05956
Step 38975: loss = 0.02887
Step 38980: loss = 0.05253
Step 38985: loss = 0.08145
Step 38990: loss = 0.04296
Step 38995: loss = 0.05444
Step 39000: loss = 0.11426
Training Data Eval:
  Num examples: 49920, Num correct: 48791, Precision @ 1: 0.9774
('Testing Data Eval: EPOCH->', 101)
  Num examples: 9984, Num correct: 7429, Precision @ 1: 0.7441
Step 39005: loss = 0.04222
Step 39010: loss = 0.05727
Step 39015: loss = 0.07249
Step 39020: loss = 0.03099
Step 39025: loss = 0.09307
Step 39030: loss = 0.07069
Step 39035: loss = 0.05514
Step 39040: loss = 0.08491
Step 39045: loss = 0.10367
Step 39050: loss = 0.02644
Step 39055: loss = 0.10337
Step 39060: loss = 0.02075
Step 39065: loss = 0.02449
Step 39070: loss = 0.05561
Step 39075: loss = 0.01393
Step 39080: loss = 0.04238
Step 39085: loss = 0.05308
Step 39090: loss = 0.09882
Step 39095: loss = 0.09074
Step 39100: loss = 0.06034
Step 39105: loss = 0.05355
Step 39110: loss = 0.05846
Step 39115: loss = 0.07174
Step 39120: loss = 0.07236
Step 39125: loss = 0.04519
Step 39130: loss = 0.06774
Step 39135: loss = 0.05275
Step 39140: loss = 0.02581
Step 39145: loss = 0.08031
Step 39150: loss = 0.10191
Step 39155: loss = 0.08385
Step 39160: loss = 0.05703
Step 39165: loss = 0.05508
Step 39170: loss = 0.03793
Step 39175: loss = 0.05473
Step 39180: loss = 0.04019
Step 39185: loss = 0.04915
Step 39190: loss = 0.11653
Step 39195: loss = 0.06553
Step 39200: loss = 0.08912
Step 39205: loss = 0.06682
Step 39210: loss = 0.12257
Step 39215: loss = 0.06808
Step 39220: loss = 0.08667
Step 39225: loss = 0.07044
Step 39230: loss = 0.03120
Step 39235: loss = 0.04596
Step 39240: loss = 0.05906
Step 39245: loss = 0.05438
Step 39250: loss = 0.05452
Step 39255: loss = 0.10136
Step 39260: loss = 0.03389
Step 39265: loss = 0.05840
Step 39270: loss = 0.04900
Step 39275: loss = 0.09156
Step 39280: loss = 0.10498
Step 39285: loss = 0.12094
Step 39290: loss = 0.05573
Step 39295: loss = 0.10126
Step 39300: loss = 0.04095
Step 39305: loss = 0.06111
Step 39310: loss = 0.08546
Step 39315: loss = 0.04888
Step 39320: loss = 0.03985
Step 39325: loss = 0.10112
Step 39330: loss = 0.13074
Step 39335: loss = 0.04573
Step 39340: loss = 0.09361
Step 39345: loss = 0.09278
Step 39350: loss = 0.04185
Step 39355: loss = 0.06290
Step 39360: loss = 0.04225
Step 39365: loss = 0.05771
Step 39370: loss = 0.10038
Step 39375: loss = 0.02893
Step 39380: loss = 0.04430
Step 39385: loss = 0.07986
Step 39390: loss = 0.09275
Training Data Eval:
  Num examples: 49920, Num correct: 48760, Precision @ 1: 0.9768
('Testing Data Eval: EPOCH->', 102)
  Num examples: 9984, Num correct: 7278, Precision @ 1: 0.7290
Step 39395: loss = 0.03669
Step 39400: loss = 0.04600
Step 39405: loss = 0.10004
Step 39410: loss = 0.03678
Step 39415: loss = 0.05639
Step 39420: loss = 0.04896
Step 39425: loss = 0.19374
Step 39430: loss = 0.04998
Step 39435: loss = 0.05122
Step 39440: loss = 0.03548
Step 39445: loss = 0.08751
Step 39450: loss = 0.07220
Step 39455: loss = 0.05852
Step 39460: loss = 0.06346
Step 39465: loss = 0.03743
Step 39470: loss = 0.09910
Step 39475: loss = 0.03933
Step 39480: loss = 0.03394
Step 39485: loss = 0.06299
Step 39490: loss = 0.04723
Step 39495: loss = 0.05627
Step 39500: loss = 0.02613
Step 39505: loss = 0.10269
Step 39510: loss = 0.06334
Step 39515: loss = 0.08896
Step 39520: loss = 0.01902
Step 39525: loss = 0.04582
Step 39530: loss = 0.02779
Step 39535: loss = 0.02677
Step 39540: loss = 0.06872
Step 39545: loss = 0.11672
Step 39550: loss = 0.14807
Step 39555: loss = 0.04223
Step 39560: loss = 0.02795
Step 39565: loss = 0.03845
Step 39570: loss = 0.06902
Step 39575: loss = 0.04973
Step 39580: loss = 0.05634
Step 39585: loss = 0.04126
Step 39590: loss = 0.01256
Step 39595: loss = 0.07609
Step 39600: loss = 0.14252
Step 39605: loss = 0.05756
Step 39610: loss = 0.03529
Step 39615: loss = 0.11880
Step 39620: loss = 0.11167
Step 39625: loss = 0.02201
Step 39630: loss = 0.15383
Step 39635: loss = 0.03185
Step 39640: loss = 0.03369
Step 39645: loss = 0.06019
Step 39650: loss = 0.05007
Step 39655: loss = 0.07670
Step 39660: loss = 0.06020
Step 39665: loss = 0.06715
Step 39670: loss = 0.09414
Step 39675: loss = 0.03264
Step 39680: loss = 0.09707
Step 39685: loss = 0.05193
Step 39690: loss = 0.07785
Step 39695: loss = 0.08461
Step 39700: loss = 0.02275
Step 39705: loss = 0.08668
Step 39710: loss = 0.02005
Step 39715: loss = 0.07116
Step 39720: loss = 0.08614
Step 39725: loss = 0.16161
Step 39730: loss = 0.12562
Step 39735: loss = 0.08751
Step 39740: loss = 0.06327
Step 39745: loss = 0.11464
Step 39750: loss = 0.04093
Step 39755: loss = 0.09923
Step 39760: loss = 0.11632
Step 39765: loss = 0.11290
Step 39770: loss = 0.04101
Step 39775: loss = 0.04546
Step 39780: loss = 0.03511
Training Data Eval:
  Num examples: 49920, Num correct: 48946, Precision @ 1: 0.9805
('Testing Data Eval: EPOCH->', 103)
  Num examples: 9984, Num correct: 7386, Precision @ 1: 0.7398
Step 39785: loss = 0.03677
Step 39790: loss = 0.03410
Step 39795: loss = 0.07645
Step 39800: loss = 0.05638
Step 39805: loss = 0.03992
Step 39810: loss = 0.06462
Step 39815: loss = 0.06769
Step 39820: loss = 0.07789
Step 39825: loss = 0.01434
Step 39830: loss = 0.02926
Step 39835: loss = 0.04076
Step 39840: loss = 0.05426
Step 39845: loss = 0.02741
Step 39850: loss = 0.06690
Step 39855: loss = 0.15081
Step 39860: loss = 0.05534
Step 39865: loss = 0.05746
Step 39870: loss = 0.03402
Step 39875: loss = 0.03547
Step 39880: loss = 0.09283
Step 39885: loss = 0.07268
Step 39890: loss = 0.08703
Step 39895: loss = 0.09557
Step 39900: loss = 0.13310
Step 39905: loss = 0.05346
Step 39910: loss = 0.11323
Step 39915: loss = 0.03918
Step 39920: loss = 0.04436
Step 39925: loss = 0.08765
Step 39930: loss = 0.23991
Step 39935: loss = 0.03751
Step 39940: loss = 0.12127
Step 39945: loss = 0.04981
Step 39950: loss = 0.10940
Step 39955: loss = 0.06835
Step 39960: loss = 0.12023
Step 39965: loss = 0.10059
Step 39970: loss = 0.04201
Step 39975: loss = 0.00952
Step 39980: loss = 0.04630
Step 39985: loss = 0.12886
Step 39990: loss = 0.05371
Step 39995: loss = 0.12811
Step 40000: loss = 0.07651
Step 40005: loss = 0.03580
Step 40010: loss = 0.05926
Step 40015: loss = 0.11537
Step 40020: loss = 0.04762
Step 40025: loss = 0.06952
Step 40030: loss = 0.05534
Step 40035: loss = 0.07694
Step 40040: loss = 0.03980
Step 40045: loss = 0.04543
Step 40050: loss = 0.08813
Step 40055: loss = 0.05225
Step 40060: loss = 0.10272
Step 40065: loss = 0.03118
Step 40070: loss = 0.02984
Step 40075: loss = 0.09572
Step 40080: loss = 0.05340
Step 40085: loss = 0.01255
Step 40090: loss = 0.07862
Step 40095: loss = 0.03261
Step 40100: loss = 0.06953
Step 40105: loss = 0.11579
Step 40110: loss = 0.04101
Step 40115: loss = 0.03295
Step 40120: loss = 0.10109
Step 40125: loss = 0.02166
Step 40130: loss = 0.05636
Step 40135: loss = 0.13906
Step 40140: loss = 0.05105
Step 40145: loss = 0.10648
Step 40150: loss = 0.04456
Step 40155: loss = 0.04422
Step 40160: loss = 0.04986
Step 40165: loss = 0.09305
Step 40170: loss = 0.05616
Training Data Eval:
  Num examples: 49920, Num correct: 48889, Precision @ 1: 0.9793
('Testing Data Eval: EPOCH->', 104)
  Num examples: 9984, Num correct: 7338, Precision @ 1: 0.7350
Step 40175: loss = 0.03806
Step 40180: loss = 0.04254
Step 40185: loss = 0.13754
Step 40190: loss = 0.07507
Step 40195: loss = 0.02640
Step 40200: loss = 0.04078
Step 40205: loss = 0.01734
Step 40210: loss = 0.05649
Step 40215: loss = 0.04651
Step 40220: loss = 0.05430
Step 40225: loss = 0.06591
Step 40230: loss = 0.04655
Step 40235: loss = 0.06698
Step 40240: loss = 0.01762
Step 40245: loss = 0.11494
Step 40250: loss = 0.07912
Step 40255: loss = 0.05297
Step 40260: loss = 0.06490
Step 40265: loss = 0.01125
Step 40270: loss = 0.15145
Step 40275: loss = 0.19163
Step 40280: loss = 0.09060
Step 40285: loss = 0.06525
Step 40290: loss = 0.11161
Step 40295: loss = 0.10488
Step 40300: loss = 0.12013
Step 40305: loss = 0.08849
Step 40310: loss = 0.05723
Step 40315: loss = 0.03092
Step 40320: loss = 0.07328
Step 40325: loss = 0.03557
Step 40330: loss = 0.11970
Step 40335: loss = 0.05597
Step 40340: loss = 0.07488
Step 40345: loss = 0.07575
Step 40350: loss = 0.04121
Step 40355: loss = 0.09895
Step 40360: loss = 0.12623
Step 40365: loss = 0.06308
Step 40370: loss = 0.07144
Step 40375: loss = 0.04879
Step 40380: loss = 0.07100
Step 40385: loss = 0.09240
Step 40390: loss = 0.09457
Step 40395: loss = 0.20608
Step 40400: loss = 0.07218
Step 40405: loss = 0.01156
Step 40410: loss = 0.10811
Step 40415: loss = 0.09348
Step 40420: loss = 0.05758
Step 40425: loss = 0.06902
Step 40430: loss = 0.08197
Step 40435: loss = 0.04768
Step 40440: loss = 0.04938
Step 40445: loss = 0.05753
Step 40450: loss = 0.09160
Step 40455: loss = 0.10864
Step 40460: loss = 0.11148
Step 40465: loss = 0.03152
Step 40470: loss = 0.09677
Step 40475: loss = 0.04666
Step 40480: loss = 0.08260
Step 40485: loss = 0.07564
Step 40490: loss = 0.01780
Step 40495: loss = 0.10068
Step 40500: loss = 0.04197
Step 40505: loss = 0.08209
Step 40510: loss = 0.09078
Step 40515: loss = 0.03592
Step 40520: loss = 0.11704
Step 40525: loss = 0.04625
Step 40530: loss = 0.12085
Step 40535: loss = 0.03695
Step 40540: loss = 0.03902
Step 40545: loss = 0.06180
Step 40550: loss = 0.11148
Step 40555: loss = 0.03112
Step 40560: loss = 0.09946
Training Data Eval:
  Num examples: 49920, Num correct: 48838, Precision @ 1: 0.9783
('Testing Data Eval: EPOCH->', 105)
  Num examples: 9984, Num correct: 7367, Precision @ 1: 0.7379
Step 40565: loss = 0.09552
Step 40570: loss = 0.02637
Step 40575: loss = 0.12465
Step 40580: loss = 0.04855
Step 40585: loss = 0.03636
Step 40590: loss = 0.06441
Step 40595: loss = 0.05128
Step 40600: loss = 0.04371
Step 40605: loss = 0.01655
Step 40610: loss = 0.01470
Step 40615: loss = 0.02198
Step 40620: loss = 0.06200
Step 40625: loss = 0.06872
Step 40630: loss = 0.07962
Step 40635: loss = 0.06980
Step 40640: loss = 0.05148
Step 40645: loss = 0.04861
Step 40650: loss = 0.01033
Step 40655: loss = 0.02593
Step 40660: loss = 0.03881
Step 40665: loss = 0.02656
Step 40670: loss = 0.07269
Step 40675: loss = 0.01303
Step 40680: loss = 0.06534
Step 40685: loss = 0.02853
Step 40690: loss = 0.03298
Step 40695: loss = 0.11573
Step 40700: loss = 0.04007
Step 40705: loss = 0.03920
Step 40710: loss = 0.07150
Step 40715: loss = 0.04638
Step 40720: loss = 0.06627
Step 40725: loss = 0.12004
Step 40730: loss = 0.08028
Step 40735: loss = 0.06105
Step 40740: loss = 0.01964
Step 40745: loss = 0.07817
Step 40750: loss = 0.06640
Step 40755: loss = 0.05759
Step 40760: loss = 0.06533
Step 40765: loss = 0.04823
Step 40770: loss = 0.08854
Step 40775: loss = 0.01040
Step 40780: loss = 0.04168
Step 40785: loss = 0.03413
Step 40790: loss = 0.11994
Step 40795: loss = 0.12327
Step 40800: loss = 0.06241
Step 40805: loss = 0.03720
Step 40810: loss = 0.07517
Step 40815: loss = 0.02564
Step 40820: loss = 0.04506
Step 40825: loss = 0.08002
Step 40830: loss = 0.02469
Step 40835: loss = 0.10214
Step 40840: loss = 0.06112
Step 40845: loss = 0.11220
Step 40850: loss = 0.02380
Step 40855: loss = 0.11496
Step 40860: loss = 0.08100
Step 40865: loss = 0.06234
Step 40870: loss = 0.05436
Step 40875: loss = 0.04005
Step 40880: loss = 0.07572
Step 40885: loss = 0.03416
Step 40890: loss = 0.11978
Step 40895: loss = 0.02325
Step 40900: loss = 0.06495
Step 40905: loss = 0.02173
Step 40910: loss = 0.11758
Step 40915: loss = 0.19563
Step 40920: loss = 0.04594
Step 40925: loss = 0.04150
Step 40930: loss = 0.11628
Step 40935: loss = 0.07058
Step 40940: loss = 0.08165
Step 40945: loss = 0.06103
Step 40950: loss = 0.06186
Training Data Eval:
  Num examples: 49920, Num correct: 48680, Precision @ 1: 0.9752
('Testing Data Eval: EPOCH->', 106)
  Num examples: 9984, Num correct: 7267, Precision @ 1: 0.7279
Step 40955: loss = 0.12696
Step 40960: loss = 0.04365
Step 40965: loss = 0.02119
Step 40970: loss = 0.02582
Step 40975: loss = 0.05493
Step 40980: loss = 0.06814
Step 40985: loss = 0.03568
Step 40990: loss = 0.07851
Step 40995: loss = 0.01104
Step 41000: loss = 0.02975
Step 41005: loss = 0.05946
Step 41010: loss = 0.11154
Step 41015: loss = 0.02994
Step 41020: loss = 0.04271
Step 41025: loss = 0.09604
Step 41030: loss = 0.01870
Step 41035: loss = 0.03836
Step 41040: loss = 0.07099
Step 41045: loss = 0.05533
Step 41050: loss = 0.06955
Step 41055: loss = 0.05233
Step 41060: loss = 0.11311
Step 41065: loss = 0.09642
Step 41070: loss = 0.06820
Step 41075: loss = 0.01189
Step 41080: loss = 0.05198
Step 41085: loss = 0.01966
Step 41090: loss = 0.07576
Step 41095: loss = 0.11352
Step 41100: loss = 0.08723
Step 41105: loss = 0.19606
Step 41110: loss = 0.09491
Step 41115: loss = 0.05410
Step 41120: loss = 0.06723
Step 41125: loss = 0.09174
Step 41130: loss = 0.04502
Step 41135: loss = 0.10339
Step 41140: loss = 0.12937
Step 41145: loss = 0.05666
Step 41150: loss = 0.04084
Step 41155: loss = 0.02096
Step 41160: loss = 0.05420
Step 41165: loss = 0.11681
Step 41170: loss = 0.03801
Step 41175: loss = 0.05778
Step 41180: loss = 0.05915
Step 41185: loss = 0.06122
Step 41190: loss = 0.01920
Step 41195: loss = 0.12942
Step 41200: loss = 0.04935
Step 41205: loss = 0.06665
Step 41210: loss = 0.04350
Step 41215: loss = 0.05018
Step 41220: loss = 0.02272
Step 41225: loss = 0.01855
Step 41230: loss = 0.02977
Step 41235: loss = 0.08411
Step 41240: loss = 0.07470
Step 41245: loss = 0.01313
Step 41250: loss = 0.08254
Step 41255: loss = 0.05363
Step 41260: loss = 0.07126
Step 41265: loss = 0.21824
Step 41270: loss = 0.11391
Step 41275: loss = 0.07110
Step 41280: loss = 0.07451
Step 41285: loss = 0.03239
Step 41290: loss = 0.10993
Step 41295: loss = 0.12710
Step 41300: loss = 0.06242
Step 41305: loss = 0.06611
Step 41310: loss = 0.01593
Step 41315: loss = 0.03577
Step 41320: loss = 0.06051
Step 41325: loss = 0.12433
Step 41330: loss = 0.08168
Step 41335: loss = 0.06236
Step 41340: loss = 0.09019
Training Data Eval:
  Num examples: 49920, Num correct: 48853, Precision @ 1: 0.9786
('Testing Data Eval: EPOCH->', 107)
  Num examples: 9984, Num correct: 7458, Precision @ 1: 0.7470
Step 41345: loss = 0.14484
Step 41350: loss = 0.12702
Step 41355: loss = 0.04328
Step 41360: loss = 0.08348
Step 41365: loss = 0.16315
Step 41370: loss = 0.04078
Step 41375: loss = 0.16189
Step 41380: loss = 0.04340
Step 41385: loss = 0.04515
Step 41390: loss = 0.07445
Step 41395: loss = 0.07177
Step 41400: loss = 0.08246
Step 41405: loss = 0.15739
Step 41410: loss = 0.02078
Step 41415: loss = 0.04229
Step 41420: loss = 0.09114
Step 41425: loss = 0.02920
Step 41430: loss = 0.05642
Step 41435: loss = 0.04790
Step 41440: loss = 0.12001
Step 41445: loss = 0.05852
Step 41450: loss = 0.05561
Step 41455: loss = 0.12406
Step 41460: loss = 0.06924
Step 41465: loss = 0.13969
Step 41470: loss = 0.09663
Step 41475: loss = 0.16846
Step 41480: loss = 0.13600
Step 41485: loss = 0.03960
Step 41490: loss = 0.04917
Step 41495: loss = 0.09365
Step 41500: loss = 0.07478
Step 41505: loss = 0.04105
Step 41510: loss = 0.07823
Step 41515: loss = 0.07085
Step 41520: loss = 0.08212
Step 41525: loss = 0.03557
Step 41530: loss = 0.03756
Step 41535: loss = 0.02828
Step 41540: loss = 0.05449
Step 41545: loss = 0.01633
Step 41550: loss = 0.06662
Step 41555: loss = 0.10092
Step 41560: loss = 0.05963
Step 41565: loss = 0.09317
Step 41570: loss = 0.09535
Step 41575: loss = 0.14781
Step 41580: loss = 0.14006
Step 41585: loss = 0.08144
Step 41590: loss = 0.08875
Step 41595: loss = 0.04835
Step 41600: loss = 0.02452
Step 41605: loss = 0.03414
Step 41610: loss = 0.03938
Step 41615: loss = 0.08715
Step 41620: loss = 0.06346
Step 41625: loss = 0.11505
Step 41630: loss = 0.01960
Step 41635: loss = 0.14017
Step 41640: loss = 0.12630
Step 41645: loss = 0.04290
Step 41650: loss = 0.08483
Step 41655: loss = 0.06707
Step 41660: loss = 0.04154
Step 41665: loss = 0.04835
Step 41670: loss = 0.03525
Step 41675: loss = 0.05790
Step 41680: loss = 0.06745
Step 41685: loss = 0.03867
Step 41690: loss = 0.07145
Step 41695: loss = 0.02020
Step 41700: loss = 0.10386
Step 41705: loss = 0.05314
Step 41710: loss = 0.03111
Step 41715: loss = 0.05655
Step 41720: loss = 0.04858
Step 41725: loss = 0.07171
Step 41730: loss = 0.03796
Training Data Eval:
  Num examples: 49920, Num correct: 48990, Precision @ 1: 0.9814
('Testing Data Eval: EPOCH->', 108)
  Num examples: 9984, Num correct: 7388, Precision @ 1: 0.7400
Step 41735: loss = 0.05818
Step 41740: loss = 0.09918
Step 41745: loss = 0.05575
Step 41750: loss = 0.04390
Step 41755: loss = 0.10322
Step 41760: loss = 0.15645
Step 41765: loss = 0.15906
Step 41770: loss = 0.06530
Step 41775: loss = 0.15472
Step 41780: loss = 0.07111
Step 41785: loss = 0.10233
Step 41790: loss = 0.05943
Step 41795: loss = 0.12938
Step 41800: loss = 0.06113
Step 41805: loss = 0.10958
Step 41810: loss = 0.06115
Step 41815: loss = 0.02670
Step 41820: loss = 0.07670
Step 41825: loss = 0.08120
Step 41830: loss = 0.05818
Step 41835: loss = 0.11192
Step 41840: loss = 0.15316
Step 41845: loss = 0.21597
Step 41850: loss = 0.05421
Step 41855: loss = 0.07325
Step 41860: loss = 0.05569
Step 41865: loss = 0.05181
Step 41870: loss = 0.10698
Step 41875: loss = 0.13646
Step 41880: loss = 0.06254
Step 41885: loss = 0.02537
Step 41890: loss = 0.01670
Step 41895: loss = 0.09613
Step 41900: loss = 0.07482
Step 41905: loss = 0.06004
Step 41910: loss = 0.16389
Step 41915: loss = 0.10345
Step 41920: loss = 0.04039
Step 41925: loss = 0.02520
Step 41930: loss = 0.06309
Step 41935: loss = 0.12938
Step 41940: loss = 0.03999
Step 41945: loss = 0.06624
Step 41950: loss = 0.01264
Step 41955: loss = 0.05335
Step 41960: loss = 0.06818
Step 41965: loss = 0.02102
Step 41970: loss = 0.03383
Step 41975: loss = 0.02073
Step 41980: loss = 0.02346
Step 41985: loss = 0.06224
Step 41990: loss = 0.07500
Step 41995: loss = 0.03736
Step 42000: loss = 0.05618
Step 42005: loss = 0.07053
Step 42010: loss = 0.03254
Step 42015: loss = 0.03365
Step 42020: loss = 0.03868
Step 42025: loss = 0.02892
Step 42030: loss = 0.02628
Step 42035: loss = 0.03260
Step 42040: loss = 0.05185
Step 42045: loss = 0.04323
Step 42050: loss = 0.05627
Step 42055: loss = 0.07663
Step 42060: loss = 0.04247
Step 42065: loss = 0.01568
Step 42070: loss = 0.17062
Step 42075: loss = 0.07104
Step 42080: loss = 0.02376
Step 42085: loss = 0.12446
Step 42090: loss = 0.07478
Step 42095: loss = 0.07356
Step 42100: loss = 0.06156
Step 42105: loss = 0.09235
Step 42110: loss = 0.05755
Step 42115: loss = 0.18149
Step 42120: loss = 0.03794
Training Data Eval:
  Num examples: 49920, Num correct: 48870, Precision @ 1: 0.9790
('Testing Data Eval: EPOCH->', 109)
  Num examples: 9984, Num correct: 7290, Precision @ 1: 0.7302
Step 42125: loss = 0.06113
Step 42130: loss = 0.06154
Step 42135: loss = 0.05094
Step 42140: loss = 0.06087
Step 42145: loss = 0.06233
Step 42150: loss = 0.02500
Step 42155: loss = 0.13911
Step 42160: loss = 0.12622
Step 42165: loss = 0.04673
Step 42170: loss = 0.07353
Step 42175: loss = 0.06304
Step 42180: loss = 0.06637
Step 42185: loss = 0.05787
Step 42190: loss = 0.05424
Step 42195: loss = 0.07253
Step 42200: loss = 0.02543
Step 42205: loss = 0.01679
Step 42210: loss = 0.05309
Step 42215: loss = 0.04147
Step 42220: loss = 0.03410
Step 42225: loss = 0.04567
Step 42230: loss = 0.02341
Step 42235: loss = 0.04460
Step 42240: loss = 0.03856
Step 42245: loss = 0.05751
Step 42250: loss = 0.15629
Step 42255: loss = 0.01999
Step 42260: loss = 0.01974
Step 42265: loss = 0.04732
Step 42270: loss = 0.02555
Step 42275: loss = 0.05661
Step 42280: loss = 0.08809
Step 42285: loss = 0.05808
Step 42290: loss = 0.04794
Step 42295: loss = 0.09262
Step 42300: loss = 0.03756
Step 42305: loss = 0.03943
Step 42310: loss = 0.09506
Step 42315: loss = 0.06234
Step 42320: loss = 0.01732
Step 42325: loss = 0.10452
Step 42330: loss = 0.05926
Step 42335: loss = 0.03039
Step 42340: loss = 0.10925
Step 42345: loss = 0.05439
Step 42350: loss = 0.05573
Step 42355: loss = 0.02409
Step 42360: loss = 0.09111
Step 42365: loss = 0.07325
Step 42370: loss = 0.07664
Step 42375: loss = 0.09695
Step 42380: loss = 0.01280
Step 42385: loss = 0.15799
Step 42390: loss = 0.06464
Step 42395: loss = 0.06219
Step 42400: loss = 0.04119
Step 42405: loss = 0.05554
Step 42410: loss = 0.04401
Step 42415: loss = 0.03393
Step 42420: loss = 0.02204
Step 42425: loss = 0.02345
Step 42430: loss = 0.02468
Step 42435: loss = 0.03042
Step 42440: loss = 0.05112
Step 42445: loss = 0.02667
Step 42450: loss = 0.14309
Step 42455: loss = 0.02470
Step 42460: loss = 0.06438
Step 42465: loss = 0.10055
Step 42470: loss = 0.12795
Step 42475: loss = 0.06259
Step 42480: loss = 0.08621
Step 42485: loss = 0.02611
Step 42490: loss = 0.10640
Step 42495: loss = 0.03546
Step 42500: loss = 0.04057
Step 42505: loss = 0.02197
Step 42510: loss = 0.02202
Training Data Eval:
  Num examples: 49920, Num correct: 49057, Precision @ 1: 0.9827
('Testing Data Eval: EPOCH->', 110)
  Num examples: 9984, Num correct: 7311, Precision @ 1: 0.7323
Step 42515: loss = 0.06552
Step 42520: loss = 0.02637
Step 42525: loss = 0.02690
Step 42530: loss = 0.01697
Step 42535: loss = 0.04697
Step 42540: loss = 0.05606
Step 42545: loss = 0.05397
Step 42550: loss = 0.01907
Step 42555: loss = 0.16867
Step 42560: loss = 0.05325
Step 42565: loss = 0.03959
Step 42570: loss = 0.07382
Step 42575: loss = 0.03983
Step 42580: loss = 0.04528
Step 42585: loss = 0.02205
Step 42590: loss = 0.04761
Step 42595: loss = 0.05123
Step 42600: loss = 0.05259
Step 42605: loss = 0.04800
Step 42610: loss = 0.07120
Step 42615: loss = 0.05974
Step 42620: loss = 0.03149
Step 42625: loss = 0.03318
Step 42630: loss = 0.06077
Step 42635: loss = 0.05411
Step 42640: loss = 0.06735
Step 42645: loss = 0.07383
Step 42650: loss = 0.01501
Step 42655: loss = 0.03491
Step 42660: loss = 0.06810
Step 42665: loss = 0.03589
Step 42670: loss = 0.07246
Step 42675: loss = 0.17853
Step 42680: loss = 0.10904
Step 42685: loss = 0.04091
Step 42690: loss = 0.08833
Step 42695: loss = 0.02931
Step 42700: loss = 0.06544
Step 42705: loss = 0.12893
Step 42710: loss = 0.05584
Step 42715: loss = 0.13579
Step 42720: loss = 0.06997
Step 42725: loss = 0.03879
Step 42730: loss = 0.05064
Step 42735: loss = 0.02418
Step 42740: loss = 0.04675
Step 42745: loss = 0.11328
Step 42750: loss = 0.03235
Step 42755: loss = 0.02509
Step 42760: loss = 0.10151
Step 42765: loss = 0.07919
Step 42770: loss = 0.02912
Step 42775: loss = 0.00591
Step 42780: loss = 0.10879
Step 42785: loss = 0.04930
Step 42790: loss = 0.03027
Step 42795: loss = 0.06326
Step 42800: loss = 0.09915
Step 42805: loss = 0.14759
Step 42810: loss = 0.02117
Step 42815: loss = 0.14629
Step 42820: loss = 0.09300
Step 42825: loss = 0.03939
Step 42830: loss = 0.05367
Step 42835: loss = 0.03664
Step 42840: loss = 0.09299
Step 42845: loss = 0.03991
Step 42850: loss = 0.02170
Step 42855: loss = 0.07091
Step 42860: loss = 0.04674
Step 42865: loss = 0.07655
Step 42870: loss = 0.06629
Step 42875: loss = 0.03088
Step 42880: loss = 0.10192
Step 42885: loss = 0.04131
Step 42890: loss = 0.04831
Step 42895: loss = 0.01715
Step 42900: loss = 0.03810
Training Data Eval:
  Num examples: 49920, Num correct: 48903, Precision @ 1: 0.9796
('Testing Data Eval: EPOCH->', 111)
  Num examples: 9984, Num correct: 7410, Precision @ 1: 0.7422
Step 42905: loss = 0.09837
Step 42910: loss = 0.02341
Step 42915: loss = 0.02372
Step 42920: loss = 0.04671
Step 42925: loss = 0.05636
Step 42930: loss = 0.02749
Step 42935: loss = 0.03972
Step 42940: loss = 0.06283
Step 42945: loss = 0.05700
Step 42950: loss = 0.08479
Step 42955: loss = 0.02910
Step 42960: loss = 0.02432
Step 42965: loss = 0.09632
Step 42970: loss = 0.05772
Step 42975: loss = 0.05683
Step 42980: loss = 0.13053
Step 42985: loss = 0.03370
Step 42990: loss = 0.18388
Step 42995: loss = 0.09756
Step 43000: loss = 0.06034
Step 43005: loss = 0.01457
Step 43010: loss = 0.10924
Step 43015: loss = 0.08258
Step 43020: loss = 0.05478
Step 43025: loss = 0.05342
Step 43030: loss = 0.13028
Step 43035: loss = 0.05062
Step 43040: loss = 0.07314
Step 43045: loss = 0.04984
Step 43050: loss = 0.02822
Step 43055: loss = 0.07502
Step 43060: loss = 0.04569
Step 43065: loss = 0.02164
Step 43070: loss = 0.04097
Step 43075: loss = 0.05714
Step 43080: loss = 0.05384
Step 43085: loss = 0.04573
Step 43090: loss = 0.07009
Step 43095: loss = 0.03465
Step 43100: loss = 0.05383
Step 43105: loss = 0.00753
Step 43110: loss = 0.09241
Step 43115: loss = 0.04911
Step 43120: loss = 0.04240
Step 43125: loss = 0.09245
Step 43130: loss = 0.13648
Step 43135: loss = 0.04571
Step 43140: loss = 0.11285
Step 43145: loss = 0.02998
Step 43150: loss = 0.04185
Step 43155: loss = 0.11614
Step 43160: loss = 0.03444
Step 43165: loss = 0.14023
Step 43170: loss = 0.05011
Step 43175: loss = 0.04724
Step 43180: loss = 0.01914
Step 43185: loss = 0.04228
Step 43190: loss = 0.05052
Step 43195: loss = 0.03418
Step 43200: loss = 0.08372
Step 43205: loss = 0.04433
Step 43210: loss = 0.07520
Step 43215: loss = 0.03298
Step 43220: loss = 0.04261
Step 43225: loss = 0.15014
Step 43230: loss = 0.05388
Step 43235: loss = 0.02628
Step 43240: loss = 0.04936
Step 43245: loss = 0.10218
Step 43250: loss = 0.08660
Step 43255: loss = 0.04154
Step 43260: loss = 0.01949
Step 43265: loss = 0.08814
Step 43270: loss = 0.03497
Step 43275: loss = 0.15540
Step 43280: loss = 0.04051
Step 43285: loss = 0.03034
Step 43290: loss = 0.04525
Training Data Eval:
  Num examples: 49920, Num correct: 48997, Precision @ 1: 0.9815
('Testing Data Eval: EPOCH->', 112)
  Num examples: 9984, Num correct: 7350, Precision @ 1: 0.7362
Step 43295: loss = 0.06445
Step 43300: loss = 0.00741
Step 43305: loss = 0.03202
Step 43310: loss = 0.05439
Step 43315: loss = 0.05420
Step 43320: loss = 0.09430
Step 43325: loss = 0.03015
Step 43330: loss = 0.06569
Step 43335: loss = 0.09872
Step 43340: loss = 0.04329
Step 43345: loss = 0.05674
Step 43350: loss = 0.05177
Step 43355: loss = 0.03267
Step 43360: loss = 0.01269
Step 43365: loss = 0.04745
Step 43370: loss = 0.07952
Step 43375: loss = 0.01112
Step 43380: loss = 0.02794
Step 43385: loss = 0.08516
Step 43390: loss = 0.04828
Step 43395: loss = 0.04731
Step 43400: loss = 0.04638
Step 43405: loss = 0.01227
Step 43410: loss = 0.04911
Step 43415: loss = 0.12266
Step 43420: loss = 0.04470
Step 43425: loss = 0.06692
Step 43430: loss = 0.07476
Step 43435: loss = 0.04048
Step 43440: loss = 0.10196
Step 43445: loss = 0.05456
Step 43450: loss = 0.08153
Step 43455: loss = 0.09510
Step 43460: loss = 0.04164
Step 43465: loss = 0.07470
Step 43470: loss = 0.02099
Step 43475: loss = 0.01524
Step 43480: loss = 0.01738
Step 43485: loss = 0.08635
Step 43490: loss = 0.05639
Step 43495: loss = 0.04499
Step 43500: loss = 0.05248
Step 43505: loss = 0.04394
Step 43510: loss = 0.03480
Step 43515: loss = 0.05822
Step 43520: loss = 0.07915
Step 43525: loss = 0.06219
Step 43530: loss = 0.05823
Step 43535: loss = 0.03942
Step 43540: loss = 0.05303
Step 43545: loss = 0.07990
Step 43550: loss = 0.11329
Step 43555: loss = 0.08151
Step 43560: loss = 0.05692
Step 43565: loss = 0.05138
Step 43570: loss = 0.08013
Step 43575: loss = 0.08841
Step 43580: loss = 0.11508
Step 43585: loss = 0.06740
Step 43590: loss = 0.07146
Step 43595: loss = 0.03338
Step 43600: loss = 0.07255
Step 43605: loss = 0.05393
Step 43610: loss = 0.02489
Step 43615: loss = 0.07018
Step 43620: loss = 0.03514
Step 43625: loss = 0.03127
Step 43630: loss = 0.12308
Step 43635: loss = 0.04527
Step 43640: loss = 0.09269
Step 43645: loss = 0.06947
Step 43650: loss = 0.09318
Step 43655: loss = 0.04047
Step 43660: loss = 0.04043
Step 43665: loss = 0.06201
Step 43670: loss = 0.07562
Step 43675: loss = 0.05799
Step 43680: loss = 0.03097
Training Data Eval:
  Num examples: 49920, Num correct: 48889, Precision @ 1: 0.9793
('Testing Data Eval: EPOCH->', 113)
  Num examples: 9984, Num correct: 7357, Precision @ 1: 0.7369
Step 43685: loss = 0.04633
Step 43690: loss = 0.07295
Step 43695: loss = 0.03195
Step 43700: loss = 0.01966
Step 43705: loss = 0.09053
Step 43710: loss = 0.10436
Step 43715: loss = 0.06017
Step 43720: loss = 0.04899
Step 43725: loss = 0.02958
Step 43730: loss = 0.03329
Step 43735: loss = 0.06435
Step 43740: loss = 0.10686
Step 43745: loss = 0.03973
Step 43750: loss = 0.03075
Step 43755: loss = 0.01959
Step 43760: loss = 0.05652
Step 43765: loss = 0.03449
Step 43770: loss = 0.12047
Step 43775: loss = 0.04944
Step 43780: loss = 0.08476
Step 43785: loss = 0.14508
Step 43790: loss = 0.11083
Step 43795: loss = 0.02780
Step 43800: loss = 0.05655
Step 43805: loss = 0.01714
Step 43810: loss = 0.01231
Step 43815: loss = 0.06275
Step 43820: loss = 0.07585
Step 43825: loss = 0.04474
Step 43830: loss = 0.08454
Step 43835: loss = 0.02261
Step 43840: loss = 0.10800
Step 43845: loss = 0.09937
Step 43850: loss = 0.11185
Step 43855: loss = 0.11721
Step 43860: loss = 0.08918
Step 43865: loss = 0.03388
Step 43870: loss = 0.02776
Step 43875: loss = 0.05594
Step 43880: loss = 0.01998
Step 43885: loss = 0.02819
Step 43890: loss = 0.01405
Step 43895: loss = 0.04935
Step 43900: loss = 0.03094
Step 43905: loss = 0.04831
Step 43910: loss = 0.04778
Step 43915: loss = 0.05339
Step 43920: loss = 0.04517
Step 43925: loss = 0.03111
Step 43930: loss = 0.04555
Step 43935: loss = 0.05679
Step 43940: loss = 0.04075
Step 43945: loss = 0.02023
Step 43950: loss = 0.02036
Step 43955: loss = 0.03328
Step 43960: loss = 0.06917
Step 43965: loss = 0.04260
Step 43970: loss = 0.02134
Step 43975: loss = 0.04683
Step 43980: loss = 0.09886
Step 43985: loss = 0.03159
Step 43990: loss = 0.10784
Step 43995: loss = 0.05715
Step 44000: loss = 0.07757
Step 44005: loss = 0.12750
Step 44010: loss = 0.06106
Step 44015: loss = 0.07310
Step 44020: loss = 0.07777
Step 44025: loss = 0.05211
Step 44030: loss = 0.04334
Step 44035: loss = 0.03891
Step 44040: loss = 0.03210
Step 44045: loss = 0.13236
Step 44050: loss = 0.03364
Step 44055: loss = 0.04175
Step 44060: loss = 0.06223
Step 44065: loss = 0.02222
Step 44070: loss = 0.00507
Training Data Eval:
  Num examples: 49920, Num correct: 49101, Precision @ 1: 0.9836
('Testing Data Eval: EPOCH->', 114)
  Num examples: 9984, Num correct: 7419, Precision @ 1: 0.7431
Step 44075: loss = 0.06049
Step 44080: loss = 0.03452
Step 44085: loss = 0.08265
Step 44090: loss = 0.06886
Step 44095: loss = 0.05827
Step 44100: loss = 0.02311
Step 44105: loss = 0.10380
Step 44110: loss = 0.03958
Step 44115: loss = 0.06024
Step 44120: loss = 0.05126
Step 44125: loss = 0.05051
Step 44130: loss = 0.01676
Step 44135: loss = 0.05191
Step 44140: loss = 0.05180
Step 44145: loss = 0.01320
Step 44150: loss = 0.01458
Step 44155: loss = 0.01473
Step 44160: loss = 0.06650
Step 44165: loss = 0.05774
Step 44170: loss = 0.04972
Step 44175: loss = 0.04160
Step 44180: loss = 0.07362
Step 44185: loss = 0.09463
Step 44190: loss = 0.04739
Step 44195: loss = 0.07780
Step 44200: loss = 0.03472
Step 44205: loss = 0.01143
Step 44210: loss = 0.00907
Step 44215: loss = 0.04526
Step 44220: loss = 0.01972
Step 44225: loss = 0.04333
Step 44230: loss = 0.08990
Step 44235: loss = 0.02093
Step 44240: loss = 0.05413
Step 44245: loss = 0.06575
Step 44250: loss = 0.12970
Step 44255: loss = 0.03356
Step 44260: loss = 0.12409
Step 44265: loss = 0.03254
Step 44270: loss = 0.04223
Step 44275: loss = 0.09275
Step 44280: loss = 0.02397
Step 44285: loss = 0.03810
Step 44290: loss = 0.13872
Step 44295: loss = 0.10540
Step 44300: loss = 0.07440
Step 44305: loss = 0.10417
Step 44310: loss = 0.04514
Step 44315: loss = 0.07362
Step 44320: loss = 0.12047
Step 44325: loss = 0.07158
Step 44330: loss = 0.16029
Step 44335: loss = 0.07437
Step 44340: loss = 0.08993
Step 44345: loss = 0.02982
Step 44350: loss = 0.01241
Step 44355: loss = 0.04116
Step 44360: loss = 0.04081
Step 44365: loss = 0.03119
Step 44370: loss = 0.03031
Step 44375: loss = 0.10243
Step 44380: loss = 0.06725
Step 44385: loss = 0.07223
Step 44390: loss = 0.01618
Step 44395: loss = 0.05013
Step 44400: loss = 0.09676
Step 44405: loss = 0.02115
Step 44410: loss = 0.07628
Step 44415: loss = 0.06743
Step 44420: loss = 0.04395
Step 44425: loss = 0.04242
Step 44430: loss = 0.04610
Step 44435: loss = 0.08107
Step 44440: loss = 0.06098
Step 44445: loss = 0.06703
Step 44450: loss = 0.10653
Step 44455: loss = 0.04320
Step 44460: loss = 0.13194
Training Data Eval:
  Num examples: 49920, Num correct: 48946, Precision @ 1: 0.9805
('Testing Data Eval: EPOCH->', 115)
  Num examples: 9984, Num correct: 7318, Precision @ 1: 0.7330
Step 44465: loss = 0.03231
Step 44470: loss = 0.04127
Step 44475: loss = 0.07387
Step 44480: loss = 0.03421
Step 44485: loss = 0.07681
Step 44490: loss = 0.05808
Step 44495: loss = 0.11747
Step 44500: loss = 0.13109
Step 44505: loss = 0.06988
Step 44510: loss = 0.08032
Step 44515: loss = 0.01892
Step 44520: loss = 0.05653
Step 44525: loss = 0.07555
Step 44530: loss = 0.02276
Step 44535: loss = 0.07245
Step 44540: loss = 0.10922
Step 44545: loss = 0.09713
Step 44550: loss = 0.02869
Step 44555: loss = 0.08427
Step 44560: loss = 0.06925
Step 44565: loss = 0.10218
Step 44570: loss = 0.03484
Step 44575: loss = 0.03065
Step 44580: loss = 0.07212
Step 44585: loss = 0.08564
Step 44590: loss = 0.02953
Step 44595: loss = 0.07941
Step 44600: loss = 0.06999
Step 44605: loss = 0.04198
Step 44610: loss = 0.00927
Step 44615: loss = 0.07376
Step 44620: loss = 0.01928
Step 44625: loss = 0.03700
Step 44630: loss = 0.08535
Step 44635: loss = 0.14681
Step 44640: loss = 0.08793
Step 44645: loss = 0.04843
Step 44650: loss = 0.05041
Step 44655: loss = 0.04509
Step 44660: loss = 0.01441
Step 44665: loss = 0.13933
Step 44670: loss = 0.08608
Step 44675: loss = 0.03849
Step 44680: loss = 0.04967
Step 44685: loss = 0.14349
Step 44690: loss = 0.03732
Step 44695: loss = 0.05270
Step 44700: loss = 0.09095
Step 44705: loss = 0.06525
Step 44710: loss = 0.09418
Step 44715: loss = 0.12792
Step 44720: loss = 0.03696
Step 44725: loss = 0.12206
Step 44730: loss = 0.20533
Step 44735: loss = 0.13053
Step 44740: loss = 0.03287
Step 44745: loss = 0.15316
Step 44750: loss = 0.05178
Step 44755: loss = 0.02617
Step 44760: loss = 0.14874
Step 44765: loss = 0.11280
Step 44770: loss = 0.04179
Step 44775: loss = 0.04730
Step 44780: loss = 0.06250
Step 44785: loss = 0.01138
Step 44790: loss = 0.04264
Step 44795: loss = 0.05995
Step 44800: loss = 0.07305
Step 44805: loss = 0.02046
Step 44810: loss = 0.08598
Step 44815: loss = 0.04128
Step 44820: loss = 0.06188
Step 44825: loss = 0.07318
Step 44830: loss = 0.08809
Step 44835: loss = 0.03352
Step 44840: loss = 0.10866
Step 44845: loss = 0.12808
Step 44850: loss = 0.03956
Training Data Eval:
  Num examples: 49920, Num correct: 48909, Precision @ 1: 0.9797
('Testing Data Eval: EPOCH->', 116)
  Num examples: 9984, Num correct: 7394, Precision @ 1: 0.7406
Step 44855: loss = 0.07931
Step 44860: loss = 0.03465
Step 44865: loss = 0.08253
Step 44870: loss = 0.07156
Step 44875: loss = 0.06293
Step 44880: loss = 0.10019
Step 44885: loss = 0.10533
Step 44890: loss = 0.02744
Step 44895: loss = 0.04889
Step 44900: loss = 0.05942
Step 44905: loss = 0.16225
Step 44910: loss = 0.06227
Step 44915: loss = 0.05167
Step 44920: loss = 0.05114
Step 44925: loss = 0.05215
Step 44930: loss = 0.14886
Step 44935: loss = 0.00646
Step 44940: loss = 0.04001
Step 44945: loss = 0.07722
Step 44950: loss = 0.06526
Step 44955: loss = 0.02943
Step 44960: loss = 0.01917
Step 44965: loss = 0.08146
Step 44970: loss = 0.03210
Step 44975: loss = 0.01750
Step 44980: loss = 0.05951
Step 44985: loss = 0.08495
Step 44990: loss = 0.03435
Step 44995: loss = 0.10133
Step 45000: loss = 0.11942
Step 45005: loss = 0.10673
Step 45010: loss = 0.06597
Step 45015: loss = 0.05668
Step 45020: loss = 0.06477
Step 45025: loss = 0.08743
Step 45030: loss = 0.02848
Step 45035: loss = 0.10414
Step 45040: loss = 0.12579
Step 45045: loss = 0.06118
Step 45050: loss = 0.03931
Step 45055: loss = 0.07503
Step 45060: loss = 0.07260
Step 45065: loss = 0.03465
Step 45070: loss = 0.06955
Step 45075: loss = 0.05415
Step 45080: loss = 0.01741
Step 45085: loss = 0.02191
Step 45090: loss = 0.02020
Step 45095: loss = 0.05736
Step 45100: loss = 0.02128
Step 45105: loss = 0.08874
Step 45110: loss = 0.06741
Step 45115: loss = 0.06686
Step 45120: loss = 0.11695
Step 45125: loss = 0.02209
Step 45130: loss = 0.09019
Step 45135: loss = 0.06898
Step 45140: loss = 0.12777
Step 45145: loss = 0.03812
Step 45150: loss = 0.04622
Step 45155: loss = 0.08353
Step 45160: loss = 0.06534
Step 45165: loss = 0.04277
Step 45170: loss = 0.03415
Step 45175: loss = 0.02660
Step 45180: loss = 0.00931
Step 45185: loss = 0.05529
Step 45190: loss = 0.06243
Step 45195: loss = 0.08146
Step 45200: loss = 0.15784
Step 45205: loss = 0.02489
Step 45210: loss = 0.04828
Step 45215: loss = 0.05232
Step 45220: loss = 0.09853
Step 45225: loss = 0.07765
Step 45230: loss = 0.04064
Step 45235: loss = 0.04745
Step 45240: loss = 0.07475
Training Data Eval:
  Num examples: 49920, Num correct: 49133, Precision @ 1: 0.9842
('Testing Data Eval: EPOCH->', 117)
  Num examples: 9984, Num correct: 7452, Precision @ 1: 0.7464
Step 45245: loss = 0.06999
Step 45250: loss = 0.05603
Step 45255: loss = 0.00796
Step 45260: loss = 0.16521
Step 45265: loss = 0.02573
Step 45270: loss = 0.05584
Step 45275: loss = 0.11134
Step 45280: loss = 0.06441
Step 45285: loss = 0.09797
Step 45290: loss = 0.08665
Step 45295: loss = 0.05423
Step 45300: loss = 0.10088
Step 45305: loss = 0.05240
Step 45310: loss = 0.01446
Step 45315: loss = 0.10506
Step 45320: loss = 0.10089
Step 45325: loss = 0.06690
Step 45330: loss = 0.06221
Step 45335: loss = 0.10083
Step 45340: loss = 0.08208
Step 45345: loss = 0.04608
Step 45350: loss = 0.04617
Step 45355: loss = 0.09367
Step 45360: loss = 0.05638
Step 45365: loss = 0.02283
Step 45370: loss = 0.03327
Step 45375: loss = 0.05067
Step 45380: loss = 0.12991
Step 45385: loss = 0.06381
Step 45390: loss = 0.05927
Step 45395: loss = 0.02018
Step 45400: loss = 0.04434
Step 45405: loss = 0.06284
Step 45410: loss = 0.02637
Step 45415: loss = 0.07695
Step 45420: loss = 0.01746
Step 45425: loss = 0.03776
Step 45430: loss = 0.03242
Step 45435: loss = 0.05989
Step 45440: loss = 0.09386
Step 45445: loss = 0.09415
Step 45450: loss = 0.09425
Step 45455: loss = 0.13985
Step 45460: loss = 0.05714
Step 45465: loss = 0.00941
Step 45470: loss = 0.14224
Step 45475: loss = 0.02477
Step 45480: loss = 0.09175
Step 45485: loss = 0.01942
Step 45490: loss = 0.01489
Step 45495: loss = 0.09333
Step 45500: loss = 0.12347
Step 45505: loss = 0.12554
Step 45510: loss = 0.08373
Step 45515: loss = 0.04198
Step 45520: loss = 0.06216
Step 45525: loss = 0.06726
Step 45530: loss = 0.07755
Step 45535: loss = 0.01023
Step 45540: loss = 0.06406
Step 45545: loss = 0.05823
Step 45550: loss = 0.05086
Step 45555: loss = 0.05835
Step 45560: loss = 0.00831
Step 45565: loss = 0.04133
Step 45570: loss = 0.03189
Step 45575: loss = 0.06283
Step 45580: loss = 0.01372
Step 45585: loss = 0.04727
Step 45590: loss = 0.03727
Step 45595: loss = 0.07448
Step 45600: loss = 0.08297
Step 45605: loss = 0.03521
Step 45610: loss = 0.10766
Step 45615: loss = 0.06007
Step 45620: loss = 0.05413
Step 45625: loss = 0.02422
Step 45630: loss = 0.03607
Training Data Eval:
  Num examples: 49920, Num correct: 49222, Precision @ 1: 0.9860
('Testing Data Eval: EPOCH->', 118)
  Num examples: 9984, Num correct: 7421, Precision @ 1: 0.7433
Step 45635: loss = 0.02720
Step 45640: loss = 0.03563
Step 45645: loss = 0.04393
Step 45650: loss = 0.04205
Step 45655: loss = 0.05796
Step 45660: loss = 0.06160
Step 45665: loss = 0.07394
Step 45670: loss = 0.04720
Step 45675: loss = 0.03916
Step 45680: loss = 0.09629
Step 45685: loss = 0.02530
Step 45690: loss = 0.10549
Step 45695: loss = 0.08466
Step 45700: loss = 0.06108
Step 45705: loss = 0.06608
Step 45710: loss = 0.13256
Step 45715: loss = 0.04410
Step 45720: loss = 0.02140
Step 45725: loss = 0.03993
Step 45730: loss = 0.02478
Step 45735: loss = 0.02917
Step 45740: loss = 0.04246
Step 45745: loss = 0.05516
Step 45750: loss = 0.02204
Step 45755: loss = 0.04934
Step 45760: loss = 0.14438
Step 45765: loss = 0.03166
Step 45770: loss = 0.10100
Step 45775: loss = 0.06474
Step 45780: loss = 0.05407
Step 45785: loss = 0.07252
Step 45790: loss = 0.03973
Step 45795: loss = 0.05950
Step 45800: loss = 0.04569
Step 45805: loss = 0.07415
Step 45810: loss = 0.08684
Step 45815: loss = 0.07212
Step 45820: loss = 0.09383
Step 45825: loss = 0.02925
Step 45830: loss = 0.03181
Step 45835: loss = 0.03496
Step 45840: loss = 0.04087
Step 45845: loss = 0.01465
Step 45850: loss = 0.09845
Step 45855: loss = 0.01544
Step 45860: loss = 0.10133
Step 45865: loss = 0.09934
Step 45870: loss = 0.10021
Step 45875: loss = 0.02640
Step 45880: loss = 0.04271
Step 45885: loss = 0.03927
Step 45890: loss = 0.01755
Step 45895: loss = 0.02944
Step 45900: loss = 0.11288
Step 45905: loss = 0.02563
Step 45910: loss = 0.02243
Step 45915: loss = 0.01882
Step 45920: loss = 0.01332
Step 45925: loss = 0.10756
Step 45930: loss = 0.03328
Step 45935: loss = 0.05060
Step 45940: loss = 0.11160
Step 45945: loss = 0.15163
Step 45950: loss = 0.05481
Step 45955: loss = 0.05464
Step 45960: loss = 0.02183
Step 45965: loss = 0.05924
Step 45970: loss = 0.06976
Step 45975: loss = 0.05670
Step 45980: loss = 0.11263
Step 45985: loss = 0.03992
Step 45990: loss = 0.04237
Step 45995: loss = 0.04705
Step 46000: loss = 0.07300
Step 46005: loss = 0.02846
Step 46010: loss = 0.07620
Step 46015: loss = 0.09645
Step 46020: loss = 0.07844
Training Data Eval:
  Num examples: 49920, Num correct: 49046, Precision @ 1: 0.9825
('Testing Data Eval: EPOCH->', 119)
  Num examples: 9984, Num correct: 7395, Precision @ 1: 0.7407
Step 46025: loss = 0.10493
Step 46030: loss = 0.09244
Step 46035: loss = 0.01648
Step 46040: loss = 0.08539
Step 46045: loss = 0.02098
Step 46050: loss = 0.01819
Step 46055: loss = 0.13733
Step 46060: loss = 0.08500
Step 46065: loss = 0.14662
Step 46070: loss = 0.08678
Step 46075: loss = 0.05134
Step 46080: loss = 0.08737
Step 46085: loss = 0.08842
Step 46090: loss = 0.06495
Step 46095: loss = 0.06733
Step 46100: loss = 0.03177
Step 46105: loss = 0.04853
Step 46110: loss = 0.09533
Step 46115: loss = 0.07235
Step 46120: loss = 0.05270
Step 46125: loss = 0.05765
Step 46130: loss = 0.08321
Step 46135: loss = 0.14090
Step 46140: loss = 0.03742
Step 46145: loss = 0.05114
Step 46150: loss = 0.03435
Step 46155: loss = 0.10073
Step 46160: loss = 0.06660
Step 46165: loss = 0.03949
Step 46170: loss = 0.06480
Step 46175: loss = 0.02954
Step 46180: loss = 0.04100
Step 46185: loss = 0.05836
Step 46190: loss = 0.03338
Step 46195: loss = 0.04735
Step 46200: loss = 0.06026
Step 46205: loss = 0.31410
Step 46210: loss = 0.06122
Step 46215: loss = 0.04304
Step 46220: loss = 0.08912
Step 46225: loss = 0.13270
Step 46230: loss = 0.09798
Step 46235: loss = 0.12534
Step 46240: loss = 0.04005
Step 46245: loss = 0.03388
Step 46250: loss = 0.07816
Step 46255: loss = 0.08696
Step 46260: loss = 0.11708
Step 46265: loss = 0.04390
Step 46270: loss = 0.06098
Step 46275: loss = 0.05267
Step 46280: loss = 0.08041
Step 46285: loss = 0.04430
Step 46290: loss = 0.03961
Step 46295: loss = 0.04234
Step 46300: loss = 0.06826
Step 46305: loss = 0.02567
Step 46310: loss = 0.02879
Step 46315: loss = 0.04545
Step 46320: loss = 0.05677
Step 46325: loss = 0.01575
Step 46330: loss = 0.07548
Step 46335: loss = 0.15750
Step 46340: loss = 0.04323
Step 46345: loss = 0.02678
Step 46350: loss = 0.03559
Step 46355: loss = 0.12397
Step 46360: loss = 0.06256
Step 46365: loss = 0.03677
Step 46370: loss = 0.01294
Step 46375: loss = 0.08807
Step 46380: loss = 0.05831
Step 46385: loss = 0.01366
Step 46390: loss = 0.10739
Step 46395: loss = 0.04784
Step 46400: loss = 0.04436
Step 46405: loss = 0.09440
Step 46410: loss = 0.02099
Training Data Eval:
  Num examples: 49920, Num correct: 49036, Precision @ 1: 0.9823
('Testing Data Eval: EPOCH->', 120)
  Num examples: 9984, Num correct: 7437, Precision @ 1: 0.7449
Step 46415: loss = 0.03304
Step 46420: loss = 0.04774
Step 46425: loss = 0.01446
Step 46430: loss = 0.07879
Step 46435: loss = 0.05242
Step 46440: loss = 0.02677
Step 46445: loss = 0.04094
Step 46450: loss = 0.11151
Step 46455: loss = 0.02925
Step 46460: loss = 0.10808
Step 46465: loss = 0.13503
Step 46470: loss = 0.04966
Step 46475: loss = 0.06734
Step 46480: loss = 0.02288
Step 46485: loss = 0.01404
Step 46490: loss = 0.07542
Step 46495: loss = 0.06244
Step 46500: loss = 0.04224
Step 46505: loss = 0.07140
Step 46510: loss = 0.05459
Step 46515: loss = 0.03994
Step 46520: loss = 0.04423
Step 46525: loss = 0.05876
Step 46530: loss = 0.04143
Step 46535: loss = 0.05928
Step 46540: loss = 0.07065
Step 46545: loss = 0.12563
Step 46550: loss = 0.01409
Step 46555: loss = 0.03233
Step 46560: loss = 0.03895
Step 46565: loss = 0.03493
Step 46570: loss = 0.02184
Step 46575: loss = 0.14279
Step 46580: loss = 0.08324
Step 46585: loss = 0.11028
Step 46590: loss = 0.03373
Step 46595: loss = 0.03639
Step 46600: loss = 0.06089
Step 46605: loss = 0.05114
Step 46610: loss = 0.06535
Step 46615: loss = 0.05692
Step 46620: loss = 0.03769
Step 46625: loss = 0.09655
Step 46630: loss = 0.07512
Step 46635: loss = 0.03519
Step 46640: loss = 0.03725
Step 46645: loss = 0.06016
Step 46650: loss = 0.13475
Step 46655: loss = 0.01257
Step 46660: loss = 0.02473
Step 46665: loss = 0.06719
Step 46670: loss = 0.03389
Step 46675: loss = 0.03009
Step 46680: loss = 0.10654
Step 46685: loss = 0.05566
Step 46690: loss = 0.01853
Step 46695: loss = 0.05946
Step 46700: loss = 0.11047
Step 46705: loss = 0.01361
Step 46710: loss = 0.10951
Step 46715: loss = 0.09060
Step 46720: loss = 0.05403
Step 46725: loss = 0.05760
Step 46730: loss = 0.10731
Step 46735: loss = 0.09297
Step 46740: loss = 0.01750
Step 46745: loss = 0.06167
Step 46750: loss = 0.09484
Step 46755: loss = 0.02451
Step 46760: loss = 0.14561
Step 46765: loss = 0.03696
Step 46770: loss = 0.06638
Step 46775: loss = 0.09522
Step 46780: loss = 0.04348
Step 46785: loss = 0.04443
Step 46790: loss = 0.00687
Step 46795: loss = 0.06063
Step 46800: loss = 0.05520
Training Data Eval:
  Num examples: 49920, Num correct: 49108, Precision @ 1: 0.9837
('Testing Data Eval: EPOCH->', 121)
  Num examples: 9984, Num correct: 7414, Precision @ 1: 0.7426
Step 46805: loss = 0.06996
Step 46810: loss = 0.01596
Step 46815: loss = 0.05431
Step 46820: loss = 0.03791
Step 46825: loss = 0.02716
Step 46830: loss = 0.04228
Step 46835: loss = 0.06272
Step 46840: loss = 0.03931
Step 46845: loss = 0.06284
Step 46850: loss = 0.03947
Step 46855: loss = 0.05230
Step 46860: loss = 0.11186
Step 46865: loss = 0.05622
Step 46870: loss = 0.02629
Step 46875: loss = 0.04435
Step 46880: loss = 0.02541
Step 46885: loss = 0.03871
Step 46890: loss = 0.08227
Step 46895: loss = 0.04108
Step 46900: loss = 0.04159
Step 46905: loss = 0.07607
Step 46910: loss = 0.01767
Step 46915: loss = 0.05348
Step 46920: loss = 0.07356
Step 46925: loss = 0.01085
Step 46930: loss = 0.06504
Step 46935: loss = 0.08410
Step 46940: loss = 0.04877
Step 46945: loss = 0.06375
Step 46950: loss = 0.08018
Step 46955: loss = 0.09231
Step 46960: loss = 0.05624
Step 46965: loss = 0.02861
Step 46970: loss = 0.03250
Step 46975: loss = 0.09998
Step 46980: loss = 0.18222
Step 46985: loss = 0.05835
Step 46990: loss = 0.05678
Step 46995: loss = 0.18516
Step 47000: loss = 0.11882
Step 47005: loss = 0.06416
Step 47010: loss = 0.08781
Step 47015: loss = 0.04226
Step 47020: loss = 0.10112
Step 47025: loss = 0.17665
Step 47030: loss = 0.05127
Step 47035: loss = 0.04245
Step 47040: loss = 0.03010
Step 47045: loss = 0.03949
Step 47050: loss = 0.08318
Step 47055: loss = 0.03859
Step 47060: loss = 0.03077
Step 47065: loss = 0.05948
Step 47070: loss = 0.02716
Step 47075: loss = 0.07671
Step 47080: loss = 0.04544
Step 47085: loss = 0.02143
Step 47090: loss = 0.11212
Step 47095: loss = 0.04626
Step 47100: loss = 0.07362
Step 47105: loss = 0.07114
Step 47110: loss = 0.06791
Step 47115: loss = 0.01214
Step 47120: loss = 0.09098
Step 47125: loss = 0.08140
Step 47130: loss = 0.07335
Step 47135: loss = 0.10687
Step 47140: loss = 0.05051
Step 47145: loss = 0.09998
Step 47150: loss = 0.08149
Step 47155: loss = 0.12781
Step 47160: loss = 0.05911
Step 47165: loss = 0.05063
Step 47170: loss = 0.07824
Step 47175: loss = 0.03176
Step 47180: loss = 0.08597
Step 47185: loss = 0.08871
Step 47190: loss = 0.08672
Training Data Eval:
  Num examples: 49920, Num correct: 49134, Precision @ 1: 0.9843
('Testing Data Eval: EPOCH->', 122)
  Num examples: 9984, Num correct: 7427, Precision @ 1: 0.7439
Step 47195: loss = 0.01898
Step 47200: loss = 0.04858
Step 47205: loss = 0.10235
Step 47210: loss = 0.02218
Step 47215: loss = 0.01659
Step 47220: loss = 0.19469
Step 47225: loss = 0.10583
Step 47230: loss = 0.04702
Step 47235: loss = 0.05103
Step 47240: loss = 0.09343
Step 47245: loss = 0.06886
Step 47250: loss = 0.03083
Step 47255: loss = 0.02871
Step 47260: loss = 0.07100
Step 47265: loss = 0.08694
Step 47270: loss = 0.01030
Step 47275: loss = 0.05509
Step 47280: loss = 0.02918
Step 47285: loss = 0.04831
Step 47290: loss = 0.02270
Step 47295: loss = 0.08577
Step 47300: loss = 0.04152
Step 47305: loss = 0.02870
Step 47310: loss = 0.02555
Step 47315: loss = 0.13836
Step 47320: loss = 0.04366
Step 47325: loss = 0.06881
Step 47330: loss = 0.03955
Step 47335: loss = 0.06941
Step 47340: loss = 0.05753
Step 47345: loss = 0.04818
Step 47350: loss = 0.02251
Step 47355: loss = 0.06889
Step 47360: loss = 0.06358
Step 47365: loss = 0.01694
Step 47370: loss = 0.02576
Step 47375: loss = 0.02813
Step 47380: loss = 0.08010
Step 47385: loss = 0.08248
Step 47390: loss = 0.11146
Step 47395: loss = 0.07538
Step 47400: loss = 0.04263
Step 47405: loss = 0.06277
Step 47410: loss = 0.04967
Step 47415: loss = 0.08953
Step 47420: loss = 0.06912
Step 47425: loss = 0.04280
Step 47430: loss = 0.12221
Step 47435: loss = 0.08861
Step 47440: loss = 0.06201
Step 47445: loss = 0.02161
Step 47450: loss = 0.02066
Step 47455: loss = 0.04795
Step 47460: loss = 0.03731
Step 47465: loss = 0.05630
Step 47470: loss = 0.10289
Step 47475: loss = 0.03489
Step 47480: loss = 0.06392
Step 47485: loss = 0.04883
Step 47490: loss = 0.03189
Step 47495: loss = 0.09648
Step 47500: loss = 0.05804
Step 47505: loss = 0.12444
Step 47510: loss = 0.02363
Step 47515: loss = 0.02461
Step 47520: loss = 0.02802
Step 47525: loss = 0.04118
Step 47530: loss = 0.04530
Step 47535: loss = 0.05067
Step 47540: loss = 0.07797
Step 47545: loss = 0.11671
Step 47550: loss = 0.04802
Step 47555: loss = 0.10963
Step 47560: loss = 0.09466
Step 47565: loss = 0.06354
Step 47570: loss = 0.05954
Step 47575: loss = 0.07920
Step 47580: loss = 0.04850
Training Data Eval:
  Num examples: 49920, Num correct: 49116, Precision @ 1: 0.9839
('Testing Data Eval: EPOCH->', 123)
  Num examples: 9984, Num correct: 7517, Precision @ 1: 0.7529
Step 47585: loss = 0.05739
Step 47590: loss = 0.02787
Step 47595: loss = 0.05137
Step 47600: loss = 0.03103
Step 47605: loss = 0.04225
Step 47610: loss = 0.02359
Step 47615: loss = 0.08898
Step 47620: loss = 0.06256
Step 47625: loss = 0.06070
Step 47630: loss = 0.02850
Step 47635: loss = 0.05066
Step 47640: loss = 0.01466
Step 47645: loss = 0.09742
Step 47650: loss = 0.01915
Step 47655: loss = 0.05347
Step 47660: loss = 0.01115
Step 47665: loss = 0.00933
Step 47670: loss = 0.02784
Step 47675: loss = 0.05268
Step 47680: loss = 0.04334
Step 47685: loss = 0.05610
Step 47690: loss = 0.05124
Step 47695: loss = 0.03592
Step 47700: loss = 0.10223
Step 47705: loss = 0.03744
Step 47710: loss = 0.09238
Step 47715: loss = 0.03596
Step 47720: loss = 0.12414
Step 47725: loss = 0.01419
Step 47730: loss = 0.05908
Step 47735: loss = 0.02839
Step 47740: loss = 0.13131
Step 47745: loss = 0.04628
Step 47750: loss = 0.05906
Step 47755: loss = 0.03475
Step 47760: loss = 0.04318
Step 47765: loss = 0.01285
Step 47770: loss = 0.06326
Step 47775: loss = 0.04455
Step 47780: loss = 0.03249
Step 47785: loss = 0.01860
Step 47790: loss = 0.07976
Step 47795: loss = 0.06596
Step 47800: loss = 0.09529
Step 47805: loss = 0.07443
Step 47810: loss = 0.07845
Step 47815: loss = 0.03008
Step 47820: loss = 0.08460
Step 47825: loss = 0.02169
Step 47830: loss = 0.02995
Step 47835: loss = 0.06240
Step 47840: loss = 0.05157
Step 47845: loss = 0.03875
Step 47850: loss = 0.05197
Step 47855: loss = 0.05024
Step 47860: loss = 0.03761
Step 47865: loss = 0.10193
Step 47870: loss = 0.03487
Step 47875: loss = 0.07922
Step 47880: loss = 0.03274
Step 47885: loss = 0.08254
Step 47890: loss = 0.01847
Step 47895: loss = 0.04651
Step 47900: loss = 0.01033
Step 47905: loss = 0.06826
Step 47910: loss = 0.02941
Step 47915: loss = 0.05482
Step 47920: loss = 0.01370
Step 47925: loss = 0.04155
Step 47930: loss = 0.08670
Step 47935: loss = 0.01027
Step 47940: loss = 0.07752
Step 47945: loss = 0.03562
Step 47950: loss = 0.07888
Step 47955: loss = 0.06365
Step 47960: loss = 0.08390
Step 47965: loss = 0.02106
Step 47970: loss = 0.03784
Training Data Eval:
  Num examples: 49920, Num correct: 49171, Precision @ 1: 0.9850
('Testing Data Eval: EPOCH->', 124)
  Num examples: 9984, Num correct: 7392, Precision @ 1: 0.7404
Step 47975: loss = 0.14025
Step 47980: loss = 0.06701
Step 47985: loss = 0.13935
Step 47990: loss = 0.06931
Step 47995: loss = 0.04748
Step 48000: loss = 0.05011
Step 48005: loss = 0.02824
Step 48010: loss = 0.04851
Step 48015: loss = 0.02129
Step 48020: loss = 0.02317
Step 48025: loss = 0.05485
Step 48030: loss = 0.04093
Step 48035: loss = 0.06773
Step 48040: loss = 0.10054
Step 48045: loss = 0.03451
Step 48050: loss = 0.04702
Step 48055: loss = 0.00986
Step 48060: loss = 0.05319
Step 48065: loss = 0.06041
Step 48070: loss = 0.05644
Step 48075: loss = 0.01927
Step 48080: loss = 0.03571
Step 48085: loss = 0.06192
Step 48090: loss = 0.01840
Step 48095: loss = 0.04420
Step 48100: loss = 0.10416
Step 48105: loss = 0.10177
Step 48110: loss = 0.02643
Step 48115: loss = 0.05287
Step 48120: loss = 0.10255
Step 48125: loss = 0.08202
Step 48130: loss = 0.06555
Step 48135: loss = 0.08737
Step 48140: loss = 0.03138
Step 48145: loss = 0.06437
Step 48150: loss = 0.07648
Step 48155: loss = 0.13034
Step 48160: loss = 0.20794
Step 48165: loss = 0.07716
Step 48170: loss = 0.08242
Step 48175: loss = 0.05480
Step 48180: loss = 0.09765
Step 48185: loss = 0.05049
Step 48190: loss = 0.14417
Step 48195: loss = 0.05137
Step 48200: loss = 0.11986
Step 48205: loss = 0.03921
Step 48210: loss = 0.03101
Step 48215: loss = 0.02322
Step 48220: loss = 0.13270
Step 48225: loss = 0.08729
Step 48230: loss = 0.03841
Step 48235: loss = 0.04117
Step 48240: loss = 0.02140
Step 48245: loss = 0.04460
Step 48250: loss = 0.04294
Step 48255: loss = 0.06318
Step 48260: loss = 0.08763
Step 48265: loss = 0.03569
Step 48270: loss = 0.02756
Step 48275: loss = 0.12249
Step 48280: loss = 0.01543
Step 48285: loss = 0.05012
Step 48290: loss = 0.04457
Step 48295: loss = 0.01677
Step 48300: loss = 0.12337
Step 48305: loss = 0.01717
Step 48310: loss = 0.03443
Step 48315: loss = 0.04894
Step 48320: loss = 0.04131
Step 48325: loss = 0.06020
Step 48330: loss = 0.02414
Step 48335: loss = 0.04545
Step 48340: loss = 0.03195
Step 48345: loss = 0.09777
Step 48350: loss = 0.01598
Step 48355: loss = 0.02542
Step 48360: loss = 0.05411
Training Data Eval:
  Num examples: 49920, Num correct: 48927, Precision @ 1: 0.9801
('Testing Data Eval: EPOCH->', 125)
  Num examples: 9984, Num correct: 7405, Precision @ 1: 0.7417
Step 48365: loss = 0.02757
Step 48370: loss = 0.03580
Step 48375: loss = 0.02357
Step 48380: loss = 0.13183
Step 48385: loss = 0.05460
Step 48390: loss = 0.04411
Step 48395: loss = 0.03454
Step 48400: loss = 0.03910
Step 48405: loss = 0.08053
Step 48410: loss = 0.04321
Step 48415: loss = 0.05406
Step 48420: loss = 0.02514
Step 48425: loss = 0.04567
Step 48430: loss = 0.05818
Step 48435: loss = 0.03846
Step 48440: loss = 0.04605
Step 48445: loss = 0.09751
Step 48450: loss = 0.05647
Step 48455: loss = 0.02903
Step 48460: loss = 0.03006
Step 48465: loss = 0.10947
Step 48470: loss = 0.08409
Step 48475: loss = 0.02612
Step 48480: loss = 0.04131
Step 48485: loss = 0.04380
Step 48490: loss = 0.07282
Step 48495: loss = 0.06073
Step 48500: loss = 0.06182
Step 48505: loss = 0.05669
Step 48510: loss = 0.05370
Step 48515: loss = 0.02761
Step 48520: loss = 0.06821
Step 48525: loss = 0.13524
Step 48530: loss = 0.13943
Step 48535: loss = 0.10111
Step 48540: loss = 0.10152
Step 48545: loss = 0.04438
Step 48550: loss = 0.04919
Step 48555: loss = 0.05105
Step 48560: loss = 0.06634
Step 48565: loss = 0.07925
Step 48570: loss = 0.01248
Step 48575: loss = 0.03526
Step 48580: loss = 0.02883
Step 48585: loss = 0.14544
Step 48590: loss = 0.03727
Step 48595: loss = 0.04513
Step 48600: loss = 0.09356
Step 48605: loss = 0.04476
Step 48610: loss = 0.02674
Step 48615: loss = 0.04270
Step 48620: loss = 0.03099
Step 48625: loss = 0.05499
Step 48630: loss = 0.01359
Step 48635: loss = 0.03721
Step 48640: loss = 0.05826
Step 48645: loss = 0.03994
Step 48650: loss = 0.07332
Step 48655: loss = 0.05122
Step 48660: loss = 0.06446
Step 48665: loss = 0.04652
Step 48670: loss = 0.02496
Step 48675: loss = 0.07952
Step 48680: loss = 0.02514
Step 48685: loss = 0.01797
Step 48690: loss = 0.03657
Step 48695: loss = 0.09801
Step 48700: loss = 0.02650
Step 48705: loss = 0.06314
Step 48710: loss = 0.03834
Step 48715: loss = 0.01934
Step 48720: loss = 0.06023
Step 48725: loss = 0.03289
Step 48730: loss = 0.03323
Step 48735: loss = 0.04053
Step 48740: loss = 0.03367
Step 48745: loss = 0.06950
Step 48750: loss = 0.13789
Training Data Eval:
  Num examples: 49920, Num correct: 49122, Precision @ 1: 0.9840
('Testing Data Eval: EPOCH->', 126)
  Num examples: 9984, Num correct: 7289, Precision @ 1: 0.7301
Step 48755: loss = 0.06853
Step 48760: loss = 0.08146
Step 48765: loss = 0.02085
Step 48770: loss = 0.03214
Step 48775: loss = 0.04202
Step 48780: loss = 0.05155
Step 48785: loss = 0.03532
Step 48790: loss = 0.05575
Step 48795: loss = 0.04022
Step 48800: loss = 0.01670
Step 48805: loss = 0.08694
Step 48810: loss = 0.02779
Step 48815: loss = 0.10619
Step 48820: loss = 0.04912
Step 48825: loss = 0.05671
Step 48830: loss = 0.06871
Step 48835: loss = 0.02982
Step 48840: loss = 0.02952
Step 48845: loss = 0.05550
Step 48850: loss = 0.05894
Step 48855: loss = 0.04738
Step 48860: loss = 0.02181
Step 48865: loss = 0.15367
Step 48870: loss = 0.04816
Step 48875: loss = 0.05943
Step 48880: loss = 0.13612
Step 48885: loss = 0.04409
Step 48890: loss = 0.01586
Step 48895: loss = 0.05165
Step 48900: loss = 0.07604
Step 48905: loss = 0.02772
Step 48910: loss = 0.05373
Step 48915: loss = 0.05292
Step 48920: loss = 0.01209
Step 48925: loss = 0.06018
Step 48930: loss = 0.03453
Step 48935: loss = 0.05360
Step 48940: loss = 0.04426
Step 48945: loss = 0.03773
Step 48950: loss = 0.05029
Step 48955: loss = 0.02827
Step 48960: loss = 0.07795
Step 48965: loss = 0.05044
Step 48970: loss = 0.01369
Step 48975: loss = 0.18938
Step 48980: loss = 0.21888
Step 48985: loss = 0.06014
Step 48990: loss = 0.05664
Step 48995: loss = 0.06731
Step 49000: loss = 0.11673
Step 49005: loss = 0.03504
Step 49010: loss = 0.05247
Step 49015: loss = 0.04871
Step 49020: loss = 0.04515
Step 49025: loss = 0.05288
Step 49030: loss = 0.05691
Step 49035: loss = 0.02508
Step 49040: loss = 0.04644
Step 49045: loss = 0.06165
Step 49050: loss = 0.05368
Step 49055: loss = 0.03904
Step 49060: loss = 0.12729
Step 49065: loss = 0.09405
Step 49070: loss = 0.07854
Step 49075: loss = 0.08201
Step 49080: loss = 0.11483
Step 49085: loss = 0.04867
Step 49090: loss = 0.03433
Step 49095: loss = 0.11317
Step 49100: loss = 0.03669
Step 49105: loss = 0.06430
Step 49110: loss = 0.04433
Step 49115: loss = 0.01081
Step 49120: loss = 0.02097
Step 49125: loss = 0.03124
Step 49130: loss = 0.00599
Step 49135: loss = 0.02043
Step 49140: loss = 0.00819
Training Data Eval:
  Num examples: 49920, Num correct: 49058, Precision @ 1: 0.9827
('Testing Data Eval: EPOCH->', 127)
  Num examples: 9984, Num correct: 7414, Precision @ 1: 0.7426
Step 49145: loss = 0.01975
Step 49150: loss = 0.01223
Step 49155: loss = 0.01816
Step 49160: loss = 0.02449
Step 49165: loss = 0.09782
Step 49170: loss = 0.04745
Step 49175: loss = 0.10005
Step 49180: loss = 0.04881
Step 49185: loss = 0.01882
Step 49190: loss = 0.09900
Step 49195: loss = 0.05227
Step 49200: loss = 0.03586
Step 49205: loss = 0.06425
Step 49210: loss = 0.02120
Step 49215: loss = 0.04731
Step 49220: loss = 0.02174
Step 49225: loss = 0.01880
Step 49230: loss = 0.05526
Step 49235: loss = 0.01345
Step 49240: loss = 0.09744
Step 49245: loss = 0.02746
Step 49250: loss = 0.11352
Step 49255: loss = 0.04851
Step 49260: loss = 0.04921
Step 49265: loss = 0.14681
Step 49270: loss = 0.07373
Step 49275: loss = 0.02318
Step 49280: loss = 0.09226
Step 49285: loss = 0.03610
Step 49290: loss = 0.08310
Step 49295: loss = 0.07116
Step 49300: loss = 0.04258
Step 49305: loss = 0.01640
Step 49310: loss = 0.08006
Step 49315: loss = 0.02349
Step 49320: loss = 0.04952
Step 49325: loss = 0.07361
Step 49330: loss = 0.06786
Step 49335: loss = 0.08385
Step 49340: loss = 0.02288
Step 49345: loss = 0.03141
Step 49350: loss = 0.05346
Step 49355: loss = 0.04919
Step 49360: loss = 0.09125
Step 49365: loss = 0.11395
Step 49370: loss = 0.12722
Step 49375: loss = 0.05581
Step 49380: loss = 0.13535
Step 49385: loss = 0.02096
Step 49390: loss = 0.01533
Step 49395: loss = 0.05123
Step 49400: loss = 0.06086
Step 49405: loss = 0.02908
Step 49410: loss = 0.09901
Step 49415: loss = 0.03503
Step 49420: loss = 0.13945
Step 49425: loss = 0.06802
Step 49430: loss = 0.08355
Step 49435: loss = 0.06304
Step 49440: loss = 0.06395
Step 49445: loss = 0.03140
Step 49450: loss = 0.07033
Step 49455: loss = 0.04909
Step 49460: loss = 0.05320
Step 49465: loss = 0.06032
Step 49470: loss = 0.03803
Step 49475: loss = 0.03181
Step 49480: loss = 0.01992
Step 49485: loss = 0.02070
Step 49490: loss = 0.06413
Step 49495: loss = 0.04921
Step 49500: loss = 0.04378
Step 49505: loss = 0.08065
Step 49510: loss = 0.06740
Step 49515: loss = 0.06855
Step 49520: loss = 0.02638
Step 49525: loss = 0.05592
Step 49530: loss = 0.05897
Training Data Eval:
  Num examples: 49920, Num correct: 49180, Precision @ 1: 0.9852
('Testing Data Eval: EPOCH->', 128)
  Num examples: 9984, Num correct: 7414, Precision @ 1: 0.7426
Step 49535: loss = 0.09215
Step 49540: loss = 0.07480
Step 49545: loss = 0.06156
Step 49550: loss = 0.02558
Step 49555: loss = 0.07487
Step 49560: loss = 0.01572
Step 49565: loss = 0.02288
Step 49570: loss = 0.05593
Step 49575: loss = 0.06414
Step 49580: loss = 0.02087
Step 49585: loss = 0.06288
Step 49590: loss = 0.07098
Step 49595: loss = 0.04540
Step 49600: loss = 0.04372
Step 49605: loss = 0.01494
Step 49610: loss = 0.04229
Step 49615: loss = 0.05562
Step 49620: loss = 0.01123
Step 49625: loss = 0.08185
Step 49630: loss = 0.02452
Step 49635: loss = 0.02230
Step 49640: loss = 0.04545
Step 49645: loss = 0.05891
Step 49650: loss = 0.12097
Step 49655: loss = 0.03438
Step 49660: loss = 0.01657
Step 49665: loss = 0.09987
Step 49670: loss = 0.07073
Step 49675: loss = 0.05265
Step 49680: loss = 0.03133
Step 49685: loss = 0.07191
Step 49690: loss = 0.02255
Step 49695: loss = 0.01352
Step 49700: loss = 0.04495
Step 49705: loss = 0.05129
Step 49710: loss = 0.03878
Step 49715: loss = 0.01911
Step 49720: loss = 0.07800
Step 49725: loss = 0.05405
Step 49730: loss = 0.05652
Step 49735: loss = 0.02516
Step 49740: loss = 0.01779
Step 49745: loss = 0.10852
Step 49750: loss = 0.10084
Step 49755: loss = 0.11157
Step 49760: loss = 0.01445
Step 49765: loss = 0.02179
Step 49770: loss = 0.05923
Step 49775: loss = 0.02706
Step 49780: loss = 0.06394
Step 49785: loss = 0.02877
Step 49790: loss = 0.03508
Step 49795: loss = 0.07802
Step 49800: loss = 0.02108
Step 49805: loss = 0.10793
Step 49810: loss = 0.07879
Step 49815: loss = 0.10980
Step 49820: loss = 0.05118
Step 49825: loss = 0.05849
Step 49830: loss = 0.06682
Step 49835: loss = 0.01695
Step 49840: loss = 0.07061
Step 49845: loss = 0.02650
Step 49850: loss = 0.04120
Step 49855: loss = 0.05893
Step 49860: loss = 0.08151
Step 49865: loss = 0.06191
Step 49870: loss = 0.02573
Step 49875: loss = 0.06318
Step 49880: loss = 0.05568
Step 49885: loss = 0.12455
Step 49890: loss = 0.02260
Step 49895: loss = 0.01723
Step 49900: loss = 0.04517
Step 49905: loss = 0.07610
Step 49910: loss = 0.04090
Step 49915: loss = 0.11424
Step 49920: loss = 0.14113
Training Data Eval:
  Num examples: 49920, Num correct: 49037, Precision @ 1: 0.9823
('Testing Data Eval: EPOCH->', 129)
  Num examples: 9984, Num correct: 7372, Precision @ 1: 0.7384
Step 49925: loss = 0.07587
Step 49930: loss = 0.13391
Step 49935: loss = 0.06078
Step 49940: loss = 0.12953
Step 49945: loss = 0.06559
Step 49950: loss = 0.05118
Step 49955: loss = 0.02159
Step 49960: loss = 0.06203
Step 49965: loss = 0.02600
Step 49970: loss = 0.04995
Step 49975: loss = 0.02576
Step 49980: loss = 0.11980
Step 49985: loss = 0.03705
Step 49990: loss = 0.07268
Step 49995: loss = 0.04440
Step 50000: loss = 0.07322
Step 50005: loss = 0.05819
Step 50010: loss = 0.10035
Step 50015: loss = 0.05591
Step 50020: loss = 0.01781
Step 50025: loss = 0.01802
Step 50030: loss = 0.09824
Step 50035: loss = 0.01495
Step 50040: loss = 0.03534
Step 50045: loss = 0.03870
Step 50050: loss = 0.06365
Step 50055: loss = 0.07094
Step 50060: loss = 0.13634
Step 50065: loss = 0.02041
Step 50070: loss = 0.03435
Step 50075: loss = 0.04689
Step 50080: loss = 0.06776
Step 50085: loss = 0.03166
Step 50090: loss = 0.06617
Step 50095: loss = 0.08565
Step 50100: loss = 0.03489
Step 50105: loss = 0.04568
Step 50110: loss = 0.04929
Step 50115: loss = 0.03033
Step 50120: loss = 0.01534
Step 50125: loss = 0.07847
Step 50130: loss = 0.11811
Step 50135: loss = 0.02812
Step 50140: loss = 0.08353
Step 50145: loss = 0.05178
Step 50150: loss = 0.01898
Step 50155: loss = 0.03228
Step 50160: loss = 0.11848
Step 50165: loss = 0.04813
Step 50170: loss = 0.03519
Step 50175: loss = 0.02976
Step 50180: loss = 0.01967
Step 50185: loss = 0.08056
Step 50190: loss = 0.02310
Step 50195: loss = 0.02073
Step 50200: loss = 0.03658
Step 50205: loss = 0.02942
Step 50210: loss = 0.05792
Step 50215: loss = 0.04574
Step 50220: loss = 0.05181
Step 50225: loss = 0.11962
Step 50230: loss = 0.02773
Step 50235: loss = 0.07714
Step 50240: loss = 0.04123
Step 50245: loss = 0.05940
Step 50250: loss = 0.14045
Step 50255: loss = 0.11750
Step 50260: loss = 0.01521
Step 50265: loss = 0.09125
Step 50270: loss = 0.06168
Step 50275: loss = 0.04479
Step 50280: loss = 0.03165
Step 50285: loss = 0.04831
Step 50290: loss = 0.07399
Step 50295: loss = 0.16794
Step 50300: loss = 0.03521
Step 50305: loss = 0.10063
Step 50310: loss = 0.01945
Training Data Eval:
  Num examples: 49920, Num correct: 49170, Precision @ 1: 0.9850
('Testing Data Eval: EPOCH->', 130)
  Num examples: 9984, Num correct: 7471, Precision @ 1: 0.7483
Step 50315: loss = 0.06590
Step 50320: loss = 0.03339
Step 50325: loss = 0.02204
Step 50330: loss = 0.04832
Step 50335: loss = 0.05184
Step 50340: loss = 0.03692
Step 50345: loss = 0.02903
Step 50350: loss = 0.01750
Step 50355: loss = 0.08995
Step 50360: loss = 0.06589
Step 50365: loss = 0.01738
Step 50370: loss = 0.05230
Step 50375: loss = 0.10333
Step 50380: loss = 0.05048
Step 50385: loss = 0.07728
Step 50390: loss = 0.07625
Step 50395: loss = 0.05271
Step 50400: loss = 0.01371
Step 50405: loss = 0.01908
Step 50410: loss = 0.04465
Step 50415: loss = 0.04117
Step 50420: loss = 0.09274
Step 50425: loss = 0.03932
Step 50430: loss = 0.03244
Step 50435: loss = 0.03680
Step 50440: loss = 0.05995
Step 50445: loss = 0.01082
Step 50450: loss = 0.03181
Step 50455: loss = 0.05849
Step 50460: loss = 0.03337
Step 50465: loss = 0.03306
Step 50470: loss = 0.04519
Step 50475: loss = 0.08051
Step 50480: loss = 0.00925
Step 50485: loss = 0.12162
Step 50490: loss = 0.04157
Step 50495: loss = 0.03016
Step 50500: loss = 0.03799
Step 50505: loss = 0.03747
Step 50510: loss = 0.11965
Step 50515: loss = 0.07441
Step 50520: loss = 0.05849
Step 50525: loss = 0.13323
Step 50530: loss = 0.01193
Step 50535: loss = 0.04590
Step 50540: loss = 0.02446
Step 50545: loss = 0.05523
Step 50550: loss = 0.04221
Step 50555: loss = 0.15808
Step 50560: loss = 0.11394
Step 50565: loss = 0.01817
Step 50570: loss = 0.05692
Step 50575: loss = 0.07879
Step 50580: loss = 0.04699
Step 50585: loss = 0.01710
Step 50590: loss = 0.09830
Step 50595: loss = 0.02014
Step 50600: loss = 0.02159
Step 50605: loss = 0.08311
Step 50610: loss = 0.07553
Step 50615: loss = 0.03195
Step 50620: loss = 0.02002
Step 50625: loss = 0.05783
Step 50630: loss = 0.02062
Step 50635: loss = 0.12319
Step 50640: loss = 0.06149
Step 50645: loss = 0.00753
Step 50650: loss = 0.06797
Step 50655: loss = 0.01682
Step 50660: loss = 0.10165
Step 50665: loss = 0.08751
Step 50670: loss = 0.04600
Step 50675: loss = 0.02217
Step 50680: loss = 0.02164
Step 50685: loss = 0.02552
Step 50690: loss = 0.05658
Step 50695: loss = 0.08033
Step 50700: loss = 0.03130
Training Data Eval:
  Num examples: 49920, Num correct: 49304, Precision @ 1: 0.9877
('Testing Data Eval: EPOCH->', 131)
  Num examples: 9984, Num correct: 7448, Precision @ 1: 0.7460
Step 50705: loss = 0.06156
Step 50710: loss = 0.02502
Step 50715: loss = 0.06724
Step 50720: loss = 0.01971
Step 50725: loss = 0.04555
Step 50730: loss = 0.01445
Step 50735: loss = 0.01949
Step 50740: loss = 0.04124
Step 50745: loss = 0.00826
Step 50750: loss = 0.02072
Step 50755: loss = 0.03182
Step 50760: loss = 0.02216
Step 50765: loss = 0.10202
Step 50770: loss = 0.11868
Step 50775: loss = 0.03151
Step 50780: loss = 0.04469
Step 50785: loss = 0.02471
Step 50790: loss = 0.03106
Step 50795: loss = 0.03892
Step 50800: loss = 0.04262
Step 50805: loss = 0.04984
Step 50810: loss = 0.05751
Step 50815: loss = 0.07013
Step 50820: loss = 0.03146
Step 50825: loss = 0.05342
Step 50830: loss = 0.01485
Step 50835: loss = 0.09121
Step 50840: loss = 0.02069
Step 50845: loss = 0.01633
Step 50850: loss = 0.04671
Step 50855: loss = 0.06622
Step 50860: loss = 0.10791
Step 50865: loss = 0.05003
Step 50870: loss = 0.02643
Step 50875: loss = 0.07160
Step 50880: loss = 0.04523
Step 50885: loss = 0.02465
Step 50890: loss = 0.03413
Step 50895: loss = 0.08827
Step 50900: loss = 0.08511
Step 50905: loss = 0.02106
Step 50910: loss = 0.06545
Step 50915: loss = 0.01768
Step 50920: loss = 0.06696
Step 50925: loss = 0.01075
Step 50930: loss = 0.06162
Step 50935: loss = 0.00943
Step 50940: loss = 0.05056
Step 50945: loss = 0.04532
Step 50950: loss = 0.02857
Step 50955: loss = 0.03693
Step 50960: loss = 0.03623
Step 50965: loss = 0.05975
Step 50970: loss = 0.01969
Step 50975: loss = 0.01955
Step 50980: loss = 0.03753
Step 50985: loss = 0.03383
Step 50990: loss = 0.04349
Step 50995: loss = 0.01734
Step 51000: loss = 0.08591
Step 51005: loss = 0.02903
Step 51010: loss = 0.04204
Step 51015: loss = 0.05843
Step 51020: loss = 0.02430
Step 51025: loss = 0.04764
Step 51030: loss = 0.10447
Step 51035: loss = 0.06546
Step 51040: loss = 0.04590
Step 51045: loss = 0.04666
Step 51050: loss = 0.03044
Step 51055: loss = 0.02113
Step 51060: loss = 0.01921
Step 51065: loss = 0.03070
Step 51070: loss = 0.17046
Step 51075: loss = 0.05531
Step 51080: loss = 0.04464
Step 51085: loss = 0.06613
Step 51090: loss = 0.27391
Training Data Eval:
  Num examples: 49920, Num correct: 49256, Precision @ 1: 0.9867
('Testing Data Eval: EPOCH->', 132)
  Num examples: 9984, Num correct: 7465, Precision @ 1: 0.7477
Step 51095: loss = 0.03148
Step 51100: loss = 0.03002
Step 51105: loss = 0.05237
Step 51110: loss = 0.04416
Step 51115: loss = 0.08574
Step 51120: loss = 0.04764
Step 51125: loss = 0.03109
Step 51130: loss = 0.02721
Step 51135: loss = 0.06175
Step 51140: loss = 0.06980
Step 51145: loss = 0.09908
Step 51150: loss = 0.12553
Step 51155: loss = 0.04612
Step 51160: loss = 0.01358
Step 51165: loss = 0.08721
Step 51170: loss = 0.10607
Step 51175: loss = 0.04291
Step 51180: loss = 0.04728
Step 51185: loss = 0.03740
Step 51190: loss = 0.10563
Step 51195: loss = 0.06454
Step 51200: loss = 0.05072
Step 51205: loss = 0.04220
Step 51210: loss = 0.03354
Step 51215: loss = 0.05341
Step 51220: loss = 0.01171
Step 51225: loss = 0.00913
Step 51230: loss = 0.10103
Step 51235: loss = 0.04215
Step 51240: loss = 0.03399
Step 51245: loss = 0.06076
Step 51250: loss = 0.15303
Step 51255: loss = 0.08609
Step 51260: loss = 0.03539
Step 51265: loss = 0.01301
Step 51270: loss = 0.08094
Step 51275: loss = 0.04259
Step 51280: loss = 0.02579
Step 51285: loss = 0.03235
Step 51290: loss = 0.07305
Step 51295: loss = 0.05217
Step 51300: loss = 0.09504
Step 51305: loss = 0.02737
Step 51310: loss = 0.04640
Step 51315: loss = 0.02471
Step 51320: loss = 0.07763
Step 51325: loss = 0.02707
Step 51330: loss = 0.07500
Step 51335: loss = 0.04533
Step 51340: loss = 0.01647
Step 51345: loss = 0.02693
Step 51350: loss = 0.03578
Step 51355: loss = 0.08989
Step 51360: loss = 0.07221
Step 51365: loss = 0.02514
Step 51370: loss = 0.02355
Step 51375: loss = 0.08298
Step 51380: loss = 0.06297
Step 51385: loss = 0.04131
Step 51390: loss = 0.04485
Step 51395: loss = 0.03552
Step 51400: loss = 0.04108
Step 51405: loss = 0.07561
Step 51410: loss = 0.04805
Step 51415: loss = 0.15896
Step 51420: loss = 0.03201
Step 51425: loss = 0.10401
Step 51430: loss = 0.04845
Step 51435: loss = 0.05527
Step 51440: loss = 0.07649
Step 51445: loss = 0.01571
Step 51450: loss = 0.04370
Step 51455: loss = 0.03795
Step 51460: loss = 0.01874
Step 51465: loss = 0.01959
Step 51470: loss = 0.06387
Step 51475: loss = 0.02419
Step 51480: loss = 0.05329
Training Data Eval:
  Num examples: 49920, Num correct: 49269, Precision @ 1: 0.9870
('Testing Data Eval: EPOCH->', 133)
  Num examples: 9984, Num correct: 7432, Precision @ 1: 0.7444
Step 51485: loss = 0.02825
Step 51490: loss = 0.06146
Step 51495: loss = 0.01278
Step 51500: loss = 0.06161
Step 51505: loss = 0.03877
Step 51510: loss = 0.03256
Step 51515: loss = 0.04365
Step 51520: loss = 0.01785
Step 51525: loss = 0.03960
Step 51530: loss = 0.01040
Step 51535: loss = 0.02946
Step 51540: loss = 0.02899
Step 51545: loss = 0.01952
Step 51550: loss = 0.05010
Step 51555: loss = 0.00691
Step 51560: loss = 0.04515
Step 51565: loss = 0.05667
Step 51570: loss = 0.04511
Step 51575: loss = 0.03722
Step 51580: loss = 0.09416
Step 51585: loss = 0.08769
Step 51590: loss = 0.06768
Step 51595: loss = 0.01147
Step 51600: loss = 0.02291
Step 51605: loss = 0.03482
Step 51610: loss = 0.02232
Step 51615: loss = 0.04513
Step 51620: loss = 0.08910
Step 51625: loss = 0.06840
Step 51630: loss = 0.00633
Step 51635: loss = 0.03554
Step 51640: loss = 0.02673
Step 51645: loss = 0.01526
Step 51650: loss = 0.05868
Step 51655: loss = 0.07011
Step 51660: loss = 0.05317
Step 51665: loss = 0.05870
Step 51670: loss = 0.03057
Step 51675: loss = 0.00813
Step 51680: loss = 0.05349
Step 51685: loss = 0.03988
Step 51690: loss = 0.04828
Step 51695: loss = 0.04964
Step 51700: loss = 0.03021
Step 51705: loss = 0.03071
Step 51710: loss = 0.05408
Step 51715: loss = 0.01883
Step 51720: loss = 0.02910
Step 51725: loss = 0.07199
Step 51730: loss = 0.03869
Step 51735: loss = 0.05982
Step 51740: loss = 0.00801
Step 51745: loss = 0.05558
Step 51750: loss = 0.01734
Step 51755: loss = 0.05817
Step 51760: loss = 0.01709
Step 51765: loss = 0.08046
Step 51770: loss = 0.05637
Step 51775: loss = 0.08203
Step 51780: loss = 0.02345
Step 51785: loss = 0.04147
Step 51790: loss = 0.04185
Step 51795: loss = 0.01851
Step 51800: loss = 0.01147
Step 51805: loss = 0.02195
Step 51810: loss = 0.05754
Step 51815: loss = 0.02618
Step 51820: loss = 0.05113
Step 51825: loss = 0.07991
Step 51830: loss = 0.02006
Step 51835: loss = 0.04433
Step 51840: loss = 0.01224
Step 51845: loss = 0.11258
Step 51850: loss = 0.07175
Step 51855: loss = 0.06550
Step 51860: loss = 0.01561
Step 51865: loss = 0.02642
Step 51870: loss = 0.14936
Training Data Eval:
  Num examples: 49920, Num correct: 49307, Precision @ 1: 0.9877
('Testing Data Eval: EPOCH->', 134)
  Num examples: 9984, Num correct: 7377, Precision @ 1: 0.7389
Step 51875: loss = 0.00849
Step 51880: loss = 0.06630
Step 51885: loss = 0.02980
Step 51890: loss = 0.03330
Step 51895: loss = 0.06505
Step 51900: loss = 0.03185
Step 51905: loss = 0.04949
Step 51910: loss = 0.01866
Step 51915: loss = 0.03163
Step 51920: loss = 0.05543
Step 51925: loss = 0.03194
Step 51930: loss = 0.01578
Step 51935: loss = 0.01733
Step 51940: loss = 0.01628
Step 51945: loss = 0.01992
Step 51950: loss = 0.01176
Step 51955: loss = 0.06578
Step 51960: loss = 0.03443
Step 51965: loss = 0.04060
Step 51970: loss = 0.05598
Step 51975: loss = 0.02985
Step 51980: loss = 0.06524
Step 51985: loss = 0.01854
Step 51990: loss = 0.07380
Step 51995: loss = 0.01336
Step 52000: loss = 0.02132
Step 52005: loss = 0.02406
Step 52010: loss = 0.04253
Step 52015: loss = 0.05463
Step 52020: loss = 0.01562
Step 52025: loss = 0.02810
Step 52030: loss = 0.09157
Step 52035: loss = 0.02473
Step 52040: loss = 0.06304
Step 52045: loss = 0.03241
Step 52050: loss = 0.01215
Step 52055: loss = 0.03594
Step 52060: loss = 0.02851
Step 52065: loss = 0.01494
Step 52070: loss = 0.02930
Step 52075: loss = 0.07716
Step 52080: loss = 0.01059
Step 52085: loss = 0.06229
Step 52090: loss = 0.04289
Step 52095: loss = 0.01661
Step 52100: loss = 0.05380
Step 52105: loss = 0.02336
Step 52110: loss = 0.04287
Step 52115: loss = 0.07912
Step 52120: loss = 0.06243
Step 52125: loss = 0.02080
Step 52130: loss = 0.01466
Step 52135: loss = 0.02697
Step 52140: loss = 0.07560
Step 52145: loss = 0.02581
Step 52150: loss = 0.06017
Step 52155: loss = 0.01969
Step 52160: loss = 0.05488
Step 52165: loss = 0.05053
Step 52170: loss = 0.06494
Step 52175: loss = 0.08197
Step 52180: loss = 0.01983
Step 52185: loss = 0.03603
Step 52190: loss = 0.11131
Step 52195: loss = 0.03392
Step 52200: loss = 0.07427
Step 52205: loss = 0.16762
Step 52210: loss = 0.08417
Step 52215: loss = 0.04185
Step 52220: loss = 0.08038
Step 52225: loss = 0.04091
Step 52230: loss = 0.05077
Step 52235: loss = 0.06059
Step 52240: loss = 0.13383
Step 52245: loss = 0.11372
Step 52250: loss = 0.04452
Step 52255: loss = 0.08929
Step 52260: loss = 0.09649
Training Data Eval:
  Num examples: 49920, Num correct: 49200, Precision @ 1: 0.9856
('Testing Data Eval: EPOCH->', 135)
  Num examples: 9984, Num correct: 7309, Precision @ 1: 0.7321
Step 52265: loss = 0.01963
Step 52270: loss = 0.00864
Step 52275: loss = 0.07389
Step 52280: loss = 0.02185
Step 52285: loss = 0.02297
Step 52290: loss = 0.03513
Step 52295: loss = 0.06297
Step 52300: loss = 0.01358
Step 52305: loss = 0.01454
Step 52310: loss = 0.06370
Step 52315: loss = 0.03139
Step 52320: loss = 0.02413
Step 52325: loss = 0.04585
Step 52330: loss = 0.02384
Step 52335: loss = 0.07271
Step 52340: loss = 0.05755
Step 52345: loss = 0.10263
Step 52350: loss = 0.01356
Step 52355: loss = 0.00926
Step 52360: loss = 0.02899
Step 52365: loss = 0.04153
Step 52370: loss = 0.09333
Step 52375: loss = 0.10536
Step 52380: loss = 0.02705
Step 52385: loss = 0.01970
Step 52390: loss = 0.03879
Step 52395: loss = 0.04929
Step 52400: loss = 0.05304
Step 52405: loss = 0.07604
Step 52410: loss = 0.01960
Step 52415: loss = 0.07585
Step 52420: loss = 0.02993
Step 52425: loss = 0.09455
Step 52430: loss = 0.06061
Step 52435: loss = 0.01891
Step 52440: loss = 0.06700
Step 52445: loss = 0.03510
Step 52450: loss = 0.02917
Step 52455: loss = 0.04349
Step 52460: loss = 0.06866
Step 52465: loss = 0.04408
Step 52470: loss = 0.04417
Step 52475: loss = 0.06896
Step 52480: loss = 0.09951
Step 52485: loss = 0.08264
Step 52490: loss = 0.08375
Step 52495: loss = 0.01586
Step 52500: loss = 0.03078
Step 52505: loss = 0.02381
Step 52510: loss = 0.02434
Step 52515: loss = 0.01879
Step 52520: loss = 0.05916
Step 52525: loss = 0.08124
Step 52530: loss = 0.15251
Step 52535: loss = 0.02470
Step 52540: loss = 0.02788
Step 52545: loss = 0.01312
Step 52550: loss = 0.06423
Step 52555: loss = 0.05830
Step 52560: loss = 0.01896
Step 52565: loss = 0.06188
Step 52570: loss = 0.04558
Step 52575: loss = 0.01998
Step 52580: loss = 0.12277
Step 52585: loss = 0.07675
Step 52590: loss = 0.01212
Step 52595: loss = 0.04157
Step 52600: loss = 0.07296
Step 52605: loss = 0.04570
Step 52610: loss = 0.05837
Step 52615: loss = 0.09480
Step 52620: loss = 0.01464
Step 52625: loss = 0.05280
Step 52630: loss = 0.05301
Step 52635: loss = 0.05746
Step 52640: loss = 0.04735
Step 52645: loss = 0.03466
Step 52650: loss = 0.09480
Training Data Eval:
  Num examples: 49920, Num correct: 49030, Precision @ 1: 0.9822
('Testing Data Eval: EPOCH->', 136)
  Num examples: 9984, Num correct: 7407, Precision @ 1: 0.7419
Step 52655: loss = 0.03000
Step 52660: loss = 0.03335
Step 52665: loss = 0.07731
Step 52670: loss = 0.06528
Step 52675: loss = 0.05667
Step 52680: loss = 0.04837
Step 52685: loss = 0.10038
Step 52690: loss = 0.02663
Step 52695: loss = 0.03297
Step 52700: loss = 0.02940
Step 52705: loss = 0.07341
Step 52710: loss = 0.04267
Step 52715: loss = 0.08245
Step 52720: loss = 0.08387
Step 52725: loss = 0.02323
Step 52730: loss = 0.04931
Step 52735: loss = 0.04648
Step 52740: loss = 0.01779
Step 52745: loss = 0.07614
Step 52750: loss = 0.08474
Step 52755: loss = 0.08081
Step 52760: loss = 0.03637
Step 52765: loss = 0.07740
Step 52770: loss = 0.03476
Step 52775: loss = 0.02615
Step 52780: loss = 0.12058
Step 52785: loss = 0.09297
Step 52790: loss = 0.05955
Step 52795: loss = 0.05527
Step 52800: loss = 0.03590
Step 52805: loss = 0.06214
Step 52810: loss = 0.12397
Step 52815: loss = 0.04704
Step 52820: loss = 0.02128
Step 52825: loss = 0.04676
Step 52830: loss = 0.01878
Step 52835: loss = 0.03699
Step 52840: loss = 0.05292
Step 52845: loss = 0.07657
Step 52850: loss = 0.10481
Step 52855: loss = 0.05474
Step 52860: loss = 0.03763
Step 52865: loss = 0.09196
Step 52870: loss = 0.09778
Step 52875: loss = 0.02365
Step 52880: loss = 0.03051
Step 52885: loss = 0.11161
Step 52890: loss = 0.05827
Step 52895: loss = 0.04401
Step 52900: loss = 0.03171
Step 52905: loss = 0.01592
Step 52910: loss = 0.03162
Step 52915: loss = 0.16616
Step 52920: loss = 0.08415
Step 52925: loss = 0.06487
Step 52930: loss = 0.04591
Step 52935: loss = 0.05767
Step 52940: loss = 0.05195
Step 52945: loss = 0.03417
Step 52950: loss = 0.04515
Step 52955: loss = 0.03514
Step 52960: loss = 0.02739
Step 52965: loss = 0.00938
Step 52970: loss = 0.04662
Step 52975: loss = 0.03309
Step 52980: loss = 0.06167
Step 52985: loss = 0.11214
Step 52990: loss = 0.02583
Step 52995: loss = 0.04562
Step 53000: loss = 0.01609
Step 53005: loss = 0.01105
Step 53010: loss = 0.06695
Step 53015: loss = 0.04419
Step 53020: loss = 0.01949
Step 53025: loss = 0.03528
Step 53030: loss = 0.03058
Step 53035: loss = 0.07083
Step 53040: loss = 0.03564
Training Data Eval:
  Num examples: 49920, Num correct: 49213, Precision @ 1: 0.9858
('Testing Data Eval: EPOCH->', 137)
  Num examples: 9984, Num correct: 7424, Precision @ 1: 0.7436
Step 53045: loss = 0.04313
Step 53050: loss = 0.09214
Step 53055: loss = 0.08941
Step 53060: loss = 0.04041
Step 53065: loss = 0.07687
Step 53070: loss = 0.11873
Step 53075: loss = 0.06174
Step 53080: loss = 0.02950
Step 53085: loss = 0.00946
Step 53090: loss = 0.03506
Step 53095: loss = 0.00919
Step 53100: loss = 0.03736
Step 53105: loss = 0.03370
Step 53110: loss = 0.05682
Step 53115: loss = 0.02814
Step 53120: loss = 0.03916
Step 53125: loss = 0.06685
Step 53130: loss = 0.00835
Step 53135: loss = 0.04127
Step 53140: loss = 0.03243
Step 53145: loss = 0.09916
Step 53150: loss = 0.02742
Step 53155: loss = 0.04107
Step 53160: loss = 0.03354
Step 53165: loss = 0.04804
Step 53170: loss = 0.06406
Step 53175: loss = 0.04085
Step 53180: loss = 0.08554
Step 53185: loss = 0.03462
Step 53190: loss = 0.05074
Step 53195: loss = 0.00643
Step 53200: loss = 0.07036
Step 53205: loss = 0.00854
Step 53210: loss = 0.05961
Step 53215: loss = 0.05281
Step 53220: loss = 0.05291
Step 53225: loss = 0.04608
Step 53230: loss = 0.05881
Step 53235: loss = 0.03467
Step 53240: loss = 0.01425
Step 53245: loss = 0.01028
Step 53250: loss = 0.04817
Step 53255: loss = 0.07551
Step 53260: loss = 0.03341
Step 53265: loss = 0.03871
Step 53270: loss = 0.10017
Step 53275: loss = 0.01829
Step 53280: loss = 0.04796
Step 53285: loss = 0.01955
Step 53290: loss = 0.03653
Step 53295: loss = 0.03504
Step 53300: loss = 0.00992
Step 53305: loss = 0.05311
Step 53310: loss = 0.04424
Step 53315: loss = 0.01875
Step 53320: loss = 0.06289
Step 53325: loss = 0.10859
Step 53330: loss = 0.04916
Step 53335: loss = 0.03171
Step 53340: loss = 0.02022
Step 53345: loss = 0.02721
Step 53350: loss = 0.01660
Step 53355: loss = 0.04015
Step 53360: loss = 0.01005
Step 53365: loss = 0.01508
Step 53370: loss = 0.04774
Step 53375: loss = 0.05415
Step 53380: loss = 0.05168
Step 53385: loss = 0.00575
Step 53390: loss = 0.03567
Step 53395: loss = 0.04097
Step 53400: loss = 0.06641
Step 53405: loss = 0.06623
Step 53410: loss = 0.02737
Step 53415: loss = 0.17447
Step 53420: loss = 0.01446
Step 53425: loss = 0.03526
Step 53430: loss = 0.04814
Training Data Eval:
  Num examples: 49920, Num correct: 49131, Precision @ 1: 0.9842
('Testing Data Eval: EPOCH->', 138)
  Num examples: 9984, Num correct: 7492, Precision @ 1: 0.7504
Step 53435: loss = 0.09593
Step 53440: loss = 0.01679
Step 53445: loss = 0.04228
Step 53450: loss = 0.02974
Step 53455: loss = 0.07706
Step 53460: loss = 0.07032
Step 53465: loss = 0.03331
Step 53470: loss = 0.04191
Step 53475: loss = 0.01363
Step 53480: loss = 0.04053
Step 53485: loss = 0.03940
Step 53490: loss = 0.07682
Step 53495: loss = 0.05375
Step 53500: loss = 0.07810
Step 53505: loss = 0.02996
Step 53510: loss = 0.04585
Step 53515: loss = 0.04893
Step 53520: loss = 0.01877
Step 53525: loss = 0.02531
Step 53530: loss = 0.00890
Step 53535: loss = 0.13387
Step 53540: loss = 0.01836
Step 53545: loss = 0.09047
Step 53550: loss = 0.02083
Step 53555: loss = 0.01237
Step 53560: loss = 0.02956
Step 53565: loss = 0.04643
Step 53570: loss = 0.07628
Step 53575: loss = 0.06060
Step 53580: loss = 0.05740
Step 53585: loss = 0.02746
Step 53590: loss = 0.01693
Step 53595: loss = 0.00839
Step 53600: loss = 0.04001
Step 53605: loss = 0.06367
Step 53610: loss = 0.02270
Step 53615: loss = 0.04420
Step 53620: loss = 0.03388
Step 53625: loss = 0.04281
Step 53630: loss = 0.07903
Step 53635: loss = 0.02820
Step 53640: loss = 0.11768
Step 53645: loss = 0.04325
Step 53650: loss = 0.01489
Step 53655: loss = 0.04840
Step 53660: loss = 0.03821
Step 53665: loss = 0.07360
Step 53670: loss = 0.01521
Step 53675: loss = 0.03328
Step 53680: loss = 0.10279
Step 53685: loss = 0.06077
Step 53690: loss = 0.09523
Step 53695: loss = 0.03573
Step 53700: loss = 0.07924
Step 53705: loss = 0.07462
Step 53710: loss = 0.01460
Step 53715: loss = 0.00915
Step 53720: loss = 0.05862
Step 53725: loss = 0.03823
Step 53730: loss = 0.01099
Step 53735: loss = 0.01027
Step 53740: loss = 0.10371
Step 53745: loss = 0.01598
Step 53750: loss = 0.11794
Step 53755: loss = 0.05548
Step 53760: loss = 0.01875
Step 53765: loss = 0.04627
Step 53770: loss = 0.05453
Step 53775: loss = 0.06564
Step 53780: loss = 0.02684
Step 53785: loss = 0.07920
Step 53790: loss = 0.03204
Step 53795: loss = 0.05131
Step 53800: loss = 0.03394
Step 53805: loss = 0.04451
Step 53810: loss = 0.07745
Step 53815: loss = 0.04112
Step 53820: loss = 0.03451
Training Data Eval:
  Num examples: 49920, Num correct: 49348, Precision @ 1: 0.9885
('Testing Data Eval: EPOCH->', 139)
  Num examples: 9984, Num correct: 7374, Precision @ 1: 0.7386
Step 53825: loss = 0.06069
Step 53830: loss = 0.02112
Step 53835: loss = 0.01184
Step 53840: loss = 0.03144
Step 53845: loss = 0.06103
Step 53850: loss = 0.01915
Step 53855: loss = 0.09406
Step 53860: loss = 0.07785
Step 53865: loss = 0.03550
Step 53870: loss = 0.00501
Step 53875: loss = 0.07810
Step 53880: loss = 0.09001
Step 53885: loss = 0.02911
Step 53890: loss = 0.06542
Step 53895: loss = 0.01054
Step 53900: loss = 0.01587
Step 53905: loss = 0.02822
Step 53910: loss = 0.14613
Step 53915: loss = 0.03679
Step 53920: loss = 0.02206
Step 53925: loss = 0.03972
Step 53930: loss = 0.06293
Step 53935: loss = 0.01857
Step 53940: loss = 0.01690
Step 53945: loss = 0.06204
Step 53950: loss = 0.04854
Step 53955: loss = 0.01776
Step 53960: loss = 0.03881
Step 53965: loss = 0.02221
Step 53970: loss = 0.03212
Step 53975: loss = 0.01437
Step 53980: loss = 0.01102
Step 53985: loss = 0.03730
Step 53990: loss = 0.07082
Step 53995: loss = 0.01324
Step 54000: loss = 0.01656
Step 54005: loss = 0.02575
Step 54010: loss = 0.04066
Step 54015: loss = 0.03143
Step 54020: loss = 0.12036
Step 54025: loss = 0.04646
Step 54030: loss = 0.04733
Step 54035: loss = 0.01669
Step 54040: loss = 0.02915
Step 54045: loss = 0.03282
Step 54050: loss = 0.12156
Step 54055: loss = 0.04868
Step 54060: loss = 0.07469
Step 54065: loss = 0.04832
Step 54070: loss = 0.05365
Step 54075: loss = 0.01042
Step 54080: loss = 0.07025
Step 54085: loss = 0.03099
Step 54090: loss = 0.03437
Step 54095: loss = 0.04044
Step 54100: loss = 0.03437
Step 54105: loss = 0.04356
Step 54110: loss = 0.02493
Step 54115: loss = 0.00932
Step 54120: loss = 0.04526
Step 54125: loss = 0.04609
Step 54130: loss = 0.06392
Step 54135: loss = 0.02823
Step 54140: loss = 0.07148
Step 54145: loss = 0.03323
Step 54150: loss = 0.07085
Step 54155: loss = 0.04609
Step 54160: loss = 0.02178
Step 54165: loss = 0.01149
Step 54170: loss = 0.03118
Step 54175: loss = 0.08650
Step 54180: loss = 0.07957
Step 54185: loss = 0.05221
Step 54190: loss = 0.01566
Step 54195: loss = 0.07601
Step 54200: loss = 0.03851
Step 54205: loss = 0.03236
Step 54210: loss = 0.01155
Training Data Eval:
  Num examples: 49920, Num correct: 49287, Precision @ 1: 0.9873
('Testing Data Eval: EPOCH->', 140)
  Num examples: 9984, Num correct: 7419, Precision @ 1: 0.7431
Step 54215: loss = 0.08799
Step 54220: loss = 0.01025
Step 54225: loss = 0.08041
Step 54230: loss = 0.14577
Step 54235: loss = 0.04293
Step 54240: loss = 0.03661
Step 54245: loss = 0.02017
Step 54250: loss = 0.01113
Step 54255: loss = 0.04415
Step 54260: loss = 0.04579
Step 54265: loss = 0.05070
Step 54270: loss = 0.01961
Step 54275: loss = 0.01285
Step 54280: loss = 0.01357
Step 54285: loss = 0.06802
Step 54290: loss = 0.01524
Step 54295: loss = 0.05826
Step 54300: loss = 0.02395
Step 54305: loss = 0.10118
Step 54310: loss = 0.02701
Step 54315: loss = 0.01342
Step 54320: loss = 0.02972
Step 54325: loss = 0.02982
Step 54330: loss = 0.01428
Step 54335: loss = 0.03657
Step 54340: loss = 0.04214
Step 54345: loss = 0.13497
Step 54350: loss = 0.04245
Step 54355: loss = 0.13864
Step 54360: loss = 0.04069
Step 54365: loss = 0.07035
Step 54370: loss = 0.04534
Step 54375: loss = 0.04879
Step 54380: loss = 0.06301
Step 54385: loss = 0.08153
Step 54390: loss = 0.01440
Step 54395: loss = 0.05044
Step 54400: loss = 0.13031
Step 54405: loss = 0.04900
Step 54410: loss = 0.07748
Step 54415: loss = 0.06077
Step 54420: loss = 0.05129
Step 54425: loss = 0.03482
Step 54430: loss = 0.12145
Step 54435: loss = 0.07252
Step 54440: loss = 0.11609
Step 54445: loss = 0.04430
Step 54450: loss = 0.01816
Step 54455: loss = 0.11296
Step 54460: loss = 0.05016
Step 54465: loss = 0.03444
Step 54470: loss = 0.06066
Step 54475: loss = 0.07425
Step 54480: loss = 0.07987
Step 54485: loss = 0.05908
Step 54490: loss = 0.02007
Step 54495: loss = 0.03881
Step 54500: loss = 0.06935
Step 54505: loss = 0.03353
Step 54510: loss = 0.06371
Step 54515: loss = 0.04683
Step 54520: loss = 0.05387
Step 54525: loss = 0.11563
Step 54530: loss = 0.01480
Step 54535: loss = 0.06777
Step 54540: loss = 0.02898
Step 54545: loss = 0.06602
Step 54550: loss = 0.03719
Step 54555: loss = 0.02524
Step 54560: loss = 0.11071
Step 54565: loss = 0.06349
Step 54570: loss = 0.01105
Step 54575: loss = 0.05771
Step 54580: loss = 0.05399
Step 54585: loss = 0.05758
Step 54590: loss = 0.03170
Step 54595: loss = 0.05063
Step 54600: loss = 0.03968
Training Data Eval:
  Num examples: 49920, Num correct: 49238, Precision @ 1: 0.9863
('Testing Data Eval: EPOCH->', 141)
  Num examples: 9984, Num correct: 7471, Precision @ 1: 0.7483
Step 54605: loss = 0.04841
Step 54610: loss = 0.04603
Step 54615: loss = 0.05793
Step 54620: loss = 0.01150
Step 54625: loss = 0.01321
Step 54630: loss = 0.03420
Step 54635: loss = 0.07354
Step 54640: loss = 0.09762
Step 54645: loss = 0.06782
Step 54650: loss = 0.02832
Step 54655: loss = 0.08780
Step 54660: loss = 0.07358
Step 54665: loss = 0.08125
Step 54670: loss = 0.03264
Step 54675: loss = 0.01626
Step 54680: loss = 0.03481
Step 54685: loss = 0.05389
Step 54690: loss = 0.03211
Step 54695: loss = 0.10479
Step 54700: loss = 0.06618
Step 54705: loss = 0.03108
Step 54710: loss = 0.07915
Step 54715: loss = 0.07146
Step 54720: loss = 0.03065
Step 54725: loss = 0.01438
Step 54730: loss = 0.00997
Step 54735: loss = 0.01202
Step 54740: loss = 0.03433
Step 54745: loss = 0.03838
Step 54750: loss = 0.10530
Step 54755: loss = 0.02424
Step 54760: loss = 0.04971
Step 54765: loss = 0.02916
Step 54770: loss = 0.01693
Step 54775: loss = 0.02614
Step 54780: loss = 0.02222
Step 54785: loss = 0.03005
Step 54790: loss = 0.01263
Step 54795: loss = 0.04840
Step 54800: loss = 0.03807
Step 54805: loss = 0.04419
Step 54810: loss = 0.04281
Step 54815: loss = 0.01805
Step 54820: loss = 0.03771
Step 54825: loss = 0.01226
Step 54830: loss = 0.04759
Step 54835: loss = 0.04474
Step 54840: loss = 0.03847
Step 54845: loss = 0.04068
Step 54850: loss = 0.02989
Step 54855: loss = 0.00463
Step 54860: loss = 0.13226
Step 54865: loss = 0.04578
Step 54870: loss = 0.07655
Step 54875: loss = 0.09957
Step 54880: loss = 0.04860
Step 54885: loss = 0.13563
Step 54890: loss = 0.16919
Step 54895: loss = 0.04830
Step 54900: loss = 0.12083
Step 54905: loss = 0.04165
Step 54910: loss = 0.04818
Step 54915: loss = 0.05787
Step 54920: loss = 0.01763
Step 54925: loss = 0.06997
Step 54930: loss = 0.03350
Step 54935: loss = 0.02875
Step 54940: loss = 0.00862
Step 54945: loss = 0.04661
Step 54950: loss = 0.04337
Step 54955: loss = 0.05362
Step 54960: loss = 0.04917
Step 54965: loss = 0.01781
Step 54970: loss = 0.03720
Step 54975: loss = 0.01874
Step 54980: loss = 0.05100
Step 54985: loss = 0.01701
Step 54990: loss = 0.10229
Training Data Eval:
  Num examples: 49920, Num correct: 49166, Precision @ 1: 0.9849
('Testing Data Eval: EPOCH->', 142)
  Num examples: 9984, Num correct: 7297, Precision @ 1: 0.7309
Step 54995: loss = 0.10211
Step 55000: loss = 0.06901
Step 55005: loss = 0.09251
Step 55010: loss = 0.05535
Step 55015: loss = 0.13172
Step 55020: loss = 0.04967
Step 55025: loss = 0.03016
Step 55030: loss = 0.05696
Step 55035: loss = 0.04249
Step 55040: loss = 0.15214
Step 55045: loss = 0.02969
Step 55050: loss = 0.01790
Step 55055: loss = 0.05784
Step 55060: loss = 0.07286
Step 55065: loss = 0.04255
Step 55070: loss = 0.03146
Step 55075: loss = 0.02534
Step 55080: loss = 0.08460
Step 55085: loss = 0.11448
Step 55090: loss = 0.02173
Step 55095: loss = 0.13305
Step 55100: loss = 0.12850
Step 55105: loss = 0.02170
Step 55110: loss = 0.04421
Step 55115: loss = 0.12067
Step 55120: loss = 0.01598
Step 55125: loss = 0.06532
Step 55130: loss = 0.02987
Step 55135: loss = 0.01170
Step 55140: loss = 0.06447
Step 55145: loss = 0.08313
Step 55150: loss = 0.04456
Step 55155: loss = 0.03556
Step 55160: loss = 0.02861
Step 55165: loss = 0.03383
Step 55170: loss = 0.04872
Step 55175: loss = 0.05667
Step 55180: loss = 0.03229
Step 55185: loss = 0.05418
Step 55190: loss = 0.06088
Step 55195: loss = 0.07001
Step 55200: loss = 0.07189
Step 55205: loss = 0.02436
Step 55210: loss = 0.01928
Step 55215: loss = 0.00966
Step 55220: loss = 0.02142
Step 55225: loss = 0.04143
Step 55230: loss = 0.10206
Step 55235: loss = 0.01280
Step 55240: loss = 0.03906
Step 55245: loss = 0.04281
Step 55250: loss = 0.04313
Step 55255: loss = 0.02675
Step 55260: loss = 0.01178
Step 55265: loss = 0.04949
Step 55270: loss = 0.03397
Step 55275: loss = 0.04915
Step 55280: loss = 0.05286
Step 55285: loss = 0.07325
Step 55290: loss = 0.05816
Step 55295: loss = 0.01846
Step 55300: loss = 0.03900
Step 55305: loss = 0.01280
Step 55310: loss = 0.01940
Step 55315: loss = 0.02052
Step 55320: loss = 0.09131
Step 55325: loss = 0.02716
Step 55330: loss = 0.03626
Step 55335: loss = 0.02434
Step 55340: loss = 0.02354
Step 55345: loss = 0.03187
Step 55350: loss = 0.02624
Step 55355: loss = 0.07029
Step 55360: loss = 0.04692
Step 55365: loss = 0.03006
Step 55370: loss = 0.03445
Step 55375: loss = 0.02367
Step 55380: loss = 0.01518
Training Data Eval:
  Num examples: 49920, Num correct: 49326, Precision @ 1: 0.9881
('Testing Data Eval: EPOCH->', 143)
  Num examples: 9984, Num correct: 7440, Precision @ 1: 0.7452
Step 55385: loss = 0.01114
Step 55390: loss = 0.06555
Step 55395: loss = 0.04275
Step 55400: loss = 0.04817
Step 55405: loss = 0.01566
Step 55410: loss = 0.02523
Step 55415: loss = 0.03803
Step 55420: loss = 0.03842
Step 55425: loss = 0.02556
Step 55430: loss = 0.05208
Step 55435: loss = 0.01565
Step 55440: loss = 0.01240
Step 55445: loss = 0.01173
Step 55450: loss = 0.02683
Step 55455: loss = 0.00823
Step 55460: loss = 0.00856
Step 55465: loss = 0.02890
Step 55470: loss = 0.07353
Step 55475: loss = 0.02749
Step 55480: loss = 0.06720
Step 55485: loss = 0.06012
Step 55490: loss = 0.09233
Step 55495: loss = 0.00774
Step 55500: loss = 0.05116
Step 55505: loss = 0.09730
Step 55510: loss = 0.05972
Step 55515: loss = 0.02764
Step 55520: loss = 0.02474
Step 55525: loss = 0.08030
Step 55530: loss = 0.02074
Step 55535: loss = 0.01049
Step 55540: loss = 0.09777
Step 55545: loss = 0.01159
Step 55550: loss = 0.06208
Step 55555: loss = 0.04708
Step 55560: loss = 0.08379
Step 55565: loss = 0.02002
Step 55570: loss = 0.08017
Step 55575: loss = 0.06757
Step 55580: loss = 0.02776
Step 55585: loss = 0.03386
Step 55590: loss = 0.06114
Step 55595: loss = 0.01101
Step 55600: loss = 0.04259
Step 55605: loss = 0.01069
Step 55610: loss = 0.02037
Step 55615: loss = 0.06456
Step 55620: loss = 0.03634
Step 55625: loss = 0.09024
Step 55630: loss = 0.04653
Step 55635: loss = 0.03331
Step 55640: loss = 0.06269
Step 55645: loss = 0.04273
Step 55650: loss = 0.07025
Step 55655: loss = 0.04154
Step 55660: loss = 0.03137
Step 55665: loss = 0.02388
Step 55670: loss = 0.05336
Step 55675: loss = 0.01598
Step 55680: loss = 0.04165
Step 55685: loss = 0.09623
Step 55690: loss = 0.07118
Step 55695: loss = 0.01933
Step 55700: loss = 0.01198
Step 55705: loss = 0.02143
Step 55710: loss = 0.04159
Step 55715: loss = 0.01541
Step 55720: loss = 0.09410
Step 55725: loss = 0.01739
Step 55730: loss = 0.03911
Step 55735: loss = 0.05060
Step 55740: loss = 0.02376
Step 55745: loss = 0.12754
Step 55750: loss = 0.05699
Step 55755: loss = 0.06406
Step 55760: loss = 0.07457
Step 55765: loss = 0.06319
Step 55770: loss = 0.05471
Training Data Eval:
  Num examples: 49920, Num correct: 49157, Precision @ 1: 0.9847
('Testing Data Eval: EPOCH->', 144)
  Num examples: 9984, Num correct: 7403, Precision @ 1: 0.7415
Step 55775: loss = 0.05290
Step 55780: loss = 0.04384
Step 55785: loss = 0.02141
Step 55790: loss = 0.04117
Step 55795: loss = 0.03011
Step 55800: loss = 0.04845
Step 55805: loss = 0.02562
Step 55810: loss = 0.06849
Step 55815: loss = 0.03991
Step 55820: loss = 0.00720
Step 55825: loss = 0.05199
Step 55830: loss = 0.03383
Step 55835: loss = 0.04415
Step 55840: loss = 0.04700
Step 55845: loss = 0.06643
Step 55850: loss = 0.03488
Step 55855: loss = 0.06548
Step 55860: loss = 0.03556
Step 55865: loss = 0.01374
Step 55870: loss = 0.07509
Step 55875: loss = 0.01380
Step 55880: loss = 0.08248
Step 55885: loss = 0.05396
Step 55890: loss = 0.07609
Step 55895: loss = 0.07020
Step 55900: loss = 0.02765
Step 55905: loss = 0.12738
Step 55910: loss = 0.02594
Step 55915: loss = 0.08423
Step 55920: loss = 0.05297
Step 55925: loss = 0.05734
Step 55930: loss = 0.05219
Step 55935: loss = 0.03517
Step 55940: loss = 0.03622
Step 55945: loss = 0.01823
Step 55950: loss = 0.02722
Step 55955: loss = 0.02595
Step 55960: loss = 0.03549
Step 55965: loss = 0.03177
Step 55970: loss = 0.04383
Step 55975: loss = 0.00472
Step 55980: loss = 0.08216
Step 55985: loss = 0.08716
Step 55990: loss = 0.05506
Step 55995: loss = 0.02272
Step 56000: loss = 0.02976
Step 56005: loss = 0.04485
Step 56010: loss = 0.02740
Step 56015: loss = 0.07156
Step 56020: loss = 0.04206
Step 56025: loss = 0.01753
Step 56030: loss = 0.07593
Step 56035: loss = 0.06634
Step 56040: loss = 0.07166
Step 56045: loss = 0.11381
Step 56050: loss = 0.07847
Step 56055: loss = 0.04805
Step 56060: loss = 0.01482
Step 56065: loss = 0.14710
Step 56070: loss = 0.02107
Step 56075: loss = 0.04426
Step 56080: loss = 0.01351
Step 56085: loss = 0.01584
Step 56090: loss = 0.06394
Step 56095: loss = 0.02022
Step 56100: loss = 0.03061
Step 56105: loss = 0.03204
Step 56110: loss = 0.05470
Step 56115: loss = 0.05475
Step 56120: loss = 0.12278
Step 56125: loss = 0.03970
Step 56130: loss = 0.01478
Step 56135: loss = 0.09691
Step 56140: loss = 0.06348
Step 56145: loss = 0.05768
Step 56150: loss = 0.05616
Step 56155: loss = 0.02731
Step 56160: loss = 0.04004
Training Data Eval:
  Num examples: 49920, Num correct: 49313, Precision @ 1: 0.9878
('Testing Data Eval: EPOCH->', 145)
  Num examples: 9984, Num correct: 7488, Precision @ 1: 0.7500
Step 56165: loss = 0.05602
Step 56170: loss = 0.06382
Step 56175: loss = 0.02739
Step 56180: loss = 0.11338
Step 56185: loss = 0.04628
Step 56190: loss = 0.05137
Step 56195: loss = 0.08646
Step 56200: loss = 0.03621
Step 56205: loss = 0.03124
Step 56210: loss = 0.01102
Step 56215: loss = 0.07359
Step 56220: loss = 0.01517
Step 56225: loss = 0.08803
Step 56230: loss = 0.03245
Step 56235: loss = 0.07279
Step 56240: loss = 0.00689
Step 56245: loss = 0.03814
Step 56250: loss = 0.01226
Step 56255: loss = 0.01844
Step 56260: loss = 0.03965
Step 56265: loss = 0.06302
Step 56270: loss = 0.01237
Step 56275: loss = 0.04436
Step 56280: loss = 0.08777
Step 56285: loss = 0.06113
Step 56290: loss = 0.03010
Step 56295: loss = 0.01310
Step 56300: loss = 0.08059
Step 56305: loss = 0.01385
Step 56310: loss = 0.01811
Step 56315: loss = 0.04048
Step 56320: loss = 0.02490
Step 56325: loss = 0.00854
Step 56330: loss = 0.01635
Step 56335: loss = 0.10057
Step 56340: loss = 0.04449
Step 56345: loss = 0.06332
Step 56350: loss = 0.05756
Step 56355: loss = 0.01344
Step 56360: loss = 0.03572
Step 56365: loss = 0.03630
Step 56370: loss = 0.02326
Step 56375: loss = 0.02467
Step 56380: loss = 0.10378
Step 56385: loss = 0.03763
Step 56390: loss = 0.01441
Step 56395: loss = 0.02639
Step 56400: loss = 0.01964
Step 56405: loss = 0.02159
Step 56410: loss = 0.04092
Step 56415: loss = 0.03798
Step 56420: loss = 0.02552
Step 56425: loss = 0.00620
Step 56430: loss = 0.04292
Step 56435: loss = 0.05992
Step 56440: loss = 0.02397
Step 56445: loss = 0.03834
Step 56450: loss = 0.03470
Step 56455: loss = 0.04963
Step 56460: loss = 0.01882
Step 56465: loss = 0.06964
Step 56470: loss = 0.02455
Step 56475: loss = 0.00989
Step 56480: loss = 0.03353
Step 56485: loss = 0.02908
Step 56490: loss = 0.01950
Step 56495: loss = 0.02682
Step 56500: loss = 0.10183
Step 56505: loss = 0.03489
Step 56510: loss = 0.04445
Step 56515: loss = 0.02660
Step 56520: loss = 0.04054
Step 56525: loss = 0.03346
Step 56530: loss = 0.01486
Step 56535: loss = 0.05375
Step 56540: loss = 0.05117
Step 56545: loss = 0.04087
Step 56550: loss = 0.03571
Training Data Eval:
  Num examples: 49920, Num correct: 49323, Precision @ 1: 0.9880
('Testing Data Eval: EPOCH->', 146)
  Num examples: 9984, Num correct: 7441, Precision @ 1: 0.7453
Step 56555: loss = 0.01254
Step 56560: loss = 0.02595
Step 56565: loss = 0.05325
Step 56570: loss = 0.10105
Step 56575: loss = 0.01582
Step 56580: loss = 0.03510
Step 56585: loss = 0.02471
Step 56590: loss = 0.01244
Step 56595: loss = 0.12020
Step 56600: loss = 0.01943
Step 56605: loss = 0.02728
Step 56610: loss = 0.02923
Step 56615: loss = 0.07532
Step 56620: loss = 0.02736
Step 56625: loss = 0.02304
Step 56630: loss = 0.01307
Step 56635: loss = 0.09089
Step 56640: loss = 0.06013
Step 56645: loss = 0.03769
Step 56650: loss = 0.00512
Step 56655: loss = 0.05341
Step 56660: loss = 0.07567
Step 56665: loss = 0.03240
Step 56670: loss = 0.09873
Step 56675: loss = 0.00857
Step 56680: loss = 0.11258
Step 56685: loss = 0.02631
Step 56690: loss = 0.03260
Step 56695: loss = 0.01214
Step 56700: loss = 0.10719
Step 56705: loss = 0.01319
Step 56710: loss = 0.07131
Step 56715: loss = 0.04803
Step 56720: loss = 0.04073
Step 56725: loss = 0.06593
Step 56730: loss = 0.05049
Step 56735: loss = 0.02541
Step 56740: loss = 0.02839
Step 56745: loss = 0.06383
Step 56750: loss = 0.02278
Step 56755: loss = 0.07694
Step 56760: loss = 0.04390
Step 56765: loss = 0.01357
Step 56770: loss = 0.02819
Step 56775: loss = 0.08413
Step 56780: loss = 0.03145
Step 56785: loss = 0.02042
Step 56790: loss = 0.01074
Step 56795: loss = 0.06162
Step 56800: loss = 0.04460
Step 56805: loss = 0.02692
Step 56810: loss = 0.01666
Step 56815: loss = 0.06641
Step 56820: loss = 0.02926
Step 56825: loss = 0.00328
Step 56830: loss = 0.05466
Step 56835: loss = 0.07703
Step 56840: loss = 0.02877
Step 56845: loss = 0.03685
Step 56850: loss = 0.10253
Step 56855: loss = 0.03830
Step 56860: loss = 0.01082
Step 56865: loss = 0.07668
Step 56870: loss = 0.01503
Step 56875: loss = 0.05232
Step 56880: loss = 0.02123
Step 56885: loss = 0.03341
Step 56890: loss = 0.10025
Step 56895: loss = 0.13452
Step 56900: loss = 0.06213
Step 56905: loss = 0.02666
Step 56910: loss = 0.13193
Step 56915: loss = 0.10847
Step 56920: loss = 0.05173
Step 56925: loss = 0.02231
Step 56930: loss = 0.03300
Step 56935: loss = 0.04941
Step 56940: loss = 0.05374
Training Data Eval:
  Num examples: 49920, Num correct: 49232, Precision @ 1: 0.9862
('Testing Data Eval: EPOCH->', 147)
  Num examples: 9984, Num correct: 7429, Precision @ 1: 0.7441
Step 56945: loss = 0.01955
Step 56950: loss = 0.03738
Step 56955: loss = 0.09506
Step 56960: loss = 0.05755
Step 56965: loss = 0.06398
Step 56970: loss = 0.02178
Step 56975: loss = 0.04097
Step 56980: loss = 0.06416
Step 56985: loss = 0.02379
Step 56990: loss = 0.05564
Step 56995: loss = 0.08650
Step 57000: loss = 0.05905
Step 57005: loss = 0.02400
Step 57010: loss = 0.04025
Step 57015: loss = 0.07379
Step 57020: loss = 0.03269
Step 57025: loss = 0.04020
Step 57030: loss = 0.02298
Step 57035: loss = 0.03486
Step 57040: loss = 0.00708
Step 57045: loss = 0.00970
Step 57050: loss = 0.03977
Step 57055: loss = 0.04939
Step 57060: loss = 0.01050
Step 57065: loss = 0.04453
Step 57070: loss = 0.01000
Step 57075: loss = 0.05312
Step 57080: loss = 0.00376
Step 57085: loss = 0.01316
Step 57090: loss = 0.03945
Step 57095: loss = 0.05783
Step 57100: loss = 0.01255
Step 57105: loss = 0.04347
Step 57110: loss = 0.04728
Step 57115: loss = 0.01648
Step 57120: loss = 0.03576
Step 57125: loss = 0.03955
Step 57130: loss = 0.06021
Step 57135: loss = 0.05804
Step 57140: loss = 0.02548
Step 57145: loss = 0.00779
Step 57150: loss = 0.02919
Step 57155: loss = 0.10025
Step 57160: loss = 0.03650
Step 57165: loss = 0.04813
Step 57170: loss = 0.02355
Step 57175: loss = 0.01888
Step 57180: loss = 0.04154
Step 57185: loss = 0.01098
Step 57190: loss = 0.02315
Step 57195: loss = 0.04813
Step 57200: loss = 0.06530
Step 57205: loss = 0.02044
Step 57210: loss = 0.01784
Step 57215: loss = 0.07424
Step 57220: loss = 0.02001
Step 57225: loss = 0.04198
Step 57230: loss = 0.01842
Step 57235: loss = 0.03596
Step 57240: loss = 0.03421
Step 57245: loss = 0.01147
Step 57250: loss = 0.03595
Step 57255: loss = 0.00791
Step 57260: loss = 0.02625
Step 57265: loss = 0.00930
Step 57270: loss = 0.04074
Step 57275: loss = 0.01304
Step 57280: loss = 0.03428
Step 57285: loss = 0.04626
Step 57290: loss = 0.01320
Step 57295: loss = 0.07994
Step 57300: loss = 0.05354
Step 57305: loss = 0.02928
Step 57310: loss = 0.06396
Step 57315: loss = 0.04140
Step 57320: loss = 0.02761
Step 57325: loss = 0.03704
Step 57330: loss = 0.02606
Training Data Eval:
  Num examples: 49920, Num correct: 49267, Precision @ 1: 0.9869
('Testing Data Eval: EPOCH->', 148)
  Num examples: 9984, Num correct: 7393, Precision @ 1: 0.7405
Step 57335: loss = 0.08184
Step 57340: loss = 0.01200
Step 57345: loss = 0.08142
Step 57350: loss = 0.05445
Step 57355: loss = 0.04027
Step 57360: loss = 0.02403
Step 57365: loss = 0.03300
Step 57370: loss = 0.07066
Step 57375: loss = 0.03779
Step 57380: loss = 0.02802
Step 57385: loss = 0.03631
Step 57390: loss = 0.02444
Step 57395: loss = 0.09056
Step 57400: loss = 0.07926
Step 57405: loss = 0.02551
Step 57410: loss = 0.05076
Step 57415: loss = 0.02549
Step 57420: loss = 0.03448
Step 57425: loss = 0.05134
Step 57430: loss = 0.01883
Step 57435: loss = 0.07493
Step 57440: loss = 0.02890
Step 57445: loss = 0.05118
Step 57450: loss = 0.05349
Step 57455: loss = 0.08991
Step 57460: loss = 0.16240
Step 57465: loss = 0.16696
Step 57470: loss = 0.10264
Step 57475: loss = 0.06414
Step 57480: loss = 0.14932
Step 57485: loss = 0.03858
Step 57490: loss = 0.06454
Step 57495: loss = 0.04009
Step 57500: loss = 0.03138
Step 57505: loss = 0.02744
Step 57510: loss = 0.08872
Step 57515: loss = 0.06063
Step 57520: loss = 0.02075
Step 57525: loss = 0.01057
Step 57530: loss = 0.01624
Step 57535: loss = 0.02923
Step 57540: loss = 0.00875
Step 57545: loss = 0.02362
Step 57550: loss = 0.12358
Step 57555: loss = 0.04230
Step 57560: loss = 0.02088
Step 57565: loss = 0.01896
Step 57570: loss = 0.06402
Step 57575: loss = 0.02399
Step 57580: loss = 0.02552
Step 57585: loss = 0.09580
Step 57590: loss = 0.08753
Step 57595: loss = 0.04316
Step 57600: loss = 0.01495
Step 57605: loss = 0.12152
Step 57610: loss = 0.00909
Step 57615: loss = 0.01328
Step 57620: loss = 0.06756
Step 57625: loss = 0.02162
Step 57630: loss = 0.02435
Step 57635: loss = 0.04139
Step 57640: loss = 0.13316
Step 57645: loss = 0.03135
Step 57650: loss = 0.04189
Step 57655: loss = 0.03516
Step 57660: loss = 0.01343
Step 57665: loss = 0.02105
Step 57670: loss = 0.05833
Step 57675: loss = 0.04745
Step 57680: loss = 0.02200
Step 57685: loss = 0.02858
Step 57690: loss = 0.02906
Step 57695: loss = 0.02586
Step 57700: loss = 0.02076
Step 57705: loss = 0.06077
Step 57710: loss = 0.03141
Step 57715: loss = 0.01615
Step 57720: loss = 0.05049
Training Data Eval:
  Num examples: 49920, Num correct: 49433, Precision @ 1: 0.9902
('Testing Data Eval: EPOCH->', 149)
  Num examples: 9984, Num correct: 7543, Precision @ 1: 0.7555
Step 57725: loss = 0.01159
Step 57730: loss = 0.02681
Step 57735: loss = 0.04089
Step 57740: loss = 0.00594
Step 57745: loss = 0.04587
Step 57750: loss = 0.00879
Step 57755: loss = 0.08717
Step 57760: loss = 0.12040
Step 57765: loss = 0.03683
Step 57770: loss = 0.00565
Step 57775: loss = 0.04594
Step 57780: loss = 0.04567
Step 57785: loss = 0.04160
Step 57790: loss = 0.05936
Step 57795: loss = 0.04250
Step 57800: loss = 0.03985
Step 57805: loss = 0.03368
Step 57810: loss = 0.08904
Step 57815: loss = 0.05240
Step 57820: loss = 0.01616
Step 57825: loss = 0.02613
Step 57830: loss = 0.07354
Step 57835: loss = 0.06685
Step 57840: loss = 0.03610
Step 57845: loss = 0.05232
Step 57850: loss = 0.04670
Step 57855: loss = 0.02447
Step 57860: loss = 0.02627
Step 57865: loss = 0.02301
Step 57870: loss = 0.03463
Step 57875: loss = 0.06755
Step 57880: loss = 0.04486
Step 57885: loss = 0.03665
Step 57890: loss = 0.02146
Step 57895: loss = 0.06403
Step 57900: loss = 0.05408
Step 57905: loss = 0.07611
Step 57910: loss = 0.02187
Step 57915: loss = 0.04176
Step 57920: loss = 0.02312
Step 57925: loss = 0.03437
Step 57930: loss = 0.03684
Step 57935: loss = 0.03053
Step 57940: loss = 0.04444
Step 57945: loss = 0.01908
Step 57950: loss = 0.00594
Step 57955: loss = 0.01383
Step 57960: loss = 0.01070
Step 57965: loss = 0.02588
Step 57970: loss = 0.01359
Step 57975: loss = 0.02682
Step 57980: loss = 0.04179
Step 57985: loss = 0.03248
Step 57990: loss = 0.04020
Step 57995: loss = 0.02336
Step 58000: loss = 0.01946
Step 58005: loss = 0.04373
Step 58010: loss = 0.02196
Step 58015: loss = 0.04155
Step 58020: loss = 0.03587
Step 58025: loss = 0.04486
Step 58030: loss = 0.03394
Step 58035: loss = 0.08878
Step 58040: loss = 0.00933
Step 58045: loss = 0.05804
Step 58050: loss = 0.02874
Step 58055: loss = 0.14310
Step 58060: loss = 0.03280
Step 58065: loss = 0.09481
Step 58070: loss = 0.01518
Step 58075: loss = 0.04097
Step 58080: loss = 0.05441
Step 58085: loss = 0.02191
Step 58090: loss = 0.05439
Step 58095: loss = 0.02375
Step 58100: loss = 0.01493
Step 58105: loss = 0.08882
Step 58110: loss = 0.01899
Training Data Eval:
  Num examples: 49920, Num correct: 49296, Precision @ 1: 0.9875
('Testing Data Eval: EPOCH->', 150)
  Num examples: 9984, Num correct: 7491, Precision @ 1: 0.7503
Step 58115: loss = 0.00986
Step 58120: loss = 0.03717
Step 58125: loss = 0.01002
Step 58130: loss = 0.06144
Step 58135: loss = 0.00723
Step 58140: loss = 0.01535
Step 58145: loss = 0.04728
Step 58150: loss = 0.01145
Step 58155: loss = 0.02845
Step 58160: loss = 0.04065
Step 58165: loss = 0.02958
Step 58170: loss = 0.03688
Step 58175: loss = 0.02719
Step 58180: loss = 0.06925
Step 58185: loss = 0.03116
Step 58190: loss = 0.09331
Step 58195: loss = 0.11749
Step 58200: loss = 0.04179
Step 58205: loss = 0.01656
Step 58210: loss = 0.04396
Step 58215: loss = 0.04879
Step 58220: loss = 0.02439
Step 58225: loss = 0.05335
Step 58230: loss = 0.06429
Step 58235: loss = 0.03061
Step 58240: loss = 0.00907
Step 58245: loss = 0.09967
Step 58250: loss = 0.04542
Step 58255: loss = 0.02013
Step 58260: loss = 0.12737
Step 58265: loss = 0.07224
Step 58270: loss = 0.02215
Step 58275: loss = 0.03193
Step 58280: loss = 0.02228
Step 58285: loss = 0.01538
Step 58290: loss = 0.07647
Step 58295: loss = 0.01435
Step 58300: loss = 0.01425
Step 58305: loss = 0.06549
Step 58310: loss = 0.02325
Step 58315: loss = 0.08040
Step 58320: loss = 0.13664
Step 58325: loss = 0.01535
Step 58330: loss = 0.05736
Step 58335: loss = 0.09829
Step 58340: loss = 0.03582
Step 58345: loss = 0.03826
Step 58350: loss = 0.00504
Step 58355: loss = 0.15511
Step 58360: loss = 0.02719
Step 58365: loss = 0.01405
Step 58370: loss = 0.01238
Step 58375: loss = 0.02325
Step 58380: loss = 0.04086
Step 58385: loss = 0.08322
Step 58390: loss = 0.01648
Step 58395: loss = 0.05670
Step 58400: loss = 0.10348
Step 58405: loss = 0.06649
Step 58410: loss = 0.07800
Step 58415: loss = 0.01633
Step 58420: loss = 0.01032
Step 58425: loss = 0.04798
Step 58430: loss = 0.02957
Step 58435: loss = 0.04759
Step 58440: loss = 0.06941
Step 58445: loss = 0.01245
Step 58450: loss = 0.01040
Step 58455: loss = 0.05014
Step 58460: loss = 0.02771
Step 58465: loss = 0.01123
Step 58470: loss = 0.05139
Step 58475: loss = 0.01559
Step 58480: loss = 0.01490
Step 58485: loss = 0.01776
Step 58490: loss = 0.10785
Step 58495: loss = 0.01702
Step 58500: loss = 0.03280
Training Data Eval:
  Num examples: 49920, Num correct: 49427, Precision @ 1: 0.9901
('Testing Data Eval: EPOCH->', 151)
  Num examples: 9984, Num correct: 7452, Precision @ 1: 0.7464
Step 58505: loss = 0.02416
Step 58510: loss = 0.04667
Step 58515: loss = 0.04462
Step 58520: loss = 0.00967
Step 58525: loss = 0.04122
Step 58530: loss = 0.10262
Step 58535: loss = 0.02440
Step 58540: loss = 0.09891
Step 58545: loss = 0.06120
Step 58550: loss = 0.02685
Step 58555: loss = 0.07313
Step 58560: loss = 0.00567
Step 58565: loss = 0.06196
Step 58570: loss = 0.03036
Step 58575: loss = 0.03009
Step 58580: loss = 0.01519
Step 58585: loss = 0.02495
Step 58590: loss = 0.03565
Step 58595: loss = 0.03207
Step 58600: loss = 0.03856
Step 58605: loss = 0.00750
Step 58610: loss = 0.01209
Step 58615: loss = 0.06266
Step 58620: loss = 0.03891
Step 58625: loss = 0.03198
Step 58630: loss = 0.04249
Step 58635: loss = 0.01525
Step 58640: loss = 0.03254
Step 58645: loss = 0.05176
Step 58650: loss = 0.10259
Step 58655: loss = 0.08478
Step 58660: loss = 0.02034
Step 58665: loss = 0.00355
Step 58670: loss = 0.02414
Step 58675: loss = 0.05827
Step 58680: loss = 0.01413
Step 58685: loss = 0.00690
Step 58690: loss = 0.06117
Step 58695: loss = 0.08952
Step 58700: loss = 0.02137
Step 58705: loss = 0.09473
Step 58710: loss = 0.01422
Step 58715: loss = 0.02181
Step 58720: loss = 0.01744
Step 58725: loss = 0.00415
Step 58730: loss = 0.05890
Step 58735: loss = 0.03382
Step 58740: loss = 0.02970
Step 58745: loss = 0.18231
Step 58750: loss = 0.03381
Step 58755: loss = 0.07052
Step 58760: loss = 0.04266
Step 58765: loss = 0.09500
Step 58770: loss = 0.03693
Step 58775: loss = 0.06424
Step 58780: loss = 0.01223
Step 58785: loss = 0.05886
Step 58790: loss = 0.07223
Step 58795: loss = 0.02968
Step 58800: loss = 0.02508
Step 58805: loss = 0.02455
Step 58810: loss = 0.01023
Step 58815: loss = 0.08468
Step 58820: loss = 0.03694
Step 58825: loss = 0.08334
Step 58830: loss = 0.17683
Step 58835: loss = 0.06033
Step 58840: loss = 0.05034
Step 58845: loss = 0.10957
Step 58850: loss = 0.04594
Step 58855: loss = 0.07236
Step 58860: loss = 0.00771
Step 58865: loss = 0.00603
Step 58870: loss = 0.03590
Step 58875: loss = 0.04021
Step 58880: loss = 0.01567
Step 58885: loss = 0.01856
Step 58890: loss = 0.02766
Training Data Eval:
  Num examples: 49920, Num correct: 49219, Precision @ 1: 0.9860
('Testing Data Eval: EPOCH->', 152)
  Num examples: 9984, Num correct: 7380, Precision @ 1: 0.7392
Step 58895: loss = 0.03789
