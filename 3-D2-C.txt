Student with hard logits approach
Step 0: loss = 47.26910
Training Data Eval:
  Num examples: 49920, Num correct: 5138, Precision @ 1: 0.1029
('Testing Data Eval: EPOCH->', 1)
  Num examples: 9984, Num correct: 995, Precision @ 1: 0.0997
Step 5: loss = 46.33414
Step 10: loss = 45.30171
Step 15: loss = 32.72942
Step 20: loss = 45.77860
Step 25: loss = 38.21163
Step 30: loss = 15.40983
Step 35: loss = 16.97143
Step 40: loss = 10.74943
Step 45: loss = 12.27082
Step 50: loss = 25.33324
Step 55: loss = 22.78808
Step 60: loss = 18.62647
Step 65: loss = 5.01086
Step 70: loss = 5.74832
Step 75: loss = 10.68139
Step 80: loss = 24.41976
Step 85: loss = 23.38013
Step 90: loss = 15.07789
Step 95: loss = 34.53686
Step 100: loss = 26.92674
Step 105: loss = 29.86273
Step 110: loss = 12.34555
Step 115: loss = 32.02282
Step 120: loss = 21.52330
Step 125: loss = 22.72293
Step 130: loss = 40.75697
Step 135: loss = 19.44054
Step 140: loss = 11.56510
Step 145: loss = 30.35620
Step 150: loss = 22.45044
Step 155: loss = 11.66410
Step 160: loss = 11.28425
Step 165: loss = 11.35319
Step 170: loss = 13.19609
Step 175: loss = 16.44588
Step 180: loss = 20.61983
Step 185: loss = 19.38466
Step 190: loss = 16.65982
Step 195: loss = 16.94733
Step 200: loss = 21.58088
Step 205: loss = 9.74121
Step 210: loss = 16.74536
Step 215: loss = 22.90719
Step 220: loss = 34.32856
Step 225: loss = 18.30330
Step 230: loss = 38.11414
Step 235: loss = 27.43356
Step 240: loss = 44.02953
Step 245: loss = 49.44608
Step 250: loss = 36.22060
Step 255: loss = 49.96891
Step 260: loss = 52.60670
Step 265: loss = 27.55409
Step 270: loss = 38.56991
Step 275: loss = 31.22985
Step 280: loss = 45.50566
Step 285: loss = 72.43465
Step 290: loss = 56.63750
Step 295: loss = 22.08665
Step 300: loss = 18.59037
Step 305: loss = 39.81058
Step 310: loss = 29.25377
Step 315: loss = 18.94446
Step 320: loss = 24.43372
Step 325: loss = 41.25897
Step 330: loss = 41.24215
Step 335: loss = 24.93248
Step 340: loss = 33.72909
Step 345: loss = 22.14770
Step 350: loss = 16.83145
Step 355: loss = 36.46304
Step 360: loss = 17.45375
Step 365: loss = 20.69840
Step 370: loss = 9.66487
Step 375: loss = 12.48088
Step 380: loss = 14.82806
Step 385: loss = 39.73094
Step 390: loss = 54.58423
Training Data Eval:
  Num examples: 49920, Num correct: 6381, Precision @ 1: 0.1278
('Testing Data Eval: EPOCH->', 2)
  Num examples: 9984, Num correct: 1332, Precision @ 1: 0.1334
Step 395: loss = 28.65321
Step 400: loss = 39.64650
Step 405: loss = 42.72748
Step 410: loss = 31.50228
Step 415: loss = 47.57864
Step 420: loss = 73.36623
Step 425: loss = 56.15648
Step 430: loss = 36.16908
Step 435: loss = 46.51036
Step 440: loss = 32.51315
Step 445: loss = 34.53075
Step 450: loss = 46.74801
Step 455: loss = 45.74486
Step 460: loss = 28.99910
Step 465: loss = 27.98344
Step 470: loss = 40.53749
Step 475: loss = 53.43488
Step 480: loss = 16.11207
Step 485: loss = 41.43840
Step 490: loss = 89.63218
Step 495: loss = 59.57890
Step 500: loss = 38.77666
Step 505: loss = 31.46894
Step 510: loss = 108.17165
Step 515: loss = 50.13518
Step 520: loss = 59.88344
Step 525: loss = 95.75402
Step 530: loss = 73.67459
Step 535: loss = 67.06143
Step 540: loss = 55.42772
Step 545: loss = 88.27448
Step 550: loss = 55.30783
Step 555: loss = 96.61111
Step 560: loss = 35.71188
Step 565: loss = 44.80671
Step 570: loss = 58.64645
Step 575: loss = 53.63885
Step 580: loss = 40.46746
Step 585: loss = 52.84900
Step 590: loss = 47.34003
Step 595: loss = 42.60621
Step 600: loss = 37.35610
Step 605: loss = 54.11513
Step 610: loss = 50.23615
Step 615: loss = 40.23040
Step 620: loss = 77.97105
Step 625: loss = 39.76394
Step 630: loss = 29.09574
Step 635: loss = 41.97163
Step 640: loss = 22.08125
Step 645: loss = 40.02063
Step 650: loss = 19.29667
Step 655: loss = 34.56172
Step 660: loss = 19.56730
Step 665: loss = 20.50549
Step 670: loss = 42.26189
Step 675: loss = 27.86535
Step 680: loss = 57.63145
Step 685: loss = 30.49162
Step 690: loss = 19.14738
Step 695: loss = 16.07240
Step 700: loss = 48.71903
Step 705: loss = 33.98843
Step 710: loss = 23.62891
Step 715: loss = 46.73859
Step 720: loss = 21.36059
Step 725: loss = 27.14472
Step 730: loss = 29.89813
Step 735: loss = 14.89170
Step 740: loss = 48.24193
Step 745: loss = 27.58879
Step 750: loss = 26.08228
Step 755: loss = 33.61076
Step 760: loss = 32.37096
Step 765: loss = 49.05125
Step 770: loss = 56.08479
Step 775: loss = 49.49622
Step 780: loss = 23.87313
Training Data Eval:
  Num examples: 49920, Num correct: 6263, Precision @ 1: 0.1255
('Testing Data Eval: EPOCH->', 3)
  Num examples: 9984, Num correct: 1319, Precision @ 1: 0.1321
Step 785: loss = 50.50221
Step 790: loss = 24.65799
Step 795: loss = 54.08764
Step 800: loss = 32.37668
Step 805: loss = 60.07415
Step 810: loss = 24.58624
Step 815: loss = 76.55119
Step 820: loss = 56.52602
Step 825: loss = 22.96594
Step 830: loss = 67.32568
Step 835: loss = 62.45235
Step 840: loss = 16.39008
Step 845: loss = 46.55906
Step 850: loss = 74.61617
Step 855: loss = 64.66137
Step 860: loss = 71.73933
Step 865: loss = 66.11658
Step 870: loss = 66.45713
Step 875: loss = 53.26892
Step 880: loss = 69.47623
Step 885: loss = 36.17570
Step 890: loss = 58.25406
Step 895: loss = 46.93081
Step 900: loss = 55.51490
Step 905: loss = 38.05208
Step 910: loss = 37.71139
Step 915: loss = 38.92324
Step 920: loss = 30.50353
Step 925: loss = 49.53472
Step 930: loss = 32.88728
Step 935: loss = 43.96332
Step 940: loss = 58.97977
Step 945: loss = 59.54841
Step 950: loss = 42.45305
Step 955: loss = 37.88275
Step 960: loss = 30.73650
Step 965: loss = 39.43759
Step 970: loss = 24.05522
Step 975: loss = 37.56890
Step 980: loss = 23.37905
Step 985: loss = 29.79992
Step 990: loss = 44.84411
Step 995: loss = 46.17143
Step 1000: loss = 72.44987
Step 1005: loss = 34.03090
Step 1010: loss = 59.38754
Step 1015: loss = 43.10626
Step 1020: loss = 34.92591
Step 1025: loss = 27.34633
Step 1030: loss = 38.58826
Step 1035: loss = 42.38606
Step 1040: loss = 49.58542
Step 1045: loss = 41.55180
Step 1050: loss = 46.13280
Step 1055: loss = 35.01080
Step 1060: loss = 36.67384
Step 1065: loss = 34.07986
Step 1070: loss = 44.13882
Step 1075: loss = 35.96399
Step 1080: loss = 29.10160
Step 1085: loss = 37.65138
Step 1090: loss = 43.99809
Step 1095: loss = 38.69853
Step 1100: loss = 47.65913
Step 1105: loss = 46.61514
Step 1110: loss = 22.67623
Step 1115: loss = 33.09125
Step 1120: loss = 60.17638
Step 1125: loss = 46.65863
Step 1130: loss = 44.48934
Step 1135: loss = 40.81542
Step 1140: loss = 27.29369
Step 1145: loss = 38.06368
Step 1150: loss = 33.09699
Step 1155: loss = 21.39645
Step 1160: loss = 19.01924
Step 1165: loss = 21.15296
Step 1170: loss = 29.58781
Training Data Eval:
  Num examples: 49920, Num correct: 6503, Precision @ 1: 0.1303
('Testing Data Eval: EPOCH->', 4)
  Num examples: 9984, Num correct: 1297, Precision @ 1: 0.1299
Step 1175: loss = 36.23017
Step 1180: loss = 34.73942
Step 1185: loss = 26.29322
Step 1190: loss = 38.33123
Step 1195: loss = 45.84697
Step 1200: loss = 39.91127
Step 1205: loss = 20.88793
Step 1210: loss = 23.89884
Step 1215: loss = 17.41825
Step 1220: loss = 37.68512
Step 1225: loss = 17.38166
Step 1230: loss = 19.46019
Step 1235: loss = 16.17068
Step 1240: loss = 42.05904
Step 1245: loss = 27.51432
Step 1250: loss = 29.22765
Step 1255: loss = 51.14112
Step 1260: loss = 30.89880
Step 1265: loss = 34.64019
Step 1270: loss = 24.94112
Step 1275: loss = 29.97147
Step 1280: loss = 29.94562
Step 1285: loss = 47.04813
Step 1290: loss = 31.65121
Step 1295: loss = 31.32722
Step 1300: loss = 41.32335
Step 1305: loss = 46.67266
Step 1310: loss = 17.24761
Step 1315: loss = 35.18333
Step 1320: loss = 32.53228
Step 1325: loss = 16.20224
Step 1330: loss = 45.68263
Step 1335: loss = 41.70358
Step 1340: loss = 24.56521
Step 1345: loss = 31.84489
Step 1350: loss = 25.02844
Step 1355: loss = 31.05089
Step 1360: loss = 36.45701
Step 1365: loss = 30.53888
Step 1370: loss = 35.01055
Step 1375: loss = 25.11257
Step 1380: loss = 32.28613
Step 1385: loss = 39.33311
Step 1390: loss = 48.31067
Step 1395: loss = 34.21846
Step 1400: loss = 34.49508
Step 1405: loss = 24.85647
Step 1410: loss = 27.62943
Step 1415: loss = 12.36679
Step 1420: loss = 20.97315
Step 1425: loss = 20.45334
Step 1430: loss = 20.01932
Step 1435: loss = 33.39169
Step 1440: loss = 21.14682
Step 1445: loss = 9.62234
Step 1450: loss = 12.66241
Step 1455: loss = 14.16041
Step 1460: loss = 16.35905
Step 1465: loss = 18.92769
Step 1470: loss = 14.11831
Step 1475: loss = 33.11562
Step 1480: loss = 28.07090
Step 1485: loss = 21.44795
Step 1490: loss = 29.02707
Step 1495: loss = 15.64118
Step 1500: loss = 28.12663
Step 1505: loss = 15.44067
Step 1510: loss = 16.68773
Step 1515: loss = 25.92066
Step 1520: loss = 25.43469
Step 1525: loss = 26.59443
Step 1530: loss = 34.76057
Step 1535: loss = 32.91816
Step 1540: loss = 39.42513
Step 1545: loss = 40.15745
Step 1550: loss = 26.09949
Step 1555: loss = 45.19022
Step 1560: loss = 42.57790
Training Data Eval:
  Num examples: 49920, Num correct: 6363, Precision @ 1: 0.1275
('Testing Data Eval: EPOCH->', 5)
  Num examples: 9984, Num correct: 1302, Precision @ 1: 0.1304
Step 1565: loss = 43.32090
Step 1570: loss = 42.24681
Step 1575: loss = 33.99936
Step 1580: loss = 35.92886
Step 1585: loss = 52.49275
Step 1590: loss = 33.02967
Step 1595: loss = 19.67316
Step 1600: loss = 22.12840
Step 1605: loss = 18.70569
Step 1610: loss = 30.29063
Step 1615: loss = 43.18364
Step 1620: loss = 47.27527
Step 1625: loss = 38.04424
Step 1630: loss = 26.51757
Step 1635: loss = 19.27776
Step 1640: loss = 30.00112
Step 1645: loss = 36.96395
Step 1650: loss = 14.35063
Step 1655: loss = 26.86405
Step 1660: loss = 18.63602
Step 1665: loss = 13.62888
Step 1670: loss = 26.02675
Step 1675: loss = 11.69936
Step 1680: loss = 17.12020
Step 1685: loss = 29.28398
Step 1690: loss = 30.33722
Step 1695: loss = 28.39571
Step 1700: loss = 15.02218
Step 1705: loss = 24.54094
Step 1710: loss = 12.78553
Step 1715: loss = 26.81160
Step 1720: loss = 24.56416
Step 1725: loss = 24.83778
Step 1730: loss = 44.94468
Step 1735: loss = 21.66193
Step 1740: loss = 40.06211
Step 1745: loss = 23.70251
Step 1750: loss = 16.13372
Step 1755: loss = 13.40054
Step 1760: loss = 45.47876
Step 1765: loss = 22.18678
Step 1770: loss = 23.87046
Step 1775: loss = 31.25670
Step 1780: loss = 19.84074
Step 1785: loss = 28.38044
Step 1790: loss = 27.05656
Step 1795: loss = 9.19457
Step 1800: loss = 16.72521
Step 1805: loss = 21.55302
Step 1810: loss = 22.25953
Step 1815: loss = 14.60781
Step 1820: loss = 18.79300
Step 1825: loss = 13.03215
Step 1830: loss = 17.16309
Step 1835: loss = 18.76236
Step 1840: loss = 25.27240
Step 1845: loss = 33.48983
Step 1850: loss = 40.03962
Step 1855: loss = 15.56876
Step 1860: loss = 37.63712
Step 1865: loss = 20.20266
Step 1870: loss = 10.31231
Step 1875: loss = 22.92439
Step 1880: loss = 14.69749
Step 1885: loss = 16.53826
Step 1890: loss = 17.84248
Step 1895: loss = 11.08976
Step 1900: loss = 11.91858
Step 1905: loss = 15.64215
Step 1910: loss = 15.22341
Step 1915: loss = 14.16177
Step 1920: loss = 9.64892
Step 1925: loss = 15.88312
Step 1930: loss = 11.13613
Step 1935: loss = 24.99245
Step 1940: loss = 23.93537
Step 1945: loss = 12.77201
Step 1950: loss = 15.81834
Training Data Eval:
  Num examples: 49920, Num correct: 6279, Precision @ 1: 0.1258
('Testing Data Eval: EPOCH->', 6)
  Num examples: 9984, Num correct: 1332, Precision @ 1: 0.1334
Step 1955: loss = 19.95337
Step 1960: loss = 21.89075
Step 1965: loss = 33.54426
Step 1970: loss = 31.88312
Step 1975: loss = 24.00032
Step 1980: loss = 17.25450
Step 1985: loss = 24.91193
Step 1990: loss = 29.77312
Step 1995: loss = 19.20500
Step 2000: loss = 15.63142
Step 2005: loss = 23.95482
Step 2010: loss = 6.85501
Step 2015: loss = 19.72053
Step 2020: loss = 15.86065
Step 2025: loss = 22.40676
Step 2030: loss = 17.46393
Step 2035: loss = 22.17302
Step 2040: loss = 47.96539
Step 2045: loss = 52.01793
Step 2050: loss = 47.39706
Step 2055: loss = 65.52920
Step 2060: loss = 63.10281
Step 2065: loss = 41.43974
Step 2070: loss = 47.77251
Step 2075: loss = 51.93117
Step 2080: loss = 75.33939
Step 2085: loss = 54.28411
Step 2090: loss = 50.11449
Step 2095: loss = 70.88837
Step 2100: loss = 78.88583
Step 2105: loss = 65.08884
Step 2110: loss = 53.58138
Step 2115: loss = 68.31491
Step 2120: loss = 31.24569
Step 2125: loss = 68.50465
Step 2130: loss = 34.97103
Step 2135: loss = 39.33710
Step 2140: loss = 63.90366
Step 2145: loss = 27.51480
Step 2150: loss = 36.51559
Step 2155: loss = 74.77671
Step 2160: loss = 20.01420
Step 2165: loss = 69.03373
Step 2170: loss = 63.23409
Step 2175: loss = 35.09237
Step 2180: loss = 53.61556
Step 2185: loss = 23.09377
Step 2190: loss = 48.17739
Step 2195: loss = 16.62496
Step 2200: loss = 51.00446
Step 2205: loss = 30.98666
Step 2210: loss = 45.59286
Step 2215: loss = 24.19928
Step 2220: loss = 50.46751
Step 2225: loss = 60.20148
Step 2230: loss = 41.03636
Step 2235: loss = 73.32841
Step 2240: loss = 27.40027
Step 2245: loss = 46.92951
Step 2250: loss = 51.11411
Step 2255: loss = 59.25276
Step 2260: loss = 77.78758
Step 2265: loss = 50.59604
Step 2270: loss = 19.24473
Step 2275: loss = 35.70483
Step 2280: loss = 23.33014
Step 2285: loss = 33.86733
Step 2290: loss = 54.89051
Step 2295: loss = 25.79681
Step 2300: loss = 34.72719
Step 2305: loss = 44.61013
Step 2310: loss = 64.97352
Step 2315: loss = 41.69986
Step 2320: loss = 90.49295
Step 2325: loss = 61.55128
Step 2330: loss = 49.04749
Step 2335: loss = 54.75756
Step 2340: loss = 48.52657
Training Data Eval:
  Num examples: 49920, Num correct: 6374, Precision @ 1: 0.1277
('Testing Data Eval: EPOCH->', 7)
  Num examples: 9984, Num correct: 1225, Precision @ 1: 0.1227
Step 2345: loss = 84.41293
Step 2350: loss = 41.76971
Step 2355: loss = 30.46165
Step 2360: loss = 75.98466
Step 2365: loss = 38.36076
Step 2370: loss = 40.17739
Step 2375: loss = 39.88638
Step 2380: loss = 26.29007
Step 2385: loss = 47.30085
Step 2390: loss = 68.17799
Step 2395: loss = 34.38701
Step 2400: loss = 27.85191
Step 2405: loss = 48.53897
Step 2410: loss = 49.32388
Step 2415: loss = 47.07749
Step 2420: loss = 26.64928
Step 2425: loss = 45.77116
Step 2430: loss = 52.42339
Step 2435: loss = 40.78737
Step 2440: loss = 25.65844
Step 2445: loss = 45.94466
Step 2450: loss = 28.46242
Step 2455: loss = 18.29848
Step 2460: loss = 36.13197
Step 2465: loss = 56.63062
Step 2470: loss = 31.42032
Step 2475: loss = 36.98106
Step 2480: loss = 20.82155
Step 2485: loss = 41.80055
Step 2490: loss = 30.29170
Step 2495: loss = 25.30928
Step 2500: loss = 41.10258
Step 2505: loss = 40.99775
Step 2510: loss = 27.63288
Step 2515: loss = 20.48575
Step 2520: loss = 25.70168
Step 2525: loss = 22.10170
Step 2530: loss = 21.56630
Step 2535: loss = 38.95724
Step 2540: loss = 6.24740
Step 2545: loss = 41.11502
Step 2550: loss = 22.66144
Step 2555: loss = 27.88654
Step 2560: loss = 32.27619
Step 2565: loss = 51.27228
Step 2570: loss = 14.82230
Step 2575: loss = 20.32266
Step 2580: loss = 29.82503
Step 2585: loss = 23.94080
Step 2590: loss = 34.53321
Step 2595: loss = 43.52895
Step 2600: loss = 53.16689
Step 2605: loss = 25.54744
Step 2610: loss = 70.55025
Step 2615: loss = 53.58002
Step 2620: loss = 78.90386
Step 2625: loss = 31.91864
Step 2630: loss = 47.82786
Step 2635: loss = 36.64291
Step 2640: loss = 41.57712
Step 2645: loss = 46.54624
Step 2650: loss = 57.45174
Step 2655: loss = 32.20560
Step 2660: loss = 2.63825
Step 2665: loss = 25.69331
Step 2670: loss = 54.05046
Step 2675: loss = 59.92364
Step 2680: loss = 52.18315
Step 2685: loss = 62.00164
Step 2690: loss = 32.32793
Step 2695: loss = 11.32635
Step 2700: loss = 44.91511
Step 2705: loss = 32.22573
Step 2710: loss = 24.23965
Step 2715: loss = 32.98282
Step 2720: loss = 16.91452
Step 2725: loss = 36.27591
Step 2730: loss = 27.82957
Training Data Eval:
  Num examples: 49920, Num correct: 6517, Precision @ 1: 0.1305
('Testing Data Eval: EPOCH->', 8)
  Num examples: 9984, Num correct: 1274, Precision @ 1: 0.1276
Step 2735: loss = 53.19090
Step 2740: loss = 25.94417
Step 2745: loss = 36.54250
Step 2750: loss = 61.18106
Step 2755: loss = 84.88232
Step 2760: loss = 60.41709
Step 2765: loss = 64.42577
Step 2770: loss = 58.15969
Step 2775: loss = 71.57386
Step 2780: loss = 65.87565
Step 2785: loss = 65.33570
Step 2790: loss = 59.94674
Step 2795: loss = 46.00669
Step 2800: loss = 57.49128
Step 2805: loss = 35.43428
Step 2810: loss = 37.23413
Step 2815: loss = 45.84163
Step 2820: loss = 65.74737
Step 2825: loss = 41.44915
Step 2830: loss = 50.51569
Step 2835: loss = 60.02871
Step 2840: loss = 44.15376
Step 2845: loss = 43.12508
Step 2850: loss = 78.68739
Step 2855: loss = 75.20329
Step 2860: loss = 60.31716
Step 2865: loss = 86.20904
Step 2870: loss = 55.86227
Step 2875: loss = 46.54472
Step 2880: loss = 79.99054
Step 2885: loss = 83.47334
Step 2890: loss = 98.60843
Step 2895: loss = 64.72201
Step 2900: loss = 35.64528
Step 2905: loss = 38.99757
Step 2910: loss = 65.43327
Step 2915: loss = 45.75086
Step 2920: loss = 53.77357
Step 2925: loss = 62.64467
Step 2930: loss = 28.34521
Step 2935: loss = 26.91327
Step 2940: loss = 41.83764
Step 2945: loss = 30.47382
Step 2950: loss = 35.82722
Step 2955: loss = 21.55661
Step 2960: loss = 14.29283
Step 2965: loss = 27.05683
Step 2970: loss = 41.86339
Step 2975: loss = 16.36654
Step 2980: loss = 49.88318
Step 2985: loss = 30.10016
Step 2990: loss = 35.55826
Step 2995: loss = 35.25357
Step 3000: loss = 22.65553
Step 3005: loss = 11.85450
Step 3010: loss = 40.47249
Step 3015: loss = 38.04871
Step 3020: loss = 41.62123
Step 3025: loss = 30.56163
Step 3030: loss = 34.00964
Step 3035: loss = 56.45803
Step 3040: loss = 51.61200
Step 3045: loss = 47.21285
Step 3050: loss = 55.54901
Step 3055: loss = 46.57266
Step 3060: loss = 41.91589
Step 3065: loss = 49.94293
Step 3070: loss = 45.82634
Step 3075: loss = 41.42991
Step 3080: loss = 46.28736
Step 3085: loss = 22.20479
Step 3090: loss = 45.33947
Step 3095: loss = 38.55621
Step 3100: loss = 17.60651
Step 3105: loss = 33.57973
Step 3110: loss = 17.26181
Step 3115: loss = 44.31817
Step 3120: loss = 23.39230
Training Data Eval:
  Num examples: 49920, Num correct: 7004, Precision @ 1: 0.1403
('Testing Data Eval: EPOCH->', 9)
  Num examples: 9984, Num correct: 1310, Precision @ 1: 0.1312
Step 3125: loss = 31.64154
Step 3130: loss = 43.46692
Step 3135: loss = 18.61153
Step 3140: loss = 28.31913
Step 3145: loss = 40.11474
Step 3150: loss = 19.63473
Step 3155: loss = 31.51505
Step 3160: loss = 13.21667
Step 3165: loss = 29.02518
Step 3170: loss = 36.94032
Step 3175: loss = 31.68940
Step 3180: loss = 24.65830
Step 3185: loss = 30.38933
Step 3190: loss = 15.60344
Step 3195: loss = 40.10339
Step 3200: loss = 13.20343
Step 3205: loss = 29.24682
Step 3210: loss = 29.22537
Step 3215: loss = 25.81287
Step 3220: loss = 15.79954
Step 3225: loss = 24.95113
Step 3230: loss = 27.08998
Step 3235: loss = 28.75099
Step 3240: loss = 33.01578
Step 3245: loss = 31.04283
Step 3250: loss = 40.09390
Step 3255: loss = 28.72903
Step 3260: loss = 39.62320
Step 3265: loss = 20.44727
Step 3270: loss = 31.80823
Step 3275: loss = 26.09885
Step 3280: loss = 38.25140
Step 3285: loss = 26.73680
Step 3290: loss = 17.41936
Step 3295: loss = 25.72745
Step 3300: loss = 16.24412
Step 3305: loss = 30.19989
Step 3310: loss = 20.31489
Step 3315: loss = 28.72150
Step 3320: loss = 21.14500
Step 3325: loss = 16.61017
Step 3330: loss = 16.67482
Step 3335: loss = 17.44221
Step 3340: loss = 18.31211
Step 3345: loss = 20.59147
Step 3350: loss = 20.12807
Step 3355: loss = 13.51345
Step 3360: loss = 20.94431
Step 3365: loss = 24.78758
Step 3370: loss = 16.38040
Step 3375: loss = 16.10907
Step 3380: loss = 26.17831
Step 3385: loss = 22.33840
Step 3390: loss = 2.98658
Step 3395: loss = 27.21872
Step 3400: loss = 22.20135
Step 3405: loss = 30.07091
Step 3410: loss = 39.15386
Step 3415: loss = 17.26915
Step 3420: loss = 34.59091
Step 3425: loss = 32.73867
Step 3430: loss = 15.79154
Step 3435: loss = 25.21421
Step 3440: loss = 29.68437
Step 3445: loss = 24.03754
Step 3450: loss = 13.78239
Step 3455: loss = 22.88033
Step 3460: loss = 22.95475
Step 3465: loss = 15.73322
Step 3470: loss = 17.68832
Step 3475: loss = 15.64769
Step 3480: loss = 23.38639
Step 3485: loss = 11.98510
Step 3490: loss = 7.22393
Step 3495: loss = 12.95481
Step 3500: loss = 29.68217
Step 3505: loss = 9.57273
Step 3510: loss = 31.40142
Training Data Eval:
  Num examples: 49920, Num correct: 5884, Precision @ 1: 0.1179
('Testing Data Eval: EPOCH->', 10)
  Num examples: 9984, Num correct: 1150, Precision @ 1: 0.1152
Step 3515: loss = 28.17923
Step 3520: loss = 49.87728
Step 3525: loss = 25.65999
Step 3530: loss = 23.85813
Step 3535: loss = 50.87659
Step 3540: loss = 47.11756
Step 3545: loss = 76.41998
Step 3550: loss = 46.15954
Step 3555: loss = 54.13641
Step 3560: loss = 56.71788
Step 3565: loss = 37.22537
Step 3570: loss = 97.64362
Step 3575: loss = 77.82525
Step 3580: loss = 36.22185
Step 3585: loss = 74.44315
Step 3590: loss = 84.87624
Step 3595: loss = 80.15092
Step 3600: loss = 65.65607
Step 3605: loss = 51.67162
Step 3610: loss = 40.94045
Step 3615: loss = 59.54260
Step 3620: loss = 35.25272
Step 3625: loss = 69.31408
Step 3630: loss = 60.61186
Step 3635: loss = 52.49319
Step 3640: loss = 64.37029
Step 3645: loss = 74.83583
Step 3650: loss = 68.04945
Step 3655: loss = 67.05573
Step 3660: loss = 12.63375
Step 3665: loss = 61.42995
Step 3670: loss = 50.22273
Step 3675: loss = 63.96847
Step 3680: loss = 82.18729
Step 3685: loss = 66.03003
Step 3690: loss = 38.93697
Step 3695: loss = 34.14926
Step 3700: loss = 35.18389
Step 3705: loss = 42.92456
Step 3710: loss = 39.43303
Step 3715: loss = 43.16132
Step 3720: loss = 17.09701
Step 3725: loss = 47.81708
Step 3730: loss = 26.51337
Step 3735: loss = 56.53691
Step 3740: loss = 59.67785
Step 3745: loss = 46.94726
Step 3750: loss = 47.28488
Step 3755: loss = 41.01574
Step 3760: loss = 61.28762
Step 3765: loss = 31.00832
Step 3770: loss = 50.06927
Step 3775: loss = 55.69085
Step 3780: loss = 29.43280
Step 3785: loss = 52.33273
Step 3790: loss = 41.78078
Step 3795: loss = 23.14123
Step 3800: loss = 21.67468
Step 3805: loss = 53.21944
Step 3810: loss = 45.27561
Step 3815: loss = 30.28169
Step 3820: loss = 30.36456
Step 3825: loss = 33.19985
Step 3830: loss = 47.26082
Step 3835: loss = 16.64858
Step 3840: loss = 40.15720
Step 3845: loss = 43.47759
Step 3850: loss = 20.89666
Step 3855: loss = 27.01389
Step 3860: loss = 18.55840
Step 3865: loss = 16.60184
Step 3870: loss = 13.57537
Step 3875: loss = 18.79541
Step 3880: loss = 18.42563
Step 3885: loss = 8.77147
Step 3890: loss = 18.00813
Step 3895: loss = 22.77146
Step 3900: loss = 17.35041
Training Data Eval:
  Num examples: 49920, Num correct: 6175, Precision @ 1: 0.1237
('Testing Data Eval: EPOCH->', 11)
  Num examples: 9984, Num correct: 1262, Precision @ 1: 0.1264
Step 3905: loss = 34.55318
Step 3910: loss = 16.60194
Step 3915: loss = 14.51319
Step 3920: loss = 15.63780
Step 3925: loss = 22.78107
Step 3930: loss = 12.51409
Step 3935: loss = 13.78662
Step 3940: loss = 18.05305
Step 3945: loss = 10.57091
Step 3950: loss = 7.38964
Step 3955: loss = 5.02744
Step 3960: loss = 12.23465
Step 3965: loss = 15.44238
Step 3970: loss = 3.09075
Step 3975: loss = 10.22946
Step 3980: loss = 9.80269
Step 3985: loss = 11.50295
Step 3990: loss = 14.60378
Step 3995: loss = 13.39747
Step 4000: loss = 14.42059
Step 4005: loss = 8.98615
Step 4010: loss = 10.60127
Step 4015: loss = 23.71670
Step 4020: loss = 25.63540
Step 4025: loss = 23.17669
Step 4030: loss = 25.90907
Step 4035: loss = 26.89293
Step 4040: loss = 34.52402
Step 4045: loss = 22.16539
Step 4050: loss = 39.27471
Step 4055: loss = 5.92775
Step 4060: loss = 27.97818
Step 4065: loss = 43.60207
Step 4070: loss = 28.76093
Step 4075: loss = 7.17516
Step 4080: loss = 28.45081
Step 4085: loss = 40.22755
Step 4090: loss = 15.59713
Step 4095: loss = 29.63468
Step 4100: loss = 33.42686
Step 4105: loss = 23.57361
Step 4110: loss = 28.91243
Step 4115: loss = 48.24710
Step 4120: loss = 7.91068
Step 4125: loss = 34.00206
Step 4130: loss = 21.47582
Step 4135: loss = 18.16452
Step 4140: loss = 22.88570
Step 4145: loss = 14.50400
Step 4150: loss = 14.29653
Step 4155: loss = 19.91893
Step 4160: loss = 24.55329
Step 4165: loss = 22.98712
Step 4170: loss = 9.48800
Step 4175: loss = 29.34353
Step 4180: loss = 26.50951
Step 4185: loss = 20.49386
Step 4190: loss = 27.87568
Step 4195: loss = 17.59525
Step 4200: loss = 18.84934
Step 4205: loss = 10.84628
Step 4210: loss = 17.37219
Step 4215: loss = 33.55840
Step 4220: loss = 16.25200
Step 4225: loss = 25.83327
Step 4230: loss = 18.96595
Step 4235: loss = 15.02012
Step 4240: loss = 12.49180
Step 4245: loss = 28.17785
Step 4250: loss = 9.95332
Step 4255: loss = 26.33058
Step 4260: loss = 10.96565
Step 4265: loss = 22.34387
Step 4270: loss = 9.97293
Step 4275: loss = 5.11413
Step 4280: loss = 17.43293
Step 4285: loss = 6.81752
Step 4290: loss = 14.93478
Training Data Eval:
  Num examples: 49920, Num correct: 6362, Precision @ 1: 0.1274
('Testing Data Eval: EPOCH->', 12)
  Num examples: 9984, Num correct: 1229, Precision @ 1: 0.1231
Step 4295: loss = 15.28074
Step 4300: loss = 15.04678
Step 4305: loss = 14.23372
Step 4310: loss = 10.65045
Step 4315: loss = 5.57846
Step 4320: loss = 12.55636
Step 4325: loss = 12.85256
Step 4330: loss = 4.72097
Step 4335: loss = 7.14172
Step 4340: loss = 3.82208
Step 4345: loss = 6.51604
Step 4350: loss = 3.03098
Step 4355: loss = 11.09556
Step 4360: loss = 7.90364
Step 4365: loss = 7.91719
Step 4370: loss = 9.43698
Step 4375: loss = 6.13479
Step 4380: loss = 14.16201
Step 4385: loss = 13.62561
Step 4390: loss = 7.74343
Step 4395: loss = 2.91894
Step 4400: loss = 5.52428
Step 4405: loss = 6.77840
Step 4410: loss = 5.82332
Step 4415: loss = 3.00406
Step 4420: loss = 6.33997
Step 4425: loss = 8.07489
Step 4430: loss = 2.91946
Step 4435: loss = 4.61327
Step 4440: loss = 3.61654
Step 4445: loss = 6.51670
Step 4450: loss = 7.03699
Step 4455: loss = 4.70240
Step 4460: loss = 2.68518
Step 4465: loss = 2.91528
Step 4470: loss = 3.61393
Step 4475: loss = 2.80501
Step 4480: loss = 3.84763
Step 4485: loss = 3.11061
Step 4490: loss = 3.12554
Step 4495: loss = 2.80746
Step 4500: loss = 2.83431
Step 4505: loss = 2.75629
Step 4510: loss = 2.82270
Step 4515: loss = 3.13084
Step 4520: loss = 2.47687
Step 4525: loss = 4.59782
Step 4530: loss = 2.71049
Step 4535: loss = 3.15094
Step 4540: loss = 3.10497
Step 4545: loss = 4.20844
Step 4550: loss = 2.97154
Step 4555: loss = 3.04083
Step 4560: loss = 3.38966
Step 4565: loss = 2.74930
Step 4570: loss = 4.04477
Step 4575: loss = 3.87738
Step 4580: loss = 3.18238
Step 4585: loss = 3.07236
Step 4590: loss = 3.15089
Step 4595: loss = 3.19452
Step 4600: loss = 2.90038
Step 4605: loss = 2.71495
Step 4610: loss = 2.77914
Step 4615: loss = 2.88828
Step 4620: loss = 3.06349
Step 4625: loss = 2.77226
Step 4630: loss = 2.58032
Step 4635: loss = 2.78137
Step 4640: loss = 2.80961
Step 4645: loss = 2.88622
Step 4650: loss = 2.85193
Step 4655: loss = 2.91579
Step 4660: loss = 2.67442
Step 4665: loss = 2.86918
Step 4670: loss = 2.83480
Step 4675: loss = 2.92003
Step 4680: loss = 3.12903
Training Data Eval:
  Num examples: 49920, Num correct: 6543, Precision @ 1: 0.1311
('Testing Data Eval: EPOCH->', 13)
  Num examples: 9984, Num correct: 1327, Precision @ 1: 0.1329
Step 4685: loss = 2.86011
Step 4690: loss = 2.68792
Step 4695: loss = 2.81532
Step 4700: loss = 3.04472
Step 4705: loss = 3.18895
Step 4710: loss = 3.06314
Step 4715: loss = 2.52873
Step 4720: loss = 2.82733
Step 4725: loss = 2.82919
Step 4730: loss = 2.72005
Step 4735: loss = 2.86018
Step 4740: loss = 2.89386
Step 4745: loss = 2.71024
Step 4750: loss = 2.89529
Step 4755: loss = 3.03059
Step 4760: loss = 2.62558
Step 4765: loss = 2.83800
Step 4770: loss = 2.79596
Step 4775: loss = 2.74008
Step 4780: loss = 2.68645
Step 4785: loss = 2.64547
Step 4790: loss = 2.51498
Step 4795: loss = 2.78580
Step 4800: loss = 2.92633
Step 4805: loss = 2.65589
Step 4810: loss = 2.75104
Step 4815: loss = 2.76197
Step 4820: loss = 2.71640
Step 4825: loss = 2.75771
Step 4830: loss = 2.92589
Step 4835: loss = 2.96858
Step 4840: loss = 2.66488
Step 4845: loss = 2.66479
Step 4850: loss = 2.57231
Step 4855: loss = 2.67649
Step 4860: loss = 2.85645
Step 4865: loss = 2.89391
Step 4870: loss = 2.68800
Step 4875: loss = 2.85582
Step 4880: loss = 3.53446
Step 4885: loss = 4.39470
Step 4890: loss = 3.55518
Step 4895: loss = 3.34272
Step 4900: loss = 3.97845
Step 4905: loss = 3.08050
Step 4910: loss = 3.51851
Step 4915: loss = 3.94849
Step 4920: loss = 3.34311
Step 4925: loss = 3.52563
Step 4930: loss = 4.32957
Step 4935: loss = 3.09916
Step 4940: loss = 3.73095
Step 4945: loss = 3.43757
Step 4950: loss = 4.80096
Step 4955: loss = 3.08142
Step 4960: loss = 2.95266
Step 4965: loss = 3.58114
Step 4970: loss = 2.75940
Step 4975: loss = 3.29333
Step 4980: loss = 3.22824
Step 4985: loss = 3.33452
Step 4990: loss = 2.63874
Step 4995: loss = 2.92255
Step 5000: loss = 2.85404
Step 5005: loss = 3.02860
Step 5010: loss = 2.66319
Step 5015: loss = 2.76003
Step 5020: loss = 2.86922
Step 5025: loss = 2.88554
Step 5030: loss = 3.11430
Step 5035: loss = 2.86761
Step 5040: loss = 2.80063
Step 5045: loss = 2.81784
Step 5050: loss = 2.79157
Step 5055: loss = 2.63639
Step 5060: loss = 2.75471
Step 5065: loss = 3.12834
Step 5070: loss = 2.75079
Training Data Eval:
  Num examples: 49920, Num correct: 6232, Precision @ 1: 0.1248
('Testing Data Eval: EPOCH->', 14)
  Num examples: 9984, Num correct: 1260, Precision @ 1: 0.1262
Step 5075: loss = 2.72679
Step 5080: loss = 2.74650
Step 5085: loss = 2.88090
Step 5090: loss = 2.97702
Step 5095: loss = 2.76150
Step 5100: loss = 2.65530
Step 5105: loss = 2.67741
Step 5110: loss = 2.97683
Step 5115: loss = 2.92536
Step 5120: loss = 2.77503
Step 5125: loss = 2.89710
Step 5130: loss = 2.96143
Step 5135: loss = 2.68485
Step 5140: loss = 2.83855
Step 5145: loss = 2.69150
Step 5150: loss = 2.81374
Step 5155: loss = 2.94523
Step 5160: loss = 3.22639
Step 5165: loss = 2.55832
Step 5170: loss = 2.74181
Step 5175: loss = 2.89695
Step 5180: loss = 2.93789
Step 5185: loss = 2.77638
Step 5190: loss = 2.52155
Step 5195: loss = 2.66321
Step 5200: loss = 2.70779
Step 5205: loss = 2.71490
Step 5210: loss = 2.78559
Step 5215: loss = 2.86807
Step 5220: loss = 2.93476
Step 5225: loss = 2.89254
Step 5230: loss = 2.72054
Step 5235: loss = 2.73864
Step 5240: loss = 2.80134
Step 5245: loss = 2.65849
Step 5250: loss = 2.83441
Step 5255: loss = 2.71927
Step 5260: loss = 2.71041
Step 5265: loss = 2.68955
Step 5270: loss = 2.88135
Step 5275: loss = 2.76005
Step 5280: loss = 2.78581
Step 5285: loss = 2.98442
Step 5290: loss = 2.51286
Step 5295: loss = 2.64981
Step 5300: loss = 2.70617
Step 5305: loss = 2.92332
Step 5310: loss = 2.64705
Step 5315: loss = 2.90127
Step 5320: loss = 2.93550
Step 5325: loss = 2.80735
Step 5330: loss = 2.83456
Step 5335: loss = 2.55328
Step 5340: loss = 2.90307
Step 5345: loss = 3.05691
Step 5350: loss = 2.76847
Step 5355: loss = 2.84427
Step 5360: loss = 3.02034
Step 5365: loss = 2.75653
Step 5370: loss = 2.86829
Step 5375: loss = 2.81699
Step 5380: loss = 2.92468
Step 5385: loss = 3.00489
Step 5390: loss = 2.55744
Step 5395: loss = 2.77547
Step 5400: loss = 2.87433
Step 5405: loss = 2.73016
Step 5410: loss = 2.77329
Step 5415: loss = 2.79243
Step 5420: loss = 2.66222
Step 5425: loss = 2.70708
Step 5430: loss = 2.88641
Step 5435: loss = 2.96061
Step 5440: loss = 2.75231
Step 5445: loss = 2.74773
Step 5450: loss = 2.86841
Step 5455: loss = 2.90593
Step 5460: loss = 2.70524
Training Data Eval:
  Num examples: 49920, Num correct: 6449, Precision @ 1: 0.1292
('Testing Data Eval: EPOCH->', 15)
  Num examples: 9984, Num correct: 1274, Precision @ 1: 0.1276
Step 5465: loss = 2.67488
Step 5470: loss = 2.96888
Step 5475: loss = 2.78713
Step 5480: loss = 2.92949
Step 5485: loss = 2.77943
Step 5490: loss = 2.80421
Step 5495: loss = 2.75128
Step 5500: loss = 2.63549
Step 5505: loss = 2.66369
Step 5510: loss = 2.82972
Step 5515: loss = 2.60994
Step 5520: loss = 3.03795
Step 5525: loss = 2.66767
Step 5530: loss = 2.71566
Step 5535: loss = 2.92773
Step 5540: loss = 2.77575
Step 5545: loss = 2.62345
Step 5550: loss = 2.70316
Step 5555: loss = 2.84219
Step 5560: loss = 2.74656
Step 5565: loss = 2.84355
Step 5570: loss = 2.88005
Step 5575: loss = 2.52194
Step 5580: loss = 2.73410
Step 5585: loss = 2.85619
Step 5590: loss = 2.87785
Step 5595: loss = 2.99296
Step 5600: loss = 2.78202
Step 5605: loss = 2.72421
Step 5610: loss = 2.91490
Step 5615: loss = 2.81278
Step 5620: loss = 2.83465
Step 5625: loss = 2.93596
Step 5630: loss = 2.82850
Step 5635: loss = 2.86394
Step 5640: loss = 2.67845
Step 5645: loss = 2.71213
Step 5650: loss = 2.81772
Step 5655: loss = 2.60075
Step 5660: loss = 2.69234
Step 5665: loss = 2.81171
Step 5670: loss = 2.81577
Step 5675: loss = 2.94085
Step 5680: loss = 2.77776
Step 5685: loss = 2.52313
Step 5690: loss = 2.67194
Step 5695: loss = 3.09736
Step 5700: loss = 2.68136
Step 5705: loss = 2.82078
Step 5710: loss = 2.68544
Step 5715: loss = 2.68772
Step 5720: loss = 2.65196
Step 5725: loss = 2.90593
Step 5730: loss = 2.69774
Step 5735: loss = 2.62450
Step 5740: loss = 2.93921
Step 5745: loss = 2.82753
Step 5750: loss = 2.69721
Step 5755: loss = 2.89584
Step 5760: loss = 2.97333
Step 5765: loss = 2.85723
Step 5770: loss = 2.98301
Step 5775: loss = 2.86279
Step 5780: loss = 2.67397
Step 5785: loss = 2.91519
Step 5790: loss = 2.64277
Step 5795: loss = 2.78463
Step 5800: loss = 2.91548
Step 5805: loss = 2.55861
Step 5810: loss = 3.34386
Step 5815: loss = 2.85213
Step 5820: loss = 2.69785
Step 5825: loss = 2.73994
Step 5830: loss = 2.52696
Step 5835: loss = 2.91218
Step 5840: loss = 2.66678
Step 5845: loss = 2.75490
Step 5850: loss = 2.70994
Training Data Eval:
  Num examples: 49920, Num correct: 6343, Precision @ 1: 0.1271
('Testing Data Eval: EPOCH->', 16)
  Num examples: 9984, Num correct: 1289, Precision @ 1: 0.1291
Step 5855: loss = 2.68723
Step 5860: loss = 2.87583
Step 5865: loss = 2.75135
Step 5870: loss = 2.55948
Step 5875: loss = 2.66487
Step 5880: loss = 2.63631
Step 5885: loss = 2.65207
Step 5890: loss = 2.67427
Step 5895: loss = 2.76970
Step 5900: loss = 2.61905
Step 5905: loss = 2.85096
Step 5910: loss = 3.00086
Step 5915: loss = 3.12422
Step 5920: loss = 2.60731
Step 5925: loss = 2.71207
Step 5930: loss = 2.90351
Step 5935: loss = 2.79506
Step 5940: loss = 2.79580
Step 5945: loss = 2.75845
Step 5950: loss = 2.63407
Step 5955: loss = 2.89524
Step 5960: loss = 2.77752
Step 5965: loss = 3.18583
Step 5970: loss = 2.93710
Step 5975: loss = 2.72824
Step 5980: loss = 2.78283
Step 5985: loss = 2.61392
Step 5990: loss = 2.71426
Step 5995: loss = 2.69587
Step 6000: loss = 2.53136
Step 6005: loss = 2.98209
Step 6010: loss = 2.77337
Step 6015: loss = 2.68966
Step 6020: loss = 2.90491
Step 6025: loss = 2.86843
Step 6030: loss = 2.62528
Step 6035: loss = 2.79908
Step 6040: loss = 2.61979
Step 6045: loss = 2.74602
Step 6050: loss = 2.87619
Step 6055: loss = 2.69533
Step 6060: loss = 2.75046
Step 6065: loss = 2.70104
Step 6070: loss = 2.58462
Step 6075: loss = 2.66220
Step 6080: loss = 2.79242
Step 6085: loss = 2.70739
Step 6090: loss = 2.73281
Step 6095: loss = 2.71360
Step 6100: loss = 2.70078
Step 6105: loss = 2.76772
Step 6110: loss = 2.70536
Step 6115: loss = 2.79143
Step 6120: loss = 2.72093
Step 6125: loss = 2.54159
Step 6130: loss = 2.78824
Step 6135: loss = 2.74082
Step 6140: loss = 2.68702
Step 6145: loss = 2.73458
Step 6150: loss = 2.74279
Step 6155: loss = 2.94568
Step 6160: loss = 2.75022
Step 6165: loss = 2.68620
Step 6170: loss = 2.77145
Step 6175: loss = 2.82382
Step 6180: loss = 2.61489
Step 6185: loss = 2.80832
Step 6190: loss = 2.79721
Step 6195: loss = 2.81788
Step 6200: loss = 2.74859
Step 6205: loss = 2.81030
Step 6210: loss = 2.92402
Step 6215: loss = 2.96986
Step 6220: loss = 2.86211
Step 6225: loss = 2.70577
Step 6230: loss = 2.71229
Step 6235: loss = 2.69400
Step 6240: loss = 2.51960
Training Data Eval:
  Num examples: 49920, Num correct: 6295, Precision @ 1: 0.1261
('Testing Data Eval: EPOCH->', 17)
  Num examples: 9984, Num correct: 1248, Precision @ 1: 0.1250
Step 6245: loss = 2.85445
Step 6250: loss = 2.79349
Step 6255: loss = 2.77472
Step 6260: loss = 2.79888
Step 6265: loss = 2.64771
Step 6270: loss = 2.77141
Step 6275: loss = 2.81185
Step 6280: loss = 2.81489
Step 6285: loss = 2.85948
Step 6290: loss = 2.81601
Step 6295: loss = 2.68359
Step 6300: loss = 2.72020
Step 6305: loss = 2.89025
Step 6310: loss = 2.73339
Step 6315: loss = 2.60537
Step 6320: loss = 2.87688
Step 6325: loss = 2.72392
Step 6330: loss = 3.02653
Step 6335: loss = 3.12321
Step 6340: loss = 2.89263
Step 6345: loss = 2.91867
Step 6350: loss = 2.84272
Step 6355: loss = 2.70710
Step 6360: loss = 2.86921
Step 6365: loss = 2.65765
Step 6370: loss = 2.72039
Step 6375: loss = 2.68273
Step 6380: loss = 2.53719
Step 6385: loss = 2.67855
Step 6390: loss = 2.73121
Step 6395: loss = 2.76846
Step 6400: loss = 2.73149
Step 6405: loss = 2.80205
Step 6410: loss = 2.61629
Step 6415: loss = 2.51682
Step 6420: loss = 2.60233
Step 6425: loss = 2.63372
Step 6430: loss = 2.72421
Step 6435: loss = 2.95226
Step 6440: loss = 2.56010
Step 6445: loss = 2.83386
Step 6450: loss = 2.77819
Step 6455: loss = 2.60416
Step 6460: loss = 2.58766
Step 6465: loss = 2.60007
Step 6470: loss = 2.95749
Step 6475: loss = 2.64785
Step 6480: loss = 2.72743
Step 6485: loss = 2.90958
Step 6490: loss = 2.81722
Step 6495: loss = 2.69996
Step 6500: loss = 2.90921
Step 6505: loss = 2.88362
Step 6510: loss = 2.55013
Step 6515: loss = 2.73310
Step 6520: loss = 2.70696
Step 6525: loss = 3.00607
Step 6530: loss = 2.93050
Step 6535: loss = 2.66084
Step 6540: loss = 2.68533
Step 6545: loss = 2.73867
Step 6550: loss = 2.60135
Step 6555: loss = 2.54179
Step 6560: loss = 2.72712
Step 6565: loss = 2.63442
Step 6570: loss = 2.63739
Step 6575: loss = 2.64583
Step 6580: loss = 2.75882
Step 6585: loss = 3.14138
Step 6590: loss = 2.87180
Step 6595: loss = 2.88261
Step 6600: loss = 2.72199
Step 6605: loss = 2.62942
Step 6610: loss = 2.79572
Step 6615: loss = 2.77331
Step 6620: loss = 2.65654
Step 6625: loss = 2.87411
Step 6630: loss = 2.65888
Training Data Eval:
  Num examples: 49920, Num correct: 6519, Precision @ 1: 0.1306
('Testing Data Eval: EPOCH->', 18)
  Num examples: 9984, Num correct: 1262, Precision @ 1: 0.1264
Step 6635: loss = 2.89980
Step 6640: loss = 2.66490
Step 6645: loss = 2.66645
Step 6650: loss = 2.59248
Step 6655: loss = 2.69205
Step 6660: loss = 2.70616
Step 6665: loss = 2.66468
Step 6670: loss = 2.70858
Step 6675: loss = 2.74113
Step 6680: loss = 2.80827
Step 6685: loss = 2.77304
Step 6690: loss = 2.57535
Step 6695: loss = 2.59180
Step 6700: loss = 2.42555
Step 6705: loss = 2.70298
Step 6710: loss = 2.79530
Step 6715: loss = 2.84163
Step 6720: loss = 2.75048
Step 6725: loss = 2.88461
Step 6730: loss = 2.77218
Step 6735: loss = 2.71003
Step 6740: loss = 2.50247
Step 6745: loss = 2.65140
Step 6750: loss = 2.74198
Step 6755: loss = 2.72259
Step 6760: loss = 2.60444
Step 6765: loss = 2.68712
Step 6770: loss = 2.65716
Step 6775: loss = 2.73686
Step 6780: loss = 2.93372
Step 6785: loss = 2.73755
Step 6790: loss = 2.68843
Step 6795: loss = 2.92499
Step 6800: loss = 2.71356
Step 6805: loss = 2.74956
Step 6810: loss = 2.85222
Step 6815: loss = 2.68404
Step 6820: loss = 2.62023
Step 6825: loss = 2.57226
Step 6830: loss = 2.94961
Step 6835: loss = 2.75950
Step 6840: loss = 2.68014
Step 6845: loss = 2.84242
Step 6850: loss = 2.67968
Step 6855: loss = 2.85589
Step 6860: loss = 2.87747
Step 6865: loss = 2.62226
Step 6870: loss = 2.51207
Step 6875: loss = 2.45412
Step 6880: loss = 2.47492
Step 6885: loss = 2.74189
Step 6890: loss = 2.53724
Step 6895: loss = 2.78932
Step 6900: loss = 2.98213
Step 6905: loss = 3.05678
Step 6910: loss = 2.70842
Step 6915: loss = 2.76593
Step 6920: loss = 2.67206
Step 6925: loss = 2.84570
Step 6930: loss = 2.81822
Step 6935: loss = 2.56740
Step 6940: loss = 2.66966
Step 6945: loss = 2.66207
Step 6950: loss = 2.72933
Step 6955: loss = 2.59506
Step 6960: loss = 2.56823
Step 6965: loss = 2.46875
Step 6970: loss = 2.62696
Step 6975: loss = 2.71611
Step 6980: loss = 2.75432
Step 6985: loss = 2.65113
Step 6990: loss = 2.59153
Step 6995: loss = 2.68812
Step 7000: loss = 2.64057
Step 7005: loss = 2.73720
Step 7010: loss = 2.70535
Step 7015: loss = 2.94566
Step 7020: loss = 2.66335
Training Data Eval:
  Num examples: 49920, Num correct: 6359, Precision @ 1: 0.1274
('Testing Data Eval: EPOCH->', 19)
  Num examples: 9984, Num correct: 1282, Precision @ 1: 0.1284
Step 7025: loss = 2.84549
Step 7030: loss = 2.82485
Step 7035: loss = 2.51098
Step 7040: loss = 2.58837
Step 7045: loss = 2.55334
Step 7050: loss = 2.73470
Step 7055: loss = 2.69609
Step 7060: loss = 2.70171
Step 7065: loss = 2.59372
Step 7070: loss = 2.84743
Step 7075: loss = 2.62417
Step 7080: loss = 2.62075
Step 7085: loss = 2.77253
Step 7090: loss = 2.76295
Step 7095: loss = 2.80917
Step 7100: loss = 2.74350
Step 7105: loss = 2.86616
Step 7110: loss = 2.84975
Step 7115: loss = 2.73156
Step 7120: loss = 2.50592
Step 7125: loss = 2.53167
Step 7130: loss = 2.77149
Step 7135: loss = 2.75880
Step 7140: loss = 2.68089
Step 7145: loss = 2.61357
Step 7150: loss = 2.66421
Step 7155: loss = 2.97087
Step 7160: loss = 2.66154
Step 7165: loss = 2.79618
Step 7170: loss = 2.94652
Step 7175: loss = 2.69328
Step 7180: loss = 2.89586
Step 7185: loss = 2.92819
Step 7190: loss = 2.77923
Step 7195: loss = 2.85901
Step 7200: loss = 2.64468
Step 7205: loss = 2.50573
Step 7210: loss = 2.55962
Step 7215: loss = 2.70157
Step 7220: loss = 2.98362
Step 7225: loss = 2.79610
Step 7230: loss = 2.58161
Step 7235: loss = 2.73759
Step 7240: loss = 2.57998
Step 7245: loss = 2.65959
Step 7250: loss = 2.52286
Step 7255: loss = 2.78818
Step 7260: loss = 2.78397
Step 7265: loss = 2.72709
Step 7270: loss = 2.54161
Step 7275: loss = 2.70822
Step 7280: loss = 2.67112
Step 7285: loss = 2.78484
Step 7290: loss = 2.73282
Step 7295: loss = 2.76635
Step 7300: loss = 2.76750
Step 7305: loss = 2.69192
Step 7310: loss = 2.68452
Step 7315: loss = 2.78130
Step 7320: loss = 2.85179
Step 7325: loss = 2.76413
Step 7330: loss = 2.64410
Step 7335: loss = 2.63739
Step 7340: loss = 2.72573
Step 7345: loss = 2.63870
Step 7350: loss = 2.69341
Step 7355: loss = 2.59356
Step 7360: loss = 2.66818
Step 7365: loss = 2.70747
Step 7370: loss = 2.77998
Step 7375: loss = 2.75300
Step 7380: loss = 2.60457
Step 7385: loss = 2.53900
Step 7390: loss = 2.97180
Step 7395: loss = 2.67318
Step 7400: loss = 2.64872
Step 7405: loss = 2.93420
Step 7410: loss = 2.69053
Training Data Eval:
  Num examples: 49920, Num correct: 6634, Precision @ 1: 0.1329
('Testing Data Eval: EPOCH->', 20)
  Num examples: 9984, Num correct: 1338, Precision @ 1: 0.1340
Step 7415: loss = 2.82306
Step 7420: loss = 2.69425
Step 7425: loss = 2.65222
Step 7430: loss = 2.61712
Step 7435: loss = 2.48836
Step 7440: loss = 2.73657
Step 7445: loss = 2.64254
Step 7450: loss = 2.67353
Step 7455: loss = 2.73388
Step 7460: loss = 2.64079
Step 7465: loss = 2.61595
Step 7470: loss = 2.62728
Step 7475: loss = 2.84738
Step 7480: loss = 2.52937
Step 7485: loss = 2.59311
Step 7490: loss = 2.67791
Step 7495: loss = 2.75177
Step 7500: loss = 2.67921
Step 7505: loss = 2.60125
Step 7510: loss = 2.67778
Step 7515: loss = 2.65631
Step 7520: loss = 2.60460
Step 7525: loss = 2.57600
Step 7530: loss = 2.82790
Step 7535: loss = 2.73128
Step 7540: loss = 2.74169
Step 7545: loss = 2.67350
Step 7550: loss = 2.63479
Step 7555: loss = 2.59141
Step 7560: loss = 2.54387
Step 7565: loss = 2.82813
Step 7570: loss = 2.74407
Step 7575: loss = 2.63664
Step 7580: loss = 2.82678
Step 7585: loss = 2.58630
Step 7590: loss = 2.62821
Step 7595: loss = 2.54059
Step 7600: loss = 2.65561
Step 7605: loss = 2.66926
Step 7610: loss = 2.84004
Step 7615: loss = 2.87479
Step 7620: loss = 2.50231
Step 7625: loss = 2.74285
Step 7630: loss = 2.67274
Step 7635: loss = 2.62356
Step 7640: loss = 2.73745
Step 7645: loss = 2.82494
Step 7650: loss = 2.72954
Step 7655: loss = 2.61992
Step 7660: loss = 2.67448
Step 7665: loss = 2.58152
Step 7670: loss = 2.59283
Step 7675: loss = 2.84733
Step 7680: loss = 2.67620
Step 7685: loss = 2.51246
Step 7690: loss = 2.67420
Step 7695: loss = 2.77426
Step 7700: loss = 2.60859
Step 7705: loss = 2.77093
Step 7710: loss = 2.66327
Step 7715: loss = 2.92410
Step 7720: loss = 2.62979
Step 7725: loss = 2.50741
Step 7730: loss = 2.64330
Step 7735: loss = 2.66991
Step 7740: loss = 2.72137
Step 7745: loss = 2.67363
Step 7750: loss = 2.64170
Step 7755: loss = 2.72832
Step 7760: loss = 2.77800
Step 7765: loss = 2.61209
Step 7770: loss = 2.75012
Step 7775: loss = 2.65701
Step 7780: loss = 2.90382
Step 7785: loss = 2.93527
Step 7790: loss = 2.53523
Step 7795: loss = 2.79234
Step 7800: loss = 2.61262
Training Data Eval:
  Num examples: 49920, Num correct: 6505, Precision @ 1: 0.1303
('Testing Data Eval: EPOCH->', 21)
  Num examples: 9984, Num correct: 1309, Precision @ 1: 0.1311
Step 7805: loss = 2.60212
Step 7810: loss = 2.46907
Step 7815: loss = 2.39996
Step 7820: loss = 2.79676
Step 7825: loss = 2.65078
Step 7830: loss = 2.62357
Step 7835: loss = 2.84568
Step 7840: loss = 2.69564
Step 7845: loss = 2.66642
Step 7850: loss = 2.71704
Step 7855: loss = 2.54768
Step 7860: loss = 2.65166
Step 7865: loss = 2.68567
Step 7870: loss = 2.62960
Step 7875: loss = 2.73701
Step 7880: loss = 2.63323
Step 7885: loss = 2.65902
Step 7890: loss = 2.64661
Step 7895: loss = 2.85065
Step 7900: loss = 2.57099
Step 7905: loss = 2.56411
Step 7910: loss = 2.64335
Step 7915: loss = 2.75829
Step 7920: loss = 2.43034
Step 7925: loss = 2.70189
Step 7930: loss = 2.72180
Step 7935: loss = 2.64590
Step 7940: loss = 2.73764
Step 7945: loss = 2.70444
Step 7950: loss = 2.62994
Step 7955: loss = 2.59235
Step 7960: loss = 2.61657
Step 7965: loss = 2.60003
Step 7970: loss = 2.69974
Step 7975: loss = 2.81575
Step 7980: loss = 2.86595
Step 7985: loss = 2.69587
Step 7990: loss = 2.65073
Step 7995: loss = 2.56846
Step 8000: loss = 2.63414
Step 8005: loss = 2.77017
Step 8010: loss = 2.72166
Step 8015: loss = 2.70735
Step 8020: loss = 2.68857
Step 8025: loss = 2.66142
Step 8030: loss = 2.66523
Step 8035: loss = 2.54305
Step 8040: loss = 2.66435
Step 8045: loss = 2.84076
Step 8050: loss = 2.81159
Step 8055: loss = 2.82579
Step 8060: loss = 2.52561
Step 8065: loss = 2.75458
Step 8070: loss = 2.69734
Step 8075: loss = 2.78925
Step 8080: loss = 2.66957
Step 8085: loss = 2.61916
Step 8090: loss = 2.55463
Step 8095: loss = 2.92018
Step 8100: loss = 2.79121
Step 8105: loss = 2.82472
Step 8110: loss = 2.76479
Step 8115: loss = 2.60931
Step 8120: loss = 2.72261
Step 8125: loss = 2.59921
Step 8130: loss = 2.50118
Step 8135: loss = 2.70729
Step 8140: loss = 2.75026
Step 8145: loss = 2.83680
Step 8150: loss = 2.79728
Step 8155: loss = 2.65843
Step 8160: loss = 2.51969
Step 8165: loss = 2.66202
Step 8170: loss = 2.93102
Step 8175: loss = 2.71455
Step 8180: loss = 2.55537
Step 8185: loss = 2.73977
Step 8190: loss = 2.50054
Training Data Eval:
  Num examples: 49920, Num correct: 6803, Precision @ 1: 0.1363
('Testing Data Eval: EPOCH->', 22)
  Num examples: 9984, Num correct: 1406, Precision @ 1: 0.1408
Step 8195: loss = 2.80677
Step 8200: loss = 2.54307
Step 8205: loss = 2.62642
Step 8210: loss = 2.60360
Step 8215: loss = 2.67529
Step 8220: loss = 2.56170
Step 8225: loss = 2.57685
Step 8230: loss = 2.75629
Step 8235: loss = 2.56718
Step 8240: loss = 2.79956
Step 8245: loss = 2.59878
Step 8250: loss = 2.51505
Step 8255: loss = 2.55624
Step 8260: loss = 2.67739
Step 8265: loss = 2.59224
Step 8270: loss = 2.84285
Step 8275: loss = 2.86830
Step 8280: loss = 2.71122
Step 8285: loss = 2.73446
Step 8290: loss = 2.63374
Step 8295: loss = 2.69449
Step 8300: loss = 2.53204
Step 8305: loss = 2.52073
Step 8310: loss = 2.74657
Step 8315: loss = 2.55114
Step 8320: loss = 2.65277
Step 8325: loss = 2.82125
Step 8330: loss = 2.62316
Step 8335: loss = 2.62906
Step 8340: loss = 2.79640
Step 8345: loss = 2.69483
Step 8350: loss = 2.73373
Step 8355: loss = 2.64382
Step 8360: loss = 2.73360
Step 8365: loss = 2.56239
Step 8370: loss = 2.66682
Step 8375: loss = 2.87604
Step 8380: loss = 2.58223
Step 8385: loss = 2.61224
Step 8390: loss = 2.67008
Step 8395: loss = 2.75478
Step 8400: loss = 3.04163
Step 8405: loss = 2.62750
Step 8410: loss = 2.80014
Step 8415: loss = 2.78144
Step 8420: loss = 2.76245
Step 8425: loss = 2.79715
Step 8430: loss = 2.72778
Step 8435: loss = 2.64662
Step 8440: loss = 2.75760
Step 8445: loss = 2.87439
Step 8450: loss = 2.95446
Step 8455: loss = 2.83170
Step 8460: loss = 3.02382
Step 8465: loss = 2.33246
Step 8470: loss = 2.67699
Step 8475: loss = 2.73951
Step 8480: loss = 2.78511
Step 8485: loss = 2.67941
Step 8490: loss = 2.60464
Step 8495: loss = 2.68701
Step 8500: loss = 2.87024
Step 8505: loss = 2.88008
Step 8510: loss = 2.80879
Step 8515: loss = 2.83630
Step 8520: loss = 2.74501
Step 8525: loss = 2.73145
Step 8530: loss = 2.52800
Step 8535: loss = 2.70676
Step 8540: loss = 2.56071
Step 8545: loss = 2.73442
Step 8550: loss = 2.80948
Step 8555: loss = 2.55217
Step 8560: loss = 2.58162
Step 8565: loss = 2.72613
Step 8570: loss = 2.80712
Step 8575: loss = 2.54918
Step 8580: loss = 2.74355
Training Data Eval:
  Num examples: 49920, Num correct: 6530, Precision @ 1: 0.1308
('Testing Data Eval: EPOCH->', 23)
  Num examples: 9984, Num correct: 1259, Precision @ 1: 0.1261
Step 8585: loss = 2.55519
Step 8590: loss = 2.63997
Step 8595: loss = 2.76191
Step 8600: loss = 2.64729
Step 8605: loss = 2.44517
Step 8610: loss = 2.89140
Step 8615: loss = 2.89746
Step 8620: loss = 2.54810
Step 8625: loss = 2.65620
Step 8630: loss = 2.65764
Step 8635: loss = 2.67400
Step 8640: loss = 2.64508
Step 8645: loss = 2.84250
Step 8650: loss = 2.68961
Step 8655: loss = 2.83265
Step 8660: loss = 2.73823
Step 8665: loss = 2.65561
Step 8670: loss = 2.84719
Step 8675: loss = 2.64727
Step 8680: loss = 2.88663
Step 8685: loss = 2.91431
Step 8690: loss = 2.56273
Step 8695: loss = 2.80111
Step 8700: loss = 2.60726
Step 8705: loss = 2.75299
Step 8710: loss = 2.70310
Step 8715: loss = 2.99386
Step 8720: loss = 2.62122
Step 8725: loss = 2.53362
Step 8730: loss = 2.76950
Step 8735: loss = 2.49920
Step 8740: loss = 2.69880
Step 8745: loss = 2.70715
Step 8750: loss = 2.55884
Step 8755: loss = 2.61933
Step 8760: loss = 2.51903
Step 8765: loss = 2.60167
Step 8770: loss = 2.87464
Step 8775: loss = 2.61343
Step 8780: loss = 2.45618
Step 8785: loss = 2.55497
Step 8790: loss = 2.58725
Step 8795: loss = 2.74259
Step 8800: loss = 2.56487
Step 8805: loss = 2.71480
Step 8810: loss = 2.66281
Step 8815: loss = 2.71521
Step 8820: loss = 2.53987
Step 8825: loss = 2.75437
Step 8830: loss = 2.57259
Step 8835: loss = 2.75277
Step 8840: loss = 2.64663
Step 8845: loss = 2.72917
Step 8850: loss = 2.44204
Step 8855: loss = 2.59309
Step 8860: loss = 2.95303
Step 8865: loss = 2.61542
Step 8870: loss = 2.68618
Step 8875: loss = 2.55998
Step 8880: loss = 2.63525
Step 8885: loss = 2.52727
Step 8890: loss = 2.72712
Step 8895: loss = 2.73794
Step 8900: loss = 2.87869
Step 8905: loss = 2.72859
Step 8910: loss = 2.94398
Step 8915: loss = 2.64662
Step 8920: loss = 2.42966
Step 8925: loss = 2.72364
Step 8930: loss = 2.67465
Step 8935: loss = 2.69445
Step 8940: loss = 2.87216
Step 8945: loss = 2.67162
Step 8950: loss = 2.59798
Step 8955: loss = 2.83534
Step 8960: loss = 2.62114
Step 8965: loss = 2.64948
Step 8970: loss = 2.58640
Training Data Eval:
  Num examples: 49920, Num correct: 6330, Precision @ 1: 0.1268
('Testing Data Eval: EPOCH->', 24)
  Num examples: 9984, Num correct: 1269, Precision @ 1: 0.1271
Step 8975: loss = 2.63189
Step 8980: loss = 2.64932
Step 8985: loss = 2.77862
Step 8990: loss = 2.78676
Step 8995: loss = 2.71619
Step 9000: loss = 2.76191
Step 9005: loss = 2.52332
Step 9010: loss = 2.55121
Step 9015: loss = 2.56899
Step 9020: loss = 2.66770
Step 9025: loss = 2.61995
Step 9030: loss = 2.72174
Step 9035: loss = 2.49317
Step 9040: loss = 2.57176
Step 9045: loss = 2.95076
Step 9050: loss = 2.68590
Step 9055: loss = 2.66640
Step 9060: loss = 2.74028
Step 9065: loss = 2.63702
Step 9070: loss = 2.71169
Step 9075: loss = 2.65683
Step 9080: loss = 2.62666
Step 9085: loss = 2.70333
Step 9090: loss = 2.73905
Step 9095: loss = 2.68237
Step 9100: loss = 2.54886
Step 9105: loss = 2.65906
Step 9110: loss = 2.57205
Step 9115: loss = 2.70271
Step 9120: loss = 2.58093
Step 9125: loss = 2.59171
Step 9130: loss = 2.60957
Step 9135: loss = 2.73827
Step 9140: loss = 2.60478
Step 9145: loss = 2.58442
Step 9150: loss = 2.73460
Step 9155: loss = 2.49450
Step 9160: loss = 2.47551
Step 9165: loss = 2.85145
Step 9170: loss = 2.58758
Step 9175: loss = 2.71568
Step 9180: loss = 2.66773
Step 9185: loss = 2.64462
Step 9190: loss = 2.58613
Step 9195: loss = 2.60409
Step 9200: loss = 2.77459
Step 9205: loss = 2.88640
Step 9210: loss = 2.77301
Step 9215: loss = 3.05422
Step 9220: loss = 2.60843
Step 9225: loss = 2.60366
Step 9230: loss = 2.72558
Step 9235: loss = 2.72864
Step 9240: loss = 2.72869
Step 9245: loss = 2.74900
Step 9250: loss = 2.60875
Step 9255: loss = 2.79290
Step 9260: loss = 2.83632
Step 9265: loss = 2.63576
Step 9270: loss = 2.79746
Step 9275: loss = 2.76482
Step 9280: loss = 2.47016
Step 9285: loss = 2.63522
Step 9290: loss = 2.55071
Step 9295: loss = 2.66307
Step 9300: loss = 2.72359
Step 9305: loss = 2.50995
Step 9310: loss = 2.71984
Step 9315: loss = 2.63326
Step 9320: loss = 2.83594
Step 9325: loss = 2.70464
Step 9330: loss = 2.60918
Step 9335: loss = 2.75295
Step 9340: loss = 3.02771
Step 9345: loss = 2.76875
Step 9350: loss = 2.87988
Step 9355: loss = 2.43418
Step 9360: loss = 2.76126
Training Data Eval:
  Num examples: 49920, Num correct: 6438, Precision @ 1: 0.1290
('Testing Data Eval: EPOCH->', 25)
  Num examples: 9984, Num correct: 1296, Precision @ 1: 0.1298
Step 9365: loss = 2.80650
Step 9370: loss = 2.62827
Step 9375: loss = 2.66218
Step 9380: loss = 2.66006
Step 9385: loss = 2.51922
Step 9390: loss = 2.67306
Step 9395: loss = 2.65460
Step 9400: loss = 2.71410
Step 9405: loss = 2.55092
Step 9410: loss = 2.80020
Step 9415: loss = 2.76713
Step 9420: loss = 2.70054
Step 9425: loss = 2.69209
Step 9430: loss = 2.66339
Step 9435: loss = 2.53993
Step 9440: loss = 2.65124
Step 9445: loss = 2.80032
Step 9450: loss = 2.72728
Step 9455: loss = 2.65645
Step 9460: loss = 2.55650
Step 9465: loss = 2.78547
Step 9470: loss = 2.63178
Step 9475: loss = 2.67474
Step 9480: loss = 2.75249
Step 9485: loss = 2.76881
Step 9490: loss = 2.39822
Step 9495: loss = 2.71337
Step 9500: loss = 2.68691
Step 9505: loss = 2.68927
Step 9510: loss = 2.77388
Step 9515: loss = 2.83049
Step 9520: loss = 2.66891
Step 9525: loss = 2.66360
Step 9530: loss = 2.49269
Step 9535: loss = 2.77421
Step 9540: loss = 2.57313
Step 9545: loss = 2.72501
Step 9550: loss = 2.51593
Step 9555: loss = 2.55768
Step 9560: loss = 2.87523
Step 9565: loss = 2.53189
Step 9570: loss = 2.51487
Step 9575: loss = 2.73531
Step 9580: loss = 2.62833
Step 9585: loss = 2.53233
Step 9590: loss = 2.66296
Step 9595: loss = 2.69324
Step 9600: loss = 2.88142
Step 9605: loss = 2.66213
Step 9610: loss = 2.63083
Step 9615: loss = 2.62650
Step 9620: loss = 2.57107
Step 9625: loss = 2.67931
Step 9630: loss = 2.63369
Step 9635: loss = 2.98665
Step 9640: loss = 2.74318
Step 9645: loss = 2.72361
Step 9650: loss = 2.73613
Step 9655: loss = 2.85943
Step 9660: loss = 2.67192
Step 9665: loss = 2.59118
Step 9670: loss = 2.73216
Step 9675: loss = 2.73299
Step 9680: loss = 2.55207
Step 9685: loss = 2.77811
Step 9690: loss = 2.82423
Step 9695: loss = 2.77870
Step 9700: loss = 2.81844
Step 9705: loss = 2.65061
Step 9710: loss = 2.65597
Step 9715: loss = 2.78028
Step 9720: loss = 2.81189
Step 9725: loss = 2.95279
Step 9730: loss = 2.63658
Step 9735: loss = 2.62552
Step 9740: loss = 2.67065
Step 9745: loss = 2.48078
Step 9750: loss = 2.64208
Training Data Eval:
  Num examples: 49920, Num correct: 6490, Precision @ 1: 0.1300
('Testing Data Eval: EPOCH->', 26)
  Num examples: 9984, Num correct: 1318, Precision @ 1: 0.1320
Step 9755: loss = 2.69836
Step 9760: loss = 2.70880
Step 9765: loss = 2.84649
Step 9770: loss = 2.64650
Step 9775: loss = 2.72112
Step 9780: loss = 2.60197
Step 9785: loss = 2.69126
Step 9790: loss = 2.73259
Step 9795: loss = 2.87419
Step 9800: loss = 2.54348
Step 9805: loss = 2.71739
Step 9810: loss = 2.70306
Step 9815: loss = 2.63290
Step 9820: loss = 2.54896
Step 9825: loss = 2.67477
Step 9830: loss = 2.74412
Step 9835: loss = 2.69762
Step 9840: loss = 2.83929
Step 9845: loss = 2.79028
Step 9850: loss = 2.73714
Step 9855: loss = 2.56620
Step 9860: loss = 2.68956
Step 9865: loss = 2.87294
Step 9870: loss = 2.67897
Step 9875: loss = 2.58884
Step 9880: loss = 2.71514
Step 9885: loss = 2.52625
Step 9890: loss = 2.61056
Step 9895: loss = 2.79268
Step 9900: loss = 2.64953
Step 9905: loss = 2.72016
Step 9910: loss = 2.68665
Step 9915: loss = 2.67948
Step 9920: loss = 2.60482
Step 9925: loss = 2.85786
Step 9930: loss = 2.87339
Step 9935: loss = 2.70794
Step 9940: loss = 2.57050
Step 9945: loss = 2.82540
Step 9950: loss = 2.79923
Step 9955: loss = 2.77765
Step 9960: loss = 2.69543
Step 9965: loss = 2.44041
Step 9970: loss = 2.86541
Step 9975: loss = 2.80190
Step 9980: loss = 2.56824
Step 9985: loss = 2.56520
Step 9990: loss = 2.83771
Step 9995: loss = 2.76095
Step 10000: loss = 2.62779
Step 10005: loss = 2.62671
Step 10010: loss = 2.72720
Step 10015: loss = 2.37278
Step 10020: loss = 2.64204
Step 10025: loss = 2.61202
Step 10030: loss = 2.68727
Step 10035: loss = 2.63622
Step 10040: loss = 2.83576
Step 10045: loss = 2.73168
Step 10050: loss = 2.66243
Step 10055: loss = 2.70020
Step 10060: loss = 2.77719
Step 10065: loss = 2.58438
Step 10070: loss = 2.74904
Step 10075: loss = 2.63230
Step 10080: loss = 2.67132
Step 10085: loss = 2.50161
Step 10090: loss = 2.68187
Step 10095: loss = 2.72591
Step 10100: loss = 2.52413
Step 10105: loss = 2.70135
Step 10110: loss = 2.83119
Step 10115: loss = 2.79913
Step 10120: loss = 2.56115
Step 10125: loss = 2.75356
Step 10130: loss = 2.59043
Step 10135: loss = 2.59857
Step 10140: loss = 2.65727
Training Data Eval:
  Num examples: 49920, Num correct: 6807, Precision @ 1: 0.1364
('Testing Data Eval: EPOCH->', 27)
  Num examples: 9984, Num correct: 1415, Precision @ 1: 0.1417
Step 10145: loss = 2.60319
Step 10150: loss = 2.69952
Step 10155: loss = 2.67726
Step 10160: loss = 2.70399
Step 10165: loss = 2.62875
Step 10170: loss = 2.63236
Step 10175: loss = 2.93925
Step 10180: loss = 2.72963
Step 10185: loss = 2.68926
Step 10190: loss = 2.79368
Step 10195: loss = 2.61576
Step 10200: loss = 2.73823
Step 10205: loss = 2.68915
Step 10210: loss = 2.66291
Step 10215: loss = 2.60182
Step 10220: loss = 2.59663
Step 10225: loss = 2.73784
Step 10230: loss = 2.73289
Step 10235: loss = 2.71515
Step 10240: loss = 2.81128
Step 10245: loss = 2.66223
Step 10250: loss = 2.91015
Step 10255: loss = 2.87115
Step 10260: loss = 2.77272
Step 10265: loss = 2.75493
Step 10270: loss = 2.66301
Step 10275: loss = 2.59110
Step 10280: loss = 2.67461
Step 10285: loss = 2.98183
Step 10290: loss = 2.68074
Step 10295: loss = 2.78568
Step 10300: loss = 2.62783
Step 10305: loss = 2.69491
Step 10310: loss = 2.74861
Step 10315: loss = 2.64912
Step 10320: loss = 2.68569
Step 10325: loss = 2.73276
Step 10330: loss = 2.87811
Step 10335: loss = 2.88936
Step 10340: loss = 2.63700
Step 10345: loss = 2.67460
Step 10350: loss = 2.73390
Step 10355: loss = 2.68632
Step 10360: loss = 2.71113
Step 10365: loss = 2.41272
Step 10370: loss = 2.75321
Step 10375: loss = 2.70863
Step 10380: loss = 2.60266
Step 10385: loss = 2.85217
Step 10390: loss = 2.68317
Step 10395: loss = 2.60535
Step 10400: loss = 2.77237
Step 10405: loss = 2.89176
Step 10410: loss = 2.73685
Step 10415: loss = 2.58917
Step 10420: loss = 2.58908
Step 10425: loss = 2.50911
Step 10430: loss = 2.87859
Step 10435: loss = 2.67413
Step 10440: loss = 2.54856
Step 10445: loss = 2.63477
Step 10450: loss = 2.75833
Step 10455: loss = 2.68145
Step 10460: loss = 2.61906
Step 10465: loss = 2.61359
Step 10470: loss = 2.53698
Step 10475: loss = 2.51161
Step 10480: loss = 2.67100
Step 10485: loss = 2.57946
Step 10490: loss = 2.79380
Step 10495: loss = 2.63460
Step 10500: loss = 2.72393
Step 10505: loss = 2.40485
Step 10510: loss = 2.82840
Step 10515: loss = 2.85755
Step 10520: loss = 2.67379
Step 10525: loss = 2.71409
Step 10530: loss = 2.66006
Training Data Eval:
  Num examples: 49920, Num correct: 6813, Precision @ 1: 0.1365
('Testing Data Eval: EPOCH->', 28)
  Num examples: 9984, Num correct: 1431, Precision @ 1: 0.1433
Step 10535: loss = 2.79639
Step 10540: loss = 2.58889
Step 10545: loss = 2.72345
Step 10550: loss = 2.63306
Step 10555: loss = 2.95974
Step 10560: loss = 2.41607
Step 10565: loss = 2.55809
Step 10570: loss = 2.70438
Step 10575: loss = 2.58170
Step 10580: loss = 2.61697
Step 10585: loss = 2.78170
Step 10590: loss = 2.57514
Step 10595: loss = 2.54090
Step 10600: loss = 2.54301
Step 10605: loss = 2.75983
Step 10610: loss = 2.81788
Step 10615: loss = 2.83515
Step 10620: loss = 2.64429
Step 10625: loss = 2.67322
Step 10630: loss = 2.55341
Step 10635: loss = 2.72362
Step 10640: loss = 2.79325
Step 10645: loss = 2.72242
Step 10650: loss = 2.80293
Step 10655: loss = 2.71691
Step 10660: loss = 2.60664
Step 10665: loss = 2.70437
Step 10670: loss = 2.67448
Step 10675: loss = 2.54232
Step 10680: loss = 2.53066
Step 10685: loss = 2.45332
Step 10690: loss = 2.68186
Step 10695: loss = 2.59177
Step 10700: loss = 2.74105
Step 10705: loss = 2.60320
Step 10710: loss = 2.89575
Step 10715: loss = 2.68847
Step 10720: loss = 2.78223
Step 10725: loss = 2.71023
Step 10730: loss = 2.68277
Step 10735: loss = 2.49715
Step 10740: loss = 2.53779
Step 10745: loss = 2.78653
Step 10750: loss = 2.57568
Step 10755: loss = 2.51217
Step 10760: loss = 2.57121
Step 10765: loss = 2.56122
Step 10770: loss = 2.82474
Step 10775: loss = 2.72402
Step 10780: loss = 2.74371
Step 10785: loss = 2.66037
Step 10790: loss = 2.76590
Step 10795: loss = 2.77591
Step 10800: loss = 2.73641
Step 10805: loss = 2.71103
Step 10810: loss = 2.77831
Step 10815: loss = 2.69548
Step 10820: loss = 2.56359
Step 10825: loss = 2.56964
Step 10830: loss = 2.65865
Step 10835: loss = 2.77498
Step 10840: loss = 2.78635
Step 10845: loss = 2.72440
Step 10850: loss = 2.62928
Step 10855: loss = 2.80987
Step 10860: loss = 2.66658
Step 10865: loss = 2.83579
Step 10870: loss = 2.83839
Step 10875: loss = 2.81786
Step 10880: loss = 2.73031
Step 10885: loss = 2.72289
Step 10890: loss = 2.69503
Step 10895: loss = 2.66907
Step 10900: loss = 2.65819
Step 10905: loss = 2.84301
Step 10910: loss = 2.73434
Step 10915: loss = 2.90301
Step 10920: loss = 2.63927
Training Data Eval:
  Num examples: 49920, Num correct: 6445, Precision @ 1: 0.1291
('Testing Data Eval: EPOCH->', 29)
  Num examples: 9984, Num correct: 1259, Precision @ 1: 0.1261
Step 10925: loss = 2.79262
Step 10930: loss = 2.71547
Step 10935: loss = 2.56223
Step 10940: loss = 2.71775
Step 10945: loss = 2.62503
Step 10950: loss = 2.81657
Step 10955: loss = 2.56645
Step 10960: loss = 2.40445
Step 10965: loss = 2.93087
Step 10970: loss = 2.53241
Step 10975: loss = 2.73555
Step 10980: loss = 2.69494
Step 10985: loss = 2.77275
Step 10990: loss = 2.64122
Step 10995: loss = 2.62173
Step 11000: loss = 2.75221
Step 11005: loss = 2.60062
Step 11010: loss = 2.74559
Step 11015: loss = 2.68843
Step 11020: loss = 2.60977
Step 11025: loss = 2.66126
Step 11030: loss = 2.70970
Step 11035: loss = 2.71086
Step 11040: loss = 2.50702
Step 11045: loss = 2.67228
Step 11050: loss = 2.76319
Step 11055: loss = 2.76991
Step 11060: loss = 2.79187
Step 11065: loss = 2.51461
Step 11070: loss = 2.48801
Step 11075: loss = 2.60455
Step 11080: loss = 2.61044
Step 11085: loss = 2.55093
Step 11090: loss = 2.69533
Step 11095: loss = 2.85729
Step 11100: loss = 2.66622
Step 11105: loss = 2.64898
Step 11110: loss = 2.47860
Step 11115: loss = 2.80062
Step 11120: loss = 2.61669
Step 11125: loss = 2.61677
Step 11130: loss = 2.77917
Step 11135: loss = 2.64120
Step 11140: loss = 2.81579
Step 11145: loss = 2.86101
Step 11150: loss = 2.83392
Step 11155: loss = 2.71604
Step 11160: loss = 2.82471
Step 11165: loss = 2.79683
Step 11170: loss = 2.87760
Step 11175: loss = 2.82091
Step 11180: loss = 2.77343
Step 11185: loss = 2.75205
Step 11190: loss = 2.76971
Step 11195: loss = 2.76703
Step 11200: loss = 2.57357
Step 11205: loss = 2.69339
Step 11210: loss = 2.61063
Step 11215: loss = 2.80413
Step 11220: loss = 2.73978
Step 11225: loss = 2.77775
Step 11230: loss = 2.76926
Step 11235: loss = 2.63058
Step 11240: loss = 2.62695
Step 11245: loss = 2.82999
Step 11250: loss = 2.70455
Step 11255: loss = 2.80882
Step 11260: loss = 2.83045
Step 11265: loss = 2.58632
Step 11270: loss = 2.66895
Step 11275: loss = 2.65118
Step 11280: loss = 2.63651
Step 11285: loss = 2.62693
Step 11290: loss = 2.79601
Step 11295: loss = 2.68143
Step 11300: loss = 2.44965
Step 11305: loss = 2.65587
Step 11310: loss = 2.60209
Training Data Eval:
  Num examples: 49920, Num correct: 6681, Precision @ 1: 0.1338
('Testing Data Eval: EPOCH->', 30)
  Num examples: 9984, Num correct: 1352, Precision @ 1: 0.1354
Step 11315: loss = 2.76632
Step 11320: loss = 2.60697
Step 11325: loss = 2.82004
Step 11330: loss = 2.86637
Step 11335: loss = 2.89669
Step 11340: loss = 2.59259
Step 11345: loss = 2.90117
Step 11350: loss = 2.74148
Step 11355: loss = 2.71515
Step 11360: loss = 2.60259
Step 11365: loss = 2.52473
Step 11370: loss = 2.59173
Step 11375: loss = 2.46205
Step 11380: loss = 2.78889
Step 11385: loss = 2.85902
Step 11390: loss = 2.70855
Step 11395: loss = 2.74387
Step 11400: loss = 2.80046
Step 11405: loss = 2.76018
Step 11410: loss = 2.63334
Step 11415: loss = 2.69576
Step 11420: loss = 2.66654
Step 11425: loss = 2.75029
Step 11430: loss = 3.00864
Step 11435: loss = 2.67783
Step 11440: loss = 2.63563
Step 11445: loss = 2.71722
Step 11450: loss = 2.84573
Step 11455: loss = 2.70456
Step 11460: loss = 2.88788
Step 11465: loss = 2.72014
Step 11470: loss = 2.63152
Step 11475: loss = 2.67410
Step 11480: loss = 2.92757
Step 11485: loss = 2.71157
Step 11490: loss = 2.66036
Step 11495: loss = 2.55943
Step 11500: loss = 2.62858
Step 11505: loss = 2.73418
Step 11510: loss = 2.47652
Step 11515: loss = 2.73607
Step 11520: loss = 2.72286
Step 11525: loss = 2.81726
Step 11530: loss = 2.86562
Step 11535: loss = 2.49306
Step 11540: loss = 2.61314
Step 11545: loss = 2.75261
Step 11550: loss = 2.72146
Step 11555: loss = 2.59212
Step 11560: loss = 2.73881
Step 11565: loss = 2.72005
Step 11570: loss = 2.74887
Step 11575: loss = 2.68358
Step 11580: loss = 2.81167
Step 11585: loss = 2.86471
Step 11590: loss = 2.68657
Step 11595: loss = 2.61297
Step 11600: loss = 2.60958
Step 11605: loss = 2.65110
Step 11610: loss = 2.74675
Step 11615: loss = 2.61421
Step 11620: loss = 2.52891
Step 11625: loss = 2.84411
Step 11630: loss = 2.70276
Step 11635: loss = 2.67266
Step 11640: loss = 2.68059
Step 11645: loss = 2.75455
Step 11650: loss = 2.75641
Step 11655: loss = 2.94491
Step 11660: loss = 2.75204
Step 11665: loss = 2.59565
Step 11670: loss = 2.41939
Step 11675: loss = 2.86655
Step 11680: loss = 3.09721
Step 11685: loss = 2.71908
Step 11690: loss = 2.78014
Step 11695: loss = 2.60439
Step 11700: loss = 2.69649
Training Data Eval:
  Num examples: 49920, Num correct: 6200, Precision @ 1: 0.1242
('Testing Data Eval: EPOCH->', 31)
  Num examples: 9984, Num correct: 1251, Precision @ 1: 0.1253
Step 11705: loss = 2.83759
Step 11710: loss = 2.59327
Step 11715: loss = 2.64076
Step 11720: loss = 2.74063
Step 11725: loss = 2.79771
Step 11730: loss = 2.75067
Step 11735: loss = 2.70547
Step 11740: loss = 2.56837
Step 11745: loss = 2.58418
Step 11750: loss = 2.73965
Step 11755: loss = 2.56080
Step 11760: loss = 2.61801
Step 11765: loss = 2.78063
Step 11770: loss = 2.69321
Step 11775: loss = 2.77527
Step 11780: loss = 2.60214
Step 11785: loss = 2.62316
Step 11790: loss = 2.48998
Step 11795: loss = 2.55840
Step 11800: loss = 2.64470
Step 11805: loss = 2.68209
Step 11810: loss = 2.62048
Step 11815: loss = 2.65799
Step 11820: loss = 2.94161
Step 11825: loss = 2.64085
Step 11830: loss = 2.61483
Step 11835: loss = 2.73998
Step 11840: loss = 2.60494
Step 11845: loss = 2.66439
Step 11850: loss = 2.49797
Step 11855: loss = 2.71332
Step 11860: loss = 2.84150
Step 11865: loss = 2.60392
Step 11870: loss = 2.89647
Step 11875: loss = 2.74131
Step 11880: loss = 2.71593
Step 11885: loss = 2.80008
Step 11890: loss = 2.64898
Step 11895: loss = 2.54763
Step 11900: loss = 2.91259
Step 11905: loss = 2.52061
Step 11910: loss = 2.68850
Step 11915: loss = 2.76369
Step 11920: loss = 2.62492
Step 11925: loss = 2.56481
Step 11930: loss = 2.51172
Step 11935: loss = 2.80054
Step 11940: loss = 2.72002
Step 11945: loss = 2.66133
Step 11950: loss = 2.81309
Step 11955: loss = 2.76357
Step 11960: loss = 3.19351
Step 11965: loss = 2.70817
Step 11970: loss = 2.57837
Step 11975: loss = 2.58568
Step 11980: loss = 2.61828
Step 11985: loss = 2.56855
Step 11990: loss = 2.71718
Step 11995: loss = 2.57798
Step 12000: loss = 2.55419
Step 12005: loss = 2.65929
Step 12010: loss = 2.79539
Step 12015: loss = 2.53421
Step 12020: loss = 2.91388
Step 12025: loss = 2.90203
Step 12030: loss = 2.70596
Step 12035: loss = 2.63561
Step 12040: loss = 2.63214
Step 12045: loss = 2.66576
Step 12050: loss = 2.88013
Step 12055: loss = 2.56445
Step 12060: loss = 2.59412
Step 12065: loss = 2.61919
Step 12070: loss = 2.66752
Step 12075: loss = 2.63507
Step 12080: loss = 2.95518
Step 12085: loss = 2.66444
Step 12090: loss = 2.65710
Training Data Eval:
  Num examples: 49920, Num correct: 6276, Precision @ 1: 0.1257
('Testing Data Eval: EPOCH->', 32)
  Num examples: 9984, Num correct: 1278, Precision @ 1: 0.1280
Step 12095: loss = 3.28279
Step 12100: loss = 2.88966
Step 12105: loss = 2.97080
Step 12110: loss = 2.79545
Step 12115: loss = 2.76936
Step 12120: loss = 2.74113
Step 12125: loss = 2.58078
Step 12130: loss = 2.52298
Step 12135: loss = 2.65785
Step 12140: loss = 2.71144
Step 12145: loss = 2.83697
Step 12150: loss = 2.84234
Step 12155: loss = 2.93349
Step 12160: loss = 2.64338
Step 12165: loss = 2.65344
Step 12170: loss = 2.58367
Step 12175: loss = 2.95491
Step 12180: loss = 2.83736
Step 12185: loss = 2.80004
Step 12190: loss = 2.86868
Step 12195: loss = 2.43693
Step 12200: loss = 2.64753
Step 12205: loss = 2.53662
Step 12210: loss = 2.71029
Step 12215: loss = 2.96262
Step 12220: loss = 2.57522
Step 12225: loss = 2.63182
Step 12230: loss = 2.54514
Step 12235: loss = 2.70046
Step 12240: loss = 2.86322
Step 12245: loss = 2.63200
Step 12250: loss = 2.65589
Step 12255: loss = 2.65326
Step 12260: loss = 2.61219
Step 12265: loss = 2.81170
Step 12270: loss = 2.76542
Step 12275: loss = 2.85383
Step 12280: loss = 2.71180
Step 12285: loss = 2.70669
Step 12290: loss = 2.95315
Step 12295: loss = 2.63499
Step 12300: loss = 2.49017
Step 12305: loss = 2.58853
Step 12310: loss = 2.71759
Step 12315: loss = 2.91135
Step 12320: loss = 2.48002
Step 12325: loss = 2.75542
Step 12330: loss = 2.80640
Step 12335: loss = 2.76261
Step 12340: loss = 2.68920
Step 12345: loss = 2.47888
Step 12350: loss = 2.61616
Step 12355: loss = 2.78615
Step 12360: loss = 2.74872
Step 12365: loss = 2.63914
Step 12370: loss = 2.73356
Step 12375: loss = 2.89522
Step 12380: loss = 2.59392
Step 12385: loss = 2.66663
Step 12390: loss = 2.68076
Step 12395: loss = 2.53772
Step 12400: loss = 2.92988
Step 12405: loss = 2.47567
Step 12410: loss = 2.79634
Step 12415: loss = 2.61418
Step 12420: loss = 2.76314
Step 12425: loss = 2.97577
Step 12430: loss = 2.66369
Step 12435: loss = 2.84050
Step 12440: loss = 2.68292
Step 12445: loss = 2.64129
Step 12450: loss = 2.43207
Step 12455: loss = 2.65465
Step 12460: loss = 2.63807
Step 12465: loss = 2.67100
Step 12470: loss = 2.72445
Step 12475: loss = 2.58396
Step 12480: loss = 2.67706
Training Data Eval:
  Num examples: 49920, Num correct: 6629, Precision @ 1: 0.1328
('Testing Data Eval: EPOCH->', 33)
  Num examples: 9984, Num correct: 1321, Precision @ 1: 0.1323
Step 12485: loss = 2.76040
Step 12490: loss = 2.55984
Step 12495: loss = 2.71634
Step 12500: loss = 2.59271
Step 12505: loss = 2.60353
Step 12510: loss = 2.68560
Step 12515: loss = 2.64821
Step 12520: loss = 2.77491
Step 12525: loss = 2.49801
Step 12530: loss = 2.80065
Step 12535: loss = 2.75618
Step 12540: loss = 2.75364
Step 12545: loss = 2.61341
Step 12550: loss = 2.55371
Step 12555: loss = 2.70097
Step 12560: loss = 2.99780
Step 12565: loss = 2.90973
Step 12570: loss = 2.58792
Step 12575: loss = 2.62821
Step 12580: loss = 2.77313
Step 12585: loss = 2.65907
Step 12590: loss = 2.75659
Step 12595: loss = 2.69133
Step 12600: loss = 2.71920
Step 12605: loss = 2.58209
Step 12610: loss = 2.86379
Step 12615: loss = 2.68046
Step 12620: loss = 2.66597
Step 12625: loss = 2.70328
Step 12630: loss = 2.47512
Step 12635: loss = 2.72902
Step 12640: loss = 2.55033
Step 12645: loss = 2.44805
Step 12650: loss = 2.77890
Step 12655: loss = 2.66167
Step 12660: loss = 2.62218
Step 12665: loss = 2.55319
Step 12670: loss = 2.68463
Step 12675: loss = 2.63760
Step 12680: loss = 2.63568
Step 12685: loss = 2.81189
Step 12690: loss = 2.64250
Step 12695: loss = 2.82887
Step 12700: loss = 2.76132
Step 12705: loss = 2.71706
Step 12710: loss = 2.81525
Step 12715: loss = 2.68981
Step 12720: loss = 2.94188
Step 12725: loss = 2.93508
Step 12730: loss = 2.67717
Step 12735: loss = 2.76301
Step 12740: loss = 2.79927
Step 12745: loss = 2.84057
Step 12750: loss = 2.76236
Step 12755: loss = 2.68305
Step 12760: loss = 2.78374
Step 12765: loss = 2.72462
Step 12770: loss = 2.54525
Step 12775: loss = 2.95825
Step 12780: loss = 2.76175
Step 12785: loss = 2.99254
Step 12790: loss = 2.81814
Step 12795: loss = 2.77090
Step 12800: loss = 2.78964
Step 12805: loss = 2.83913
Step 12810: loss = 2.69652
Step 12815: loss = 2.74108
Step 12820: loss = 2.42529
Step 12825: loss = 2.61550
Step 12830: loss = 2.82493
Step 12835: loss = 2.75060
Step 12840: loss = 2.74464
Step 12845: loss = 2.91316
Step 12850: loss = 2.89204
Step 12855: loss = 2.96085
Step 12860: loss = 2.81596
Step 12865: loss = 2.73678
Step 12870: loss = 2.55334
Training Data Eval:
  Num examples: 49920, Num correct: 6231, Precision @ 1: 0.1248
('Testing Data Eval: EPOCH->', 34)
  Num examples: 9984, Num correct: 1333, Precision @ 1: 0.1335
Step 12875: loss = 2.65920
Step 12880: loss = 2.68611
Step 12885: loss = 2.62905
Step 12890: loss = 2.80885
Step 12895: loss = 3.05229
Step 12900: loss = 2.87986
Step 12905: loss = 2.55613
Step 12910: loss = 2.54941
Step 12915: loss = 2.83626
Step 12920: loss = 2.93964
Step 12925: loss = 2.62101
Step 12930: loss = 2.62693
Step 12935: loss = 2.54484
Step 12940: loss = 2.62588
Step 12945: loss = 2.95505
Step 12950: loss = 2.90906
Step 12955: loss = 2.77532
Step 12960: loss = 2.77224
Step 12965: loss = 2.62567
Step 12970: loss = 2.71509
Step 12975: loss = 2.84032
Step 12980: loss = 2.70500
Step 12985: loss = 2.61645
Step 12990: loss = 2.82555
Step 12995: loss = 2.94743
Step 13000: loss = 2.46647
Step 13005: loss = 2.84480
Step 13010: loss = 2.88673
Step 13015: loss = 2.88255
Step 13020: loss = 2.91710
Step 13025: loss = 2.85270
Step 13030: loss = 2.79413
Step 13035: loss = 2.99944
Step 13040: loss = 2.91622
Step 13045: loss = 2.64347
Step 13050: loss = 2.87323
Step 13055: loss = 2.82691
Step 13060: loss = 2.64496
Step 13065: loss = 2.51576
Step 13070: loss = 2.91843
Step 13075: loss = 2.76330
Step 13080: loss = 2.84054
Step 13085: loss = 2.99966
Step 13090: loss = 2.76893
Step 13095: loss = 2.80359
Step 13100: loss = 2.87685
Step 13105: loss = 2.66814
Step 13110: loss = 2.71031
Step 13115: loss = 2.58497
Step 13120: loss = 2.76796
Step 13125: loss = 2.86021
Step 13130: loss = 2.93863
Step 13135: loss = 2.78263
Step 13140: loss = 2.75762
Step 13145: loss = 2.97382
Step 13150: loss = 2.77982
Step 13155: loss = 2.95289
Step 13160: loss = 2.78109
Step 13165: loss = 2.86217
Step 13170: loss = 2.58883
Step 13175: loss = 3.01748
Step 13180: loss = 2.69772
Step 13185: loss = 2.63645
Step 13190: loss = 2.85922
Step 13195: loss = 2.53447
Step 13200: loss = 2.68033
Step 13205: loss = 2.99603
Step 13210: loss = 2.78364
Step 13215: loss = 2.85040
Step 13220: loss = 2.69757
Step 13225: loss = 2.70401
Step 13230: loss = 2.61114
Step 13235: loss = 2.82888
Step 13240: loss = 2.55155
Step 13245: loss = 2.64006
Step 13250: loss = 2.81338
Step 13255: loss = 2.70929
Step 13260: loss = 2.87101
Training Data Eval:
  Num examples: 49920, Num correct: 6445, Precision @ 1: 0.1291
('Testing Data Eval: EPOCH->', 35)
  Num examples: 9984, Num correct: 1297, Precision @ 1: 0.1299
Step 13265: loss = 2.67749
Step 13270: loss = 2.76418
Step 13275: loss = 2.89630
Step 13280: loss = 2.57361
Step 13285: loss = 2.60314
Step 13290: loss = 2.83417
Step 13295: loss = 2.86656
Step 13300: loss = 2.60329
Step 13305: loss = 2.61137
Step 13310: loss = 2.78611
Step 13315: loss = 2.65690
Step 13320: loss = 2.53413
Step 13325: loss = 2.99335
Step 13330: loss = 2.70552
Step 13335: loss = 2.82165
Step 13340: loss = 2.65110
Step 13345: loss = 2.83394
Step 13350: loss = 2.68076
Step 13355: loss = 2.69998
Step 13360: loss = 2.68089
Step 13365: loss = 2.79536
Step 13370: loss = 2.57850
Step 13375: loss = 2.70467
Step 13380: loss = 2.56624
Step 13385: loss = 2.73666
Step 13390: loss = 2.85835
Step 13395: loss = 2.71019
Step 13400: loss = 2.63059
Step 13405: loss = 2.65382
Step 13410: loss = 2.51299
Step 13415: loss = 2.65171
Step 13420: loss = 2.81583
Step 13425: loss = 2.68235
Step 13430: loss = 2.58657
Step 13435: loss = 2.87774
Step 13440: loss = 2.67599
Step 13445: loss = 2.71208
Step 13450: loss = 2.66662
Step 13455: loss = 2.65871
Step 13460: loss = 2.78164
Step 13465: loss = 2.68405
Step 13470: loss = 2.89366
Step 13475: loss = 2.77621
Step 13480: loss = 2.94513
Step 13485: loss = 2.91146
Step 13490: loss = 2.68261
Step 13495: loss = 2.45394
Step 13500: loss = 2.71107
Step 13505: loss = 2.70133
Step 13510: loss = 2.79548
Step 13515: loss = 2.87878
Step 13520: loss = 2.80333
Step 13525: loss = 2.80950
Step 13530: loss = 2.66395
Step 13535: loss = 2.96630
Step 13540: loss = 2.79917
Step 13545: loss = 2.84168
Step 13550: loss = 2.63688
Step 13555: loss = 2.74706
Step 13560: loss = 2.75250
Step 13565: loss = 2.91641
Step 13570: loss = 2.58676
Step 13575: loss = 2.99524
Step 13580: loss = 2.69881
Step 13585: loss = 2.84986
Step 13590: loss = 2.75095
Step 13595: loss = 2.73715
Step 13600: loss = 2.52944
Step 13605: loss = 2.77680
Step 13610: loss = 2.69388
Step 13615: loss = 2.55305
Step 13620: loss = 2.57233
Step 13625: loss = 2.71366
Step 13630: loss = 2.70430
Step 13635: loss = 2.78040
Step 13640: loss = 2.89490
Step 13645: loss = 2.61130
Step 13650: loss = 2.70124
Training Data Eval:
  Num examples: 49920, Num correct: 6459, Precision @ 1: 0.1294
('Testing Data Eval: EPOCH->', 36)
  Num examples: 9984, Num correct: 1342, Precision @ 1: 0.1344
Step 13655: loss = 2.71639
Step 13660: loss = 2.85707
Step 13665: loss = 2.91177
Step 13670: loss = 2.68467
Step 13675: loss = 2.88231
Step 13680: loss = 2.61789
Step 13685: loss = 2.68519
Step 13690: loss = 2.54717
Step 13695: loss = 2.55385
Step 13700: loss = 2.79427
Step 13705: loss = 2.76738
Step 13710: loss = 2.81233
Step 13715: loss = 2.81787
Step 13720: loss = 2.77991
Step 13725: loss = 2.68105
Step 13730: loss = 2.93546
Step 13735: loss = 2.64373
Step 13740: loss = 2.52938
Step 13745: loss = 2.60673
Step 13750: loss = 2.83638
Step 13755: loss = 2.86692
Step 13760: loss = 2.69612
Step 13765: loss = 2.67358
Step 13770: loss = 2.71521
Step 13775: loss = 2.54191
Step 13780: loss = 2.71496
Step 13785: loss = 2.96842
Step 13790: loss = 2.76189
Step 13795: loss = 2.75272
Step 13800: loss = 2.75949
Step 13805: loss = 2.54155
Step 13810: loss = 3.10347
Step 13815: loss = 2.89178
Step 13820: loss = 2.40017
Step 13825: loss = 2.41425
Step 13830: loss = 2.83220
Step 13835: loss = 2.61304
Step 13840: loss = 2.75588
Step 13845: loss = 2.66979
Step 13850: loss = 2.59118
Step 13855: loss = 2.63498
Step 13860: loss = 2.82291
Step 13865: loss = 2.81210
Step 13870: loss = 2.63255
Step 13875: loss = 2.91266
Step 13880: loss = 2.58013
Step 13885: loss = 3.02242
Step 13890: loss = 3.13997
Step 13895: loss = 2.85504
Step 13900: loss = 2.66690
Step 13905: loss = 2.84214
Step 13910: loss = 2.81704
Step 13915: loss = 2.61800
Step 13920: loss = 2.69256
Step 13925: loss = 2.75137
Step 13930: loss = 2.90250
Step 13935: loss = 2.64653
Step 13940: loss = 2.63202
Step 13945: loss = 2.67256
Step 13950: loss = 2.82907
Step 13955: loss = 2.85053
Step 13960: loss = 2.68815
Step 13965: loss = 2.63268
Step 13970: loss = 2.70879
Step 13975: loss = 2.60240
Step 13980: loss = 2.82452
Step 13985: loss = 2.54511
Step 13990: loss = 2.70689
Step 13995: loss = 2.60785
Step 14000: loss = 2.64076
Step 14005: loss = 2.71869
Step 14010: loss = 2.69869
Step 14015: loss = 2.67115
Step 14020: loss = 2.76104
Step 14025: loss = 2.81827
Step 14030: loss = 2.81472
Step 14035: loss = 2.98106
Step 14040: loss = 2.78968
Training Data Eval:
  Num examples: 49920, Num correct: 6169, Precision @ 1: 0.1236
('Testing Data Eval: EPOCH->', 37)
  Num examples: 9984, Num correct: 1284, Precision @ 1: 0.1286
Step 14045: loss = 2.60735
Step 14050: loss = 2.62746
Step 14055: loss = 2.52860
Step 14060: loss = 2.74778
Step 14065: loss = 2.64659
Step 14070: loss = 2.74500
Step 14075: loss = 2.84971
Step 14080: loss = 2.70262
Step 14085: loss = 2.75614
Step 14090: loss = 2.79341
Step 14095: loss = 2.74365
Step 14100: loss = 2.90690
Step 14105: loss = 2.80428
Step 14110: loss = 2.66432
Step 14115: loss = 2.52631
Step 14120: loss = 2.66580
Step 14125: loss = 3.04079
Step 14130: loss = 2.72806
Step 14135: loss = 2.68757
Step 14140: loss = 2.74878
Step 14145: loss = 2.86216
Step 14150: loss = 2.58062
Step 14155: loss = 2.70419
Step 14160: loss = 2.67966
Step 14165: loss = 2.80185
Step 14170: loss = 2.74871
Step 14175: loss = 2.67794
Step 14180: loss = 2.73845
Step 14185: loss = 2.77320
Step 14190: loss = 2.85172
Step 14195: loss = 2.56201
Step 14200: loss = 2.75420
Step 14205: loss = 2.82306
Step 14210: loss = 2.80145
Step 14215: loss = 2.70033
Step 14220: loss = 2.77140
Step 14225: loss = 2.90632
Step 14230: loss = 2.65965
Step 14235: loss = 2.61130
Step 14240: loss = 2.79703
Step 14245: loss = 2.75891
Step 14250: loss = 2.79273
Step 14255: loss = 2.70169
Step 14260: loss = 2.86533
Step 14265: loss = 3.04872
Step 14270: loss = 2.61197
Step 14275: loss = 2.69151
Step 14280: loss = 2.70285
Step 14285: loss = 3.05344
Step 14290: loss = 2.60415
Step 14295: loss = 2.52135
Step 14300: loss = 2.68877
Step 14305: loss = 2.68319
Step 14310: loss = 2.56124
Step 14315: loss = 2.87508
Step 14320: loss = 2.73881
Step 14325: loss = 2.66812
Step 14330: loss = 2.74470
Step 14335: loss = 2.72178
Step 14340: loss = 2.60214
Step 14345: loss = 2.57074
Step 14350: loss = 2.80405
Step 14355: loss = 2.59799
Step 14360: loss = 2.65401
Step 14365: loss = 2.74918
Step 14370: loss = 2.68133
Step 14375: loss = 2.80750
Step 14380: loss = 2.60057
Step 14385: loss = 2.79259
Step 14390: loss = 2.88177
Step 14395: loss = 2.79165
Step 14400: loss = 2.95376
Step 14405: loss = 2.87234
Step 14410: loss = 2.65523
Step 14415: loss = 2.75176
Step 14420: loss = 2.54362
Step 14425: loss = 2.52212
Step 14430: loss = 2.63657
Training Data Eval:
  Num examples: 49920, Num correct: 6484, Precision @ 1: 0.1299
('Testing Data Eval: EPOCH->', 38)
  Num examples: 9984, Num correct: 1332, Precision @ 1: 0.1334
Step 14435: loss = 2.85772
Step 14440: loss = 2.75826
Step 14445: loss = 2.64158
Step 14450: loss = 2.69864
Step 14455: loss = 2.72800
Step 14460: loss = 2.90829
Step 14465: loss = 2.70702
Step 14470: loss = 2.63961
Step 14475: loss = 2.51111
Step 14480: loss = 2.78292
Step 14485: loss = 2.52086
Step 14490: loss = 2.63251
Step 14495: loss = 2.54741
Step 14500: loss = 2.59552
Step 14505: loss = 2.62916
Step 14510: loss = 2.68356
Step 14515: loss = 2.77286
Step 14520: loss = 2.68556
Step 14525: loss = 2.62488
Step 14530: loss = 2.78776
Step 14535: loss = 2.89535
Step 14540: loss = 2.90391
Step 14545: loss = 2.77748
Step 14550: loss = 2.64858
Step 14555: loss = 2.51728
Step 14560: loss = 2.63471
Step 14565: loss = 2.83450
Step 14570: loss = 2.69511
Step 14575: loss = 2.47862
Step 14580: loss = 2.79316
Step 14585: loss = 2.92041
Step 14590: loss = 2.76801
Step 14595: loss = 2.80584
Step 14600: loss = 2.56020
Step 14605: loss = 2.50996
Step 14610: loss = 3.03450
Step 14615: loss = 2.97073
Step 14620: loss = 2.58635
Step 14625: loss = 2.76709
Step 14630: loss = 2.58672
Step 14635: loss = 2.84141
Step 14640: loss = 2.67350
Step 14645: loss = 2.66545
Step 14650: loss = 2.78450
Step 14655: loss = 2.74934
Step 14660: loss = 2.76873
Step 14665: loss = 2.36878
Step 14670: loss = 2.80818
Step 14675: loss = 2.82048
Step 14680: loss = 2.72325
Step 14685: loss = 2.76449
Step 14690: loss = 2.71875
Step 14695: loss = 2.85902
Step 14700: loss = 2.88654
Step 14705: loss = 2.75447
Step 14710: loss = 2.71291
Step 14715: loss = 2.63837
Step 14720: loss = 2.59880
Step 14725: loss = 2.60978
Step 14730: loss = 2.72450
Step 14735: loss = 2.84396
Step 14740: loss = 2.62987
Step 14745: loss = 2.70016
Step 14750: loss = 2.66714
Step 14755: loss = 2.78519
Step 14760: loss = 2.85675
Step 14765: loss = 2.57226
Step 14770: loss = 2.58106
Step 14775: loss = 2.80495
Step 14780: loss = 2.69649
Step 14785: loss = 2.75911
Step 14790: loss = 2.59384
Step 14795: loss = 2.91104
Step 14800: loss = 2.73926
Step 14805: loss = 2.83333
Step 14810: loss = 2.64842
Step 14815: loss = 2.51116
Step 14820: loss = 2.72027
Training Data Eval:
  Num examples: 49920, Num correct: 6733, Precision @ 1: 0.1349
('Testing Data Eval: EPOCH->', 39)
  Num examples: 9984, Num correct: 1320, Precision @ 1: 0.1322
Step 14825: loss = 2.76662
Step 14830: loss = 2.80281
Step 14835: loss = 2.91632
Step 14840: loss = 2.40221
Step 14845: loss = 2.68966
Step 14850: loss = 3.01594
Step 14855: loss = 2.50256
Step 14860: loss = 2.68753
Step 14865: loss = 2.56225
Step 14870: loss = 2.87571
Step 14875: loss = 2.51686
Step 14880: loss = 2.65198
Step 14885: loss = 2.89255
Step 14890: loss = 2.87916
Step 14895: loss = 2.82587
Step 14900: loss = 2.66194
Step 14905: loss = 2.80850
Step 14910: loss = 2.74470
Step 14915: loss = 2.60462
Step 14920: loss = 2.51851
Step 14925: loss = 2.67240
Step 14930: loss = 2.56706
Step 14935: loss = 2.84908
Step 14940: loss = 2.85801
Step 14945: loss = 2.60669
Step 14950: loss = 2.72163
Step 14955: loss = 2.77338
Step 14960: loss = 2.59453
Step 14965: loss = 2.68941
Step 14970: loss = 2.99298
Step 14975: loss = 2.84764
Step 14980: loss = 2.63927
Step 14985: loss = 2.84865
Step 14990: loss = 2.76860
Step 14995: loss = 2.80431
Step 15000: loss = 2.63382
Step 15005: loss = 2.74492
Step 15010: loss = 2.68237
Step 15015: loss = 2.62563
Step 15020: loss = 2.74378
Step 15025: loss = 2.58226
Step 15030: loss = 2.66295
Step 15035: loss = 2.60088
Step 15040: loss = 2.74009
Step 15045: loss = 2.66755
Step 15050: loss = 2.67883
Step 15055: loss = 2.54507
Step 15060: loss = 2.54467
Step 15065: loss = 2.74820
Step 15070: loss = 2.79825
Step 15075: loss = 2.75792
Step 15080: loss = 2.70201
Step 15085: loss = 2.72730
Step 15090: loss = 2.65189
Step 15095: loss = 2.97049
Step 15100: loss = 2.82853
Step 15105: loss = 2.81042
Step 15110: loss = 2.93651
Step 15115: loss = 2.62841
Step 15120: loss = 2.73952
Step 15125: loss = 2.75722
Step 15130: loss = 2.63957
Step 15135: loss = 3.03248
Step 15140: loss = 2.60666
Step 15145: loss = 2.64351
Step 15150: loss = 2.52847
Step 15155: loss = 2.88482
Step 15160: loss = 2.67057
Step 15165: loss = 2.58237
Step 15170: loss = 2.59665
Step 15175: loss = 2.70933
Step 15180: loss = 2.57865
Step 15185: loss = 2.69892
Step 15190: loss = 2.72954
Step 15195: loss = 2.70406
Step 15200: loss = 2.47969
Step 15205: loss = 2.68749
Step 15210: loss = 2.74248
Training Data Eval:
  Num examples: 49920, Num correct: 6385, Precision @ 1: 0.1279
('Testing Data Eval: EPOCH->', 40)
  Num examples: 9984, Num correct: 1292, Precision @ 1: 0.1294
Step 15215: loss = 2.95434
Step 15220: loss = 2.50686
Step 15225: loss = 2.78135
Step 15230: loss = 2.92930
Step 15235: loss = 2.90557
Step 15240: loss = 2.62634
Step 15245: loss = 2.71514
Step 15250: loss = 2.64193
Step 15255: loss = 2.60981
Step 15260: loss = 2.60044
Step 15265: loss = 2.70860
Step 15270: loss = 2.83156
Step 15275: loss = 2.66148
Step 15280: loss = 2.50243
Step 15285: loss = 2.77177
Step 15290: loss = 2.81934
Step 15295: loss = 2.91737
Step 15300: loss = 2.91057
Step 15305: loss = 2.58006
Step 15310: loss = 2.69882
Step 15315: loss = 2.58312
Step 15320: loss = 2.62874
Step 15325: loss = 2.62577
Step 15330: loss = 2.64976
Step 15335: loss = 2.63211
Step 15340: loss = 2.76485
Step 15345: loss = 2.56473
Step 15350: loss = 2.67201
Step 15355: loss = 2.53862
Step 15360: loss = 2.62221
Step 15365: loss = 2.93474
Step 15370: loss = 2.54067
Step 15375: loss = 2.67891
Step 15380: loss = 2.63184
Step 15385: loss = 2.73093
Step 15390: loss = 2.80186
Step 15395: loss = 2.73355
Step 15400: loss = 2.59837
Step 15405: loss = 2.63818
Step 15410: loss = 2.78118
Step 15415: loss = 2.52077
Step 15420: loss = 2.66471
Step 15425: loss = 2.66823
Step 15430: loss = 2.65706
Step 15435: loss = 2.65723
Step 15440: loss = 2.74687
Step 15445: loss = 2.63336
Step 15450: loss = 2.75465
Step 15455: loss = 2.57196
Step 15460: loss = 2.56068
Step 15465: loss = 2.86956
Step 15470: loss = 2.78762
Step 15475: loss = 2.87331
Step 15480: loss = 2.50648
Step 15485: loss = 2.73660
Step 15490: loss = 2.77866
Step 15495: loss = 2.83517
Step 15500: loss = 2.73849
Step 15505: loss = 2.60264
Step 15510: loss = 2.86291
Step 15515: loss = 2.85775
Step 15520: loss = 2.85860
Step 15525: loss = 2.81504
Step 15530: loss = 2.60918
Step 15535: loss = 2.63628
Step 15540: loss = 2.70923
Step 15545: loss = 2.66567
Step 15550: loss = 2.91572
Step 15555: loss = 2.63208
Step 15560: loss = 2.73317
Step 15565: loss = 2.71384
Step 15570: loss = 2.78816
Step 15575: loss = 2.65743
Step 15580: loss = 2.71124
Step 15585: loss = 2.54405
Step 15590: loss = 2.56942
Step 15595: loss = 2.72818
Step 15600: loss = 2.82189
Training Data Eval:
  Num examples: 49920, Num correct: 6589, Precision @ 1: 0.1320
('Testing Data Eval: EPOCH->', 41)
  Num examples: 9984, Num correct: 1397, Precision @ 1: 0.1399
Step 15605: loss = 2.67791
Step 15610: loss = 2.76240
Step 15615: loss = 2.40087
Step 15620: loss = 2.56016
Step 15625: loss = 2.68803
Step 15630: loss = 2.75201
Step 15635: loss = 2.79484
Step 15640: loss = 2.77574
Step 15645: loss = 2.81306
Step 15650: loss = 2.62564
Step 15655: loss = 2.59704
Step 15660: loss = 2.74558
Step 15665: loss = 2.49262
Step 15670: loss = 2.69585
Step 15675: loss = 2.57438
Step 15680: loss = 2.75417
Step 15685: loss = 2.54332
Step 15690: loss = 2.62483
Step 15695: loss = 2.58620
Step 15700: loss = 2.54423
Step 15705: loss = 2.67800
Step 15710: loss = 2.82318
Step 15715: loss = 2.50260
Step 15720: loss = 2.64369
Step 15725: loss = 2.68064
Step 15730: loss = 2.77166
Step 15735: loss = 2.52805
Step 15740: loss = 2.68751
Step 15745: loss = 2.68043
Step 15750: loss = 2.87123
Step 15755: loss = 2.95083
Step 15760: loss = 2.86574
Step 15765: loss = 2.81876
Step 15770: loss = 2.84194
Step 15775: loss = 2.71725
Step 15780: loss = 2.62574
Step 15785: loss = 2.69003
Step 15790: loss = 2.61571
Step 15795: loss = 2.71425
Step 15800: loss = 2.67522
Step 15805: loss = 2.78604
Step 15810: loss = 2.84527
Step 15815: loss = 2.61151
Step 15820: loss = 2.66399
Step 15825: loss = 2.73069
Step 15830: loss = 2.91465
Step 15835: loss = 2.83170
Step 15840: loss = 2.89956
Step 15845: loss = 3.07162
Step 15850: loss = 3.21875
Step 15855: loss = 2.82436
Step 15860: loss = 2.85700
Step 15865: loss = 2.87247
Step 15870: loss = 2.75225
Step 15875: loss = 2.74632
Step 15880: loss = 2.93165
Step 15885: loss = 2.85246
Step 15890: loss = 2.61930
Step 15895: loss = 2.88722
Step 15900: loss = 2.86042
Step 15905: loss = 2.70067
Step 15910: loss = 2.99080
Step 15915: loss = 2.70541
Step 15920: loss = 2.54575
Step 15925: loss = 2.52296
Step 15930: loss = 2.58421
Step 15935: loss = 2.73794
Step 15940: loss = 2.86157
Step 15945: loss = 2.64096
Step 15950: loss = 2.77559
Step 15955: loss = 2.76682
Step 15960: loss = 2.55834
Step 15965: loss = 2.66785
Step 15970: loss = 2.78089
Step 15975: loss = 2.71521
Step 15980: loss = 2.85835
Step 15985: loss = 2.68874
Step 15990: loss = 2.78969
Training Data Eval:
  Num examples: 49920, Num correct: 6537, Precision @ 1: 0.1309
('Testing Data Eval: EPOCH->', 42)
  Num examples: 9984, Num correct: 1249, Precision @ 1: 0.1251
Step 15995: loss = 2.70488
Step 16000: loss = 2.65120
Step 16005: loss = 2.56883
Step 16010: loss = 2.89573
Step 16015: loss = 2.71377
Step 16020: loss = 2.75932
Step 16025: loss = 2.73953
Step 16030: loss = 2.58162
Step 16035: loss = 2.57900
Step 16040: loss = 2.55453
Step 16045: loss = 2.75556
Step 16050: loss = 2.63536
Step 16055: loss = 2.82506
Step 16060: loss = 3.06830
Step 16065: loss = 2.73218
Step 16070: loss = 2.53411
Step 16075: loss = 3.01150
Step 16080: loss = 2.54508
Step 16085: loss = 2.78010
Step 16090: loss = 2.71780
Step 16095: loss = 2.80036
Step 16100: loss = 2.51744
Step 16105: loss = 2.64941
Step 16110: loss = 2.76174
Step 16115: loss = 2.62736
Step 16120: loss = 2.69595
Step 16125: loss = 2.65885
Step 16130: loss = 2.46119
Step 16135: loss = 2.86544
Step 16140: loss = 2.78873
Step 16145: loss = 2.66261
Step 16150: loss = 2.88688
Step 16155: loss = 2.80689
Step 16160: loss = 2.65786
Step 16165: loss = 2.75855
Step 16170: loss = 3.23038
Step 16175: loss = 2.77708
Step 16180: loss = 2.73545
Step 16185: loss = 2.65773
Step 16190: loss = 2.60246
Step 16195: loss = 2.62303
Step 16200: loss = 2.84207
Step 16205: loss = 3.00642
Step 16210: loss = 2.49860
Step 16215: loss = 2.71922
Step 16220: loss = 2.57564
Step 16225: loss = 2.68578
Step 16230: loss = 2.72689
Step 16235: loss = 2.79592
Step 16240: loss = 2.74282
Step 16245: loss = 2.65612
Step 16250: loss = 2.64642
Step 16255: loss = 2.55503
Step 16260: loss = 2.73132
Step 16265: loss = 2.73176
Step 16270: loss = 2.85104
Step 16275: loss = 2.51574
Step 16280: loss = 2.69209
Step 16285: loss = 2.82029
Step 16290: loss = 2.69726
Step 16295: loss = 2.75271
Step 16300: loss = 2.63694
Step 16305: loss = 2.71701
Step 16310: loss = 2.75525
Step 16315: loss = 2.77106
Step 16320: loss = 2.62414
Step 16325: loss = 2.62728
Step 16330: loss = 2.55693
Step 16335: loss = 2.65799
Step 16340: loss = 2.63514
Step 16345: loss = 2.83995
Step 16350: loss = 2.70838
Step 16355: loss = 2.62646
Step 16360: loss = 2.75922
Step 16365: loss = 2.63118
Step 16370: loss = 2.70536
Step 16375: loss = 2.82611
Step 16380: loss = 2.65259
Training Data Eval:
  Num examples: 49920, Num correct: 6337, Precision @ 1: 0.1269
('Testing Data Eval: EPOCH->', 43)
  Num examples: 9984, Num correct: 1341, Precision @ 1: 0.1343
Step 16385: loss = 2.79086
Step 16390: loss = 2.74492
Step 16395: loss = 2.61619
Step 16400: loss = 2.81419
Step 16405: loss = 2.81515
Step 16410: loss = 3.00812
Step 16415: loss = 2.76930
Step 16420: loss = 2.69907
Step 16425: loss = 2.83116
Step 16430: loss = 2.67679
Step 16435: loss = 2.79152
Step 16440: loss = 2.48237
Step 16445: loss = 2.82531
Step 16450: loss = 2.65921
Step 16455: loss = 2.82877
Step 16460: loss = 2.81676
Step 16465: loss = 2.81876
Step 16470: loss = 2.75224
Step 16475: loss = 2.64823
Step 16480: loss = 2.82922
Step 16485: loss = 2.68717
Step 16490: loss = 2.77462
Step 16495: loss = 2.71145
Step 16500: loss = 2.84170
Step 16505: loss = 3.00567
Step 16510: loss = 2.47729
Step 16515: loss = 2.69513
Step 16520: loss = 2.53800
Step 16525: loss = 2.77425
Step 16530: loss = 2.59110
Step 16535: loss = 2.70420
Step 16540: loss = 2.59551
Step 16545: loss = 2.61405
Step 16550: loss = 2.57962
Step 16555: loss = 2.62334
Step 16560: loss = 2.74421
Step 16565: loss = 2.75691
Step 16570: loss = 2.59430
Step 16575: loss = 2.59717
Step 16580: loss = 2.68814
Step 16585: loss = 2.70409
Step 16590: loss = 2.60092
Step 16595: loss = 2.74514
Step 16600: loss = 2.77848
Step 16605: loss = 2.80368
Step 16610: loss = 2.61212
Step 16615: loss = 2.59559
Step 16620: loss = 2.64464
Step 16625: loss = 2.65503
Step 16630: loss = 2.53779
Step 16635: loss = 2.70477
Step 16640: loss = 2.90843
Step 16645: loss = 2.76837
Step 16650: loss = 2.68801
Step 16655: loss = 3.12276
Step 16660: loss = 2.65919
Step 16665: loss = 2.67318
Step 16670: loss = 2.89060
Step 16675: loss = 2.62378
Step 16680: loss = 2.78290
Step 16685: loss = 2.66133
Step 16690: loss = 2.75824
Step 16695: loss = 2.81274
Step 16700: loss = 2.72290
Step 16705: loss = 2.87152
Step 16710: loss = 2.71370
Step 16715: loss = 2.81964
Step 16720: loss = 2.68357
Step 16725: loss = 2.89448
Step 16730: loss = 2.59540
Step 16735: loss = 2.73316
Step 16740: loss = 2.44787
Step 16745: loss = 2.71686
Step 16750: loss = 2.63318
Step 16755: loss = 2.49706
Step 16760: loss = 2.70804
Step 16765: loss = 2.75556
Step 16770: loss = 3.00181
Training Data Eval:
  Num examples: 49920, Num correct: 6433, Precision @ 1: 0.1289
('Testing Data Eval: EPOCH->', 44)
  Num examples: 9984, Num correct: 1288, Precision @ 1: 0.1290
Step 16775: loss = 2.61143
Step 16780: loss = 2.62744
Step 16785: loss = 2.74231
Step 16790: loss = 2.54654
Step 16795: loss = 2.78110
Step 16800: loss = 2.77605
Step 16805: loss = 2.83852
Step 16810: loss = 2.82139
Step 16815: loss = 2.85520
Step 16820: loss = 2.78216
Step 16825: loss = 2.79820
Step 16830: loss = 2.60177
Step 16835: loss = 2.56788
Step 16840: loss = 2.67400
Step 16845: loss = 2.72644
Step 16850: loss = 2.81146
Step 16855: loss = 2.60803
Step 16860: loss = 2.92677
Step 16865: loss = 2.64866
Step 16870: loss = 2.73255
Step 16875: loss = 2.66897
Step 16880: loss = 2.70300
Step 16885: loss = 2.82241
Step 16890: loss = 2.76365
Step 16895: loss = 2.64291
Step 16900: loss = 2.72572
Step 16905: loss = 2.61994
Step 16910: loss = 2.51919
Step 16915: loss = 2.93523
Step 16920: loss = 2.67345
Step 16925: loss = 2.51883
Step 16930: loss = 2.78977
Step 16935: loss = 2.93343
Step 16940: loss = 2.78480
Step 16945: loss = 2.71838
Step 16950: loss = 2.66010
Step 16955: loss = 2.59560
Step 16960: loss = 2.65075
Step 16965: loss = 2.71580
Step 16970: loss = 2.73723
Step 16975: loss = 2.67350
Step 16980: loss = 2.85158
Step 16985: loss = 2.77459
Step 16990: loss = 2.82497
Step 16995: loss = 2.75443
Step 17000: loss = 2.57296
Step 17005: loss = 2.64662
Step 17010: loss = 2.76361
Step 17015: loss = 2.45815
Step 17020: loss = 2.79825
Step 17025: loss = 2.64253
Step 17030: loss = 2.78019
Step 17035: loss = 2.42071
Step 17040: loss = 2.58429
Step 17045: loss = 2.76580
Step 17050: loss = 2.64770
Step 17055: loss = 2.67919
Step 17060: loss = 2.63398
Step 17065: loss = 2.75400
Step 17070: loss = 2.69363
Step 17075: loss = 2.65534
Step 17080: loss = 2.98565
Step 17085: loss = 2.73892
Step 17090: loss = 2.68402
Step 17095: loss = 2.75675
Step 17100: loss = 2.61201
Step 17105: loss = 2.59371
Step 17110: loss = 2.58004
Step 17115: loss = 2.67888
Step 17120: loss = 2.57773
Step 17125: loss = 2.75585
Step 17130: loss = 2.78623
Step 17135: loss = 2.67860
Step 17140: loss = 2.60679
Step 17145: loss = 2.64710
Step 17150: loss = 2.82385
Step 17155: loss = 2.69592
Step 17160: loss = 2.68498
Training Data Eval:
  Num examples: 49920, Num correct: 6626, Precision @ 1: 0.1327
('Testing Data Eval: EPOCH->', 45)
  Num examples: 9984, Num correct: 1328, Precision @ 1: 0.1330
Step 17165: loss = 2.79257
Step 17170: loss = 2.54250
Step 17175: loss = 2.62843
Step 17180: loss = 2.79416
Step 17185: loss = 2.62979
Step 17190: loss = 2.50228
Step 17195: loss = 2.71073
Step 17200: loss = 2.83208
Step 17205: loss = 2.77894
Step 17210: loss = 2.67308
Step 17215: loss = 2.76043
Step 17220: loss = 2.70664
Step 17225: loss = 2.85384
Step 17230: loss = 2.63284
Step 17235: loss = 2.61675
Step 17240: loss = 2.59015
Step 17245: loss = 2.63541
Step 17250: loss = 2.55792
Step 17255: loss = 2.88394
Step 17260: loss = 2.57897
Step 17265: loss = 2.42563
Step 17270: loss = 2.74942
Step 17275: loss = 2.68374
Step 17280: loss = 2.75953
Step 17285: loss = 2.68689
Step 17290: loss = 2.78696
Step 17295: loss = 2.62921
Step 17300: loss = 2.58862
Step 17305: loss = 2.72912
Step 17310: loss = 2.71619
Step 17315: loss = 2.74045
Step 17320: loss = 2.78535
Step 17325: loss = 2.61885
Step 17330: loss = 2.67583
Step 17335: loss = 2.77446
Step 17340: loss = 2.61800
Step 17345: loss = 2.85636
Step 17350: loss = 2.60790
Step 17355: loss = 2.64789
Step 17360: loss = 2.63934
Step 17365: loss = 3.03229
Step 17370: loss = 2.63383
Step 17375: loss = 2.54069
Step 17380: loss = 3.04774
Step 17385: loss = 2.49394
Step 17390: loss = 2.69645
Step 17395: loss = 2.63420
Step 17400: loss = 2.64618
Step 17405: loss = 2.76021
Step 17410: loss = 2.73351
Step 17415: loss = 2.68969
Step 17420: loss = 2.72638
Step 17425: loss = 2.92389
Step 17430: loss = 2.93517
Step 17435: loss = 2.64449
Step 17440: loss = 2.80617
Step 17445: loss = 2.89941
Step 17450: loss = 2.51178
Step 17455: loss = 2.82187
Step 17460: loss = 2.87830
Step 17465: loss = 2.88023
Step 17470: loss = 2.74076
Step 17475: loss = 3.03187
Step 17480: loss = 2.96523
Step 17485: loss = 2.86223
Step 17490: loss = 2.45152
Step 17495: loss = 2.61945
Step 17500: loss = 2.67002
Step 17505: loss = 2.67999
Step 17510: loss = 2.76133
Step 17515: loss = 2.71636
Step 17520: loss = 3.05163
Step 17525: loss = 2.72816
Step 17530: loss = 2.62776
Step 17535: loss = 2.68601
Step 17540: loss = 2.62807
Step 17545: loss = 2.65176
Step 17550: loss = 2.69106
Training Data Eval:
  Num examples: 49920, Num correct: 6636, Precision @ 1: 0.1329
('Testing Data Eval: EPOCH->', 46)
  Num examples: 9984, Num correct: 1346, Precision @ 1: 0.1348
Step 17555: loss = 2.69117
Step 17560: loss = 2.64259
Step 17565: loss = 2.62867
Step 17570: loss = 2.80249
Step 17575: loss = 2.77698
Step 17580: loss = 2.84216
Step 17585: loss = 2.46452
Step 17590: loss = 2.69704
Step 17595: loss = 2.64713
Step 17600: loss = 2.84574
Step 17605: loss = 2.64666
Step 17610: loss = 2.70146
Step 17615: loss = 2.66183
Step 17620: loss = 2.67632
Step 17625: loss = 2.81824
Step 17630: loss = 2.53260
Step 17635: loss = 2.63615
Step 17640: loss = 2.68095
Step 17645: loss = 2.88514
Step 17650: loss = 2.70684
Step 17655: loss = 2.82065
Step 17660: loss = 2.79522
Step 17665: loss = 2.91528
Step 17670: loss = 2.67811
Step 17675: loss = 2.68165
Step 17680: loss = 2.65569
Step 17685: loss = 2.72781
Step 17690: loss = 2.66100
Step 17695: loss = 2.97236
Step 17700: loss = 2.85199
Step 17705: loss = 2.70299
Step 17710: loss = 2.56466
Step 17715: loss = 2.94563
Step 17720: loss = 2.50064
Step 17725: loss = 2.56538
Step 17730: loss = 2.79436
Step 17735: loss = 2.41552
Step 17740: loss = 2.60263
Step 17745: loss = 2.57406
Step 17750: loss = 2.79111
Step 17755: loss = 2.66809
Step 17760: loss = 2.60889
Step 17765: loss = 2.48088
Step 17770: loss = 2.59804
Step 17775: loss = 2.72721
Step 17780: loss = 2.51919
Step 17785: loss = 2.92645
Step 17790: loss = 2.76912
Step 17795: loss = 2.81095
Step 17800: loss = 2.72934
Step 17805: loss = 2.77404
Step 17810: loss = 2.96374
Step 17815: loss = 2.64773
Step 17820: loss = 2.63617
Step 17825: loss = 2.62612
Step 17830: loss = 2.75042
Step 17835: loss = 2.91679
Step 17840: loss = 2.88257
Step 17845: loss = 2.56336
Step 17850: loss = 2.96216
Step 17855: loss = 2.66374
Step 17860: loss = 2.72019
Step 17865: loss = 2.63300
Step 17870: loss = 2.68178
Step 17875: loss = 2.54369
Step 17880: loss = 2.70601
Step 17885: loss = 2.79158
Step 17890: loss = 2.59659
Step 17895: loss = 2.57353
Step 17900: loss = 2.72667
Step 17905: loss = 2.78859
Step 17910: loss = 2.78117
Step 17915: loss = 2.54645
Step 17920: loss = 2.74575
Step 17925: loss = 2.64467
Step 17930: loss = 2.75779
Step 17935: loss = 2.72139
Step 17940: loss = 2.73750
Training Data Eval:
  Num examples: 49920, Num correct: 6716, Precision @ 1: 0.1345
('Testing Data Eval: EPOCH->', 47)
  Num examples: 9984, Num correct: 1341, Precision @ 1: 0.1343
Step 17945: loss = 2.69628
Step 17950: loss = 2.73882
Step 17955: loss = 2.86669
Step 17960: loss = 2.60431
Step 17965: loss = 2.78968
Step 17970: loss = 2.61187
Step 17975: loss = 2.73926
Step 17980: loss = 2.89566
Step 17985: loss = 2.60143
Step 17990: loss = 2.72652
Step 17995: loss = 2.93259
Step 18000: loss = 2.77104
Step 18005: loss = 2.65386
Step 18010: loss = 2.73888
Step 18015: loss = 2.84551
Step 18020: loss = 2.62958
Step 18025: loss = 2.74096
Step 18030: loss = 2.78038
Step 18035: loss = 2.67292
Step 18040: loss = 2.77165
Step 18045: loss = 2.56748
Step 18050: loss = 2.81514
Step 18055: loss = 2.77075
Step 18060: loss = 2.87802
Step 18065: loss = 2.80399
Step 18070: loss = 2.72254
Step 18075: loss = 2.75117
Step 18080: loss = 2.68869
Step 18085: loss = 2.76738
Step 18090: loss = 2.53707
Step 18095: loss = 2.95464
Step 18100: loss = 2.98532
Step 18105: loss = 2.63513
Step 18110: loss = 2.73815
Step 18115: loss = 2.83003
Step 18120: loss = 3.06091
Step 18125: loss = 2.53340
Step 18130: loss = 2.60337
Step 18135: loss = 2.65435
Step 18140: loss = 2.67800
Step 18145: loss = 2.73604
Step 18150: loss = 2.58433
Step 18155: loss = 2.62335
Step 18160: loss = 2.75101
Step 18165: loss = 2.51222
Step 18170: loss = 2.70424
Step 18175: loss = 2.73428
Step 18180: loss = 2.64771
Step 18185: loss = 2.55879
Step 18190: loss = 2.77025
Step 18195: loss = 2.69242
Step 18200: loss = 2.67182
Step 18205: loss = 2.79541
Step 18210: loss = 2.79866
Step 18215: loss = 2.57924
Step 18220: loss = 2.62596
Step 18225: loss = 2.77594
Step 18230: loss = 2.96381
Step 18235: loss = 2.73525
Step 18240: loss = 2.63321
Step 18245: loss = 2.62194
Step 18250: loss = 2.50068
Step 18255: loss = 2.66508
Step 18260: loss = 2.82748
Step 18265: loss = 2.60189
Step 18270: loss = 2.62251
Step 18275: loss = 2.70448
Step 18280: loss = 2.70161
Step 18285: loss = 2.74935
Step 18290: loss = 2.79297
Step 18295: loss = 2.85883
Step 18300: loss = 2.92254
Step 18305: loss = 2.65124
Step 18310: loss = 2.73980
Step 18315: loss = 2.85024
Step 18320: loss = 2.83001
Step 18325: loss = 2.80551
Step 18330: loss = 2.66172
Training Data Eval:
  Num examples: 49920, Num correct: 6745, Precision @ 1: 0.1351
('Testing Data Eval: EPOCH->', 48)
  Num examples: 9984, Num correct: 1320, Precision @ 1: 0.1322
Step 18335: loss = 2.57751
Step 18340: loss = 2.55220
Step 18345: loss = 2.47408
Step 18350: loss = 2.73836
Step 18355: loss = 2.52644
Step 18360: loss = 2.74465
Step 18365: loss = 2.75520
Step 18370: loss = 2.95529
Step 18375: loss = 2.90479
Step 18380: loss = 2.84880
Step 18385: loss = 2.61844
Step 18390: loss = 2.80619
Step 18395: loss = 2.56973
Step 18400: loss = 2.61048
Step 18405: loss = 3.02240
Step 18410: loss = 2.92853
Step 18415: loss = 2.79816
Step 18420: loss = 2.97366
Step 18425: loss = 2.72112
Step 18430: loss = 2.76435
Step 18435: loss = 2.76284
Step 18440: loss = 2.85004
Step 18445: loss = 2.80114
Step 18450: loss = 2.80304
Step 18455: loss = 2.61759
Step 18460: loss = 2.69311
Step 18465: loss = 2.66772
Step 18470: loss = 2.90177
Step 18475: loss = 2.63350
Step 18480: loss = 2.69695
Step 18485: loss = 2.58406
Step 18490: loss = 2.76745
Step 18495: loss = 2.79025
Step 18500: loss = 2.93579
Step 18505: loss = 2.87092
Step 18510: loss = 2.65946
Step 18515: loss = 2.80941
Step 18520: loss = 3.01646
Step 18525: loss = 2.62969
Step 18530: loss = 2.69836
Step 18535: loss = 2.76716
Step 18540: loss = 2.57037
Step 18545: loss = 2.68421
Step 18550: loss = 2.82629
Step 18555: loss = 2.80514
Step 18560: loss = 2.73250
Step 18565: loss = 2.62824
Step 18570: loss = 2.61309
Step 18575: loss = 2.53900
Step 18580: loss = 2.77234
Step 18585: loss = 2.75189
Step 18590: loss = 2.75731
Step 18595: loss = 2.88814
Step 18600: loss = 2.56681
Step 18605: loss = 2.81275
Step 18610: loss = 2.63819
Step 18615: loss = 2.60716
Step 18620: loss = 2.89029
Step 18625: loss = 2.66916
Step 18630: loss = 2.76203
Step 18635: loss = 2.67326
Step 18640: loss = 2.41946
Step 18645: loss = 2.60574
Step 18650: loss = 2.70519
Step 18655: loss = 2.66928
Step 18660: loss = 2.47263
Step 18665: loss = 2.67642
Step 18670: loss = 2.63660
Step 18675: loss = 2.50578
Step 18680: loss = 2.54713
Step 18685: loss = 3.09945
Step 18690: loss = 2.94686
Step 18695: loss = 2.80172
Step 18700: loss = 2.60750
Step 18705: loss = 2.98457
Step 18710: loss = 2.67310
Step 18715: loss = 2.93034
Step 18720: loss = 2.97023
Training Data Eval:
  Num examples: 49920, Num correct: 6247, Precision @ 1: 0.1251
('Testing Data Eval: EPOCH->', 49)
  Num examples: 9984, Num correct: 1309, Precision @ 1: 0.1311
Step 18725: loss = 2.66573
Step 18730: loss = 2.61942
Step 18735: loss = 2.53410
Step 18740: loss = 2.56819
Step 18745: loss = 2.82691
Step 18750: loss = 2.80977
Step 18755: loss = 2.79694
Step 18760: loss = 2.73318
Step 18765: loss = 2.74499
Step 18770: loss = 2.54038
Step 18775: loss = 2.87364
Step 18780: loss = 2.81108
Step 18785: loss = 2.82589
Step 18790: loss = 2.59626
Step 18795: loss = 2.80411
Step 18800: loss = 2.52015
Step 18805: loss = 2.71134
Step 18810: loss = 2.72485
Step 18815: loss = 2.86091
Step 18820: loss = 2.60501
Step 18825: loss = 2.83066
Step 18830: loss = 2.73917
Step 18835: loss = 2.77732
Step 18840: loss = 2.63553
Step 18845: loss = 2.90646
Step 18850: loss = 2.64454
Step 18855: loss = 2.82626
Step 18860: loss = 2.53998
Step 18865: loss = 2.59012
Step 18870: loss = 2.55263
Step 18875: loss = 2.68591
Step 18880: loss = 2.76328
Step 18885: loss = 2.70676
Step 18890: loss = 2.50380
Step 18895: loss = 2.57824
Step 18900: loss = 2.56637
Step 18905: loss = 2.60526
Step 18910: loss = 2.67123
Step 18915: loss = 2.53151
Step 18920: loss = 2.55736
Step 18925: loss = 2.65334
Step 18930: loss = 2.53706
Step 18935: loss = 2.74102
Step 18940: loss = 2.74916
Step 18945: loss = 2.79671
Step 18950: loss = 2.79558
Step 18955: loss = 2.66365
Step 18960: loss = 2.47690
Step 18965: loss = 2.73987
Step 18970: loss = 2.90967
Step 18975: loss = 2.64798
Step 18980: loss = 2.73568
Step 18985: loss = 2.71050
Step 18990: loss = 2.99869
Step 18995: loss = 2.66031
Step 19000: loss = 2.71581
Step 19005: loss = 2.77960
Step 19010: loss = 2.71800
Step 19015: loss = 2.63817
Step 19020: loss = 2.73164
Step 19025: loss = 2.67334
Step 19030: loss = 2.74764
Step 19035: loss = 2.67809
Step 19040: loss = 2.59305
Step 19045: loss = 2.79915
Step 19050: loss = 2.71080
Step 19055: loss = 2.70598
Step 19060: loss = 2.77783
Step 19065: loss = 2.63030
Step 19070: loss = 2.64307
Step 19075: loss = 2.71972
Step 19080: loss = 2.74356
Step 19085: loss = 2.77886
Step 19090: loss = 2.78394
Step 19095: loss = 2.71008
Step 19100: loss = 2.93210
Step 19105: loss = 2.88967
Step 19110: loss = 2.78884
Training Data Eval:
  Num examples: 49920, Num correct: 6455, Precision @ 1: 0.1293
('Testing Data Eval: EPOCH->', 50)
  Num examples: 9984, Num correct: 1322, Precision @ 1: 0.1324
Step 19115: loss = 2.72827
Step 19120: loss = 2.83548
Step 19125: loss = 2.76984
Step 19130: loss = 2.66855
Step 19135: loss = 2.79171
Step 19140: loss = 2.82818
Step 19145: loss = 2.78008
Step 19150: loss = 2.61688
Step 19155: loss = 2.61878
Step 19160: loss = 2.52006
Step 19165: loss = 2.56610
Step 19170: loss = 2.60379
Step 19175: loss = 2.69420
Step 19180: loss = 2.75250
Step 19185: loss = 2.69684
Step 19190: loss = 2.72249
Step 19195: loss = 2.65830
Step 19200: loss = 2.83394
Step 19205: loss = 3.05158
Step 19210: loss = 2.67209
Step 19215: loss = 2.85181
Step 19220: loss = 2.61555
Step 19225: loss = 2.65541
Step 19230: loss = 2.84506
Step 19235: loss = 2.79335
Step 19240: loss = 2.79260
Step 19245: loss = 2.85651
Step 19250: loss = 2.60633
Step 19255: loss = 2.67415
Step 19260: loss = 2.64234
Step 19265: loss = 2.67120
Step 19270: loss = 2.55018
Step 19275: loss = 2.64875
Step 19280: loss = 2.87995
Step 19285: loss = 2.72387
Step 19290: loss = 2.91575
Step 19295: loss = 2.58333
Step 19300: loss = 2.57425
Step 19305: loss = 2.70294
Step 19310: loss = 2.89868
Step 19315: loss = 2.64722
Step 19320: loss = 2.72340
Step 19325: loss = 2.92240
Step 19330: loss = 2.84544
Step 19335: loss = 2.72089
Step 19340: loss = 2.62133
Step 19345: loss = 2.61718
Step 19350: loss = 2.59754
Step 19355: loss = 2.81618
Step 19360: loss = 2.64509
Step 19365: loss = 2.78900
Step 19370: loss = 2.76378
Step 19375: loss = 2.82854
Step 19380: loss = 2.81342
Step 19385: loss = 2.72769
Step 19390: loss = 2.56270
Step 19395: loss = 2.60873
Step 19400: loss = 2.66959
Step 19405: loss = 2.82297
Step 19410: loss = 2.86214
Step 19415: loss = 2.59989
Step 19420: loss = 2.58969
Step 19425: loss = 2.83774
Step 19430: loss = 2.75977
Step 19435: loss = 2.66157
Step 19440: loss = 2.61245
Step 19445: loss = 2.72458
Step 19450: loss = 2.87802
Step 19455: loss = 2.79458
Step 19460: loss = 2.79159
Step 19465: loss = 2.75992
Step 19470: loss = 2.69587
Step 19475: loss = 2.87382
Step 19480: loss = 2.61496
Step 19485: loss = 2.68046
Step 19490: loss = 2.61496
Step 19495: loss = 2.47318
Step 19500: loss = 2.52710
Training Data Eval:
  Num examples: 49920, Num correct: 6506, Precision @ 1: 0.1303
('Testing Data Eval: EPOCH->', 51)
  Num examples: 9984, Num correct: 1274, Precision @ 1: 0.1276
Step 19505: loss = 2.82322
Step 19510: loss = 2.58437
Step 19515: loss = 2.79220
Step 19520: loss = 2.78691
Step 19525: loss = 2.87532
Step 19530: loss = 2.85052
Step 19535: loss = 2.80641
Step 19540: loss = 2.76036
Step 19545: loss = 2.90750
Step 19550: loss = 2.84069
Step 19555: loss = 2.94995
Step 19560: loss = 2.63237
Step 19565: loss = 2.59932
Step 19570: loss = 2.86090
Step 19575: loss = 2.70592
Step 19580: loss = 2.55810
Step 19585: loss = 2.82841
Step 19590: loss = 2.84777
Step 19595: loss = 2.83332
Step 19600: loss = 2.63257
Step 19605: loss = 2.52262
Step 19610: loss = 2.66534
Step 19615: loss = 2.87612
Step 19620: loss = 2.68495
Step 19625: loss = 2.73137
Step 19630: loss = 2.66320
Step 19635: loss = 2.61747
Step 19640: loss = 2.76151
Step 19645: loss = 2.71037
Step 19650: loss = 2.61652
Step 19655: loss = 2.84741
Step 19660: loss = 2.82099
Step 19665: loss = 2.88123
Step 19670: loss = 2.90723
Step 19675: loss = 2.58899
Step 19680: loss = 2.75321
Step 19685: loss = 2.81007
Step 19690: loss = 2.74403
Step 19695: loss = 2.64328
Step 19700: loss = 2.58567
Step 19705: loss = 2.55366
Step 19710: loss = 2.61254
Step 19715: loss = 2.77508
Step 19720: loss = 2.80352
Step 19725: loss = 2.86662
Step 19730: loss = 2.89528
Step 19735: loss = 2.70296
Step 19740: loss = 2.86617
Step 19745: loss = 2.90843
Step 19750: loss = 2.78734
Step 19755: loss = 2.81443
Step 19760: loss = 2.73865
Step 19765: loss = 2.66272
Step 19770: loss = 2.69191
Step 19775: loss = 2.81302
Step 19780: loss = 2.88098
Step 19785: loss = 2.77889
Step 19790: loss = 2.85626
Step 19795: loss = 2.64281
Step 19800: loss = 2.64532
Step 19805: loss = 2.85898
Step 19810: loss = 2.84816
Step 19815: loss = 2.65311
Step 19820: loss = 2.65160
Step 19825: loss = 2.61129
Step 19830: loss = 2.53243
Step 19835: loss = 2.64171
Step 19840: loss = 2.99036
Step 19845: loss = 2.87113
Step 19850: loss = 3.03465
Step 19855: loss = 2.63972
Step 19860: loss = 2.99337
Step 19865: loss = 2.65665
Step 19870: loss = 2.72665
Step 19875: loss = 2.64267
Step 19880: loss = 2.72784
Step 19885: loss = 2.54019
Step 19890: loss = 2.72176
Training Data Eval:
  Num examples: 49920, Num correct: 6607, Precision @ 1: 0.1324
('Testing Data Eval: EPOCH->', 52)
  Num examples: 9984, Num correct: 1309, Precision @ 1: 0.1311
Step 19895: loss = 2.48341
Step 19900: loss = 2.81025
Step 19905: loss = 2.74095
Step 19910: loss = 2.77399
Step 19915: loss = 2.66968
Step 19920: loss = 2.75071
Step 19925: loss = 2.41399
Step 19930: loss = 2.55947
Step 19935: loss = 2.63498
Step 19940: loss = 3.07739
Step 19945: loss = 2.58621
Step 19950: loss = 2.71474
Step 19955: loss = 2.60709
Step 19960: loss = 2.89505
Step 19965: loss = 2.48298
Step 19970: loss = 2.91481
Step 19975: loss = 2.81495
Step 19980: loss = 2.84002
Step 19985: loss = 2.76579
Step 19990: loss = 2.65221
Step 19995: loss = 2.80560
Step 20000: loss = 2.61303
Step 20005: loss = 2.84880
Step 20010: loss = 2.61856
Step 20015: loss = 2.58816
Step 20020: loss = 2.75422
Step 20025: loss = 2.66636
Step 20030: loss = 2.84143
Step 20035: loss = 2.69101
Step 20040: loss = 2.82614
Step 20045: loss = 2.79039
Step 20050: loss = 2.79136
Step 20055: loss = 2.75477
Step 20060: loss = 2.77467
Step 20065: loss = 2.65834
Step 20070: loss = 2.84368
Step 20075: loss = 2.79929
Step 20080: loss = 2.80076
Step 20085: loss = 2.57958
Step 20090: loss = 2.68950
Step 20095: loss = 2.64290
Step 20100: loss = 2.83749
Step 20105: loss = 2.84913
Step 20110: loss = 2.58153
Step 20115: loss = 2.68378
Step 20120: loss = 2.48796
Step 20125: loss = 2.73099
Step 20130: loss = 2.77935
Step 20135: loss = 2.50073
Step 20140: loss = 2.82741
Step 20145: loss = 2.99258
Step 20150: loss = 2.46987
Step 20155: loss = 2.87488
Step 20160: loss = 2.66119
Step 20165: loss = 2.70704
Step 20170: loss = 2.80920
Step 20175: loss = 2.81949
Step 20180: loss = 2.63509
Step 20185: loss = 2.63851
Step 20190: loss = 2.72056
Step 20195: loss = 2.76645
Step 20200: loss = 2.88839
Step 20205: loss = 2.69359
Step 20210: loss = 2.71376
Step 20215: loss = 2.79365
Step 20220: loss = 2.93852
Step 20225: loss = 2.72009
Step 20230: loss = 2.77026
Step 20235: loss = 2.57657
Step 20240: loss = 2.96139
Step 20245: loss = 2.81046
Step 20250: loss = 2.76527
Step 20255: loss = 2.69457
Step 20260: loss = 2.73814
Step 20265: loss = 2.74643
Step 20270: loss = 2.64743
Step 20275: loss = 2.64308
Step 20280: loss = 2.70066
Training Data Eval:
  Num examples: 49920, Num correct: 6680, Precision @ 1: 0.1338
('Testing Data Eval: EPOCH->', 53)
  Num examples: 9984, Num correct: 1475, Precision @ 1: 0.1477
Step 20285: loss = 2.75581
Step 20290: loss = 2.72936
Step 20295: loss = 2.65056
Step 20300: loss = 2.91413
Step 20305: loss = 2.66470
Step 20310: loss = 2.72851
Step 20315: loss = 2.69650
Step 20320: loss = 2.90303
Step 20325: loss = 2.72539
Step 20330: loss = 2.88380
Step 20335: loss = 2.87774
Step 20340: loss = 2.71191
Step 20345: loss = 2.87195
Step 20350: loss = 2.65954
Step 20355: loss = 2.60972
Step 20360: loss = 2.65901
Step 20365: loss = 2.64042
Step 20370: loss = 3.04961
Step 20375: loss = 2.68568
Step 20380: loss = 2.68689
Step 20385: loss = 2.64821
Step 20390: loss = 2.52350
Step 20395: loss = 2.97380
Step 20400: loss = 2.75299
Step 20405: loss = 2.72330
Step 20410: loss = 2.65460
Step 20415: loss = 2.72547
Step 20420: loss = 2.57852
Step 20425: loss = 2.59875
Step 20430: loss = 2.68155
Step 20435: loss = 2.74838
Step 20440: loss = 2.67029
Step 20445: loss = 2.60525
Step 20450: loss = 2.49584
Step 20455: loss = 2.75382
Step 20460: loss = 2.75752
Step 20465: loss = 2.82565
Step 20470: loss = 2.65338
Step 20475: loss = 2.77907
Step 20480: loss = 2.61590
Step 20485: loss = 2.68735
Step 20490: loss = 2.76029
Step 20495: loss = 2.81038
Step 20500: loss = 2.88322
Step 20505: loss = 2.65274
Step 20510: loss = 2.63098
Step 20515: loss = 2.63383
Step 20520: loss = 2.98321
Step 20525: loss = 2.87232
Step 20530: loss = 2.64952
Step 20535: loss = 2.75036
Step 20540: loss = 2.90283
Step 20545: loss = 2.80075
Step 20550: loss = 2.62795
Step 20555: loss = 2.51012
Step 20560: loss = 2.56388
Step 20565: loss = 2.70508
Step 20570: loss = 2.69997
Step 20575: loss = 2.64285
Step 20580: loss = 2.89821
Step 20585: loss = 2.77831
Step 20590: loss = 2.69113
Step 20595: loss = 2.79532
Step 20600: loss = 2.83583
Step 20605: loss = 2.84426
Step 20610: loss = 2.68369
Step 20615: loss = 2.85586
Step 20620: loss = 2.70270
Step 20625: loss = 2.82364
Step 20630: loss = 2.60739
Step 20635: loss = 2.62533
Step 20640: loss = 2.78318
Step 20645: loss = 2.79299
Step 20650: loss = 2.91414
Step 20655: loss = 2.73685
Step 20660: loss = 2.56383
Step 20665: loss = 2.68654
Step 20670: loss = 2.64616
Training Data Eval:
  Num examples: 49920, Num correct: 6544, Precision @ 1: 0.1311
('Testing Data Eval: EPOCH->', 54)
  Num examples: 9984, Num correct: 1379, Precision @ 1: 0.1381
Step 20675: loss = 2.81934
Step 20680: loss = 2.84845
Step 20685: loss = 2.83384
Step 20690: loss = 2.50424
Step 20695: loss = 2.82283
Step 20700: loss = 2.71857
Step 20705: loss = 2.40005
Step 20710: loss = 2.87054
Step 20715: loss = 2.67509
Step 20720: loss = 2.57115
Step 20725: loss = 2.76591
Step 20730: loss = 2.93254
Step 20735: loss = 2.76796
Step 20740: loss = 2.74241
Step 20745: loss = 2.65682
Step 20750: loss = 2.53747
Step 20755: loss = 2.61145
Step 20760: loss = 2.76564
Step 20765: loss = 3.00477
Step 20770: loss = 2.86262
Step 20775: loss = 2.65760
Step 20780: loss = 2.66685
Step 20785: loss = 2.77575
Step 20790: loss = 2.60394
Step 20795: loss = 2.66270
Step 20800: loss = 2.74595
Step 20805: loss = 2.82964
Step 20810: loss = 2.65365
Step 20815: loss = 2.73694
Step 20820: loss = 2.74066
Step 20825: loss = 2.72698
Step 20830: loss = 2.71573
Step 20835: loss = 2.75736
Step 20840: loss = 2.65353
Step 20845: loss = 2.75971
Step 20850: loss = 2.71084
Step 20855: loss = 2.77600
Step 20860: loss = 2.71341
Step 20865: loss = 2.93404
Step 20870: loss = 2.70454
Step 20875: loss = 2.80786
Step 20880: loss = 2.62212
Step 20885: loss = 2.70970
Step 20890: loss = 2.64609
Step 20895: loss = 2.72324
Step 20900: loss = 2.71260
Step 20905: loss = 2.71264
Step 20910: loss = 2.75064
Step 20915: loss = 2.74457
Step 20920: loss = 2.79564
Step 20925: loss = 2.70149
Step 20930: loss = 2.77915
Step 20935: loss = 2.66561
Step 20940: loss = 2.77660
Step 20945: loss = 2.70412
Step 20950: loss = 2.60474
Step 20955: loss = 2.56835
Step 20960: loss = 2.61255
Step 20965: loss = 2.85747
Step 20970: loss = 2.74213
Step 20975: loss = 2.54297
Step 20980: loss = 3.00619
Step 20985: loss = 2.80515
Step 20990: loss = 2.81568
Step 20995: loss = 2.51601
Step 21000: loss = 2.81148
Step 21005: loss = 2.75802
Step 21010: loss = 2.83337
Step 21015: loss = 2.66168
Step 21020: loss = 2.77414
Step 21025: loss = 2.73525
Step 21030: loss = 2.66304
Step 21035: loss = 2.76781
Step 21040: loss = 2.89923
Step 21045: loss = 2.82167
Step 21050: loss = 2.57671
Step 21055: loss = 2.83618
Step 21060: loss = 2.68790
Training Data Eval:
  Num examples: 49920, Num correct: 6657, Precision @ 1: 0.1334
('Testing Data Eval: EPOCH->', 55)
  Num examples: 9984, Num correct: 1376, Precision @ 1: 0.1378
Step 21065: loss = 2.77524
Step 21070: loss = 2.69338
Step 21075: loss = 2.58955
Step 21080: loss = 2.84784
Step 21085: loss = 2.76998
Step 21090: loss = 2.93349
Step 21095: loss = 2.66233
Step 21100: loss = 2.75024
Step 21105: loss = 2.54233
Step 21110: loss = 2.80634
Step 21115: loss = 2.90173
Step 21120: loss = 2.84988
Step 21125: loss = 2.75273
Step 21130: loss = 2.72993
Step 21135: loss = 2.72619
Step 21140: loss = 2.62855
Step 21145: loss = 2.63244
Step 21150: loss = 3.19454
Step 21155: loss = 2.78394
Step 21160: loss = 2.72199
Step 21165: loss = 2.87193
Step 21170: loss = 2.84491
Step 21175: loss = 2.88540
Step 21180: loss = 2.86108
Step 21185: loss = 2.58739
Step 21190: loss = 2.80829
Step 21195: loss = 2.72548
Step 21200: loss = 2.69023
Step 21205: loss = 2.76724
Step 21210: loss = 2.74517
Step 21215: loss = 2.91776
Step 21220: loss = 2.71533
Step 21225: loss = 2.71446
Step 21230: loss = 2.62102
Step 21235: loss = 2.70458
Step 21240: loss = 2.67969
Step 21245: loss = 2.88861
Step 21250: loss = 2.64761
Step 21255: loss = 2.61251
Step 21260: loss = 2.48208
Step 21265: loss = 2.89249
Step 21270: loss = 2.86716
Step 21275: loss = 2.74923
Step 21280: loss = 2.94725
Step 21285: loss = 2.74677
Step 21290: loss = 2.77848
Step 21295: loss = 2.56076
Step 21300: loss = 2.49671
Step 21305: loss = 2.77464
Step 21310: loss = 2.61570
Step 21315: loss = 2.81772
Step 21320: loss = 2.77241
Step 21325: loss = 2.61963
Step 21330: loss = 2.86351
Step 21335: loss = 2.90832
Step 21340: loss = 2.98699
Step 21345: loss = 2.84451
Step 21350: loss = 2.93093
Step 21355: loss = 2.68424
Step 21360: loss = 2.73925
Step 21365: loss = 2.60671
Step 21370: loss = 2.83965
Step 21375: loss = 2.80714
Step 21380: loss = 2.81281
Step 21385: loss = 2.74457
Step 21390: loss = 2.77571
Step 21395: loss = 2.89718
Step 21400: loss = 2.81278
Step 21405: loss = 2.76009
Step 21410: loss = 2.69387
Step 21415: loss = 2.85605
Step 21420: loss = 2.80419
Step 21425: loss = 2.73514
Step 21430: loss = 2.91264
Step 21435: loss = 3.02915
Step 21440: loss = 2.62689
Step 21445: loss = 2.74802
Step 21450: loss = 2.80504
Training Data Eval:
  Num examples: 49920, Num correct: 6461, Precision @ 1: 0.1294
('Testing Data Eval: EPOCH->', 56)
  Num examples: 9984, Num correct: 1370, Precision @ 1: 0.1372
Step 21455: loss = 2.93701
Step 21460: loss = 2.63229
Step 21465: loss = 2.72264
Step 21470: loss = 2.76985
Step 21475: loss = 2.67236
Step 21480: loss = 2.88190
Step 21485: loss = 2.73265
Step 21490: loss = 2.81856
Step 21495: loss = 2.99152
Step 21500: loss = 3.00106
Step 21505: loss = 2.82011
Step 21510: loss = 2.62537
Step 21515: loss = 2.40532
Step 21520: loss = 2.72405
Step 21525: loss = 2.96575
Step 21530: loss = 2.81986
Step 21535: loss = 2.89919
Step 21540: loss = 2.86014
Step 21545: loss = 2.78003
Step 21550: loss = 2.80889
Step 21555: loss = 2.75948
Step 21560: loss = 2.55531
Step 21565: loss = 3.06788
Step 21570: loss = 2.69316
Step 21575: loss = 2.85769
Step 21580: loss = 3.02792
Step 21585: loss = 2.74313
Step 21590: loss = 2.94844
Step 21595: loss = 2.76098
Step 21600: loss = 2.70414
Step 21605: loss = 2.67006
Step 21610: loss = 2.93625
Step 21615: loss = 2.77713
Step 21620: loss = 2.80138
Step 21625: loss = 2.74470
Step 21630: loss = 2.77687
Step 21635: loss = 2.67549
Step 21640: loss = 3.01750
Step 21645: loss = 2.78274
Step 21650: loss = 2.70888
Step 21655: loss = 2.80242
Step 21660: loss = 2.70112
Step 21665: loss = 2.85159
Step 21670: loss = 2.72290
Step 21675: loss = 2.64614
Step 21680: loss = 2.62041
Step 21685: loss = 2.78391
Step 21690: loss = 2.56004
Step 21695: loss = 2.81970
Step 21700: loss = 2.73520
Step 21705: loss = 2.63704
Step 21710: loss = 2.74824
Step 21715: loss = 2.67376
Step 21720: loss = 2.70060
Step 21725: loss = 2.49262
Step 21730: loss = 2.94090
Step 21735: loss = 2.87127
Step 21740: loss = 2.67967
Step 21745: loss = 2.75449
Step 21750: loss = 2.86929
Step 21755: loss = 2.70721
Step 21760: loss = 3.00714
Step 21765: loss = 2.78207
Step 21770: loss = 2.51729
Step 21775: loss = 2.81413
Step 21780: loss = 2.92087
Step 21785: loss = 2.66632
Step 21790: loss = 2.76686
Step 21795: loss = 2.57565
Step 21800: loss = 2.68278
Step 21805: loss = 2.63858
Step 21810: loss = 2.74085
Step 21815: loss = 2.89214
Step 21820: loss = 2.79371
Step 21825: loss = 2.71011
Step 21830: loss = 2.68872
Step 21835: loss = 2.87332
Step 21840: loss = 2.64387
Training Data Eval:
  Num examples: 49920, Num correct: 6784, Precision @ 1: 0.1359
('Testing Data Eval: EPOCH->', 57)
  Num examples: 9984, Num correct: 1331, Precision @ 1: 0.1333
Step 21845: loss = 2.65583
Step 21850: loss = 2.60405
Step 21855: loss = 2.96710
Step 21860: loss = 2.81138
Step 21865: loss = 2.63328
Step 21870: loss = 2.84618
Step 21875: loss = 2.87233
Step 21880: loss = 3.06182
Step 21885: loss = 2.78703
Step 21890: loss = 2.65386
Step 21895: loss = 2.48441
Step 21900: loss = 2.59884
Step 21905: loss = 2.60024
Step 21910: loss = 2.77791
Step 21915: loss = 2.61810
Step 21920: loss = 2.56697
Step 21925: loss = 2.90340
Step 21930: loss = 2.79527
Step 21935: loss = 2.59158
Step 21940: loss = 2.71262
Step 21945: loss = 2.64307
Step 21950: loss = 2.75818
Step 21955: loss = 2.70695
Step 21960: loss = 2.75390
Step 21965: loss = 2.71884
Step 21970: loss = 2.68754
Step 21975: loss = 2.61076
Step 21980: loss = 2.72543
Step 21985: loss = 2.87252
Step 21990: loss = 2.78147
Step 21995: loss = 2.58767
Step 22000: loss = 2.58274
Step 22005: loss = 2.58399
Step 22010: loss = 2.78268
Step 22015: loss = 2.69080
Step 22020: loss = 2.72558
Step 22025: loss = 2.60226
Step 22030: loss = 2.76032
Step 22035: loss = 2.59668
Step 22040: loss = 2.94061
Step 22045: loss = 2.73991
Step 22050: loss = 2.63109
Step 22055: loss = 2.59986
Step 22060: loss = 2.78794
Step 22065: loss = 2.81046
Step 22070: loss = 2.72904
Step 22075: loss = 2.75680
Step 22080: loss = 2.63711
Step 22085: loss = 2.60311
Step 22090: loss = 2.63076
Step 22095: loss = 2.79583
Step 22100: loss = 2.81572
Step 22105: loss = 2.82740
Step 22110: loss = 2.61391
Step 22115: loss = 2.67095
Step 22120: loss = 2.69109
Step 22125: loss = 2.66066
Step 22130: loss = 2.57250
Step 22135: loss = 2.89847
Step 22140: loss = 2.78097
Step 22145: loss = 3.00860
Step 22150: loss = 2.70413
Step 22155: loss = 2.73366
Step 22160: loss = 2.58563
Step 22165: loss = 2.83996
Step 22170: loss = 2.77448
Step 22175: loss = 2.78380
Step 22180: loss = 2.79870
Step 22185: loss = 2.65346
Step 22190: loss = 2.76190
Step 22195: loss = 2.62885
Step 22200: loss = 2.80957
Step 22205: loss = 2.59099
Step 22210: loss = 2.63372
Step 22215: loss = 2.96762
Step 22220: loss = 2.75585
Step 22225: loss = 2.56763
Step 22230: loss = 2.53677
Training Data Eval:
  Num examples: 49920, Num correct: 6671, Precision @ 1: 0.1336
('Testing Data Eval: EPOCH->', 58)
  Num examples: 9984, Num correct: 1391, Precision @ 1: 0.1393
Step 22235: loss = 2.76398
Step 22240: loss = 2.58487
Step 22245: loss = 2.71627
Step 22250: loss = 2.45070
Step 22255: loss = 2.43326
Step 22260: loss = 2.64530
Step 22265: loss = 2.66322
Step 22270: loss = 2.67302
Step 22275: loss = 2.81456
Step 22280: loss = 2.66357
Step 22285: loss = 2.62798
Step 22290: loss = 2.76056
Step 22295: loss = 2.94510
Step 22300: loss = 2.73277
Step 22305: loss = 2.74986
Step 22310: loss = 2.50216
Step 22315: loss = 2.80807
Step 22320: loss = 2.80476
Step 22325: loss = 2.73227
Step 22330: loss = 2.56372
Step 22335: loss = 2.89235
Step 22340: loss = 2.78753
Step 22345: loss = 2.66751
Step 22350: loss = 2.67832
Step 22355: loss = 2.60328
Step 22360: loss = 3.02279
Step 22365: loss = 2.74706
Step 22370: loss = 2.86267
Step 22375: loss = 2.68858
Step 22380: loss = 2.89305
Step 22385: loss = 2.66337
Step 22390: loss = 2.86164
Step 22395: loss = 3.16769
Step 22400: loss = 2.77203
Step 22405: loss = 2.72066
Step 22410: loss = 2.66556
Step 22415: loss = 2.78584
Step 22420: loss = 2.41833
Step 22425: loss = 2.58045
Step 22430: loss = 2.69416
Step 22435: loss = 2.58390
Step 22440: loss = 2.66708
Step 22445: loss = 2.69621
Step 22450: loss = 2.55863
Step 22455: loss = 2.79278
Step 22460: loss = 2.83635
Step 22465: loss = 2.63549
Step 22470: loss = 2.62333
Step 22475: loss = 2.65710
Step 22480: loss = 3.02851
Step 22485: loss = 2.71312
Step 22490: loss = 2.82953
Step 22495: loss = 2.69317
Step 22500: loss = 2.67319
Step 22505: loss = 2.56670
Step 22510: loss = 2.66116
Step 22515: loss = 2.72144
Step 22520: loss = 2.74533
Step 22525: loss = 2.67249
Step 22530: loss = 2.65436
Step 22535: loss = 2.71103
Step 22540: loss = 2.77088
Step 22545: loss = 2.75562
Step 22550: loss = 2.54310
Step 22555: loss = 2.92593
Step 22560: loss = 2.56047
Step 22565: loss = 2.79232
Step 22570: loss = 2.84074
Step 22575: loss = 2.56456
Step 22580: loss = 2.75204
Step 22585: loss = 2.57497
Step 22590: loss = 2.83354
Step 22595: loss = 2.65899
Step 22600: loss = 2.71574
Step 22605: loss = 2.65817
Step 22610: loss = 2.83761
Step 22615: loss = 2.73164
Step 22620: loss = 2.70429
Training Data Eval:
  Num examples: 49920, Num correct: 6771, Precision @ 1: 0.1356
('Testing Data Eval: EPOCH->', 59)
  Num examples: 9984, Num correct: 1366, Precision @ 1: 0.1368
Step 22625: loss = 2.69233
Step 22630: loss = 2.64953
Step 22635: loss = 2.73933
Step 22640: loss = 2.68701
Step 22645: loss = 2.74236
Step 22650: loss = 2.58640
Step 22655: loss = 2.48396
Step 22660: loss = 2.80142
Step 22665: loss = 3.00721
Step 22670: loss = 2.84879
Step 22675: loss = 2.75288
Step 22680: loss = 2.97305
Step 22685: loss = 2.74689
Step 22690: loss = 2.72363
Step 22695: loss = 2.77639
Step 22700: loss = 2.83453
Step 22705: loss = 2.83651
Step 22710: loss = 2.58451
Step 22715: loss = 2.46273
Step 22720: loss = 2.48824
Step 22725: loss = 2.73292
Step 22730: loss = 2.93114
Step 22735: loss = 2.73085
Step 22740: loss = 2.81600
Step 22745: loss = 2.76382
Step 22750: loss = 2.81886
Step 22755: loss = 2.59993
Step 22760: loss = 2.79481
Step 22765: loss = 2.68334
Step 22770: loss = 2.68718
Step 22775: loss = 2.59981
Step 22780: loss = 2.60678
Step 22785: loss = 2.93384
Step 22790: loss = 2.63232
Step 22795: loss = 2.95298
Step 22800: loss = 2.76612
Step 22805: loss = 2.87007
Step 22810: loss = 2.75581
Step 22815: loss = 2.83298
Step 22820: loss = 2.81579
Step 22825: loss = 2.92928
Step 22830: loss = 2.78011
Step 22835: loss = 2.74034
Step 22840: loss = 2.63611
Step 22845: loss = 2.59752
Step 22850: loss = 2.64524
Step 22855: loss = 2.85954
Step 22860: loss = 2.83004
Step 22865: loss = 2.77205
Step 22870: loss = 2.79256
Step 22875: loss = 2.63410
Step 22880: loss = 2.58786
Step 22885: loss = 2.80935
Step 22890: loss = 2.59049
Step 22895: loss = 2.72529
Step 22900: loss = 2.79978
Step 22905: loss = 2.65969
Step 22910: loss = 2.91835
Step 22915: loss = 2.61331
Step 22920: loss = 2.68180
Step 22925: loss = 2.62148
Step 22930: loss = 2.59753
Step 22935: loss = 2.49968
Step 22940: loss = 2.77998
Step 22945: loss = 2.68260
Step 22950: loss = 2.63684
Step 22955: loss = 2.85588
Step 22960: loss = 2.73181
Step 22965: loss = 2.79098
Step 22970: loss = 2.73298
Step 22975: loss = 2.86756
Step 22980: loss = 2.81949
Step 22985: loss = 2.78864
Step 22990: loss = 2.68385
Step 22995: loss = 2.68895
Step 23000: loss = 2.57794
Step 23005: loss = 2.75689
Step 23010: loss = 2.72258
Training Data Eval:
  Num examples: 49920, Num correct: 6845, Precision @ 1: 0.1371
('Testing Data Eval: EPOCH->', 60)
  Num examples: 9984, Num correct: 1397, Precision @ 1: 0.1399
Step 23015: loss = 2.73326
Step 23020: loss = 2.61943
Step 23025: loss = 2.83489
Step 23030: loss = 2.89957
Step 23035: loss = 2.77812
Step 23040: loss = 2.69374
Step 23045: loss = 2.86517
Step 23050: loss = 2.69108
Step 23055: loss = 2.73549
Step 23060: loss = 2.54855
Step 23065: loss = 2.45068
Step 23070: loss = 2.89108
Step 23075: loss = 2.77557
Step 23080: loss = 2.66916
Step 23085: loss = 3.07112
Step 23090: loss = 2.89450
Step 23095: loss = 2.66781
Step 23100: loss = 2.80160
Step 23105: loss = 2.93985
Step 23110: loss = 2.87046
Step 23115: loss = 2.69342
Step 23120: loss = 2.58263
Step 23125: loss = 2.71970
Step 23130: loss = 2.53499
Step 23135: loss = 2.73047
Step 23140: loss = 2.68219
Step 23145: loss = 2.91018
Step 23150: loss = 2.67807
Step 23155: loss = 2.70627
Step 23160: loss = 2.66915
Step 23165: loss = 2.79611
Step 23170: loss = 2.80313
Step 23175: loss = 2.76413
Step 23180: loss = 2.89711
Step 23185: loss = 2.85317
Step 23190: loss = 2.75538
Step 23195: loss = 2.62626
Step 23200: loss = 2.51235
Step 23205: loss = 2.82733
Step 23210: loss = 2.72650
Step 23215: loss = 2.99877
Step 23220: loss = 2.76440
Step 23225: loss = 2.91426
Step 23230: loss = 2.68443
Step 23235: loss = 2.96612
Step 23240: loss = 2.81807
Step 23245: loss = 2.74199
Step 23250: loss = 3.25461
Step 23255: loss = 3.90480
Step 23260: loss = 2.69264
Step 23265: loss = 4.22396
Step 23270: loss = 2.88426
Step 23275: loss = 4.37885
Step 23280: loss = 3.79507
Step 23285: loss = 6.82116
Step 23290: loss = 2.90466
Step 23295: loss = 4.90932
Step 23300: loss = 6.31643
Step 23305: loss = 5.26814
Step 23310: loss = 2.81226
Step 23315: loss = 4.46431
Step 23320: loss = 5.38782
Step 23325: loss = 3.22480
Step 23330: loss = 2.92419
Step 23335: loss = 2.77385
Step 23340: loss = 2.98075
Step 23345: loss = 2.92943
Step 23350: loss = 2.83772
Step 23355: loss = 2.78255
Step 23360: loss = 2.81652
Step 23365: loss = 2.55650
Step 23370: loss = 2.70766
Step 23375: loss = 2.75523
Step 23380: loss = 2.82381
Step 23385: loss = 2.76429
Step 23390: loss = 2.97618
Step 23395: loss = 2.64486
Step 23400: loss = 2.73200
Training Data Eval:
  Num examples: 49920, Num correct: 6556, Precision @ 1: 0.1313
('Testing Data Eval: EPOCH->', 61)
  Num examples: 9984, Num correct: 1341, Precision @ 1: 0.1343
Step 23405: loss = 2.84280
Step 23410: loss = 3.08941
Step 23415: loss = 2.77144
Step 23420: loss = 3.03716
Step 23425: loss = 2.84059
Step 23430: loss = 3.95741
Step 23435: loss = 3.76330
Step 23440: loss = 3.55399
Step 23445: loss = 2.84566
Step 23450: loss = 2.59134
Step 23455: loss = 2.71826
Step 23460: loss = 2.72593
Step 23465: loss = 2.80161
Step 23470: loss = 2.65408
Step 23475: loss = 2.68797
Step 23480: loss = 2.70178
Step 23485: loss = 2.69631
Step 23490: loss = 2.88025
Step 23495: loss = 2.68817
Step 23500: loss = 2.69101
Step 23505: loss = 2.73488
Step 23510: loss = 2.70580
Step 23515: loss = 2.81060
Step 23520: loss = 3.02934
Step 23525: loss = 2.59155
Step 23530: loss = 2.57347
Step 23535: loss = 2.81898
Step 23540: loss = 2.74635
Step 23545: loss = 2.78658
Step 23550: loss = 2.67180
Step 23555: loss = 2.85795
Step 23560: loss = 2.75602
Step 23565: loss = 2.66542
Step 23570: loss = 2.73632
Step 23575: loss = 2.58575
Step 23580: loss = 2.80210
Step 23585: loss = 2.74994
Step 23590: loss = 2.80857
Step 23595: loss = 2.80291
Step 23600: loss = 2.70396
Step 23605: loss = 2.66257
Step 23610: loss = 2.77046
Step 23615: loss = 2.62229
Step 23620: loss = 2.63436
Step 23625: loss = 2.90319
Step 23630: loss = 2.65209
Step 23635: loss = 2.70668
Step 23640: loss = 2.72721
Step 23645: loss = 2.82926
Step 23650: loss = 2.69003
Step 23655: loss = 2.77057
Step 23660: loss = 2.64633
Step 23665: loss = 2.88664
Step 23670: loss = 2.86516
Step 23675: loss = 2.68457
Step 23680: loss = 2.72610
Step 23685: loss = 2.75717
Step 23690: loss = 2.94773
Step 23695: loss = 2.78519
Step 23700: loss = 2.86732
Step 23705: loss = 2.65060
Step 23710: loss = 2.59554
Step 23715: loss = 2.96251
Step 23720: loss = 2.79453
Step 23725: loss = 2.72360
Step 23730: loss = 2.85580
Step 23735: loss = 2.58256
Step 23740: loss = 2.79966
Step 23745: loss = 2.86071
Step 23750: loss = 2.90370
Step 23755: loss = 2.72855
Step 23760: loss = 2.88657
Step 23765: loss = 2.77627
Step 23770: loss = 2.62236
Step 23775: loss = 2.65257
Step 23780: loss = 2.64128
Step 23785: loss = 2.75824
Step 23790: loss = 2.59894
Training Data Eval:
  Num examples: 49920, Num correct: 7040, Precision @ 1: 0.1410
('Testing Data Eval: EPOCH->', 62)
  Num examples: 9984, Num correct: 1452, Precision @ 1: 0.1454
Step 23795: loss = 2.68203
Step 23800: loss = 2.62897
Step 23805: loss = 2.78575
Step 23810: loss = 2.77202
Step 23815: loss = 2.75586
Step 23820: loss = 2.71397
Step 23825: loss = 2.78320
Step 23830: loss = 2.91182
Step 23835: loss = 2.58571
Step 23840: loss = 2.80926
Step 23845: loss = 2.76179
Step 23850: loss = 2.94928
Step 23855: loss = 2.57695
Step 23860: loss = 2.59876
Step 23865: loss = 2.76032
Step 23870: loss = 2.77550
Step 23875: loss = 2.61265
Step 23880: loss = 2.70200
Step 23885: loss = 2.54644
Step 23890: loss = 2.78066
Step 23895: loss = 2.85392
Step 23900: loss = 2.61870
Step 23905: loss = 2.86734
Step 23910: loss = 2.56285
Step 23915: loss = 2.51099
Step 23920: loss = 2.69725
Step 23925: loss = 2.87920
Step 23930: loss = 2.77820
Step 23935: loss = 2.56442
Step 23940: loss = 2.59930
Step 23945: loss = 2.70511
Step 23950: loss = 2.53783
Step 23955: loss = 2.80234
Step 23960: loss = 2.69366
Step 23965: loss = 2.87630
Step 23970: loss = 2.80760
Step 23975: loss = 2.73945
Step 23980: loss = 2.66526
Step 23985: loss = 2.90236
Step 23990: loss = 3.03381
Step 23995: loss = 2.71055
Step 24000: loss = 2.53005
Step 24005: loss = 2.70464
Step 24010: loss = 2.46571
Step 24015: loss = 2.71523
Step 24020: loss = 2.74520
Step 24025: loss = 2.50307
Step 24030: loss = 2.76886
Step 24035: loss = 2.84988
Step 24040: loss = 2.78677
Step 24045: loss = 2.72888
Step 24050: loss = 2.86628
Step 24055: loss = 2.74779
Step 24060: loss = 2.62653
Step 24065: loss = 2.67217
Step 24070: loss = 2.70209
Step 24075: loss = 2.67855
Step 24080: loss = 2.83036
Step 24085: loss = 2.90851
Step 24090: loss = 2.55840
Step 24095: loss = 2.71521
Step 24100: loss = 2.50150
Step 24105: loss = 2.73207
Step 24110: loss = 2.81803
Step 24115: loss = 2.89805
Step 24120: loss = 2.74104
Step 24125: loss = 2.66044
Step 24130: loss = 2.86005
Step 24135: loss = 2.67619
Step 24140: loss = 2.95133
Step 24145: loss = 2.67961
Step 24150: loss = 2.88756
Step 24155: loss = 2.64284
Step 24160: loss = 3.08603
Step 24165: loss = 2.76824
Step 24170: loss = 2.88079
Step 24175: loss = 3.03225
Step 24180: loss = 2.72793
Training Data Eval:
  Num examples: 49920, Num correct: 6966, Precision @ 1: 0.1395
('Testing Data Eval: EPOCH->', 63)
  Num examples: 9984, Num correct: 1396, Precision @ 1: 0.1398
Step 24185: loss = 2.66823
Step 24190: loss = 2.63810
Step 24195: loss = 2.92141
Step 24200: loss = 2.91715
Step 24205: loss = 2.66999
Step 24210: loss = 2.62721
Step 24215: loss = 2.80305
Step 24220: loss = 2.67197
Step 24225: loss = 2.74004
Step 24230: loss = 3.09096
Step 24235: loss = 2.62364
Step 24240: loss = 2.66233
Step 24245: loss = 2.72993
Step 24250: loss = 2.79373
Step 24255: loss = 2.65897
Step 24260: loss = 2.60691
Step 24265: loss = 2.71591
Step 24270: loss = 2.60781
Step 24275: loss = 2.89631
Step 24280: loss = 2.77230
Step 24285: loss = 2.89407
Step 24290: loss = 2.80448
Step 24295: loss = 2.55274
Step 24300: loss = 2.66514
Step 24305: loss = 2.94120
Step 24310: loss = 2.68792
Step 24315: loss = 2.56292
Step 24320: loss = 2.72274
Step 24325: loss = 2.85048
Step 24330: loss = 2.84411
Step 24335: loss = 2.76278
Step 24340: loss = 2.63104
Step 24345: loss = 2.87066
Step 24350: loss = 2.67430
Step 24355: loss = 2.57043
Step 24360: loss = 2.69530
Step 24365: loss = 2.81693
Step 24370: loss = 2.80502
Step 24375: loss = 2.98166
Step 24380: loss = 2.83312
Step 24385: loss = 2.81278
Step 24390: loss = 2.71106
Step 24395: loss = 2.63722
Step 24400: loss = 2.95793
Step 24405: loss = 2.65730
Step 24410: loss = 2.56258
Step 24415: loss = 2.78947
Step 24420: loss = 2.92712
Step 24425: loss = 2.67877
Step 24430: loss = 2.69606
Step 24435: loss = 2.84272
Step 24440: loss = 2.87188
Step 24445: loss = 2.70013
Step 24450: loss = 2.81933
Step 24455: loss = 2.74547
Step 24460: loss = 2.78815
Step 24465: loss = 2.73081
Step 24470: loss = 2.79141
Step 24475: loss = 2.76330
Step 24480: loss = 2.67103
Step 24485: loss = 2.65223
Step 24490: loss = 2.66791
Step 24495: loss = 2.63539
Step 24500: loss = 2.79385
Step 24505: loss = 2.74455
Step 24510: loss = 2.66919
Step 24515: loss = 2.61225
Step 24520: loss = 2.77640
Step 24525: loss = 2.90693
Step 24530: loss = 2.92754
Step 24535: loss = 2.67639
Step 24540: loss = 2.70271
Step 24545: loss = 2.82773
Step 24550: loss = 2.91871
Step 24555: loss = 2.50715
Step 24560: loss = 2.72211
Step 24565: loss = 2.84519
Step 24570: loss = 2.64892
Training Data Eval:
  Num examples: 49920, Num correct: 6626, Precision @ 1: 0.1327
('Testing Data Eval: EPOCH->', 64)
  Num examples: 9984, Num correct: 1290, Precision @ 1: 0.1292
Step 24575: loss = 2.88511
Step 24580: loss = 3.10250
Step 24585: loss = 2.75417
Step 24590: loss = 2.57941
Step 24595: loss = 2.77770
Step 24600: loss = 2.55240
Step 24605: loss = 2.69018
Step 24610: loss = 2.76882
Step 24615: loss = 2.85413
Step 24620: loss = 2.84017
Step 24625: loss = 2.86295
Step 24630: loss = 2.77512
Step 24635: loss = 2.92301
Step 24640: loss = 2.84800
Step 24645: loss = 2.65320
Step 24650: loss = 2.80180
Step 24655: loss = 2.49190
Step 24660: loss = 2.89907
Step 24665: loss = 3.07783
Step 24670: loss = 2.71966
Step 24675: loss = 2.76406
Step 24680: loss = 2.66355
Step 24685: loss = 2.66860
Step 24690: loss = 2.83385
Step 24695: loss = 2.69221
Step 24700: loss = 2.83151
Step 24705: loss = 2.88902
Step 24710: loss = 2.96260
Step 24715: loss = 2.69216
Step 24720: loss = 2.89086
Step 24725: loss = 2.58405
Step 24730: loss = 2.90450
Step 24735: loss = 2.67049
Step 24740: loss = 2.71696
Step 24745: loss = 2.86098
Step 24750: loss = 2.69882
Step 24755: loss = 2.64527
Step 24760: loss = 2.71276
Step 24765: loss = 2.98207
Step 24770: loss = 2.74676
Step 24775: loss = 2.49962
Step 24780: loss = 2.85172
Step 24785: loss = 2.51803
Step 24790: loss = 2.88187
Step 24795: loss = 2.87060
Step 24800: loss = 2.84418
Step 24805: loss = 2.72224
Step 24810: loss = 2.87534
Step 24815: loss = 2.62581
Step 24820: loss = 2.70582
Step 24825: loss = 2.66396
Step 24830: loss = 2.48097
Step 24835: loss = 2.72716
Step 24840: loss = 2.74547
Step 24845: loss = 2.85653
Step 24850: loss = 2.79272
Step 24855: loss = 2.75031
Step 24860: loss = 2.72418
Step 24865: loss = 2.57091
Step 24870: loss = 2.74565
Step 24875: loss = 2.83976
Step 24880: loss = 2.91007
Step 24885: loss = 2.98599
Step 24890: loss = 2.86421
Step 24895: loss = 2.78735
Step 24900: loss = 2.94629
Step 24905: loss = 2.75420
Step 24910: loss = 2.72865
Step 24915: loss = 2.61437
Step 24920: loss = 2.81553
Step 24925: loss = 2.70832
Step 24930: loss = 2.80710
Step 24935: loss = 2.89161
Step 24940: loss = 2.89458
Step 24945: loss = 2.66499
Step 24950: loss = 2.67300
Step 24955: loss = 2.67004
Step 24960: loss = 2.69172
Training Data Eval:
  Num examples: 49920, Num correct: 6924, Precision @ 1: 0.1387
('Testing Data Eval: EPOCH->', 65)
  Num examples: 9984, Num correct: 1369, Precision @ 1: 0.1371
Step 24965: loss = 2.75462
Step 24970: loss = 2.88645
Step 24975: loss = 2.64724
Step 24980: loss = 2.75703
Step 24985: loss = 2.87287
Step 24990: loss = 2.74662
Step 24995: loss = 2.58702
Step 25000: loss = 2.75664
Step 25005: loss = 2.58110
Step 25010: loss = 2.64339
Step 25015: loss = 2.74887
Step 25020: loss = 2.63589
Step 25025: loss = 2.71639
Step 25030: loss = 2.62925
Step 25035: loss = 2.77921
Step 25040: loss = 2.93795
Step 25045: loss = 2.75137
Step 25050: loss = 2.81336
Step 25055: loss = 2.94322
Step 25060: loss = 2.51705
Step 25065: loss = 2.63066
Step 25070: loss = 2.76579
Step 25075: loss = 2.72386
Step 25080: loss = 2.96965
Step 25085: loss = 2.77206
Step 25090: loss = 2.83230
Step 25095: loss = 3.00318
Step 25100: loss = 2.91074
Step 25105: loss = 2.74354
Step 25110: loss = 2.59732
Step 25115: loss = 3.01614
Step 25120: loss = 2.91789
Step 25125: loss = 2.93722
Step 25130: loss = 2.75146
Step 25135: loss = 2.83620
Step 25140: loss = 2.77161
Step 25145: loss = 2.83266
Step 25150: loss = 2.76136
Step 25155: loss = 2.85339
Step 25160: loss = 2.85489
Step 25165: loss = 2.72128
Step 25170: loss = 2.55172
Step 25175: loss = 3.00434
Step 25180: loss = 2.95204
Step 25185: loss = 2.64221
Step 25190: loss = 2.52885
Step 25195: loss = 2.61735
Step 25200: loss = 2.70839
Step 25205: loss = 2.87519
Step 25210: loss = 2.82585
Step 25215: loss = 2.95164
Step 25220: loss = 2.78478
Step 25225: loss = 2.59945
Step 25230: loss = 2.70205
Step 25235: loss = 2.89611
Step 25240: loss = 2.63022
Step 25245: loss = 2.79308
Step 25250: loss = 2.82890
Step 25255: loss = 2.75240
Step 25260: loss = 2.81135
Step 25265: loss = 2.70993
Step 25270: loss = 2.77420
Step 25275: loss = 2.82907
Step 25280: loss = 2.58226
Step 25285: loss = 2.71034
Step 25290: loss = 2.88606
Step 25295: loss = 2.81506
Step 25300: loss = 2.75670
Step 25305: loss = 2.63247
Step 25310: loss = 2.61962
Step 25315: loss = 2.83527
Step 25320: loss = 2.90328
Step 25325: loss = 2.86727
Step 25330: loss = 2.69340
Step 25335: loss = 2.77419
Step 25340: loss = 2.74883
Step 25345: loss = 2.75395
Step 25350: loss = 2.74468
Training Data Eval:
  Num examples: 49920, Num correct: 6801, Precision @ 1: 0.1362
('Testing Data Eval: EPOCH->', 66)
  Num examples: 9984, Num correct: 1407, Precision @ 1: 0.1409
Step 25355: loss = 2.78502
Step 25360: loss = 2.93417
Step 25365: loss = 2.82453
Step 25370: loss = 2.78008
Step 25375: loss = 2.76596
Step 25380: loss = 2.66975
Step 25385: loss = 2.83927
Step 25390: loss = 2.73090
Step 25395: loss = 2.55247
Step 25400: loss = 2.68991
Step 25405: loss = 2.66185
Step 25410: loss = 2.76651
Step 25415: loss = 2.87848
Step 25420: loss = 2.79656
Step 25425: loss = 2.60439
Step 25430: loss = 2.80836
Step 25435: loss = 2.79348
Step 25440: loss = 2.59946
Step 25445: loss = 2.50736
Step 25450: loss = 2.64554
Step 25455: loss = 2.68095
Step 25460: loss = 2.79078
Step 25465: loss = 2.69978
Step 25470: loss = 2.74122
Step 25475: loss = 2.68137
Step 25480: loss = 2.67094
Step 25485: loss = 2.59566
Step 25490: loss = 3.01427
Step 25495: loss = 2.70570
Step 25500: loss = 2.90883
Step 25505: loss = 3.05593
Step 25510: loss = 2.72997
Step 25515: loss = 2.83684
Step 25520: loss = 2.87577
Step 25525: loss = 2.85673
Step 25530: loss = 2.97070
Step 25535: loss = 2.86970
Step 25540: loss = 2.58166
Step 25545: loss = 2.76064
Step 25550: loss = 2.69491
Step 25555: loss = 2.63406
Step 25560: loss = 2.89919
Step 25565: loss = 2.93396
Step 25570: loss = 2.82336
Step 25575: loss = 3.12978
Step 25580: loss = 2.71361
Step 25585: loss = 2.73681
Step 25590: loss = 2.88004
Step 25595: loss = 2.85129
Step 25600: loss = 2.91534
Step 25605: loss = 3.00927
Step 25610: loss = 2.84018
Step 25615: loss = 2.86447
Step 25620: loss = 2.80091
Step 25625: loss = 2.66063
Step 25630: loss = 2.80854
Step 25635: loss = 2.67278
Step 25640: loss = 2.63887
Step 25645: loss = 2.90704
Step 25650: loss = 2.85488
Step 25655: loss = 2.54475
Step 25660: loss = 3.02952
Step 25665: loss = 2.77802
Step 25670: loss = 2.80668
Step 25675: loss = 2.65722
Step 25680: loss = 2.77253
Step 25685: loss = 2.76513
Step 25690: loss = 2.78987
Step 25695: loss = 2.94826
Step 25700: loss = 2.80737
Step 25705: loss = 2.95649
Step 25710: loss = 2.68910
Step 25715: loss = 2.77771
Step 25720: loss = 2.65089
Step 25725: loss = 2.60219
Step 25730: loss = 2.82928
Step 25735: loss = 2.88921
Step 25740: loss = 2.94254
Training Data Eval:
  Num examples: 49920, Num correct: 7154, Precision @ 1: 0.1433
('Testing Data Eval: EPOCH->', 67)
  Num examples: 9984, Num correct: 1444, Precision @ 1: 0.1446
Step 25745: loss = 2.67389
Step 25750: loss = 2.66643
Step 25755: loss = 2.76399
Step 25760: loss = 2.84235
Step 25765: loss = 2.73228
Step 25770: loss = 2.66820
Step 25775: loss = 2.72121
Step 25780: loss = 2.72455
Step 25785: loss = 2.90264
Step 25790: loss = 2.82048
Step 25795: loss = 2.75901
Step 25800: loss = 2.66447
Step 25805: loss = 2.70251
Step 25810: loss = 2.79667
Step 25815: loss = 2.61226
Step 25820: loss = 2.62191
Step 25825: loss = 2.59410
Step 25830: loss = 2.77256
Step 25835: loss = 2.70712
Step 25840: loss = 2.87377
Step 25845: loss = 3.08580
Step 25850: loss = 2.68245
Step 25855: loss = 2.72210
Step 25860: loss = 2.68292
Step 25865: loss = 2.71403
Step 25870: loss = 3.08491
Step 25875: loss = 2.70563
Step 25880: loss = 2.72335
Step 25885: loss = 2.79178
Step 25890: loss = 2.71361
Step 25895: loss = 2.71409
Step 25900: loss = 2.74177
Step 25905: loss = 2.61600
Step 25910: loss = 2.90735
Step 25915: loss = 2.84889
Step 25920: loss = 2.79925
Step 25925: loss = 3.17536
Step 25930: loss = 2.57485
Step 25935: loss = 2.71761
Step 25940: loss = 2.87856
Step 25945: loss = 2.69184
Step 25950: loss = 2.48763
Step 25955: loss = 2.69263
Step 25960: loss = 2.70336
Step 25965: loss = 2.80178
Step 25970: loss = 2.72052
Step 25975: loss = 2.76121
Step 25980: loss = 2.82415
Step 25985: loss = 2.70330
Step 25990: loss = 2.70581
Step 25995: loss = 2.90611
Step 26000: loss = 3.03031
Step 26005: loss = 3.02210
Step 26010: loss = 2.73635
Step 26015: loss = 2.59763
Step 26020: loss = 2.80930
Step 26025: loss = 2.57673
Step 26030: loss = 2.67574
Step 26035: loss = 2.60396
Step 26040: loss = 2.81902
Step 26045: loss = 2.58544
Step 26050: loss = 2.73618
Step 26055: loss = 2.59474
Step 26060: loss = 2.79269
Step 26065: loss = 2.76009
Step 26070: loss = 2.87345
Step 26075: loss = 2.57472
Step 26080: loss = 2.54346
Step 26085: loss = 2.77021
Step 26090: loss = 2.79355
Step 26095: loss = 2.92048
Step 26100: loss = 2.80896
Step 26105: loss = 2.65038
Step 26110: loss = 2.48226
Step 26115: loss = 2.61605
Step 26120: loss = 2.69576
Step 26125: loss = 2.93774
Step 26130: loss = 2.65717
Training Data Eval:
  Num examples: 49920, Num correct: 7097, Precision @ 1: 0.1422
('Testing Data Eval: EPOCH->', 68)
  Num examples: 9984, Num correct: 1452, Precision @ 1: 0.1454
Step 26135: loss = 3.02542
Step 26140: loss = 2.74814
Step 26145: loss = 2.71660
Step 26150: loss = 2.69293
Step 26155: loss = 2.65728
Step 26160: loss = 2.75543
Step 26165: loss = 2.75670
Step 26170: loss = 2.87263
Step 26175: loss = 2.96845
Step 26180: loss = 2.67068
Step 26185: loss = 2.72906
Step 26190: loss = 2.94339
Step 26195: loss = 2.83596
Step 26200: loss = 2.87034
Step 26205: loss = 2.60940
Step 26210: loss = 2.74256
Step 26215: loss = 2.92573
Step 26220: loss = 2.68353
Step 26225: loss = 2.68285
Step 26230: loss = 2.70937
Step 26235: loss = 2.64380
Step 26240: loss = 2.68146
Step 26245: loss = 2.86585
Step 26250: loss = 2.70488
Step 26255: loss = 3.08189
Step 26260: loss = 2.98608
Step 26265: loss = 2.79745
Step 26270: loss = 2.63308
Step 26275: loss = 2.52368
Step 26280: loss = 3.01526
Step 26285: loss = 3.13112
Step 26290: loss = 2.79530
Step 26295: loss = 2.76073
Step 26300: loss = 2.71700
Step 26305: loss = 2.68257
Step 26310: loss = 2.72960
Step 26315: loss = 2.85648
Step 26320: loss = 2.93991
Step 26325: loss = 2.70014
Step 26330: loss = 2.69721
Step 26335: loss = 2.85070
Step 26340: loss = 2.75033
Step 26345: loss = 2.68368
Step 26350: loss = 2.79787
Step 26355: loss = 2.59733
Step 26360: loss = 2.79341
Step 26365: loss = 2.62802
Step 26370: loss = 2.65157
Step 26375: loss = 2.86254
Step 26380: loss = 2.73154
Step 26385: loss = 2.75713
Step 26390: loss = 2.84428
Step 26395: loss = 3.00786
Step 26400: loss = 2.72355
Step 26405: loss = 2.69241
Step 26410: loss = 2.83585
Step 26415: loss = 2.70822
Step 26420: loss = 2.68088
Step 26425: loss = 2.70490
Step 26430: loss = 2.58740
Step 26435: loss = 2.64998
Step 26440: loss = 2.75327
Step 26445: loss = 2.75944
Step 26450: loss = 2.60072
Step 26455: loss = 3.00647
Step 26460: loss = 2.73246
Step 26465: loss = 2.59473
Step 26470: loss = 2.99926
Step 26475: loss = 2.79077
Step 26480: loss = 2.70425
Step 26485: loss = 2.85921
Step 26490: loss = 2.74801
Step 26495: loss = 2.66989
Step 26500: loss = 3.05958
Step 26505: loss = 2.73531
Step 26510: loss = 2.73269
Step 26515: loss = 2.82389
Step 26520: loss = 2.66484
Training Data Eval:
  Num examples: 49920, Num correct: 6829, Precision @ 1: 0.1368
('Testing Data Eval: EPOCH->', 69)
  Num examples: 9984, Num correct: 1429, Precision @ 1: 0.1431
Step 26525: loss = 2.98089
Step 26530: loss = 2.89601
Step 26535: loss = 2.52499
Step 26540: loss = 2.78102
Step 26545: loss = 2.73236
Step 26550: loss = 2.70057
Step 26555: loss = 2.71328
Step 26560: loss = 2.77755
Step 26565: loss = 2.93528
Step 26570: loss = 2.62853
Step 26575: loss = 2.72535
Step 26580: loss = 2.75630
Step 26585: loss = 2.86937
Step 26590: loss = 2.91949
Step 26595: loss = 2.87188
Step 26600: loss = 2.97062
Step 26605: loss = 2.79315
Step 26610: loss = 2.56364
Step 26615: loss = 2.76416
Step 26620: loss = 2.98813
Step 26625: loss = 2.74992
Step 26630: loss = 2.64316
Step 26635: loss = 2.71140
Step 26640: loss = 2.92121
Step 26645: loss = 2.68004
Step 26650: loss = 2.55362
Step 26655: loss = 2.72769
Step 26660: loss = 2.89065
Step 26665: loss = 2.93560
Step 26670: loss = 2.58547
Step 26675: loss = 2.95103
Step 26680: loss = 2.98059
Step 26685: loss = 2.77697
Step 26690: loss = 2.85862
Step 26695: loss = 2.98025
Step 26700: loss = 2.84347
Step 26705: loss = 2.68372
Step 26710: loss = 2.57666
Step 26715: loss = 2.82669
Step 26720: loss = 2.80620
Step 26725: loss = 2.65915
Step 26730: loss = 2.77594
Step 26735: loss = 2.73482
Step 26740: loss = 2.54764
Step 26745: loss = 2.74307
Step 26750: loss = 3.12152
Step 26755: loss = 2.74574
Step 26760: loss = 2.83072
Step 26765: loss = 2.85116
Step 26770: loss = 2.63710
Step 26775: loss = 2.73567
Step 26780: loss = 3.04718
Step 26785: loss = 2.70085
Step 26790: loss = 2.82355
Step 26795: loss = 2.72963
Step 26800: loss = 2.69580
Step 26805: loss = 2.99254
Step 26810: loss = 2.63778
Step 26815: loss = 2.71840
Step 26820: loss = 2.86206
Step 26825: loss = 2.64437
Step 26830: loss = 2.90156
Step 26835: loss = 2.74365
Step 26840: loss = 2.88140
Step 26845: loss = 2.90405
Step 26850: loss = 2.88975
Step 26855: loss = 2.84551
Step 26860: loss = 2.86278
Step 26865: loss = 2.68407
Step 26870: loss = 2.94951
Step 26875: loss = 2.99726
Step 26880: loss = 2.78037
Step 26885: loss = 2.59500
Step 26890: loss = 2.64996
Step 26895: loss = 2.76686
Step 26900: loss = 2.72098
Step 26905: loss = 2.46168
Step 26910: loss = 2.72861
Training Data Eval:
  Num examples: 49920, Num correct: 6980, Precision @ 1: 0.1398
('Testing Data Eval: EPOCH->', 70)
  Num examples: 9984, Num correct: 1417, Precision @ 1: 0.1419
Step 26915: loss = 2.69523
Step 26920: loss = 2.58090
Step 26925: loss = 2.59457
Step 26930: loss = 2.86555
Step 26935: loss = 2.44764
Step 26940: loss = 2.99888
Step 26945: loss = 2.79544
Step 26950: loss = 2.88030
Step 26955: loss = 2.87913
Step 26960: loss = 2.86666
Step 26965: loss = 2.75528
Step 26970: loss = 2.64522
Step 26975: loss = 2.63973
Step 26980: loss = 2.82356
Step 26985: loss = 2.64197
Step 26990: loss = 2.60093
Step 26995: loss = 2.79722
Step 27000: loss = 2.82461
Step 27005: loss = 2.68955
Step 27010: loss = 2.85336
Step 27015: loss = 2.63680
Step 27020: loss = 2.81370
Step 27025: loss = 2.52906
Step 27030: loss = 2.63562
Step 27035: loss = 2.63298
Step 27040: loss = 2.49355
Step 27045: loss = 2.59367
Step 27050: loss = 2.73686
Step 27055: loss = 2.75186
Step 27060: loss = 2.61913
Step 27065: loss = 2.64433
Step 27070: loss = 2.52581
Step 27075: loss = 2.67649
Step 27080: loss = 2.77590
Step 27085: loss = 2.45673
Step 27090: loss = 2.59199
Step 27095: loss = 2.82827
Step 27100: loss = 2.73920
Step 27105: loss = 2.90752
Step 27110: loss = 3.12725
Step 27115: loss = 3.01806
Step 27120: loss = 2.65728
Step 27125: loss = 2.80852
Step 27130: loss = 2.80967
Step 27135: loss = 2.64658
Step 27140: loss = 2.64600
Step 27145: loss = 2.75489
Step 27150: loss = 2.83105
Step 27155: loss = 2.71626
Step 27160: loss = 2.68350
Step 27165: loss = 2.60962
Step 27170: loss = 2.60952
Step 27175: loss = 2.55526
Step 27180: loss = 2.66005
Step 27185: loss = 2.86149
Step 27190: loss = 2.81814
Step 27195: loss = 2.97053
Step 27200: loss = 2.63893
Step 27205: loss = 2.90819
Step 27210: loss = 2.83028
Step 27215: loss = 3.02944
Step 27220: loss = 2.71982
Step 27225: loss = 2.79179
Step 27230: loss = 2.78024
Step 27235: loss = 3.14339
Step 27240: loss = 2.80101
Step 27245: loss = 2.98977
Step 27250: loss = 2.93856
Step 27255: loss = 2.68673
Step 27260: loss = 2.80185
Step 27265: loss = 2.76236
Step 27270: loss = 2.76433
Step 27275: loss = 2.74089
Step 27280: loss = 2.81847
Step 27285: loss = 2.73420
Step 27290: loss = 2.71804
Step 27295: loss = 2.60644
Step 27300: loss = 2.87091
Training Data Eval:
  Num examples: 49920, Num correct: 6905, Precision @ 1: 0.1383
('Testing Data Eval: EPOCH->', 71)
  Num examples: 9984, Num correct: 1414, Precision @ 1: 0.1416
Step 27305: loss = 2.72464
Step 27310: loss = 2.75460
Step 27315: loss = 2.99850
Step 27320: loss = 2.79208
Step 27325: loss = 2.69525
Step 27330: loss = 2.75869
Step 27335: loss = 2.79606
Step 27340: loss = 2.88455
Step 27345: loss = 2.53152
Step 27350: loss = 2.60195
Step 27355: loss = 2.80808
Step 27360: loss = 2.71252
Step 27365: loss = 2.62989
Step 27370: loss = 2.76563
Step 27375: loss = 2.77295
Step 27380: loss = 2.65083
Step 27385: loss = 2.88673
Step 27390: loss = 2.89373
Step 27395: loss = 2.46587
Step 27400: loss = 2.73357
Step 27405: loss = 2.80984
Step 27410: loss = 2.95879
Step 27415: loss = 3.01659
Step 27420: loss = 2.72659
Step 27425: loss = 2.54724
Step 27430: loss = 2.74984
Step 27435: loss = 2.85561
Step 27440: loss = 2.72262
Step 27445: loss = 2.73296
Step 27450: loss = 2.71964
Step 27455: loss = 2.63307
Step 27460: loss = 2.98799
Step 27465: loss = 2.90836
Step 27470: loss = 2.98409
Step 27475: loss = 2.78789
Step 27480: loss = 2.84318
Step 27485: loss = 2.67815
Step 27490: loss = 2.71079
Step 27495: loss = 2.94491
Step 27500: loss = 2.80078
Step 27505: loss = 2.77252
Step 27510: loss = 2.66407
Step 27515: loss = 2.66609
Step 27520: loss = 2.74690
Step 27525: loss = 2.90479
Step 27530: loss = 2.72712
Step 27535: loss = 2.80380
Step 27540: loss = 2.63752
Step 27545: loss = 2.78549
Step 27550: loss = 2.59051
Step 27555: loss = 2.71117
Step 27560: loss = 2.68988
Step 27565: loss = 2.72931
Step 27570: loss = 2.81650
Step 27575: loss = 2.78335
Step 27580: loss = 2.73174
Step 27585: loss = 2.71828
Step 27590: loss = 2.85537
Step 27595: loss = 2.71410
Step 27600: loss = 2.86571
Step 27605: loss = 2.62949
Step 27610: loss = 2.60282
Step 27615: loss = 2.72884
Step 27620: loss = 2.84209
Step 27625: loss = 2.77391
Step 27630: loss = 2.89334
Step 27635: loss = 2.55433
Step 27640: loss = 2.74278
Step 27645: loss = 2.74751
Step 27650: loss = 2.93501
Step 27655: loss = 2.82684
Step 27660: loss = 2.72993
Step 27665: loss = 2.67048
Step 27670: loss = 2.67448
Step 27675: loss = 2.58535
Step 27680: loss = 2.99500
Step 27685: loss = 2.82605
Step 27690: loss = 2.93556
Training Data Eval:
  Num examples: 49920, Num correct: 6710, Precision @ 1: 0.1344
('Testing Data Eval: EPOCH->', 72)
  Num examples: 9984, Num correct: 1352, Precision @ 1: 0.1354
Step 27695: loss = 2.62630
Step 27700: loss = 2.66430
Step 27705: loss = 2.65530
Step 27710: loss = 2.55945
Step 27715: loss = 2.70357
Step 27720: loss = 2.74255
Step 27725: loss = 2.89845
Step 27730: loss = 3.00228
Step 27735: loss = 2.65167
Step 27740: loss = 2.90954
Step 27745: loss = 2.60827
Step 27750: loss = 2.88651
Step 27755: loss = 2.59334
Step 27760: loss = 2.76950
Step 27765: loss = 2.77332
Step 27770: loss = 2.65761
Step 27775: loss = 2.72822
Step 27780: loss = 2.94124
Step 27785: loss = 2.67341
Step 27790: loss = 2.65749
Step 27795: loss = 2.75282
Step 27800: loss = 2.66282
Step 27805: loss = 2.78547
Step 27810: loss = 2.73099
Step 27815: loss = 2.63672
Step 27820: loss = 2.77851
Step 27825: loss = 2.72624
Step 27830: loss = 2.95170
Step 27835: loss = 2.92729
Step 27840: loss = 2.66852
Step 27845: loss = 2.99356
Step 27850: loss = 2.93785
Step 27855: loss = 2.86735
Step 27860: loss = 2.58527
Step 27865: loss = 2.74849
Step 27870: loss = 2.72204
Step 27875: loss = 3.00967
Step 27880: loss = 2.84476
Step 27885: loss = 2.80049
Step 27890: loss = 2.81853
Step 27895: loss = 2.55116
Step 27900: loss = 2.78932
Step 27905: loss = 2.62267
Step 27910: loss = 2.67167
Step 27915: loss = 2.70547
Step 27920: loss = 2.71341
Step 27925: loss = 2.60808
Step 27930: loss = 2.73292
Step 27935: loss = 2.75870
Step 27940: loss = 2.65194
Step 27945: loss = 2.51564
Step 27950: loss = 2.75891
Step 27955: loss = 3.00235
Step 27960: loss = 2.67221
Step 27965: loss = 2.71856
Step 27970: loss = 2.83752
Step 27975: loss = 2.62123
Step 27980: loss = 2.86319
Step 27985: loss = 2.84208
Step 27990: loss = 2.61223
Step 27995: loss = 2.75153
Step 28000: loss = 2.84026
Step 28005: loss = 2.69547
Step 28010: loss = 3.00439
Step 28015: loss = 2.82424
Step 28020: loss = 2.74418
Step 28025: loss = 2.68801
Step 28030: loss = 2.80284
Step 28035: loss = 2.92363
Step 28040: loss = 2.83246
Step 28045: loss = 2.82011
Step 28050: loss = 2.66630
Step 28055: loss = 2.68529
Step 28060: loss = 2.80112
Step 28065: loss = 2.96710
Step 28070: loss = 2.71041
Step 28075: loss = 2.85545
Step 28080: loss = 3.01475
Training Data Eval:
  Num examples: 49920, Num correct: 6482, Precision @ 1: 0.1298
('Testing Data Eval: EPOCH->', 73)
  Num examples: 9984, Num correct: 1306, Precision @ 1: 0.1308
Step 28085: loss = 2.81187
Step 28090: loss = 2.70911
Step 28095: loss = 2.80693
Step 28100: loss = 2.87908
Step 28105: loss = 2.95534
Step 28110: loss = 2.52678
Step 28115: loss = 2.72801
Step 28120: loss = 2.86112
Step 28125: loss = 2.69057
Step 28130: loss = 2.75331
Step 28135: loss = 2.74795
Step 28140: loss = 2.65689
Step 28145: loss = 2.59588
Step 28150: loss = 2.65415
Step 28155: loss = 2.78487
Step 28160: loss = 2.88240
Step 28165: loss = 2.64346
Step 28170: loss = 2.50957
Step 28175: loss = 2.87090
Step 28180: loss = 2.84400
Step 28185: loss = 2.60177
Step 28190: loss = 2.76366
Step 28195: loss = 2.71409
Step 28200: loss = 2.61924
Step 28205: loss = 2.53379
Step 28210: loss = 2.84787
Step 28215: loss = 2.66595
Step 28220: loss = 2.79183
Step 28225: loss = 2.79041
Step 28230: loss = 2.65634
Step 28235: loss = 2.59755
Step 28240: loss = 2.68618
Step 28245: loss = 2.72472
Step 28250: loss = 2.66178
Step 28255: loss = 2.67446
Step 28260: loss = 2.67769
Step 28265: loss = 2.67950
Step 28270: loss = 2.80355
Step 28275: loss = 2.76902
Step 28280: loss = 2.80364
Step 28285: loss = 2.52210
Step 28290: loss = 3.00820
Step 28295: loss = 2.84951
Step 28300: loss = 2.44653
Step 28305: loss = 2.87006
Step 28310: loss = 2.86234
Step 28315: loss = 2.53229
Step 28320: loss = 2.65468
Step 28325: loss = 2.73790
Step 28330: loss = 2.84892
Step 28335: loss = 2.66878
Step 28340: loss = 2.87094
Step 28345: loss = 2.60293
Step 28350: loss = 2.79577
Step 28355: loss = 2.74858
Step 28360: loss = 2.75762
Step 28365: loss = 2.86186
Step 28370: loss = 2.78227
Step 28375: loss = 2.78819
Step 28380: loss = 2.74237
Step 28385: loss = 2.68151
Step 28390: loss = 2.86921
Step 28395: loss = 2.74699
Step 28400: loss = 2.83779
Step 28405: loss = 2.85735
Step 28410: loss = 2.55219
Step 28415: loss = 2.72400
Step 28420: loss = 2.98838
Step 28425: loss = 3.00554
Step 28430: loss = 2.68590
Step 28435: loss = 2.86952
Step 28440: loss = 2.70188
Step 28445: loss = 3.02609
Step 28450: loss = 2.73939
Step 28455: loss = 2.81135
Step 28460: loss = 2.96806
Step 28465: loss = 2.84339
Step 28470: loss = 2.68625
Training Data Eval:
  Num examples: 49920, Num correct: 6856, Precision @ 1: 0.1373
('Testing Data Eval: EPOCH->', 74)
  Num examples: 9984, Num correct: 1355, Precision @ 1: 0.1357
Step 28475: loss = 2.83027
Step 28480: loss = 2.77937
Step 28485: loss = 2.86534
Step 28490: loss = 2.80117
Step 28495: loss = 2.68108
Step 28500: loss = 2.55473
Step 28505: loss = 2.90943
Step 28510: loss = 2.75199
Step 28515: loss = 3.12423
Step 28520: loss = 2.72554
Step 28525: loss = 2.63620
Step 28530: loss = 2.63699
Step 28535: loss = 2.97883
Step 28540: loss = 2.73718
Step 28545: loss = 2.84472
Step 28550: loss = 2.72050
Step 28555: loss = 2.71389
Step 28560: loss = 2.66877
Step 28565: loss = 2.68589
Step 28570: loss = 2.65264
Step 28575: loss = 2.91299
Step 28580: loss = 2.76392
Step 28585: loss = 2.74094
Step 28590: loss = 2.74900
Step 28595: loss = 2.68385
Step 28600: loss = 2.85110
Step 28605: loss = 2.80464
Step 28610: loss = 2.71426
Step 28615: loss = 2.80985
Step 28620: loss = 3.01571
Step 28625: loss = 2.62478
Step 28630: loss = 2.80861
Step 28635: loss = 2.60416
Step 28640: loss = 2.65782
Step 28645: loss = 2.91611
Step 28650: loss = 2.67693
Step 28655: loss = 2.69086
Step 28660: loss = 2.58983
Step 28665: loss = 2.74318
Step 28670: loss = 2.73646
Step 28675: loss = 2.64484
Step 28680: loss = 2.67556
Step 28685: loss = 2.86274
Step 28690: loss = 2.85357
Step 28695: loss = 2.67735
Step 28700: loss = 2.62401
Step 28705: loss = 2.80055
Step 28710: loss = 2.73741
Step 28715: loss = 2.85232
Step 28720: loss = 2.82120
Step 28725: loss = 2.90457
Step 28730: loss = 2.84912
Step 28735: loss = 2.97875
Step 28740: loss = 2.98411
Step 28745: loss = 2.76950
Step 28750: loss = 2.65944
Step 28755: loss = 2.81290
Step 28760: loss = 2.80209
Step 28765: loss = 3.00726
Step 28770: loss = 2.83920
Step 28775: loss = 2.69397
Step 28780: loss = 2.91885
Step 28785: loss = 2.65727
Step 28790: loss = 2.59528
Step 28795: loss = 2.57750
Step 28800: loss = 2.88080
Step 28805: loss = 3.02605
Step 28810: loss = 2.76159
Step 28815: loss = 2.96730
Step 28820: loss = 2.79721
Step 28825: loss = 2.93796
Step 28830: loss = 2.83207
Step 28835: loss = 2.86824
Step 28840: loss = 2.95927
Step 28845: loss = 2.94225
Step 28850: loss = 2.61029
Step 28855: loss = 2.69464
Step 28860: loss = 2.84683
Training Data Eval:
  Num examples: 49920, Num correct: 6460, Precision @ 1: 0.1294
('Testing Data Eval: EPOCH->', 75)
  Num examples: 9984, Num correct: 1351, Precision @ 1: 0.1353
Step 28865: loss = 2.79072
Step 28870: loss = 2.82706
Step 28875: loss = 2.82248
Step 28880: loss = 2.84997
Step 28885: loss = 2.63362
Step 28890: loss = 2.68292
Step 28895: loss = 2.86377
Step 28900: loss = 2.60784
Step 28905: loss = 2.70917
Step 28910: loss = 2.82556
Step 28915: loss = 2.82346
Step 28920: loss = 2.72655
Step 28925: loss = 2.76360
Step 28930: loss = 2.89723
Step 28935: loss = 2.63478
Step 28940: loss = 2.82943
Step 28945: loss = 2.59067
Step 28950: loss = 2.63991
Step 28955: loss = 2.68258
Step 28960: loss = 2.68521
Step 28965: loss = 2.71940
Step 28970: loss = 2.94770
Step 28975: loss = 2.91464
Step 28980: loss = 2.65619
Step 28985: loss = 2.83687
Step 28990: loss = 2.86034
Step 28995: loss = 2.76289
Step 29000: loss = 2.89300
Step 29005: loss = 2.73090
Step 29010: loss = 2.83551
Step 29015: loss = 2.65221
Step 29020: loss = 2.70498
Step 29025: loss = 2.81973
Step 29030: loss = 2.76732
Step 29035: loss = 2.69295
Step 29040: loss = 2.59033
Step 29045: loss = 2.64883
Step 29050: loss = 2.78206
Step 29055: loss = 2.97696
Step 29060: loss = 2.70107
Step 29065: loss = 2.66330
Step 29070: loss = 2.96271
Step 29075: loss = 2.89790
Step 29080: loss = 2.70725
Step 29085: loss = 2.72764
Step 29090: loss = 2.92840
Step 29095: loss = 2.84739
Step 29100: loss = 2.72465
Step 29105: loss = 2.43931
Step 29110: loss = 3.05074
Step 29115: loss = 2.79787
Step 29120: loss = 2.58732
Step 29125: loss = 2.76135
Step 29130: loss = 2.62084
Step 29135: loss = 2.74664
Step 29140: loss = 2.83401
Step 29145: loss = 2.79348
Step 29150: loss = 2.79551
Step 29155: loss = 2.85350
Step 29160: loss = 2.69783
Step 29165: loss = 2.62323
Step 29170: loss = 2.91458
Step 29175: loss = 2.72428
Step 29180: loss = 3.01821
Step 29185: loss = 2.63816
Step 29190: loss = 2.63817
Step 29195: loss = 2.78943
Step 29200: loss = 2.66192
Step 29205: loss = 2.73302
Step 29210: loss = 2.71247
Step 29215: loss = 2.94042
Step 29220: loss = 2.84709
Step 29225: loss = 2.80698
Step 29230: loss = 2.81545
Step 29235: loss = 2.53285
Step 29240: loss = 2.65512
Step 29245: loss = 2.80201
Step 29250: loss = 2.87208
Training Data Eval:
  Num examples: 49920, Num correct: 6879, Precision @ 1: 0.1378
('Testing Data Eval: EPOCH->', 76)
  Num examples: 9984, Num correct: 1341, Precision @ 1: 0.1343
Step 29255: loss = 2.72062
Step 29260: loss = 2.75032
Step 29265: loss = 2.74151
Step 29270: loss = 2.84727
Step 29275: loss = 2.80239
Step 29280: loss = 2.90315
Step 29285: loss = 2.89342
Step 29290: loss = 2.83770
Step 29295: loss = 2.71233
Step 29300: loss = 2.75115
Step 29305: loss = 2.80650
Step 29310: loss = 2.59224
Step 29315: loss = 2.69097
Step 29320: loss = 2.65507
Step 29325: loss = 2.82699
Step 29330: loss = 2.74579
Step 29335: loss = 3.04464
Step 29340: loss = 2.69113
Step 29345: loss = 2.77105
Step 29350: loss = 2.74764
Step 29355: loss = 2.80372
Step 29360: loss = 2.69703
Step 29365: loss = 2.81186
Step 29370: loss = 2.91765
Step 29375: loss = 2.79467
Step 29380: loss = 2.68046
Step 29385: loss = 2.80626
Step 29390: loss = 2.55445
Step 29395: loss = 2.87441
Step 29400: loss = 2.78672
Step 29405: loss = 2.86568
Step 29410: loss = 3.10610
Step 29415: loss = 2.85911
Step 29420: loss = 2.67685
Step 29425: loss = 2.78029
Step 29430: loss = 3.05250
Step 29435: loss = 2.69660
Step 29440: loss = 2.85354
Step 29445: loss = 2.97814
Step 29450: loss = 2.81024
Step 29455: loss = 2.78090
Step 29460: loss = 2.55794
Step 29465: loss = 2.67752
Step 29470: loss = 2.82417
Step 29475: loss = 2.79929
Step 29480: loss = 2.67536
Step 29485: loss = 2.75797
Step 29490: loss = 2.85178
Step 29495: loss = 2.97899
Step 29500: loss = 2.72205
Step 29505: loss = 2.74360
Step 29510: loss = 2.81594
Step 29515: loss = 2.71363
Step 29520: loss = 2.77126
Step 29525: loss = 2.66466
Step 29530: loss = 2.60639
Step 29535: loss = 2.80347
Step 29540: loss = 2.74353
Step 29545: loss = 2.72572
Step 29550: loss = 2.86141
Step 29555: loss = 2.62485
Step 29560: loss = 2.69608
Step 29565: loss = 3.00025
Step 29570: loss = 2.79906
Step 29575: loss = 2.95058
Step 29580: loss = 2.70269
Step 29585: loss = 2.65615
Step 29590: loss = 2.56840
Step 29595: loss = 2.58766
Step 29600: loss = 2.67962
Step 29605: loss = 2.76623
Step 29610: loss = 2.61164
Step 29615: loss = 2.93057
Step 29620: loss = 2.65201
Step 29625: loss = 2.78347
Step 29630: loss = 2.65836
Step 29635: loss = 2.63424
Step 29640: loss = 2.81096
Training Data Eval:
  Num examples: 49920, Num correct: 6693, Precision @ 1: 0.1341
('Testing Data Eval: EPOCH->', 77)
  Num examples: 9984, Num correct: 1319, Precision @ 1: 0.1321
Step 29645: loss = 2.71460
Step 29650: loss = 2.81490
Step 29655: loss = 2.76083
Step 29660: loss = 2.58045
Step 29665: loss = 2.62978
Step 29670: loss = 2.77148
Step 29675: loss = 2.80283
Step 29680: loss = 2.84307
Step 29685: loss = 2.70451
Step 29690: loss = 2.74927
Step 29695: loss = 2.70613
Step 29700: loss = 2.75407
Step 29705: loss = 3.01958
Step 29710: loss = 2.76506
Step 29715: loss = 2.75725
Step 29720: loss = 3.09205
Step 29725: loss = 2.58299
Step 29730: loss = 2.86744
Step 29735: loss = 2.86838
Step 29740: loss = 2.50717
Step 29745: loss = 2.78177
Step 29750: loss = 2.80308
Step 29755: loss = 2.73881
Step 29760: loss = 2.77736
Step 29765: loss = 2.69227
Step 29770: loss = 2.88645
Step 29775: loss = 2.95155
Step 29780: loss = 2.57790
Step 29785: loss = 2.69274
Step 29790: loss = 2.65383
Step 29795: loss = 2.71082
Step 29800: loss = 2.68861
Step 29805: loss = 2.82008
Step 29810: loss = 2.66850
Step 29815: loss = 2.67576
Step 29820: loss = 2.70514
Step 29825: loss = 2.53393
Step 29830: loss = 2.85242
Step 29835: loss = 2.61962
Step 29840: loss = 2.84503
Step 29845: loss = 2.66329
Step 29850: loss = 2.85093
Step 29855: loss = 2.67611
Step 29860: loss = 2.66367
Step 29865: loss = 2.78247
Step 29870: loss = 2.53504
Step 29875: loss = 2.69703
Step 29880: loss = 2.91169
Step 29885: loss = 2.92333
Step 29890: loss = 2.72933
Step 29895: loss = 2.69988
Step 29900: loss = 2.52471
Step 29905: loss = 2.73025
Step 29910: loss = 2.66617
Step 29915: loss = 2.73258
Step 29920: loss = 2.73736
Step 29925: loss = 2.85246
Step 29930: loss = 2.76626
Step 29935: loss = 3.05989
Step 29940: loss = 2.80654
Step 29945: loss = 2.79068
Step 29950: loss = 2.77309
Step 29955: loss = 2.92459
Step 29960: loss = 2.95293
Step 29965: loss = 2.67374
Step 29970: loss = 2.53085
Step 29975: loss = 2.60306
Step 29980: loss = 2.55964
Step 29985: loss = 2.79275
Step 29990: loss = 2.64805
Step 29995: loss = 2.83736
Step 30000: loss = 2.74044
Step 30005: loss = 2.84806
Step 30010: loss = 2.78723
Step 30015: loss = 2.69508
Step 30020: loss = 2.99133
Step 30025: loss = 2.80830
Step 30030: loss = 2.78155
Training Data Eval:
  Num examples: 49920, Num correct: 6794, Precision @ 1: 0.1361
('Testing Data Eval: EPOCH->', 78)
  Num examples: 9984, Num correct: 1383, Precision @ 1: 0.1385
Step 30035: loss = 2.65698
Step 30040: loss = 2.75465
Step 30045: loss = 2.75842
Step 30050: loss = 2.90246
Step 30055: loss = 2.80063
Step 30060: loss = 2.80738
Step 30065: loss = 2.69261
Step 30070: loss = 2.63697
Step 30075: loss = 2.88395
Step 30080: loss = 2.97982
Step 30085: loss = 2.68554
Step 30090: loss = 2.75968
Step 30095: loss = 2.73493
Step 30100: loss = 2.96323
Step 30105: loss = 2.46983
Step 30110: loss = 2.99678
Step 30115: loss = 2.96950
Step 30120: loss = 2.82522
Step 30125: loss = 2.90689
Step 30130: loss = 2.60109
Step 30135: loss = 2.81415
Step 30140: loss = 2.69305
Step 30145: loss = 2.83519
Step 30150: loss = 2.70348
Step 30155: loss = 2.73477
Step 30160: loss = 2.71300
Step 30165: loss = 2.72198
Step 30170: loss = 2.59581
Step 30175: loss = 2.88511
Step 30180: loss = 2.93133
Step 30185: loss = 2.79272
Step 30190: loss = 3.25722
Step 30195: loss = 2.88396
Step 30200: loss = 2.65682
Step 30205: loss = 2.67485
Step 30210: loss = 2.65187
Step 30215: loss = 2.63454
Step 30220: loss = 2.73189
Step 30225: loss = 2.70237
Step 30230: loss = 2.92217
Step 30235: loss = 2.80332
Step 30240: loss = 2.65804
Step 30245: loss = 2.95158
Step 30250: loss = 2.99981
Step 30255: loss = 2.80685
Step 30260: loss = 2.57447
Step 30265: loss = 2.98664
Step 30270: loss = 2.71045
Step 30275: loss = 2.76890
Step 30280: loss = 2.84627
Step 30285: loss = 2.71033
Step 30290: loss = 2.76140
Step 30295: loss = 2.57008
Step 30300: loss = 3.00047
Step 30305: loss = 2.68373
Step 30310: loss = 2.87599
Step 30315: loss = 2.86094
Step 30320: loss = 2.60229
Step 30325: loss = 3.06135
Step 30330: loss = 2.90328
Step 30335: loss = 2.87204
Step 30340: loss = 2.58978
Step 30345: loss = 2.78691
Step 30350: loss = 2.86234
Step 30355: loss = 2.84830
Step 30360: loss = 2.79350
Step 30365: loss = 2.90637
Step 30370: loss = 2.74340
Step 30375: loss = 2.74944
Step 30380: loss = 2.78685
Step 30385: loss = 2.91669
Step 30390: loss = 2.75356
Step 30395: loss = 2.75966
Step 30400: loss = 2.78715
Step 30405: loss = 2.61754
Step 30410: loss = 2.59186
Step 30415: loss = 2.81024
Step 30420: loss = 2.81445
Training Data Eval:
  Num examples: 49920, Num correct: 6796, Precision @ 1: 0.1361
('Testing Data Eval: EPOCH->', 79)
  Num examples: 9984, Num correct: 1356, Precision @ 1: 0.1358
Step 30425: loss = 2.78062
Step 30430: loss = 2.64704
Step 30435: loss = 2.79834
Step 30440: loss = 2.79689
Step 30445: loss = 2.80433
Step 30450: loss = 2.84688
Step 30455: loss = 2.79729
Step 30460: loss = 2.91306
Step 30465: loss = 2.93866
Step 30470: loss = 2.61122
Step 30475: loss = 3.01118
Step 30480: loss = 2.80868
Step 30485: loss = 2.88841
Step 30490: loss = 2.63967
Step 30495: loss = 2.65737
Step 30500: loss = 2.71619
Step 30505: loss = 2.69088
Step 30510: loss = 2.57891
Step 30515: loss = 2.73146
Step 30520: loss = 2.59683
Step 30525: loss = 2.51760
Step 30530: loss = 2.69428
Step 30535: loss = 2.73650
Step 30540: loss = 2.66531
Step 30545: loss = 3.02993
Step 30550: loss = 2.89661
Step 30555: loss = 2.86987
Step 30560: loss = 2.67201
Step 30565: loss = 2.57912
Step 30570: loss = 2.84061
Step 30575: loss = 2.63389
Step 30580: loss = 2.78890
Step 30585: loss = 2.65176
Step 30590: loss = 2.87176
Step 30595: loss = 2.89061
Step 30600: loss = 2.70684
Step 30605: loss = 2.67569
Step 30610: loss = 2.77912
Step 30615: loss = 2.83002
Step 30620: loss = 2.90624
Step 30625: loss = 2.61436
Step 30630: loss = 2.92686
Step 30635: loss = 2.64012
Step 30640: loss = 2.86516
Step 30645: loss = 2.80441
Step 30650: loss = 2.91608
Step 30655: loss = 2.78444
Step 30660: loss = 2.68828
Step 30665: loss = 2.77103
Step 30670: loss = 2.66273
Step 30675: loss = 2.75566
Step 30680: loss = 2.72846
Step 30685: loss = 2.76812
Step 30690: loss = 2.54819
Step 30695: loss = 2.95139
Step 30700: loss = 2.49430
Step 30705: loss = 2.64562
Step 30710: loss = 2.87432
Step 30715: loss = 2.81901
Step 30720: loss = 2.82410
Step 30725: loss = 2.66645
Step 30730: loss = 2.51722
Step 30735: loss = 2.95651
Step 30740: loss = 2.79985
Step 30745: loss = 2.66911
Step 30750: loss = 2.49464
Step 30755: loss = 2.62960
Step 30760: loss = 2.68159
Step 30765: loss = 2.67366
Step 30770: loss = 2.61510
Step 30775: loss = 2.83313
Step 30780: loss = 2.84942
Step 30785: loss = 2.68702
Step 30790: loss = 2.85612
Step 30795: loss = 2.92381
Step 30800: loss = 2.77502
Step 30805: loss = 2.75414
Step 30810: loss = 2.79070
Training Data Eval:
  Num examples: 49920, Num correct: 6529, Precision @ 1: 0.1308
('Testing Data Eval: EPOCH->', 80)
  Num examples: 9984, Num correct: 1301, Precision @ 1: 0.1303
Step 30815: loss = 2.67534
Step 30820: loss = 2.82762
Step 30825: loss = 2.80058
Step 30830: loss = 2.65282
Step 30835: loss = 2.81643
Step 30840: loss = 2.95661
Step 30845: loss = 2.81735
Step 30850: loss = 2.74697
Step 30855: loss = 2.72932
Step 30860: loss = 2.75066
Step 30865: loss = 2.80633
Step 30870: loss = 2.90040
Step 30875: loss = 2.54258
Step 30880: loss = 2.87012
Step 30885: loss = 2.73958
Step 30890: loss = 2.76440
Step 30895: loss = 2.77818
Step 30900: loss = 2.66186
Step 30905: loss = 2.69312
Step 30910: loss = 2.87611
Step 30915: loss = 2.69248
Step 30920: loss = 2.77069
Step 30925: loss = 2.69644
Step 30930: loss = 2.71607
Step 30935: loss = 2.71798
Step 30940: loss = 2.72091
Step 30945: loss = 2.70910
Step 30950: loss = 2.85546
Step 30955: loss = 2.71376
Step 30960: loss = 2.83384
Step 30965: loss = 2.88323
Step 30970: loss = 2.64355
Step 30975: loss = 2.71875
Step 30980: loss = 2.84689
Step 30985: loss = 2.67039
Step 30990: loss = 2.61806
Step 30995: loss = 2.91081
Step 31000: loss = 2.71066
Step 31005: loss = 2.80147
Step 31010: loss = 2.84676
Step 31015: loss = 2.85010
Step 31020: loss = 2.76792
Step 31025: loss = 2.92639
Step 31030: loss = 3.23940
Step 31035: loss = 2.87274
Step 31040: loss = 2.55621
Step 31045: loss = 2.73622
Step 31050: loss = 2.84626
Step 31055: loss = 3.21110
Step 31060: loss = 2.82280
Step 31065: loss = 3.03087
Step 31070: loss = 2.62505
Step 31075: loss = 2.58336
Step 31080: loss = 2.77214
Step 31085: loss = 2.77990
Step 31090: loss = 2.85522
Step 31095: loss = 2.87439
Step 31100: loss = 2.65219
Step 31105: loss = 2.74750
Step 31110: loss = 2.88161
Step 31115: loss = 2.83991
Step 31120: loss = 2.81785
Step 31125: loss = 2.93954
Step 31130: loss = 2.81875
Step 31135: loss = 2.65582
Step 31140: loss = 2.84923
Step 31145: loss = 2.92776
Step 31150: loss = 2.93710
Step 31155: loss = 2.63095
Step 31160: loss = 2.95886
Step 31165: loss = 2.78578
Step 31170: loss = 2.60246
Step 31175: loss = 2.77095
Step 31180: loss = 2.58073
Step 31185: loss = 2.78371
Step 31190: loss = 2.75773
Step 31195: loss = 2.55079
Step 31200: loss = 2.79856
Training Data Eval:
  Num examples: 49920, Num correct: 6975, Precision @ 1: 0.1397
('Testing Data Eval: EPOCH->', 81)
  Num examples: 9984, Num correct: 1403, Precision @ 1: 0.1405
Step 31205: loss = 2.71943
Step 31210: loss = 3.04844
Step 31215: loss = 2.72647
Step 31220: loss = 2.86280
Step 31225: loss = 2.74213
Step 31230: loss = 2.88198
Step 31235: loss = 2.82802
Step 31240: loss = 3.01245
Step 31245: loss = 2.87825
Step 31250: loss = 2.68554
Step 31255: loss = 2.68163
Step 31260: loss = 2.74635
Step 31265: loss = 2.80870
Step 31270: loss = 2.65184
Step 31275: loss = 2.63583
Step 31280: loss = 2.90802
Step 31285: loss = 2.86260
Step 31290: loss = 2.78128
Step 31295: loss = 2.61474
Step 31300: loss = 2.72286
Step 31305: loss = 2.75210
Step 31310: loss = 2.87570
Step 31315: loss = 2.78538
Step 31320: loss = 2.77678
Step 31325: loss = 2.82770
Step 31330: loss = 2.62526
Step 31335: loss = 3.00498
Step 31340: loss = 2.71070
Step 31345: loss = 2.78192
Step 31350: loss = 2.82642
Step 31355: loss = 2.69587
Step 31360: loss = 2.70273
Step 31365: loss = 2.76291
Step 31370: loss = 2.87031
Step 31375: loss = 3.21950
Step 31380: loss = 2.89260
Step 31385: loss = 2.84622
Step 31390: loss = 2.77678
Step 31395: loss = 2.94141
Step 31400: loss = 3.27886
Step 31405: loss = 2.73184
Step 31410: loss = 2.96269
Step 31415: loss = 2.74741
Step 31420: loss = 2.68819
Step 31425: loss = 2.51944
Step 31430: loss = 2.68689
Step 31435: loss = 3.03669
Step 31440: loss = 2.80919
Step 31445: loss = 2.70655
Step 31450: loss = 2.73696
Step 31455: loss = 2.76227
Step 31460: loss = 2.86959
Step 31465: loss = 2.68188
Step 31470: loss = 2.74943
Step 31475: loss = 2.60992
Step 31480: loss = 2.81607
Step 31485: loss = 2.82673
Step 31490: loss = 3.06980
Step 31495: loss = 2.74568
Step 31500: loss = 2.71469
Step 31505: loss = 2.81372
Step 31510: loss = 2.72813
Step 31515: loss = 2.71613
Step 31520: loss = 2.87179
Step 31525: loss = 2.79358
Step 31530: loss = 2.96545
Step 31535: loss = 2.82285
Step 31540: loss = 3.01900
Step 31545: loss = 3.16806
Step 31550: loss = 2.65536
Step 31555: loss = 2.72731
Step 31560: loss = 2.81786
Step 31565: loss = 2.88642
Step 31570: loss = 2.85423
Step 31575: loss = 2.86555
Step 31580: loss = 2.63044
Step 31585: loss = 2.64796
Step 31590: loss = 2.60880
Training Data Eval:
  Num examples: 49920, Num correct: 6840, Precision @ 1: 0.1370
('Testing Data Eval: EPOCH->', 82)
  Num examples: 9984, Num correct: 1392, Precision @ 1: 0.1394
Step 31595: loss = 3.05162
Step 31600: loss = 2.63253
Step 31605: loss = 2.68588
Step 31610: loss = 2.84919
Step 31615: loss = 2.65296
Step 31620: loss = 2.58106
Step 31625: loss = 2.72370
Step 31630: loss = 2.72547
Step 31635: loss = 2.88540
Step 31640: loss = 2.97371
Step 31645: loss = 2.93790
Step 31650: loss = 2.91418
Step 31655: loss = 2.52204
Step 31660: loss = 2.69257
Step 31665: loss = 2.59899
Step 31670: loss = 2.62563
Step 31675: loss = 2.71313
Step 31680: loss = 2.98226
Step 31685: loss = 2.80887
Step 31690: loss = 2.63509
Step 31695: loss = 2.63390
Step 31700: loss = 2.66800
Step 31705: loss = 2.69970
Step 31710: loss = 2.73337
Step 31715: loss = 2.72434
Step 31720: loss = 2.72551
Step 31725: loss = 2.67817
Step 31730: loss = 2.79108
Step 31735: loss = 2.78007
Step 31740: loss = 2.67354
Step 31745: loss = 3.09744
Step 31750: loss = 3.15423
Step 31755: loss = 2.67369
Step 31760: loss = 2.67584
Step 31765: loss = 2.77025
Step 31770: loss = 2.75082
Step 31775: loss = 2.76819
Step 31780: loss = 2.98851
Step 31785: loss = 2.62503
Step 31790: loss = 2.63227
Step 31795: loss = 2.51326
Step 31800: loss = 2.67227
Step 31805: loss = 2.68163
Step 31810: loss = 2.67714
Step 31815: loss = 2.71881
Step 31820: loss = 2.74536
Step 31825: loss = 2.79683
Step 31830: loss = 2.59370
Step 31835: loss = 2.82811
Step 31840: loss = 2.67183
Step 31845: loss = 2.82737
Step 31850: loss = 2.83578
Step 31855: loss = 2.79126
Step 31860: loss = 2.80658
Step 31865: loss = 2.66976
Step 31870: loss = 2.51698
Step 31875: loss = 2.64858
Step 31880: loss = 2.59727
Step 31885: loss = 3.06359
Step 31890: loss = 2.70989
Step 31895: loss = 2.66446
Step 31900: loss = 3.03957
Step 31905: loss = 2.62333
Step 31910: loss = 3.02260
Step 31915: loss = 2.95568
Step 31920: loss = 2.84815
Step 31925: loss = 2.76109
Step 31930: loss = 2.85713
Step 31935: loss = 2.88065
Step 31940: loss = 2.55780
Step 31945: loss = 2.76518
Step 31950: loss = 2.91825
Step 31955: loss = 3.02951
Step 31960: loss = 2.70791
Step 31965: loss = 2.92907
Step 31970: loss = 2.71010
Step 31975: loss = 2.66521
Step 31980: loss = 2.62807
Training Data Eval:
  Num examples: 49920, Num correct: 6791, Precision @ 1: 0.1360
('Testing Data Eval: EPOCH->', 83)
  Num examples: 9984, Num correct: 1304, Precision @ 1: 0.1306
Step 31985: loss = 2.90870
Step 31990: loss = 2.57481
Step 31995: loss = 2.70070
Step 32000: loss = 2.75632
Step 32005: loss = 2.84886
Step 32010: loss = 2.61636
Step 32015: loss = 3.07915
Step 32020: loss = 2.89213
Step 32025: loss = 2.73276
Step 32030: loss = 2.87074
Step 32035: loss = 2.91025
Step 32040: loss = 2.93311
Step 32045: loss = 2.62255
Step 32050: loss = 3.01589
Step 32055: loss = 2.88644
Step 32060: loss = 2.64911
Step 32065: loss = 2.69591
Step 32070: loss = 2.80511
Step 32075: loss = 2.84034
Step 32080: loss = 2.87149
Step 32085: loss = 2.80020
Step 32090: loss = 2.66821
Step 32095: loss = 2.88071
Step 32100: loss = 2.75707
Step 32105: loss = 2.66270
Step 32110: loss = 2.82089
Step 32115: loss = 2.81216
Step 32120: loss = 2.87594
Step 32125: loss = 2.74905
Step 32130: loss = 2.80795
Step 32135: loss = 2.77978
Step 32140: loss = 2.86571
Step 32145: loss = 2.69910
Step 32150: loss = 2.97506
Step 32155: loss = 2.91555
Step 32160: loss = 2.66723
Step 32165: loss = 2.46863
Step 32170: loss = 2.99551
Step 32175: loss = 2.81158
Step 32180: loss = 2.68659
Step 32185: loss = 2.63653
Step 32190: loss = 2.87351
Step 32195: loss = 2.72952
Step 32200: loss = 2.64700
Step 32205: loss = 3.02706
Step 32210: loss = 2.63323
Step 32215: loss = 2.63111
Step 32220: loss = 2.61485
Step 32225: loss = 2.70003
Step 32230: loss = 2.70241
Step 32235: loss = 2.63629
Step 32240: loss = 2.76309
Step 32245: loss = 2.60358
Step 32250: loss = 2.79254
Step 32255: loss = 2.81328
Step 32260: loss = 2.70199
Step 32265: loss = 2.87724
Step 32270: loss = 2.88651
Step 32275: loss = 2.94898
Step 32280: loss = 2.86495
Step 32285: loss = 2.59341
Step 32290: loss = 2.93125
Step 32295: loss = 2.77367
Step 32300: loss = 2.79964
Step 32305: loss = 2.76090
Step 32310: loss = 2.74262
Step 32315: loss = 3.10974
Step 32320: loss = 2.70644
Step 32325: loss = 2.86174
Step 32330: loss = 2.56956
Step 32335: loss = 2.76203
Step 32340: loss = 2.61039
Step 32345: loss = 2.83535
Step 32350: loss = 2.84947
Step 32355: loss = 2.70469
Step 32360: loss = 2.81013
Step 32365: loss = 2.91399
Step 32370: loss = 2.82383
Training Data Eval:
  Num examples: 49920, Num correct: 6878, Precision @ 1: 0.1378
('Testing Data Eval: EPOCH->', 84)
  Num examples: 9984, Num correct: 1370, Precision @ 1: 0.1372
Step 32375: loss = 2.58361
Step 32380: loss = 2.68256
Step 32385: loss = 2.71836
Step 32390: loss = 2.67130
Step 32395: loss = 2.78001
Step 32400: loss = 2.63284
Step 32405: loss = 2.66228
Step 32410: loss = 2.88184
Step 32415: loss = 2.73994
Step 32420: loss = 2.65804
Step 32425: loss = 2.73872
Step 32430: loss = 2.80142
Step 32435: loss = 2.77436
Step 32440: loss = 2.89689
Step 32445: loss = 2.62617
Step 32450: loss = 2.82795
Step 32455: loss = 2.68768
Step 32460: loss = 2.62165
Step 32465: loss = 2.74225
Step 32470: loss = 2.87051
Step 32475: loss = 2.60523
Step 32480: loss = 2.67987
Step 32485: loss = 2.69121
Step 32490: loss = 2.81188
Step 32495: loss = 2.68192
Step 32500: loss = 2.53270
Step 32505: loss = 2.80802
Step 32510: loss = 2.65369
Step 32515: loss = 2.79683
Step 32520: loss = 2.90419
Step 32525: loss = 2.80138
Step 32530: loss = 2.70110
Step 32535: loss = 2.84439
Step 32540: loss = 2.50666
Step 32545: loss = 2.85660
Step 32550: loss = 2.66653
Step 32555: loss = 2.72351
Step 32560: loss = 2.87988
Step 32565: loss = 2.61247
Step 32570: loss = 2.77832
Step 32575: loss = 2.87813
Step 32580: loss = 2.98040
Step 32585: loss = 2.73591
Step 32590: loss = 2.76168
Step 32595: loss = 2.79049
Step 32600: loss = 2.61881
Step 32605: loss = 2.50779
Step 32610: loss = 2.70224
Step 32615: loss = 2.84648
Step 32620: loss = 2.84245
Step 32625: loss = 2.58468
Step 32630: loss = 2.69527
Step 32635: loss = 2.88767
Step 32640: loss = 3.27896
Step 32645: loss = 2.79259
Step 32650: loss = 2.79279
Step 32655: loss = 2.75039
Step 32660: loss = 2.83323
Step 32665: loss = 3.19482
Step 32670: loss = 2.61099
Step 32675: loss = 2.75983
Step 32680: loss = 2.83060
Step 32685: loss = 2.72256
Step 32690: loss = 2.95504
Step 32695: loss = 2.66711
Step 32700: loss = 2.93909
Step 32705: loss = 2.65155
Step 32710: loss = 2.65376
Step 32715: loss = 2.87862
Step 32720: loss = 2.90644
Step 32725: loss = 2.95690
Step 32730: loss = 2.81602
Step 32735: loss = 2.88226
Step 32740: loss = 2.91484
Step 32745: loss = 2.88245
Step 32750: loss = 2.89823
Step 32755: loss = 2.79038
Step 32760: loss = 2.90834
Training Data Eval:
  Num examples: 49920, Num correct: 6428, Precision @ 1: 0.1288
('Testing Data Eval: EPOCH->', 85)
  Num examples: 9984, Num correct: 1352, Precision @ 1: 0.1354
Step 32765: loss = 2.79033
Step 32770: loss = 2.69379
Step 32775: loss = 2.79681
Step 32780: loss = 2.86321
Step 32785: loss = 3.00761
Step 32790: loss = 2.82875
Step 32795: loss = 2.84890
Step 32800: loss = 2.65364
Step 32805: loss = 2.78195
Step 32810: loss = 2.65812
Step 32815: loss = 2.84654
Step 32820: loss = 2.64816
Step 32825: loss = 2.61585
Step 32830: loss = 2.66939
Step 32835: loss = 2.81013
Step 32840: loss = 2.41207
Step 32845: loss = 2.54942
Step 32850: loss = 2.78719
Step 32855: loss = 2.95148
Step 32860: loss = 2.63546
Step 32865: loss = 2.73506
Step 32870: loss = 2.61517
Step 32875: loss = 2.85730
Step 32880: loss = 2.61810
Step 32885: loss = 3.00060
Step 32890: loss = 2.67895
Step 32895: loss = 2.69552
Step 32900: loss = 2.75715
Step 32905: loss = 2.86707
Step 32910: loss = 2.86338
Step 32915: loss = 2.78366
Step 32920: loss = 2.71155
Step 32925: loss = 2.90934
Step 32930: loss = 3.08895
Step 32935: loss = 2.79596
Step 32940: loss = 2.55009
Step 32945: loss = 2.76943
Step 32950: loss = 2.80343
Step 32955: loss = 2.75986
Step 32960: loss = 2.66691
Step 32965: loss = 2.63315
Step 32970: loss = 2.56875
Step 32975: loss = 2.68358
Step 32980: loss = 2.88316
Step 32985: loss = 2.85936
Step 32990: loss = 2.78824
Step 32995: loss = 2.96734
Step 33000: loss = 2.49744
Step 33005: loss = 2.84935
Step 33010: loss = 2.72790
Step 33015: loss = 2.78620
Step 33020: loss = 2.74384
Step 33025: loss = 2.83874
Step 33030: loss = 2.93849
Step 33035: loss = 2.79947
Step 33040: loss = 2.75220
Step 33045: loss = 2.55591
Step 33050: loss = 2.69041
Step 33055: loss = 2.83784
Step 33060: loss = 2.73166
Step 33065: loss = 2.87381
Step 33070: loss = 2.66940
Step 33075: loss = 2.64087
Step 33080: loss = 2.60295
Step 33085: loss = 2.61734
Step 33090: loss = 2.90242
Step 33095: loss = 2.66384
Step 33100: loss = 2.89257
Step 33105: loss = 2.80337
Step 33110: loss = 2.71020
Step 33115: loss = 2.76131
Step 33120: loss = 2.79058
Step 33125: loss = 2.74962
Step 33130: loss = 2.65468
Step 33135: loss = 2.86579
Step 33140: loss = 2.75726
Step 33145: loss = 2.89454
Step 33150: loss = 2.72183
Training Data Eval:
  Num examples: 49920, Num correct: 7091, Precision @ 1: 0.1420
('Testing Data Eval: EPOCH->', 86)
  Num examples: 9984, Num correct: 1431, Precision @ 1: 0.1433
Step 33155: loss = 2.65673
Step 33160: loss = 2.50414
Step 33165: loss = 2.52421
Step 33170: loss = 2.75933
Step 33175: loss = 2.83507
Step 33180: loss = 2.86323
Step 33185: loss = 2.72821
Step 33190: loss = 2.79750
Step 33195: loss = 2.84291
Step 33200: loss = 2.68772
Step 33205: loss = 2.99355
Step 33210: loss = 2.82678
Step 33215: loss = 2.80846
Step 33220: loss = 2.54537
Step 33225: loss = 2.72814
Step 33230: loss = 2.65628
Step 33235: loss = 2.87367
Step 33240: loss = 2.49731
Step 33245: loss = 2.87228
Step 33250: loss = 2.84299
Step 33255: loss = 2.69166
Step 33260: loss = 2.68303
Step 33265: loss = 2.81156
Step 33270: loss = 2.74208
Step 33275: loss = 2.63122
Step 33280: loss = 2.90099
Step 33285: loss = 2.68843
Step 33290: loss = 2.67036
Step 33295: loss = 3.06713
Step 33300: loss = 2.70748
Step 33305: loss = 2.66427
Step 33310: loss = 3.01904
Step 33315: loss = 2.53198
Step 33320: loss = 2.78206
Step 33325: loss = 2.81553
Step 33330: loss = 2.56121
Step 33335: loss = 2.83840
Step 33340: loss = 2.78606
Step 33345: loss = 2.89794
Step 33350: loss = 2.85008
Step 33355: loss = 2.62510
Step 33360: loss = 2.77719
Step 33365: loss = 2.75020
Step 33370: loss = 2.64034
Step 33375: loss = 2.88148
Step 33380: loss = 2.70246
Step 33385: loss = 2.69619
Step 33390: loss = 2.70523
Step 33395: loss = 2.76981
Step 33400: loss = 2.60342
Step 33405: loss = 2.82110
Step 33410: loss = 2.75687
Step 33415: loss = 3.06429
Step 33420: loss = 2.75978
Step 33425: loss = 2.97044
Step 33430: loss = 2.82844
Step 33435: loss = 2.79590
Step 33440: loss = 2.64519
Step 33445: loss = 2.87745
Step 33450: loss = 2.65689
Step 33455: loss = 2.89891
Step 33460: loss = 2.72698
Step 33465: loss = 2.63656
Step 33470: loss = 2.87676
Step 33475: loss = 2.75474
Step 33480: loss = 2.66888
Step 33485: loss = 2.87476
Step 33490: loss = 2.62916
Step 33495: loss = 2.57538
Step 33500: loss = 2.75741
Step 33505: loss = 2.56541
Step 33510: loss = 2.64971
Step 33515: loss = 2.96075
Step 33520: loss = 2.98589
Step 33525: loss = 2.72751
Step 33530: loss = 2.80318
Step 33535: loss = 2.88541
Step 33540: loss = 2.58709
Training Data Eval:
  Num examples: 49920, Num correct: 6830, Precision @ 1: 0.1368
('Testing Data Eval: EPOCH->', 87)
  Num examples: 9984, Num correct: 1330, Precision @ 1: 0.1332
Step 33545: loss = 2.77771
Step 33550: loss = 2.70237
Step 33555: loss = 2.94066
Step 33560: loss = 2.79205
Step 33565: loss = 2.75781
Step 33570: loss = 2.69238
Step 33575: loss = 2.81604
Step 33580: loss = 2.82896
Step 33585: loss = 2.71816
Step 33590: loss = 2.85935
Step 33595: loss = 2.84976
Step 33600: loss = 2.79176
Step 33605: loss = 2.77234
Step 33610: loss = 2.87103
Step 33615: loss = 2.68966
Step 33620: loss = 2.84537
Step 33625: loss = 2.89863
Step 33630: loss = 2.88971
Step 33635: loss = 2.69517
Step 33640: loss = 2.86302
Step 33645: loss = 2.67466
Step 33650: loss = 2.58924
Step 33655: loss = 2.70084
Step 33660: loss = 2.66411
Step 33665: loss = 3.09492
Step 33670: loss = 2.84243
Step 33675: loss = 2.80494
Step 33680: loss = 2.82711
Step 33685: loss = 2.77822
Step 33690: loss = 2.65316
Step 33695: loss = 2.74858
Step 33700: loss = 2.64701
Step 33705: loss = 2.81694
Step 33710: loss = 2.89081
Step 33715: loss = 2.54434
Step 33720: loss = 2.59896
Step 33725: loss = 2.82897
Step 33730: loss = 2.83892
Step 33735: loss = 2.62732
Step 33740: loss = 2.69053
Step 33745: loss = 2.78736
Step 33750: loss = 3.09632
Step 33755: loss = 3.01357
Step 33760: loss = 2.67797
Step 33765: loss = 2.55669
Step 33770: loss = 2.72028
Step 33775: loss = 2.70313
Step 33780: loss = 2.89309
Step 33785: loss = 2.69857
Step 33790: loss = 2.73551
Step 33795: loss = 2.60253
Step 33800: loss = 2.63100
Step 33805: loss = 2.75203
Step 33810: loss = 2.75993
Step 33815: loss = 3.05443
Step 33820: loss = 2.73727
Step 33825: loss = 2.40028
Step 33830: loss = 2.77932
Step 33835: loss = 3.08287
Step 33840: loss = 2.67142
Step 33845: loss = 2.87294
Step 33850: loss = 2.68368
Step 33855: loss = 2.85629
Step 33860: loss = 2.85386
Step 33865: loss = 2.72634
Step 33870: loss = 2.94842
Step 33875: loss = 2.67783
Step 33880: loss = 2.85511
Step 33885: loss = 2.70583
Step 33890: loss = 2.80231
Step 33895: loss = 2.77266
Step 33900: loss = 2.89006
Step 33905: loss = 2.76040
Step 33910: loss = 2.64412
Step 33915: loss = 2.64849
Step 33920: loss = 2.58038
Step 33925: loss = 2.68395
Step 33930: loss = 2.60143
Training Data Eval:
  Num examples: 49920, Num correct: 6485, Precision @ 1: 0.1299
('Testing Data Eval: EPOCH->', 88)
  Num examples: 9984, Num correct: 1375, Precision @ 1: 0.1377
Step 33935: loss = 2.63434
Step 33940: loss = 2.67148
Step 33945: loss = 2.87893
Step 33950: loss = 2.82114
Step 33955: loss = 2.54826
Step 33960: loss = 2.84908
Step 33965: loss = 2.69993
Step 33970: loss = 2.83860
Step 33975: loss = 2.82120
Step 33980: loss = 2.73887
Step 33985: loss = 2.81885
Step 33990: loss = 2.71807
Step 33995: loss = 2.80747
Step 34000: loss = 2.89073
Step 34005: loss = 2.86342
Step 34010: loss = 2.85882
Step 34015: loss = 2.65358
Step 34020: loss = 2.82498
Step 34025: loss = 2.92877
Step 34030: loss = 2.83245
Step 34035: loss = 2.71196
Step 34040: loss = 2.73186
Step 34045: loss = 2.86124
Step 34050: loss = 2.71569
Step 34055: loss = 2.77547
Step 34060: loss = 2.85029
Step 34065: loss = 2.87336
Step 34070: loss = 2.63791
Step 34075: loss = 2.58041
Step 34080: loss = 2.65575
Step 34085: loss = 3.05119
Step 34090: loss = 3.35439
Step 34095: loss = 2.58649
Step 34100: loss = 2.72672
Step 34105: loss = 2.83783
Step 34110: loss = 2.77986
Step 34115: loss = 2.76006
Step 34120: loss = 2.81826
Step 34125: loss = 2.75587
Step 34130: loss = 2.70383
Step 34135: loss = 2.76696
Step 34140: loss = 2.80423
Step 34145: loss = 2.78052
Step 34150: loss = 2.93492
Step 34155: loss = 2.81183
Step 34160: loss = 2.56301
Step 34165: loss = 2.54316
Step 34170: loss = 2.94362
Step 34175: loss = 2.77444
Step 34180: loss = 2.71364
Step 34185: loss = 2.73719
Step 34190: loss = 2.80053
Step 34195: loss = 2.76846
Step 34200: loss = 2.91721
Step 34205: loss = 2.81987
Step 34210: loss = 2.71127
Step 34215: loss = 2.72674
Step 34220: loss = 2.77318
Step 34225: loss = 2.67830
Step 34230: loss = 2.78244
Step 34235: loss = 2.68892
Step 34240: loss = 2.67818
Step 34245: loss = 2.64060
Step 34250: loss = 2.57722
Step 34255: loss = 3.03337
Step 34260: loss = 2.78299
Step 34265: loss = 2.62010
Step 34270: loss = 2.88344
Step 34275: loss = 2.58490
Step 34280: loss = 2.69600
Step 34285: loss = 2.59969
Step 34290: loss = 2.91773
Step 34295: loss = 2.92059
Step 34300: loss = 2.75916
Step 34305: loss = 2.84612
Step 34310: loss = 2.81383
Step 34315: loss = 2.67151
Step 34320: loss = 2.79302
Training Data Eval:
  Num examples: 49920, Num correct: 6785, Precision @ 1: 0.1359
('Testing Data Eval: EPOCH->', 89)
  Num examples: 9984, Num correct: 1366, Precision @ 1: 0.1368
Step 34325: loss = 2.60951
Step 34330: loss = 2.90146
Step 34335: loss = 2.76501
Step 34340: loss = 2.75604
Step 34345: loss = 2.61902
Step 34350: loss = 2.76385
Step 34355: loss = 2.55754
Step 34360: loss = 2.84829
Step 34365: loss = 2.81285
Step 34370: loss = 2.73537
Step 34375: loss = 2.75730
Step 34380: loss = 2.72933
Step 34385: loss = 2.63593
Step 34390: loss = 2.94479
Step 34395: loss = 2.67227
Step 34400: loss = 2.86585
Step 34405: loss = 2.93021
Step 34410: loss = 2.87586
Step 34415: loss = 2.57977
Step 34420: loss = 2.75995
Step 34425: loss = 2.75111
Step 34430: loss = 2.64158
Step 34435: loss = 2.81319
Step 34440: loss = 2.69795
Step 34445: loss = 2.70202
Step 34450: loss = 2.82853
Step 34455: loss = 2.77143
Step 34460: loss = 2.85896
Step 34465: loss = 2.84298
Step 34470: loss = 2.76340
Step 34475: loss = 2.72912
Step 34480: loss = 2.80358
Step 34485: loss = 2.80277
Step 34490: loss = 2.84189
Step 34495: loss = 2.82874
Step 34500: loss = 2.63700
Step 34505: loss = 2.81157
Step 34510: loss = 2.61909
Step 34515: loss = 2.57831
Step 34520: loss = 2.70119
Step 34525: loss = 2.94947
Step 34530: loss = 2.99309
Step 34535: loss = 2.70676
Step 34540: loss = 2.65775
Step 34545: loss = 2.66267
Step 34550: loss = 2.82216
Step 34555: loss = 2.77764
Step 34560: loss = 2.77505
Step 34565: loss = 3.00441
Step 34570: loss = 2.69924
Step 34575: loss = 2.77382
Step 34580: loss = 2.79106
Step 34585: loss = 2.62381
Step 34590: loss = 2.62449
Step 34595: loss = 2.79450
Step 34600: loss = 2.88647
Step 34605: loss = 2.74889
Step 34610: loss = 2.71617
Step 34615: loss = 2.49614
Step 34620: loss = 3.02712
Step 34625: loss = 2.76768
Step 34630: loss = 2.77263
Step 34635: loss = 2.84708
Step 34640: loss = 2.71146
Step 34645: loss = 2.91345
Step 34650: loss = 2.64478
Step 34655: loss = 3.05423
Step 34660: loss = 2.86462
Step 34665: loss = 2.98300
Step 34670: loss = 2.85497
Step 34675: loss = 2.61042
Step 34680: loss = 2.70270
Step 34685: loss = 2.67417
Step 34690: loss = 2.71934
Step 34695: loss = 2.84149
Step 34700: loss = 2.91864
Step 34705: loss = 2.94182
Step 34710: loss = 2.94597
Training Data Eval:
  Num examples: 49920, Num correct: 6539, Precision @ 1: 0.1310
('Testing Data Eval: EPOCH->', 90)
  Num examples: 9984, Num correct: 1347, Precision @ 1: 0.1349
Step 34715: loss = 2.58548
Step 34720: loss = 2.82254
Step 34725: loss = 2.76965
Step 34730: loss = 2.83297
Step 34735: loss = 2.67199
Step 34740: loss = 2.82791
Step 34745: loss = 2.82154
Step 34750: loss = 2.87489
Step 34755: loss = 2.73781
Step 34760: loss = 2.68964
Step 34765: loss = 2.76472
Step 34770: loss = 2.60266
Step 34775: loss = 3.09405
Step 34780: loss = 2.70864
Step 34785: loss = 2.62788
Step 34790: loss = 2.76856
Step 34795: loss = 2.73360
Step 34800: loss = 2.58010
Step 34805: loss = 2.78092
Step 34810: loss = 2.78398
Step 34815: loss = 2.77373
Step 34820: loss = 2.63005
Step 34825: loss = 2.67972
Step 34830: loss = 2.62951
Step 34835: loss = 2.80311
Step 34840: loss = 2.80444
Step 34845: loss = 2.73041
Step 34850: loss = 2.92933
Step 34855: loss = 2.70287
Step 34860: loss = 2.80060
Step 34865: loss = 2.86520
Step 34870: loss = 2.68963
Step 34875: loss = 2.75649
Step 34880: loss = 2.79040
Step 34885: loss = 2.60448
Step 34890: loss = 2.83366
Step 34895: loss = 3.02229
Step 34900: loss = 2.89955
Step 34905: loss = 2.82391
Step 34910: loss = 2.64808
Step 34915: loss = 2.91541
Step 34920: loss = 2.76276
Step 34925: loss = 2.89946
Step 34930: loss = 2.87017
Step 34935: loss = 3.11065
Step 34940: loss = 2.81360
Step 34945: loss = 2.93033
Step 34950: loss = 2.74547
Step 34955: loss = 2.86645
Step 34960: loss = 2.76010
Step 34965: loss = 2.84018
Step 34970: loss = 2.77901
Step 34975: loss = 2.85222
Step 34980: loss = 2.65275
Step 34985: loss = 2.75575
Step 34990: loss = 2.52772
Step 34995: loss = 2.68920
Step 35000: loss = 2.63289
Step 35005: loss = 2.85099
Step 35010: loss = 2.93326
Step 35015: loss = 2.88208
Step 35020: loss = 2.79135
Step 35025: loss = 2.96830
Step 35030: loss = 2.58288
Step 35035: loss = 2.66450
Step 35040: loss = 2.64303
Step 35045: loss = 3.03808
Step 35050: loss = 2.80846
Step 35055: loss = 2.66644
Step 35060: loss = 2.90797
Step 35065: loss = 2.63424
Step 35070: loss = 2.80201
Step 35075: loss = 2.90617
Step 35080: loss = 2.68041
Step 35085: loss = 2.72945
Step 35090: loss = 2.73617
Step 35095: loss = 2.78932
Step 35100: loss = 2.50633
Training Data Eval:
  Num examples: 49920, Num correct: 6754, Precision @ 1: 0.1353
('Testing Data Eval: EPOCH->', 91)
  Num examples: 9984, Num correct: 1341, Precision @ 1: 0.1343
Step 35105: loss = 3.01171
Step 35110: loss = 2.89104
Step 35115: loss = 2.68460
Step 35120: loss = 2.68543
Step 35125: loss = 2.74046
Step 35130: loss = 2.72238
Step 35135: loss = 2.70153
Step 35140: loss = 2.80883
Step 35145: loss = 2.74589
Step 35150: loss = 2.89977
Step 35155: loss = 2.71301
Step 35160: loss = 2.81131
Step 35165: loss = 2.82618
Step 35170: loss = 2.93211
Step 35175: loss = 2.67761
Step 35180: loss = 2.81304
Step 35185: loss = 2.78637
Step 35190: loss = 2.88593
Step 35195: loss = 2.76190
Step 35200: loss = 2.85684
Step 35205: loss = 2.97395
Step 35210: loss = 2.81242
Step 35215: loss = 2.75963
Step 35220: loss = 2.68459
Step 35225: loss = 2.90868
Step 35230: loss = 2.72368
Step 35235: loss = 2.61222
Step 35240: loss = 2.73569
Step 35245: loss = 2.88468
Step 35250: loss = 2.86258
Step 35255: loss = 2.65464
Step 35260: loss = 2.79664
Step 35265: loss = 2.74073
Step 35270: loss = 2.78675
Step 35275: loss = 2.54347
Step 35280: loss = 2.76067
Step 35285: loss = 2.78044
Step 35290: loss = 2.71729
Step 35295: loss = 2.87851
Step 35300: loss = 2.76063
Step 35305: loss = 2.93697
Step 35310: loss = 2.68204
Step 35315: loss = 2.64745
Step 35320: loss = 2.63283
Step 35325: loss = 2.80880
Step 35330: loss = 2.94857
Step 35335: loss = 2.72896
Step 35340: loss = 2.50897
Step 35345: loss = 2.77906
Step 35350: loss = 2.98780
Step 35355: loss = 2.98647
Step 35360: loss = 2.87602
Step 35365: loss = 2.85551
Step 35370: loss = 2.82015
Step 35375: loss = 2.68592
Step 35380: loss = 2.62439
Step 35385: loss = 2.75170
Step 35390: loss = 2.72391
Step 35395: loss = 2.90725
Step 35400: loss = 2.89058
Step 35405: loss = 2.77613
Step 35410: loss = 2.57730
Step 35415: loss = 2.91626
Step 35420: loss = 2.67938
Step 35425: loss = 2.62129
Step 35430: loss = 3.00421
Step 35435: loss = 2.61334
Step 35440: loss = 2.84599
Step 35445: loss = 2.66984
Step 35450: loss = 2.82357
Step 35455: loss = 2.83432
Step 35460: loss = 2.66360
Step 35465: loss = 2.88746
Step 35470: loss = 2.58592
Step 35475: loss = 2.70682
Step 35480: loss = 2.72976
Step 35485: loss = 2.83841
Step 35490: loss = 2.82246
Training Data Eval:
  Num examples: 49920, Num correct: 6921, Precision @ 1: 0.1386
('Testing Data Eval: EPOCH->', 92)
  Num examples: 9984, Num correct: 1324, Precision @ 1: 0.1326
Step 35495: loss = 2.62621
Step 35500: loss = 2.72870
Step 35505: loss = 2.78771
Step 35510: loss = 2.68713
Step 35515: loss = 2.83148
Step 35520: loss = 2.79206
Step 35525: loss = 2.79694
Step 35530: loss = 2.82173
Step 35535: loss = 2.57768
Step 35540: loss = 2.68622
Step 35545: loss = 2.71345
Step 35550: loss = 2.44175
Step 35555: loss = 2.60493
Step 35560: loss = 2.74051
Step 35565: loss = 2.81521
Step 35570: loss = 2.68510
Step 35575: loss = 2.85026
Step 35580: loss = 2.81560
Step 35585: loss = 2.90610
Step 35590: loss = 2.75893
Step 35595: loss = 2.57390
Step 35600: loss = 2.71646
Step 35605: loss = 2.88910
Step 35610: loss = 2.78952
Step 35615: loss = 2.78177
Step 35620: loss = 3.00255
Step 35625: loss = 2.76073
Step 35630: loss = 2.80138
Step 35635: loss = 2.80478
Step 35640: loss = 2.68931
Step 35645: loss = 2.82969
Step 35650: loss = 2.69663
Step 35655: loss = 2.78258
Step 35660: loss = 2.77877
Step 35665: loss = 2.69291
Step 35670: loss = 2.72941
Step 35675: loss = 2.57465
Step 35680: loss = 2.71153
Step 35685: loss = 2.67285
Step 35690: loss = 2.89042
Step 35695: loss = 2.76766
Step 35700: loss = 2.75139
Step 35705: loss = 2.86047
Step 35710: loss = 2.91833
Step 35715: loss = 2.74336
Step 35720: loss = 2.64125
Step 35725: loss = 2.86637
Step 35730: loss = 2.65958
Step 35735: loss = 2.81838
Step 35740: loss = 2.79247
Step 35745: loss = 2.76255
Step 35750: loss = 2.68220
Step 35755: loss = 2.78053
Step 35760: loss = 2.73724
Step 35765: loss = 2.78320
Step 35770: loss = 2.94622
Step 35775: loss = 2.64175
Step 35780: loss = 2.55342
Step 35785: loss = 2.65957
Step 35790: loss = 2.64446
Step 35795: loss = 2.88935
Step 35800: loss = 2.78324
Step 35805: loss = 2.62077
Step 35810: loss = 2.87946
Step 35815: loss = 2.68431
Step 35820: loss = 2.86240
Step 35825: loss = 2.96014
Step 35830: loss = 2.57364
Step 35835: loss = 2.70278
Step 35840: loss = 2.85529
Step 35845: loss = 2.71541
Step 35850: loss = 2.99187
Step 35855: loss = 2.67500
Step 35860: loss = 2.67580
Step 35865: loss = 3.02273
Step 35870: loss = 2.82789
Step 35875: loss = 2.87436
Step 35880: loss = 2.84807
Training Data Eval:
  Num examples: 49920, Num correct: 6706, Precision @ 1: 0.1343
('Testing Data Eval: EPOCH->', 93)
  Num examples: 9984, Num correct: 1371, Precision @ 1: 0.1373
Step 35885: loss = 2.86893
Step 35890: loss = 2.75010
Step 35895: loss = 2.70130
Step 35900: loss = 2.88110
Step 35905: loss = 2.67185
Step 35910: loss = 2.69900
Step 35915: loss = 2.86658
Step 35920: loss = 2.85777
Step 35925: loss = 2.75246
Step 35930: loss = 2.83734
Step 35935: loss = 2.71110
Step 35940: loss = 2.58797
Step 35945: loss = 2.66166
Step 35950: loss = 2.84540
Step 35955: loss = 2.79663
Step 35960: loss = 2.68264
Step 35965: loss = 2.62364
Step 35970: loss = 2.70906
Step 35975: loss = 2.78306
Step 35980: loss = 2.92267
Step 35985: loss = 2.75591
Step 35990: loss = 2.83014
Step 35995: loss = 2.85994
Step 36000: loss = 2.74979
Step 36005: loss = 2.89875
Step 36010: loss = 2.64220
Step 36015: loss = 2.90183
Step 36020: loss = 2.56189
Step 36025: loss = 2.77103
Step 36030: loss = 2.97635
Step 36035: loss = 2.77941
Step 36040: loss = 3.00011
Step 36045: loss = 2.60864
Step 36050: loss = 2.91292
Step 36055: loss = 2.99273
Step 36060: loss = 2.70999
Step 36065: loss = 2.81178
Step 36070: loss = 2.82684
Step 36075: loss = 2.78997
Step 36080: loss = 2.76990
Step 36085: loss = 2.56803
Step 36090: loss = 2.77171
Step 36095: loss = 2.93590
Step 36100: loss = 2.68822
Step 36105: loss = 2.87293
Step 36110: loss = 2.92653
Step 36115: loss = 2.63957
Step 36120: loss = 2.57637
Step 36125: loss = 2.79796
Step 36130: loss = 2.75212
Step 36135: loss = 2.79502
Step 36140: loss = 2.91472
Step 36145: loss = 2.75279
Step 36150: loss = 2.79829
Step 36155: loss = 2.80384
Step 36160: loss = 3.01044
Step 36165: loss = 2.79755
Step 36170: loss = 2.89134
Step 36175: loss = 2.74759
Step 36180: loss = 2.86586
Step 36185: loss = 2.79526
Step 36190: loss = 2.82018
Step 36195: loss = 2.71235
Step 36200: loss = 2.84829
Step 36205: loss = 2.87060
Step 36210: loss = 2.79774
Step 36215: loss = 2.69095
Step 36220: loss = 2.70961
Step 36225: loss = 2.86868
Step 36230: loss = 2.93687
Step 36235: loss = 2.71832
Step 36240: loss = 2.65560
Step 36245: loss = 2.89706
Step 36250: loss = 2.73003
Step 36255: loss = 2.79948
Step 36260: loss = 2.83543
Step 36265: loss = 2.87918
Step 36270: loss = 2.78757
Training Data Eval:
  Num examples: 49920, Num correct: 7170, Precision @ 1: 0.1436
('Testing Data Eval: EPOCH->', 94)
  Num examples: 9984, Num correct: 1421, Precision @ 1: 0.1423
Step 36275: loss = 2.63148
Step 36280: loss = 2.85310
Step 36285: loss = 2.77283
Step 36290: loss = 2.90563
Step 36295: loss = 2.72564
Step 36300: loss = 2.78836
Step 36305: loss = 2.86630
Step 36310: loss = 2.92588
Step 36315: loss = 2.74849
Step 36320: loss = 2.77624
Step 36325: loss = 2.91500
Step 36330: loss = 2.61690
Step 36335: loss = 2.97089
Step 36340: loss = 3.06121
Step 36345: loss = 2.76026
Step 36350: loss = 2.66505
Step 36355: loss = 2.81882
Step 36360: loss = 2.81781
Step 36365: loss = 2.76987
Step 36370: loss = 2.81221
Step 36375: loss = 2.85316
Step 36380: loss = 2.89230
Step 36385: loss = 2.92348
Step 36390: loss = 2.56227
Step 36395: loss = 2.58383
Step 36400: loss = 2.65706
Step 36405: loss = 2.83794
Step 36410: loss = 2.73712
Step 36415: loss = 2.88517
Step 36420: loss = 2.57247
Step 36425: loss = 2.84999
Step 36430: loss = 3.01691
Step 36435: loss = 2.77771
Step 36440: loss = 2.85552
Step 36445: loss = 2.82123
Step 36450: loss = 2.78817
Step 36455: loss = 2.88730
Step 36460: loss = 2.81047
Step 36465: loss = 2.75000
Step 36470: loss = 2.62384
Step 36475: loss = 2.86743
Step 36480: loss = 2.82808
Step 36485: loss = 2.71147
Step 36490: loss = 2.90833
Step 36495: loss = 2.60736
Step 36500: loss = 2.58828
Step 36505: loss = 2.62047
Step 36510: loss = 2.64847
Step 36515: loss = 2.87480
Step 36520: loss = 3.01784
Step 36525: loss = 2.90865
Step 36530: loss = 2.67496
Step 36535: loss = 2.71608
Step 36540: loss = 3.02758
Step 36545: loss = 2.97549
Step 36550: loss = 2.82673
Step 36555: loss = 2.83435
Step 36560: loss = 2.70799
Step 36565: loss = 2.75690
Step 36570: loss = 2.63716
Step 36575: loss = 2.80898
Step 36580: loss = 2.76338
Step 36585: loss = 2.78165
Step 36590: loss = 2.71550
Step 36595: loss = 2.72407
Step 36600: loss = 2.84701
Step 36605: loss = 2.93027
Step 36610: loss = 2.64435
Step 36615: loss = 2.79460
Step 36620: loss = 2.76847
Step 36625: loss = 2.63458
Step 36630: loss = 2.52864
Step 36635: loss = 2.63766
Step 36640: loss = 2.83634
Step 36645: loss = 2.75702
Step 36650: loss = 2.53939
Step 36655: loss = 2.76472
Step 36660: loss = 2.77701
Training Data Eval:
  Num examples: 49920, Num correct: 6611, Precision @ 1: 0.1324
('Testing Data Eval: EPOCH->', 95)
  Num examples: 9984, Num correct: 1303, Precision @ 1: 0.1305
Step 36665: loss = 2.78879
Step 36670: loss = 2.82489
Step 36675: loss = 2.59811
Step 36680: loss = 2.91162
Step 36685: loss = 2.79509
Step 36690: loss = 2.70443
Step 36695: loss = 3.02754
Step 36700: loss = 2.65320
Step 36705: loss = 2.77404
Step 36710: loss = 2.88343
Step 36715: loss = 2.68843
Step 36720: loss = 2.77858
Step 36725: loss = 2.68466
Step 36730: loss = 2.68458
Step 36735: loss = 2.85900
Step 36740: loss = 2.78194
Step 36745: loss = 2.73406
Step 36750: loss = 2.87002
Step 36755: loss = 2.77746
Step 36760: loss = 2.82165
Step 36765: loss = 3.03573
Step 36770: loss = 2.90302
Step 36775: loss = 2.69323
Step 36780: loss = 3.04419
Step 36785: loss = 2.81766
Step 36790: loss = 2.79418
Step 36795: loss = 2.94058
Step 36800: loss = 2.70814
Step 36805: loss = 2.69739
Step 36810: loss = 2.64941
Step 36815: loss = 2.76324
Step 36820: loss = 2.48569
Step 36825: loss = 2.79292
Step 36830: loss = 2.83097
Step 36835: loss = 2.81838
Step 36840: loss = 3.15035
Step 36845: loss = 2.80681
Step 36850: loss = 2.62897
Step 36855: loss = 2.92448
Step 36860: loss = 2.90361
Step 36865: loss = 2.77230
Step 36870: loss = 2.74613
Step 36875: loss = 2.84903
Step 36880: loss = 2.88137
Step 36885: loss = 2.72440
Step 36890: loss = 2.89567
Step 36895: loss = 2.84243
Step 36900: loss = 2.72959
Step 36905: loss = 2.77349
Step 36910: loss = 2.76162
Step 36915: loss = 2.64321
Step 36920: loss = 3.01848
Step 36925: loss = 2.77542
Step 36930: loss = 2.51422
Step 36935: loss = 2.75139
Step 36940: loss = 2.64316
Step 36945: loss = 2.52386
Step 36950: loss = 2.93167
Step 36955: loss = 2.89968
Step 36960: loss = 2.76661
Step 36965: loss = 2.75674
Step 36970: loss = 2.76678
Step 36975: loss = 2.85315
Step 36980: loss = 2.82106
Step 36985: loss = 2.88964
Step 36990: loss = 2.77945
Step 36995: loss = 2.83815
Step 37000: loss = 3.08212
Step 37005: loss = 2.85591
Step 37010: loss = 2.79566
Step 37015: loss = 2.93649
Step 37020: loss = 2.67488
Step 37025: loss = 2.56330
Step 37030: loss = 2.67536
Step 37035: loss = 2.81403
Step 37040: loss = 2.72315
Step 37045: loss = 2.70503
Step 37050: loss = 2.69898
Training Data Eval:
  Num examples: 49920, Num correct: 6662, Precision @ 1: 0.1335
('Testing Data Eval: EPOCH->', 96)
  Num examples: 9984, Num correct: 1278, Precision @ 1: 0.1280
Step 37055: loss = 2.67522
Step 37060: loss = 2.70498
Step 37065: loss = 2.70697
Step 37070: loss = 2.78711
Step 37075: loss = 2.66918
Step 37080: loss = 2.93460
Step 37085: loss = 2.71218
Step 37090: loss = 2.80221
Step 37095: loss = 2.67447
Step 37100: loss = 2.84948
Step 37105: loss = 2.54125
Step 37110: loss = 2.71800
Step 37115: loss = 2.75364
Step 37120: loss = 2.85193
Step 37125: loss = 2.90051
Step 37130: loss = 2.65038
Step 37135: loss = 2.57943
Step 37140: loss = 2.72725
Step 37145: loss = 2.83550
Step 37150: loss = 2.69474
Step 37155: loss = 2.65364
Step 37160: loss = 2.70607
Step 37165: loss = 2.92498
Step 37170: loss = 2.61147
Step 37175: loss = 2.83394
Step 37180: loss = 2.66798
Step 37185: loss = 2.67447
Step 37190: loss = 2.76718
Step 37195: loss = 3.02622
Step 37200: loss = 2.59738
Step 37205: loss = 2.71538
Step 37210: loss = 2.67829
Step 37215: loss = 2.78000
Step 37220: loss = 2.65836
Step 37225: loss = 2.93720
Step 37230: loss = 2.68118
Step 37235: loss = 2.90813
Step 37240: loss = 2.79306
Step 37245: loss = 2.75752
Step 37250: loss = 2.84828
Step 37255: loss = 2.86802
Step 37260: loss = 2.66931
Step 37265: loss = 2.61600
Step 37270: loss = 2.59758
Step 37275: loss = 2.83575
Step 37280: loss = 2.57031
Step 37285: loss = 2.64529
Step 37290: loss = 2.62719
Step 37295: loss = 2.60905
Step 37300: loss = 2.77521
Step 37305: loss = 2.69756
Step 37310: loss = 2.80670
Step 37315: loss = 2.71299
Step 37320: loss = 2.51816
Step 37325: loss = 2.73183
Step 37330: loss = 2.87255
Step 37335: loss = 2.75399
Step 37340: loss = 2.64335
Step 37345: loss = 2.91462
Step 37350: loss = 2.82451
Step 37355: loss = 2.85352
Step 37360: loss = 3.01739
Step 37365: loss = 2.71648
Step 37370: loss = 2.75444
Step 37375: loss = 3.02741
Step 37380: loss = 3.03656
Step 37385: loss = 2.75545
Step 37390: loss = 2.80295
Step 37395: loss = 2.75228
Step 37400: loss = 2.59300
Step 37405: loss = 2.66140
Step 37410: loss = 2.95072
Step 37415: loss = 3.03847
Step 37420: loss = 2.73542
Step 37425: loss = 2.68911
Step 37430: loss = 2.92250
Step 37435: loss = 2.62955
Step 37440: loss = 2.78818
Training Data Eval:
  Num examples: 49920, Num correct: 6975, Precision @ 1: 0.1397
('Testing Data Eval: EPOCH->', 97)
  Num examples: 9984, Num correct: 1362, Precision @ 1: 0.1364
Step 37445: loss = 2.76466
Step 37450: loss = 2.74324
Step 37455: loss = 2.71411
Step 37460: loss = 2.79547
Step 37465: loss = 2.83955
Step 37470: loss = 2.88356
Step 37475: loss = 2.94689
Step 37480: loss = 2.95869
Step 37485: loss = 2.88585
Step 37490: loss = 2.81692
Step 37495: loss = 2.90006
Step 37500: loss = 2.80142
Step 37505: loss = 2.80204
Step 37510: loss = 2.97868
Step 37515: loss = 2.89488
Step 37520: loss = 2.86397
Step 37525: loss = 2.84409
Step 37530: loss = 2.75711
Step 37535: loss = 2.83159
Step 37540: loss = 2.93313
Step 37545: loss = 2.61107
Step 37550: loss = 2.90301
Step 37555: loss = 2.68260
Step 37560: loss = 2.72351
Step 37565: loss = 2.76303
Step 37570: loss = 2.91427
Step 37575: loss = 2.86389
Step 37580: loss = 2.96407
Step 37585: loss = 2.96070
Step 37590: loss = 2.71304
Step 37595: loss = 2.68256
Step 37600: loss = 2.75887
Step 37605: loss = 2.62975
Step 37610: loss = 2.86973
Step 37615: loss = 2.81422
Step 37620: loss = 2.83659
Step 37625: loss = 2.80379
Step 37630: loss = 2.59038
Step 37635: loss = 2.64277
Step 37640: loss = 2.73995
Step 37645: loss = 2.86162
Step 37650: loss = 2.74732
Step 37655: loss = 2.76172
Step 37660: loss = 3.13405
Step 37665: loss = 3.17320
Step 37670: loss = 2.75999
Step 37675: loss = 2.76046
Step 37680: loss = 2.72785
Step 37685: loss = 2.91366
Step 37690: loss = 3.01895
Step 37695: loss = 2.69351
Step 37700: loss = 2.79599
Step 37705: loss = 2.61107
Step 37710: loss = 2.80191
Step 37715: loss = 2.74626
Step 37720: loss = 2.79661
Step 37725: loss = 2.75600
Step 37730: loss = 2.73369
Step 37735: loss = 2.76737
Step 37740: loss = 2.66604
Step 37745: loss = 2.61815
Step 37750: loss = 2.94333
Step 37755: loss = 2.92890
Step 37760: loss = 2.83141
Step 37765: loss = 2.73929
Step 37770: loss = 2.90028
Step 37775: loss = 2.75403
Step 37780: loss = 2.75361
Step 37785: loss = 2.94409
Step 37790: loss = 2.85360
Step 37795: loss = 2.94292
Step 37800: loss = 2.89757
Step 37805: loss = 2.72310
Step 37810: loss = 2.77865
Step 37815: loss = 2.89789
Step 37820: loss = 2.88180
Step 37825: loss = 2.78325
Step 37830: loss = 2.84269
Training Data Eval:
  Num examples: 49920, Num correct: 6823, Precision @ 1: 0.1367
('Testing Data Eval: EPOCH->', 98)
  Num examples: 9984, Num correct: 1383, Precision @ 1: 0.1385
Step 37835: loss = 3.01423
Step 37840: loss = 2.82452
Step 37845: loss = 2.77440
Step 37850: loss = 2.68560
Step 37855: loss = 2.95212
Step 37860: loss = 2.81715
Step 37865: loss = 2.91752
Step 37870: loss = 2.68789
Step 37875: loss = 2.73551
Step 37880: loss = 2.61371
Step 37885: loss = 2.73294
Step 37890: loss = 2.74249
Step 37895: loss = 2.81414
Step 37900: loss = 2.68931
Step 37905: loss = 2.82721
Step 37910: loss = 2.84745
Step 37915: loss = 2.86361
Step 37920: loss = 2.62065
Step 37925: loss = 2.61582
Step 37930: loss = 2.64913
Step 37935: loss = 2.65306
Step 37940: loss = 2.74171
Step 37945: loss = 2.88590
Step 37950: loss = 3.01459
Step 37955: loss = 2.65432
Step 37960: loss = 2.52810
Step 37965: loss = 3.04455
Step 37970: loss = 2.83000
Step 37975: loss = 2.67128
Step 37980: loss = 2.62850
Step 37985: loss = 2.68218
Step 37990: loss = 2.75671
Step 37995: loss = 2.95760
Step 38000: loss = 2.69030
Step 38005: loss = 3.08120
Step 38010: loss = 2.68155
Step 38015: loss = 2.82102
Step 38020: loss = 2.84176
Step 38025: loss = 2.74154
Step 38030: loss = 2.99862
Step 38035: loss = 2.71914
Step 38040: loss = 2.74715
Step 38045: loss = 2.72449
Step 38050: loss = 2.58335
Step 38055: loss = 2.60130
Step 38060: loss = 2.55067
Step 38065: loss = 2.65160
Step 38070: loss = 2.81539
Step 38075: loss = 2.59813
Step 38080: loss = 2.60820
Step 38085: loss = 2.74313
Step 38090: loss = 2.96055
Step 38095: loss = 2.90585
Step 38100: loss = 2.76971
Step 38105: loss = 2.86959
Step 38110: loss = 2.95371
Step 38115: loss = 2.79011
Step 38120: loss = 2.81080
Step 38125: loss = 2.80063
Step 38130: loss = 2.80180
Step 38135: loss = 2.79039
Step 38140: loss = 2.88325
Step 38145: loss = 2.80917
Step 38150: loss = 2.56923
Step 38155: loss = 2.86605
Step 38160: loss = 2.78262
Step 38165: loss = 2.88262
Step 38170: loss = 2.61512
Step 38175: loss = 2.71985
Step 38180: loss = 2.63737
Step 38185: loss = 2.84401
Step 38190: loss = 2.82220
Step 38195: loss = 2.59841
Step 38200: loss = 2.75651
Step 38205: loss = 2.88279
Step 38210: loss = 2.55753
Step 38215: loss = 2.81908
Step 38220: loss = 2.75594
Training Data Eval:
  Num examples: 49920, Num correct: 6893, Precision @ 1: 0.1381
('Testing Data Eval: EPOCH->', 99)
  Num examples: 9984, Num correct: 1360, Precision @ 1: 0.1362
Step 38225: loss = 2.85311
Step 38230: loss = 2.71897
Step 38235: loss = 2.64575
Step 38240: loss = 2.76311
Step 38245: loss = 3.01392
Step 38250: loss = 2.91435
Step 38255: loss = 2.59936
Step 38260: loss = 2.70206
Step 38265: loss = 2.87071
Step 38270: loss = 2.96295
Step 38275: loss = 2.45821
Step 38280: loss = 2.82714
Step 38285: loss = 2.67689
Step 38290: loss = 2.83987
Step 38295: loss = 3.00801
Step 38300: loss = 2.92216
Step 38305: loss = 3.09035
Step 38310: loss = 2.90253
Step 38315: loss = 2.92124
Step 38320: loss = 2.69304
Step 38325: loss = 2.80023
Step 38330: loss = 2.77132
Step 38335: loss = 3.06335
Step 38340: loss = 2.75645
Step 38345: loss = 3.13006
Step 38350: loss = 2.94906
Step 38355: loss = 2.86832
Step 38360: loss = 2.50138
Step 38365: loss = 2.79692
Step 38370: loss = 3.15028
Step 38375: loss = 2.76888
Step 38380: loss = 2.96402
Step 38385: loss = 2.85882
Step 38390: loss = 2.83527
Step 38395: loss = 2.72366
Step 38400: loss = 2.79361
Step 38405: loss = 2.61891
Step 38410: loss = 2.82646
Step 38415: loss = 2.66368
Step 38420: loss = 2.76362
Step 38425: loss = 2.86625
Step 38430: loss = 2.88348
Step 38435: loss = 2.81472
Step 38440: loss = 2.72242
Step 38445: loss = 2.64775
Step 38450: loss = 2.72984
Step 38455: loss = 2.94200
Step 38460: loss = 2.96623
Step 38465: loss = 2.83214
Step 38470: loss = 2.80172
Step 38475: loss = 2.74868
Step 38480: loss = 2.80166
Step 38485: loss = 2.69714
Step 38490: loss = 2.73262
Step 38495: loss = 2.98948
Step 38500: loss = 2.63873
Step 38505: loss = 3.05698
Step 38510: loss = 2.85770
Step 38515: loss = 2.87710
Step 38520: loss = 2.82428
Step 38525: loss = 2.73135
Step 38530: loss = 2.71898
Step 38535: loss = 2.77167
Step 38540: loss = 2.90421
Step 38545: loss = 2.72956
Step 38550: loss = 2.77223
Step 38555: loss = 2.77302
Step 38560: loss = 2.95933
Step 38565: loss = 2.78530
Step 38570: loss = 2.71688
Step 38575: loss = 2.81748
Step 38580: loss = 2.83702
Step 38585: loss = 2.58576
Step 38590: loss = 2.61606
Step 38595: loss = 2.93516
Step 38600: loss = 2.74420
Step 38605: loss = 2.73563
Step 38610: loss = 2.76347
Training Data Eval:
  Num examples: 49920, Num correct: 6769, Precision @ 1: 0.1356
('Testing Data Eval: EPOCH->', 100)
  Num examples: 9984, Num correct: 1359, Precision @ 1: 0.1361
Step 38615: loss = 2.94705
Step 38620: loss = 2.69590
Step 38625: loss = 2.52388
Step 38630: loss = 2.73946
Step 38635: loss = 2.80236
Step 38640: loss = 2.76477
Step 38645: loss = 2.86748
Step 38650: loss = 2.87091
Step 38655: loss = 3.01830
Step 38660: loss = 2.78975
Step 38665: loss = 3.08998
Step 38670: loss = 2.87329
Step 38675: loss = 2.85048
Step 38680: loss = 2.71618
Step 38685: loss = 2.65379
Step 38690: loss = 2.71585
Step 38695: loss = 2.87676
Step 38700: loss = 2.86717
Step 38705: loss = 2.92998
Step 38710: loss = 2.85804
Step 38715: loss = 2.89979
Step 38720: loss = 2.94016
Step 38725: loss = 2.70102
Step 38730: loss = 2.60887
Step 38735: loss = 3.03613
Step 38740: loss = 2.68282
Step 38745: loss = 2.51185
Step 38750: loss = 2.58417
Step 38755: loss = 2.73254
Step 38760: loss = 2.88033
Step 38765: loss = 2.90653
Step 38770: loss = 2.64978
Step 38775: loss = 2.77726
Step 38780: loss = 2.90592
Step 38785: loss = 2.99077
Step 38790: loss = 2.84114
Step 38795: loss = 3.02940
Step 38800: loss = 2.81720
Step 38805: loss = 2.87317
Step 38810: loss = 2.66781
Step 38815: loss = 2.78320
Step 38820: loss = 2.69093
Step 38825: loss = 3.07109
Step 38830: loss = 2.69688
Step 38835: loss = 2.77891
Step 38840: loss = 2.71038
Step 38845: loss = 2.81538
Step 38850: loss = 2.78651
Step 38855: loss = 2.88315
Step 38860: loss = 2.63351
Step 38865: loss = 2.81447
Step 38870: loss = 2.89730
Step 38875: loss = 2.96189
Step 38880: loss = 2.89302
Step 38885: loss = 2.77043
Step 38890: loss = 2.86134
Step 38895: loss = 2.94032
Step 38900: loss = 2.96064
Step 38905: loss = 2.84746
Step 38910: loss = 2.75497
Step 38915: loss = 2.79090
Step 38920: loss = 2.71733
Step 38925: loss = 2.88168
Step 38930: loss = 2.83216
Step 38935: loss = 2.77889
Step 38940: loss = 2.83737
Step 38945: loss = 2.55731
Step 38950: loss = 2.67395
Step 38955: loss = 3.01102
Step 38960: loss = 2.66135
Step 38965: loss = 2.97443
Step 38970: loss = 2.62974
Step 38975: loss = 2.56014
Step 38980: loss = 2.92991
Step 38985: loss = 2.73621
Step 38990: loss = 2.86765
Step 38995: loss = 2.66846
Step 39000: loss = 2.84250
Training Data Eval:
  Num examples: 49920, Num correct: 6859, Precision @ 1: 0.1374
('Testing Data Eval: EPOCH->', 101)
  Num examples: 9984, Num correct: 1358, Precision @ 1: 0.1360
Step 39005: loss = 2.80415
Step 39010: loss = 2.80042
Step 39015: loss = 2.86525
Step 39020: loss = 3.18521
Step 39025: loss = 2.88539
Step 39030: loss = 2.72514
Step 39035: loss = 2.92607
Step 39040: loss = 3.00863
Step 39045: loss = 2.72424
Step 39050: loss = 2.93299
Step 39055: loss = 2.68157
Step 39060: loss = 2.97023
Step 39065: loss = 2.74898
Step 39070: loss = 2.77769
Step 39075: loss = 2.63890
Step 39080: loss = 2.66716
Step 39085: loss = 2.77666
Step 39090: loss = 2.75413
Step 39095: loss = 2.81054
Step 39100: loss = 2.62954
Step 39105: loss = 2.74508
Step 39110: loss = 2.79671
Step 39115: loss = 2.76318
Step 39120: loss = 2.75276
Step 39125: loss = 2.65211
Step 39130: loss = 2.83334
Step 39135: loss = 2.90908
Step 39140: loss = 2.77430
Step 39145: loss = 2.76934
Step 39150: loss = 2.83319
Step 39155: loss = 3.06373
Step 39160: loss = 2.82553
Step 39165: loss = 2.69819
Step 39170: loss = 2.54225
Step 39175: loss = 2.64847
Step 39180: loss = 2.73753
Step 39185: loss = 2.81316
Step 39190: loss = 2.89268
Step 39195: loss = 2.95068
Step 39200: loss = 2.84332
Step 39205: loss = 2.77915
Step 39210: loss = 2.72297
Step 39215: loss = 2.86869
Step 39220: loss = 2.65782
Step 39225: loss = 2.73613
Step 39230: loss = 2.56182
Step 39235: loss = 2.64140
Step 39240: loss = 2.86442
Step 39245: loss = 2.58547
Step 39250: loss = 2.73246
Step 39255: loss = 2.56498
Step 39260: loss = 2.87302
Step 39265: loss = 2.68836
Step 39270: loss = 2.75921
Step 39275: loss = 2.98845
Step 39280: loss = 2.85715
Step 39285: loss = 2.79924
Step 39290: loss = 2.84398
Step 39295: loss = 2.66435
Step 39300: loss = 2.89239
Step 39305: loss = 3.06565
Step 39310: loss = 2.77855
Step 39315: loss = 3.06733
Step 39320: loss = 2.98571
Step 39325: loss = 2.94392
Step 39330: loss = 2.75171
Step 39335: loss = 2.91793
Step 39340: loss = 2.80798
Step 39345: loss = 2.78107
Step 39350: loss = 2.76863
Step 39355: loss = 2.58749
Step 39360: loss = 2.64518
Step 39365: loss = 2.98258
Step 39370: loss = 2.94594
Step 39375: loss = 2.78948
Step 39380: loss = 2.63478
Step 39385: loss = 2.69212
Step 39390: loss = 2.69849
Training Data Eval:
  Num examples: 49920, Num correct: 6701, Precision @ 1: 0.1342
('Testing Data Eval: EPOCH->', 102)
  Num examples: 9984, Num correct: 1411, Precision @ 1: 0.1413
Step 39395: loss = 3.08766
Step 39400: loss = 2.82694
Step 39405: loss = 2.79429
Step 39410: loss = 2.79638
Step 39415: loss = 2.71740
Step 39420: loss = 2.92583
Step 39425: loss = 2.93652
Step 39430: loss = 2.56484
Step 39435: loss = 2.78043
Step 39440: loss = 2.82868
Step 39445: loss = 2.68186
Step 39450: loss = 2.99034
Step 39455: loss = 2.84872
Step 39460: loss = 2.90188
Step 39465: loss = 2.82259
Step 39470: loss = 2.83947
Step 39475: loss = 2.90680
Step 39480: loss = 2.90748
Step 39485: loss = 2.68886
Step 39490: loss = 2.75588
Step 39495: loss = 3.04977
Step 39500: loss = 2.96733
Step 39505: loss = 2.78404
Step 39510: loss = 2.75778
Step 39515: loss = 2.92693
Step 39520: loss = 2.77629
Step 39525: loss = 2.77970
Step 39530: loss = 2.66534
Step 39535: loss = 2.78055
Step 39540: loss = 2.89617
Step 39545: loss = 2.88426
Step 39550: loss = 2.66836
Step 39555: loss = 2.98419
Step 39560: loss = 2.96987
Step 39565: loss = 3.01394
Step 39570: loss = 2.78788
Step 39575: loss = 2.83913
Step 39580: loss = 2.64751
Step 39585: loss = 2.82953
Step 39590: loss = 2.63923
Step 39595: loss = 2.65297
Step 39600: loss = 2.75070
Step 39605: loss = 3.04393
Step 39610: loss = 3.06785
Step 39615: loss = 2.81560
Step 39620: loss = 2.60343
Step 39625: loss = 2.66942
Step 39630: loss = 2.71198
Step 39635: loss = 2.59679
Step 39640: loss = 2.72990
Step 39645: loss = 2.71335
Step 39650: loss = 2.69335
Step 39655: loss = 2.77089
Step 39660: loss = 2.74922
Step 39665: loss = 2.77915
Step 39670: loss = 2.88804
Step 39675: loss = 2.74246
Step 39680: loss = 2.85762
Step 39685: loss = 2.87074
Step 39690: loss = 2.61961
Step 39695: loss = 2.71911
Step 39700: loss = 2.64388
Step 39705: loss = 2.81756
Step 39710: loss = 2.80424
Step 39715: loss = 2.97061
Step 39720: loss = 2.78325
Step 39725: loss = 2.81567
Step 39730: loss = 2.41451
Step 39735: loss = 2.80832
Step 39740: loss = 2.63895
Step 39745: loss = 2.74874
Step 39750: loss = 3.11427
Step 39755: loss = 2.83881
Step 39760: loss = 2.66597
Step 39765: loss = 2.62871
Step 39770: loss = 2.64021
Step 39775: loss = 2.77464
Step 39780: loss = 2.75788
Training Data Eval:
  Num examples: 49920, Num correct: 6575, Precision @ 1: 0.1317
('Testing Data Eval: EPOCH->', 103)
  Num examples: 9984, Num correct: 1343, Precision @ 1: 0.1345
Step 39785: loss = 2.76876
Step 39790: loss = 2.70094
Step 39795: loss = 2.72744
Step 39800: loss = 2.75715
Step 39805: loss = 2.79663
Step 39810: loss = 2.89176
Step 39815: loss = 2.76726
Step 39820: loss = 2.83531
Step 39825: loss = 2.88908
Step 39830: loss = 2.71899
Step 39835: loss = 2.69142
Step 39840: loss = 2.55994
Step 39845: loss = 2.79148
Step 39850: loss = 2.99491
Step 39855: loss = 2.87506
Step 39860: loss = 2.85421
Step 39865: loss = 3.02439
Step 39870: loss = 2.93491
Step 39875: loss = 2.91897
Step 39880: loss = 2.74298
Step 39885: loss = 3.07363
Step 39890: loss = 2.65544
Step 39895: loss = 2.72239
Step 39900: loss = 2.90437
Step 39905: loss = 2.83229
Step 39910: loss = 2.69517
Step 39915: loss = 2.77618
Step 39920: loss = 3.15805
Step 39925: loss = 2.70395
Step 39930: loss = 2.96945
Step 39935: loss = 2.90318
Step 39940: loss = 2.71814
Step 39945: loss = 3.01964
Step 39950: loss = 3.01004
Step 39955: loss = 2.80724
Step 39960: loss = 2.59057
Step 39965: loss = 2.60708
Step 39970: loss = 2.79791
Step 39975: loss = 2.69701
Step 39980: loss = 3.09594
Step 39985: loss = 2.65354
Step 39990: loss = 2.82869
Step 39995: loss = 3.00787
Step 40000: loss = 2.82008
Step 40005: loss = 2.66700
Step 40010: loss = 2.77634
Step 40015: loss = 2.85920
Step 40020: loss = 2.88326
Step 40025: loss = 2.69461
Step 40030: loss = 2.65265
Step 40035: loss = 2.78532
Step 40040: loss = 2.68813
Step 40045: loss = 2.81680
Step 40050: loss = 2.62594
Step 40055: loss = 2.86434
Step 40060: loss = 2.55422
Step 40065: loss = 2.70904
Step 40070: loss = 2.81714
Step 40075: loss = 2.89948
Step 40080: loss = 2.68450
Step 40085: loss = 2.81938
Step 40090: loss = 2.80648
Step 40095: loss = 2.88857
Step 40100: loss = 2.77143
Step 40105: loss = 2.88094
Step 40110: loss = 2.73892
Step 40115: loss = 2.89653
Step 40120: loss = 2.87235
Step 40125: loss = 3.10075
Step 40130: loss = 2.63035
Step 40135: loss = 2.70174
Step 40140: loss = 2.85244
Step 40145: loss = 2.86185
Step 40150: loss = 2.71688
Step 40155: loss = 2.82097
Step 40160: loss = 2.88444
Step 40165: loss = 2.79967
Step 40170: loss = 3.09742
Training Data Eval:
  Num examples: 49920, Num correct: 6706, Precision @ 1: 0.1343
('Testing Data Eval: EPOCH->', 104)
  Num examples: 9984, Num correct: 1347, Precision @ 1: 0.1349
Step 40175: loss = 2.53662
Step 40180: loss = 2.79676
Step 40185: loss = 2.59615
Step 40190: loss = 2.69218
Step 40195: loss = 2.67245
Step 40200: loss = 2.78937
Step 40205: loss = 2.67845
Step 40210: loss = 2.69744
Step 40215: loss = 2.74432
Step 40220: loss = 2.64932
Step 40225: loss = 2.81193
Step 40230: loss = 2.92485
Step 40235: loss = 2.73001
Step 40240: loss = 2.82768
Step 40245: loss = 2.83529
Step 40250: loss = 2.81397
Step 40255: loss = 2.80398
Step 40260: loss = 2.98830
Step 40265: loss = 2.69455
Step 40270: loss = 2.76070
Step 40275: loss = 3.02895
Step 40280: loss = 2.84118
Step 40285: loss = 2.70802
Step 40290: loss = 2.88208
Step 40295: loss = 2.80124
Step 40300: loss = 2.84173
Step 40305: loss = 2.80821
Step 40310: loss = 2.73659
Step 40315: loss = 2.76054
Step 40320: loss = 2.67235
Step 40325: loss = 2.55315
Step 40330: loss = 2.69889
Step 40335: loss = 2.82569
Step 40340: loss = 2.80369
Step 40345: loss = 2.81019
Step 40350: loss = 2.91345
Step 40355: loss = 2.91220
Step 40360: loss = 2.78816
Step 40365: loss = 2.73669
Step 40370: loss = 3.17192
Step 40375: loss = 2.78288
Step 40380: loss = 2.79383
Step 40385: loss = 2.64309
Step 40390: loss = 2.74148
Step 40395: loss = 2.69358
Step 40400: loss = 2.87842
Step 40405: loss = 2.56413
Step 40410: loss = 3.10373
Step 40415: loss = 2.66299
Step 40420: loss = 2.67306
Step 40425: loss = 2.69231
Step 40430: loss = 2.76861
Step 40435: loss = 2.75671
Step 40440: loss = 3.05405
Step 40445: loss = 2.89240
Step 40450: loss = 2.90940
Step 40455: loss = 2.93920
Step 40460: loss = 2.92528
Step 40465: loss = 2.89960
Step 40470: loss = 2.88165
Step 40475: loss = 2.80515
Step 40480: loss = 2.80401
Step 40485: loss = 2.83925
Step 40490: loss = 2.98963
Step 40495: loss = 2.76971
Step 40500: loss = 2.74537
Step 40505: loss = 2.91511
Step 40510: loss = 2.64601
Step 40515: loss = 2.88360
Step 40520: loss = 2.78279
Step 40525: loss = 2.86713
Step 40530: loss = 2.88858
Step 40535: loss = 2.79520
Step 40540: loss = 3.14386
Step 40545: loss = 2.80566
Step 40550: loss = 2.94716
Step 40555: loss = 2.63066
Step 40560: loss = 2.78917
Training Data Eval:
  Num examples: 49920, Num correct: 6894, Precision @ 1: 0.1381
('Testing Data Eval: EPOCH->', 105)
  Num examples: 9984, Num correct: 1394, Precision @ 1: 0.1396
Step 40565: loss = 2.84099
Step 40570: loss = 2.67341
Step 40575: loss = 3.00032
Step 40580: loss = 2.85116
Step 40585: loss = 2.74404
Step 40590: loss = 2.82284
Step 40595: loss = 2.85913
Step 40600: loss = 2.79664
Step 40605: loss = 2.83657
Step 40610: loss = 2.83144
Step 40615: loss = 2.89525
Step 40620: loss = 2.56590
Step 40625: loss = 2.71296
Step 40630: loss = 2.83988
Step 40635: loss = 2.76461
Step 40640: loss = 2.83773
Step 40645: loss = 2.69414
Step 40650: loss = 2.78658
Step 40655: loss = 2.93150
Step 40660: loss = 2.79523
Step 40665: loss = 2.81340
Step 40670: loss = 2.68269
Step 40675: loss = 2.75839
Step 40680: loss = 2.70501
Step 40685: loss = 2.84162
Step 40690: loss = 2.90861
Step 40695: loss = 2.91300
Step 40700: loss = 2.87724
Step 40705: loss = 2.79247
Step 40710: loss = 2.87981
Step 40715: loss = 2.65622
Step 40720: loss = 2.89344
Step 40725: loss = 2.82219
Step 40730: loss = 2.97097
Step 40735: loss = 2.79952
Step 40740: loss = 2.62576
Step 40745: loss = 2.60086
Step 40750: loss = 2.72836
Step 40755: loss = 2.77528
Step 40760: loss = 2.74464
Step 40765: loss = 2.62959
Step 40770: loss = 3.16047
Step 40775: loss = 2.87203
Step 40780: loss = 2.65154
Step 40785: loss = 2.67676
Step 40790: loss = 2.84299
Step 40795: loss = 2.74432
Step 40800: loss = 2.94978
Step 40805: loss = 2.91487
Step 40810: loss = 2.79628
Step 40815: loss = 2.79131
Step 40820: loss = 2.75021
Step 40825: loss = 2.69973
Step 40830: loss = 2.87917
Step 40835: loss = 2.84134
Step 40840: loss = 2.80784
Step 40845: loss = 3.05361
Step 40850: loss = 2.96559
Step 40855: loss = 2.79630
Step 40860: loss = 2.55994
Step 40865: loss = 2.75253
Step 40870: loss = 2.63314
Step 40875: loss = 3.03950
Step 40880: loss = 2.68376
Step 40885: loss = 2.85153
Step 40890: loss = 2.84224
Step 40895: loss = 2.86728
Step 40900: loss = 2.78593
Step 40905: loss = 2.92551
Step 40910: loss = 2.86628
Step 40915: loss = 2.68780
Step 40920: loss = 3.04268
Step 40925: loss = 2.67491
Step 40930: loss = 2.85125
Step 40935: loss = 2.87127
Step 40940: loss = 2.83576
Step 40945: loss = 2.98836
Step 40950: loss = 2.97486
Training Data Eval:
  Num examples: 49920, Num correct: 7010, Precision @ 1: 0.1404
('Testing Data Eval: EPOCH->', 106)
  Num examples: 9984, Num correct: 1408, Precision @ 1: 0.1410
Step 40955: loss = 2.71604
Step 40960: loss = 2.80530
Step 40965: loss = 2.74016
Step 40970: loss = 3.01277
Step 40975: loss = 2.68613
Step 40980: loss = 2.80351
Step 40985: loss = 2.70724
Step 40990: loss = 2.87044
Step 40995: loss = 2.66215
Step 41000: loss = 2.63226
Step 41005: loss = 2.74788
Step 41010: loss = 2.74863
Step 41015: loss = 2.78787
Step 41020: loss = 2.81375
Step 41025: loss = 2.72403
Step 41030: loss = 2.70110
Step 41035: loss = 2.79335
Step 41040: loss = 2.99755
Step 41045: loss = 2.83510
Step 41050: loss = 2.73992
Step 41055: loss = 2.74425
Step 41060: loss = 2.90237
Step 41065: loss = 2.57874
Step 41070: loss = 2.84057
Step 41075: loss = 2.65180
Step 41080: loss = 2.84826
Step 41085: loss = 2.88503
Step 41090: loss = 2.90976
Step 41095: loss = 2.90234
Step 41100: loss = 2.79927
Step 41105: loss = 2.57018
Step 41110: loss = 2.77205
Step 41115: loss = 2.90727
Step 41120: loss = 2.74150
Step 41125: loss = 2.76105
Step 41130: loss = 2.80378
Step 41135: loss = 2.75898
Step 41140: loss = 2.62613
Step 41145: loss = 2.59332
Step 41150: loss = 2.81700
Step 41155: loss = 2.53994
Step 41160: loss = 2.89385
Step 41165: loss = 2.85646
Step 41170: loss = 2.79901
Step 41175: loss = 2.93215
Step 41180: loss = 2.89100
Step 41185: loss = 2.69213
Step 41190: loss = 2.67610
Step 41195: loss = 2.67934
Step 41200: loss = 2.73986
Step 41205: loss = 2.63455
Step 41210: loss = 2.60237
Step 41215: loss = 2.79684
Step 41220: loss = 2.69801
Step 41225: loss = 2.73191
Step 41230: loss = 2.96612
Step 41235: loss = 2.97534
Step 41240: loss = 2.72993
Step 41245: loss = 2.76651
Step 41250: loss = 2.87911
Step 41255: loss = 2.92392
Step 41260: loss = 2.92793
Step 41265: loss = 3.02227
Step 41270: loss = 2.68122
Step 41275: loss = 2.94348
Step 41280: loss = 2.90811
Step 41285: loss = 2.88453
Step 41290: loss = 2.72188
Step 41295: loss = 2.63535
Step 41300: loss = 2.64113
Step 41305: loss = 2.49373
Step 41310: loss = 2.66821
Step 41315: loss = 2.76853
Step 41320: loss = 2.93902
Step 41325: loss = 2.86118
Step 41330: loss = 2.84030
Step 41335: loss = 2.64607
Step 41340: loss = 2.81212
Training Data Eval:
  Num examples: 49920, Num correct: 6668, Precision @ 1: 0.1336
('Testing Data Eval: EPOCH->', 107)
  Num examples: 9984, Num correct: 1308, Precision @ 1: 0.1310
Step 41345: loss = 2.64532
Step 41350: loss = 2.79518
Step 41355: loss = 2.72490
Step 41360: loss = 2.58550
Step 41365: loss = 2.72187
Step 41370: loss = 2.72274
Step 41375: loss = 2.93284
Step 41380: loss = 3.01043
Step 41385: loss = 2.64191
Step 41390: loss = 2.92508
Step 41395: loss = 3.01522
Step 41400: loss = 2.91818
Step 41405: loss = 2.79045
Step 41410: loss = 2.55565
Step 41415: loss = 2.86881
Step 41420: loss = 2.75009
Step 41425: loss = 2.84034
Step 41430: loss = 2.72787
Step 41435: loss = 2.87576
Step 41440: loss = 2.75840
Step 41445: loss = 3.06076
Step 41450: loss = 2.76365
Step 41455: loss = 2.62142
Step 41460: loss = 2.73888
Step 41465: loss = 2.87129
Step 41470: loss = 2.97654
Step 41475: loss = 2.85023
Step 41480: loss = 2.58577
Step 41485: loss = 3.02066
Step 41490: loss = 2.97435
Step 41495: loss = 2.95858
Step 41500: loss = 2.77407
Step 41505: loss = 2.79031
Step 41510: loss = 2.87106
Step 41515: loss = 2.86643
Step 41520: loss = 2.86298
Step 41525: loss = 2.85930
Step 41530: loss = 2.88756
Step 41535: loss = 2.71341
Step 41540: loss = 2.59286
Step 41545: loss = 2.76864
Step 41550: loss = 2.77822
Step 41555: loss = 2.75336
Step 41560: loss = 2.89792
Step 41565: loss = 2.69392
Step 41570: loss = 2.81837
Step 41575: loss = 2.61475
Step 41580: loss = 2.93781
Step 41585: loss = 2.72102
Step 41590: loss = 2.70230
Step 41595: loss = 2.73816
Step 41600: loss = 2.53469
Step 41605: loss = 2.82960
Step 41610: loss = 2.70240
Step 41615: loss = 2.82121
Step 41620: loss = 2.81518
Step 41625: loss = 2.80227
Step 41630: loss = 2.80083
Step 41635: loss = 2.72126
Step 41640: loss = 2.61827
Step 41645: loss = 2.55966
Step 41650: loss = 2.81547
Step 41655: loss = 3.14262
Step 41660: loss = 2.86428
Step 41665: loss = 2.69537
Step 41670: loss = 2.72493
Step 41675: loss = 2.84522
Step 41680: loss = 2.77838
Step 41685: loss = 3.10769
Step 41690: loss = 2.95075
Step 41695: loss = 2.97478
Step 41700: loss = 2.57914
Step 41705: loss = 2.78775
Step 41710: loss = 2.68940
Step 41715: loss = 2.93510
Step 41720: loss = 2.92564
Step 41725: loss = 2.85209
Step 41730: loss = 2.77749
Training Data Eval:
  Num examples: 49920, Num correct: 6812, Precision @ 1: 0.1365
('Testing Data Eval: EPOCH->', 108)
  Num examples: 9984, Num correct: 1371, Precision @ 1: 0.1373
Step 41735: loss = 2.73891
Step 41740: loss = 2.72388
Step 41745: loss = 2.83756
Step 41750: loss = 2.83921
Step 41755: loss = 2.75926
Step 41760: loss = 2.81674
Step 41765: loss = 2.98165
Step 41770: loss = 2.83771
Step 41775: loss = 2.87132
Step 41780: loss = 2.63924
Step 41785: loss = 2.70433
Step 41790: loss = 2.74791
Step 41795: loss = 2.70163
Step 41800: loss = 2.94230
Step 41805: loss = 3.10490
Step 41810: loss = 2.87437
Step 41815: loss = 2.81915
Step 41820: loss = 2.42917
Step 41825: loss = 2.91898
Step 41830: loss = 2.74354
Step 41835: loss = 2.73262
Step 41840: loss = 2.83252
Step 41845: loss = 2.72295
Step 41850: loss = 2.85341
Step 41855: loss = 2.94911
Step 41860: loss = 2.71829
Step 41865: loss = 2.59509
Step 41870: loss = 2.85922
Step 41875: loss = 2.71059
Step 41880: loss = 2.73167
Step 41885: loss = 2.69658
Step 41890: loss = 2.92372
Step 41895: loss = 2.71704
Step 41900: loss = 2.73088
Step 41905: loss = 2.76625
Step 41910: loss = 2.62112
Step 41915: loss = 2.69109
Step 41920: loss = 2.87931
Step 41925: loss = 2.88752
Step 41930: loss = 2.84281
Step 41935: loss = 2.79960
Step 41940: loss = 2.78526
Step 41945: loss = 3.01674
Step 41950: loss = 2.84440
Step 41955: loss = 2.68927
Step 41960: loss = 2.66802
Step 41965: loss = 2.71824
Step 41970: loss = 2.60073
Step 41975: loss = 2.69599
Step 41980: loss = 2.74253
Step 41985: loss = 2.96117
Step 41990: loss = 2.94531
Step 41995: loss = 2.75971
Step 42000: loss = 2.87971
Step 42005: loss = 2.87090
Step 42010: loss = 2.62052
Step 42015: loss = 2.82322
Step 42020: loss = 2.57348
Step 42025: loss = 2.85355
Step 42030: loss = 2.82131
Step 42035: loss = 2.50586
Step 42040: loss = 2.66299
Step 42045: loss = 2.69328
Step 42050: loss = 3.01063
Step 42055: loss = 2.75642
Step 42060: loss = 2.90345
Step 42065: loss = 2.78963
Step 42070: loss = 2.67591
Step 42075: loss = 3.03260
Step 42080: loss = 2.81612
Step 42085: loss = 2.86749
Step 42090: loss = 2.90225
Step 42095: loss = 2.94760
Step 42100: loss = 2.82003
Step 42105: loss = 2.79810
Step 42110: loss = 2.82828
Step 42115: loss = 2.79943
Step 42120: loss = 2.80840
Training Data Eval:
  Num examples: 49920, Num correct: 6912, Precision @ 1: 0.1385
('Testing Data Eval: EPOCH->', 109)
  Num examples: 9984, Num correct: 1391, Precision @ 1: 0.1393
Step 42125: loss = 2.81277
Step 42130: loss = 2.77173
Step 42135: loss = 2.79945
Step 42140: loss = 2.58140
Step 42145: loss = 2.43921
Step 42150: loss = 2.79836
Step 42155: loss = 2.82896
Step 42160: loss = 2.58502
Step 42165: loss = 2.87404
Step 42170: loss = 2.79829
Step 42175: loss = 2.61640
Step 42180: loss = 2.78446
Step 42185: loss = 2.73219
Step 42190: loss = 2.75367
Step 42195: loss = 2.98863
Step 42200: loss = 2.70396
Step 42205: loss = 2.86805
Step 42210: loss = 2.81315
Step 42215: loss = 2.71003
Step 42220: loss = 2.80586
Step 42225: loss = 3.11428
Step 42230: loss = 2.68941
Step 42235: loss = 2.91846
Step 42240: loss = 2.78666
Step 42245: loss = 2.63720
Step 42250: loss = 2.64525
Step 42255: loss = 2.82997
Step 42260: loss = 2.76923
Step 42265: loss = 2.72052
Step 42270: loss = 2.86069
Step 42275: loss = 2.91460
Step 42280: loss = 2.88583
Step 42285: loss = 2.62060
Step 42290: loss = 2.75396
Step 42295: loss = 2.58547
Step 42300: loss = 2.86303
Step 42305: loss = 3.01706
Step 42310: loss = 3.03878
Step 42315: loss = 2.86678
Step 42320: loss = 2.73501
Step 42325: loss = 2.64455
Step 42330: loss = 2.65981
Step 42335: loss = 2.72956
Step 42340: loss = 2.90719
Step 42345: loss = 3.08057
Step 42350: loss = 2.63207
Step 42355: loss = 2.79271
Step 42360: loss = 2.70370
Step 42365: loss = 2.85018
Step 42370: loss = 2.83712
Step 42375: loss = 2.98283
Step 42380: loss = 2.73363
Step 42385: loss = 2.78616
Step 42390: loss = 2.62821
Step 42395: loss = 3.04538
Step 42400: loss = 2.96184
Step 42405: loss = 2.91591
Step 42410: loss = 2.64213
Step 42415: loss = 2.95775
Step 42420: loss = 2.84474
Step 42425: loss = 2.80602
Step 42430: loss = 2.70759
Step 42435: loss = 2.69041
Step 42440: loss = 2.67980
Step 42445: loss = 2.82901
Step 42450: loss = 2.85713
Step 42455: loss = 2.93202
Step 42460: loss = 2.78957
Step 42465: loss = 2.66784
Step 42470: loss = 2.84523
Step 42475: loss = 2.60424
Step 42480: loss = 2.84324
Step 42485: loss = 2.72158
Step 42490: loss = 2.75576
Step 42495: loss = 2.76634
Step 42500: loss = 2.63025
Step 42505: loss = 2.80090
Step 42510: loss = 2.81801
Training Data Eval:
  Num examples: 49920, Num correct: 6838, Precision @ 1: 0.1370
('Testing Data Eval: EPOCH->', 110)
  Num examples: 9984, Num correct: 1331, Precision @ 1: 0.1333
Step 42515: loss = 2.58797
Step 42520: loss = 2.67965
Step 42525: loss = 2.65633
Step 42530: loss = 2.72024
Step 42535: loss = 2.69925
Step 42540: loss = 2.83854
Step 42545: loss = 2.96863
Step 42550: loss = 2.93931
Step 42555: loss = 2.64957
Step 42560: loss = 2.54203
Step 42565: loss = 2.76786
Step 42570: loss = 2.70084
Step 42575: loss = 2.65847
Step 42580: loss = 2.69120
Step 42585: loss = 2.85811
Step 42590: loss = 2.83848
Step 42595: loss = 2.90606
Step 42600: loss = 2.82795
Step 42605: loss = 2.56776
Step 42610: loss = 2.60029
Step 42615: loss = 3.04236
Step 42620: loss = 2.90842
Step 42625: loss = 2.94960
Step 42630: loss = 2.50309
Step 42635: loss = 2.80294
Step 42640: loss = 2.73150
Step 42645: loss = 2.92448
Step 42650: loss = 2.87416
Step 42655: loss = 3.00259
Step 42660: loss = 2.64276
Step 42665: loss = 2.68427
Step 42670: loss = 2.70249
Step 42675: loss = 3.00721
Step 42680: loss = 2.79030
Step 42685: loss = 2.72953
Step 42690: loss = 2.78160
Step 42695: loss = 2.79135
Step 42700: loss = 2.78125
Step 42705: loss = 2.67932
Step 42710: loss = 2.75776
Step 42715: loss = 2.62618
Step 42720: loss = 2.87137
Step 42725: loss = 2.80525
Step 42730: loss = 3.24077
Step 42735: loss = 2.69337
Step 42740: loss = 2.88522
Step 42745: loss = 2.91887
Step 42750: loss = 2.63275
Step 42755: loss = 2.75844
Step 42760: loss = 2.82291
Step 42765: loss = 2.86363
Step 42770: loss = 2.95034
Step 42775: loss = 2.80903
Step 42780: loss = 2.99517
Step 42785: loss = 2.87007
Step 42790: loss = 2.67610
Step 42795: loss = 2.69724
Step 42800: loss = 2.70319
Step 42805: loss = 2.85408
Step 42810: loss = 2.81761
Step 42815: loss = 2.71512
Step 42820: loss = 2.95435
Step 42825: loss = 2.98342
Step 42830: loss = 2.94025
Step 42835: loss = 2.73756
Step 42840: loss = 2.80668
Step 42845: loss = 2.95403
Step 42850: loss = 2.92758
Step 42855: loss = 2.68439
Step 42860: loss = 2.63533
Step 42865: loss = 2.86230
Step 42870: loss = 2.74389
Step 42875: loss = 2.55686
Step 42880: loss = 2.99565
Step 42885: loss = 2.88981
Step 42890: loss = 2.90130
Step 42895: loss = 2.92442
Step 42900: loss = 2.49444
Training Data Eval:
  Num examples: 49920, Num correct: 6841, Precision @ 1: 0.1370
('Testing Data Eval: EPOCH->', 111)
  Num examples: 9984, Num correct: 1389, Precision @ 1: 0.1391
Step 42905: loss = 2.85500
Step 42910: loss = 2.72217
Step 42915: loss = 2.87073
Step 42920: loss = 3.11605
Step 42925: loss = 2.60615
Step 42930: loss = 2.61323
Step 42935: loss = 2.89184
Step 42940: loss = 2.88447
Step 42945: loss = 2.85477
Step 42950: loss = 2.70007
Step 42955: loss = 2.79549
Step 42960: loss = 2.87212
Step 42965: loss = 2.87759
Step 42970: loss = 2.81850
Step 42975: loss = 2.86036
Step 42980: loss = 2.87766
Step 42985: loss = 2.63471
Step 42990: loss = 2.88513
Step 42995: loss = 2.84454
Step 43000: loss = 2.83302
Step 43005: loss = 2.81146
Step 43010: loss = 2.80442
Step 43015: loss = 2.94292
Step 43020: loss = 2.72383
Step 43025: loss = 2.80623
Step 43030: loss = 2.70387
Step 43035: loss = 2.79170
Step 43040: loss = 2.94841
Step 43045: loss = 2.85316
Step 43050: loss = 2.71581
Step 43055: loss = 2.65143
Step 43060: loss = 2.66295
Step 43065: loss = 2.70611
Step 43070: loss = 2.81767
Step 43075: loss = 2.63959
Step 43080: loss = 2.82485
Step 43085: loss = 2.77045
Step 43090: loss = 2.85473
Step 43095: loss = 2.82607
Step 43100: loss = 2.80355
Step 43105: loss = 3.11665
Step 43110: loss = 2.82580
Step 43115: loss = 2.94103
Step 43120: loss = 2.81434
Step 43125: loss = 2.95227
Step 43130: loss = 2.58783
Step 43135: loss = 2.81733
Step 43140: loss = 3.01237
Step 43145: loss = 2.77087
Step 43150: loss = 2.88629
Step 43155: loss = 2.47261
Step 43160: loss = 2.60827
Step 43165: loss = 2.96290
Step 43170: loss = 2.55334
Step 43175: loss = 2.81032
Step 43180: loss = 2.78554
Step 43185: loss = 2.59627
Step 43190: loss = 2.85182
Step 43195: loss = 2.95029
Step 43200: loss = 3.13464
Step 43205: loss = 3.08268
Step 43210: loss = 2.93838
Step 43215: loss = 2.72056
Step 43220: loss = 3.07106
Step 43225: loss = 2.66442
Step 43230: loss = 2.72783
Step 43235: loss = 2.61526
Step 43240: loss = 2.84959
Step 43245: loss = 2.75932
Step 43250: loss = 2.76054
Step 43255: loss = 2.65130
Step 43260: loss = 3.04463
Step 43265: loss = 2.68275
Step 43270: loss = 2.81357
Step 43275: loss = 2.80721
Step 43280: loss = 2.80992
Step 43285: loss = 2.95492
Step 43290: loss = 2.69463
Training Data Eval:
  Num examples: 49920, Num correct: 6845, Precision @ 1: 0.1371
('Testing Data Eval: EPOCH->', 112)
  Num examples: 9984, Num correct: 1385, Precision @ 1: 0.1387
Step 43295: loss = 2.69244
Step 43300: loss = 2.89759
Step 43305: loss = 2.91863
Step 43310: loss = 2.74716
Step 43315: loss = 2.97439
Step 43320: loss = 2.66759
Step 43325: loss = 2.79607
Step 43330: loss = 2.89344
Step 43335: loss = 2.77471
Step 43340: loss = 3.12285
Step 43345: loss = 2.72774
Step 43350: loss = 2.76484
Step 43355: loss = 2.96281
Step 43360: loss = 2.64966
Step 43365: loss = 2.74246
Step 43370: loss = 2.98761
Step 43375: loss = 2.90081
Step 43380: loss = 2.65852
Step 43385: loss = 2.87247
Step 43390: loss = 2.72011
Step 43395: loss = 2.81878
Step 43400: loss = 2.79539
Step 43405: loss = 3.05144
Step 43410: loss = 3.01943
Step 43415: loss = 2.90028
Step 43420: loss = 2.86981
Step 43425: loss = 2.73286
Step 43430: loss = 2.85304
Step 43435: loss = 3.15292
Step 43440: loss = 2.98994
Step 43445: loss = 3.03938
Step 43450: loss = 2.90968
Step 43455: loss = 2.75034
Step 43460: loss = 2.60332
Step 43465: loss = 2.87700
Step 43470: loss = 2.88355
Step 43475: loss = 2.75820
Step 43480: loss = 2.68534
Step 43485: loss = 2.68718
Step 43490: loss = 2.82029
Step 43495: loss = 2.80518
Step 43500: loss = 2.96606
Step 43505: loss = 2.95418
Step 43510: loss = 2.58336
Step 43515: loss = 2.80676
Step 43520: loss = 2.76658
Step 43525: loss = 2.78546
Step 43530: loss = 3.00512
Step 43535: loss = 3.27609
Step 43540: loss = 2.82655
Step 43545: loss = 3.08918
Step 43550: loss = 2.76679
Step 43555: loss = 2.62533
Step 43560: loss = 2.60805
Step 43565: loss = 2.79549
Step 43570: loss = 2.73647
Step 43575: loss = 2.78225
Step 43580: loss = 2.62955
Step 43585: loss = 2.68972
Step 43590: loss = 2.79018
Step 43595: loss = 2.77021
Step 43600: loss = 2.69259
Step 43605: loss = 2.85848
Step 43610: loss = 2.89576
Step 43615: loss = 2.93683
Step 43620: loss = 2.76091
Step 43625: loss = 2.82337
Step 43630: loss = 3.03023
Step 43635: loss = 2.89419
Step 43640: loss = 2.83687
Step 43645: loss = 2.93206
Step 43650: loss = 3.11361
Step 43655: loss = 2.97632
Step 43660: loss = 3.08447
Step 43665: loss = 2.87057
Step 43670: loss = 2.94883
Step 43675: loss = 2.76825
Step 43680: loss = 2.78490
Training Data Eval:
  Num examples: 49920, Num correct: 6720, Precision @ 1: 0.1346
('Testing Data Eval: EPOCH->', 113)
  Num examples: 9984, Num correct: 1287, Precision @ 1: 0.1289
Step 43685: loss = 2.62306
Step 43690: loss = 2.87285
Step 43695: loss = 2.71000
Step 43700: loss = 2.71186
Step 43705: loss = 2.68545
Step 43710: loss = 2.67193
Step 43715: loss = 2.47672
Step 43720: loss = 2.69159
Step 43725: loss = 2.91770
Step 43730: loss = 2.80773
Step 43735: loss = 2.68188
Step 43740: loss = 2.70556
Step 43745: loss = 2.83646
Step 43750: loss = 2.71628
Step 43755: loss = 2.90137
Step 43760: loss = 2.78465
Step 43765: loss = 2.78492
Step 43770: loss = 2.87153
Step 43775: loss = 2.76532
Step 43780: loss = 2.61315
Step 43785: loss = 2.84523
Step 43790: loss = 2.76070
Step 43795: loss = 2.60244
Step 43800: loss = 2.73362
Step 43805: loss = 2.70932
Step 43810: loss = 2.88326
Step 43815: loss = 2.81176
Step 43820: loss = 2.82356
Step 43825: loss = 2.84963
Step 43830: loss = 2.93366
Step 43835: loss = 2.85827
Step 43840: loss = 2.80068
Step 43845: loss = 2.67380
Step 43850: loss = 2.69209
Step 43855: loss = 2.92238
Step 43860: loss = 2.73319
Step 43865: loss = 2.98146
Step 43870: loss = 2.69207
Step 43875: loss = 2.65957
Step 43880: loss = 2.78562
Step 43885: loss = 2.77929
Step 43890: loss = 2.72400
Step 43895: loss = 2.81048
Step 43900: loss = 2.90252
Step 43905: loss = 2.64704
Step 43910: loss = 2.82162
Step 43915: loss = 2.80920
Step 43920: loss = 2.83043
Step 43925: loss = 2.89957
Step 43930: loss = 2.61023
Step 43935: loss = 2.76539
Step 43940: loss = 2.77040
Step 43945: loss = 2.73493
Step 43950: loss = 2.85367
Step 43955: loss = 2.59504
Step 43960: loss = 2.80505
Step 43965: loss = 2.62625
Step 43970: loss = 2.74001
Step 43975: loss = 2.72109
Step 43980: loss = 2.84954
Step 43985: loss = 2.89844
Step 43990: loss = 2.72576
Step 43995: loss = 2.88476
Step 44000: loss = 2.84864
Step 44005: loss = 2.82153
Step 44010: loss = 2.46179
Step 44015: loss = 2.77882
Step 44020: loss = 2.85345
Step 44025: loss = 2.81660
Step 44030: loss = 2.54403
Step 44035: loss = 2.71871
Step 44040: loss = 3.01272
Step 44045: loss = 2.93093
Step 44050: loss = 3.00701
Step 44055: loss = 2.92694
Step 44060: loss = 2.71704
Step 44065: loss = 2.82489
Step 44070: loss = 2.97070
Training Data Eval:
  Num examples: 49920, Num correct: 6518, Precision @ 1: 0.1306
('Testing Data Eval: EPOCH->', 114)
  Num examples: 9984, Num correct: 1333, Precision @ 1: 0.1335
Step 44075: loss = 2.51455
Step 44080: loss = 2.90535
Step 44085: loss = 2.68494
Step 44090: loss = 3.01184
Step 44095: loss = 2.69619
Step 44100: loss = 2.66552
Step 44105: loss = 2.79896
Step 44110: loss = 2.92913
Step 44115: loss = 2.91770
Step 44120: loss = 2.75207
Step 44125: loss = 2.68237
Step 44130: loss = 2.80278
Step 44135: loss = 2.68449
Step 44140: loss = 2.77298
Step 44145: loss = 2.89438
Step 44150: loss = 2.83075
Step 44155: loss = 2.86657
Step 44160: loss = 2.95862
Step 44165: loss = 2.75716
Step 44170: loss = 2.92221
Step 44175: loss = 3.02020
Step 44180: loss = 3.06705
Step 44185: loss = 2.69206
Step 44190: loss = 2.81491
Step 44195: loss = 3.09780
Step 44200: loss = 3.24399
Step 44205: loss = 2.95442
Step 44210: loss = 2.81594
Step 44215: loss = 2.79769
Step 44220: loss = 2.69020
Step 44225: loss = 2.78405
Step 44230: loss = 2.81702
Step 44235: loss = 2.69360
Step 44240: loss = 2.93503
Step 44245: loss = 2.74697
Step 44250: loss = 2.85125
Step 44255: loss = 2.89828
Step 44260: loss = 2.66309
Step 44265: loss = 2.57822
Step 44270: loss = 2.55902
Step 44275: loss = 2.71115
Step 44280: loss = 2.67957
Step 44285: loss = 2.54729
Step 44290: loss = 2.71457
Step 44295: loss = 2.76551
Step 44300: loss = 2.74155
Step 44305: loss = 2.77759
Step 44310: loss = 2.57656
Step 44315: loss = 2.86280
Step 44320: loss = 2.84462
Step 44325: loss = 2.70342
Step 44330: loss = 2.95369
Step 44335: loss = 2.79772
Step 44340: loss = 2.98462
Step 44345: loss = 2.80591
Step 44350: loss = 2.91766
Step 44355: loss = 2.69481
Step 44360: loss = 2.76175
Step 44365: loss = 2.90703
Step 44370: loss = 2.62196
Step 44375: loss = 3.10791
Step 44380: loss = 2.73749
Step 44385: loss = 2.73006
Step 44390: loss = 2.93190
Step 44395: loss = 2.81516
Step 44400: loss = 2.88985
Step 44405: loss = 2.66641
Step 44410: loss = 2.69538
Step 44415: loss = 2.65584
Step 44420: loss = 2.60737
Step 44425: loss = 2.73525
Step 44430: loss = 2.74248
Step 44435: loss = 2.92229
Step 44440: loss = 2.68410
Step 44445: loss = 2.94681
Step 44450: loss = 2.76526
Step 44455: loss = 2.73136
Step 44460: loss = 2.77966
Training Data Eval:
  Num examples: 49920, Num correct: 6803, Precision @ 1: 0.1363
('Testing Data Eval: EPOCH->', 115)
  Num examples: 9984, Num correct: 1403, Precision @ 1: 0.1405
Step 44465: loss = 2.66035
Step 44470: loss = 2.73653
Step 44475: loss = 2.74034
Step 44480: loss = 2.78263
Step 44485: loss = 2.66112
Step 44490: loss = 2.71889
Step 44495: loss = 2.84547
Step 44500: loss = 2.97661
Step 44505: loss = 2.91516
Step 44510: loss = 2.78823
Step 44515: loss = 2.79461
Step 44520: loss = 2.81341
Step 44525: loss = 2.71784
Step 44530: loss = 2.78548
Step 44535: loss = 2.70670
Step 44540: loss = 2.90191
Step 44545: loss = 3.04085
Step 44550: loss = 2.58612
Step 44555: loss = 2.78733
Step 44560: loss = 2.77539
Step 44565: loss = 2.79336
Step 44570: loss = 2.59534
Step 44575: loss = 2.66264
Step 44580: loss = 2.89876
Step 44585: loss = 2.82426
Step 44590: loss = 2.81240
Step 44595: loss = 2.90452
Step 44600: loss = 2.82343
Step 44605: loss = 2.86335
Step 44610: loss = 2.88016
Step 44615: loss = 2.87751
Step 44620: loss = 2.77910
Step 44625: loss = 2.77342
Step 44630: loss = 2.69468
Step 44635: loss = 2.65901
Step 44640: loss = 2.68880
Step 44645: loss = 2.78178
Step 44650: loss = 2.78134
Step 44655: loss = 2.68214
Step 44660: loss = 2.67022
Step 44665: loss = 2.62245
Step 44670: loss = 2.70856
Step 44675: loss = 2.73550
Step 44680: loss = 2.73880
Step 44685: loss = 2.85447
Step 44690: loss = 2.74513
Step 44695: loss = 2.93024
Step 44700: loss = 2.62071
Step 44705: loss = 2.80944
Step 44710: loss = 2.57651
Step 44715: loss = 2.76434
Step 44720: loss = 2.67379
Step 44725: loss = 2.55739
Step 44730: loss = 3.07381
Step 44735: loss = 2.83932
Step 44740: loss = 2.87838
Step 44745: loss = 2.64050
Step 44750: loss = 2.78597
Step 44755: loss = 2.91324
Step 44760: loss = 2.90638
Step 44765: loss = 2.91464
Step 44770: loss = 2.95271
Step 44775: loss = 2.67889
Step 44780: loss = 3.05880
Step 44785: loss = 2.88899
Step 44790: loss = 2.75788
Step 44795: loss = 2.83655
Step 44800: loss = 2.62552
Step 44805: loss = 2.82393
Step 44810: loss = 2.83474
Step 44815: loss = 2.84800
Step 44820: loss = 2.84160
Step 44825: loss = 2.87338
Step 44830: loss = 2.76847
Step 44835: loss = 2.66855
Step 44840: loss = 2.68275
Step 44845: loss = 2.73366
Step 44850: loss = 2.73812
Training Data Eval:
  Num examples: 49920, Num correct: 6884, Precision @ 1: 0.1379
('Testing Data Eval: EPOCH->', 116)
  Num examples: 9984, Num correct: 1354, Precision @ 1: 0.1356
Step 44855: loss = 3.27407
Step 44860: loss = 2.85592
Step 44865: loss = 2.65721
Step 44870: loss = 2.92580
Step 44875: loss = 2.70421
Step 44880: loss = 2.71187
Step 44885: loss = 2.92876
Step 44890: loss = 2.87372
Step 44895: loss = 2.89183
Step 44900: loss = 2.85220
Step 44905: loss = 2.82666
Step 44910: loss = 2.81407
Step 44915: loss = 2.89890
Step 44920: loss = 2.74885
Step 44925: loss = 2.64148
Step 44930: loss = 2.58052
Step 44935: loss = 2.64514
Step 44940: loss = 2.70308
Step 44945: loss = 2.62330
Step 44950: loss = 2.75309
Step 44955: loss = 2.74045
Step 44960: loss = 2.72081
Step 44965: loss = 2.83564
Step 44970: loss = 2.79039
Step 44975: loss = 2.79969
Step 44980: loss = 2.68600
Step 44985: loss = 2.97888
Step 44990: loss = 2.71766
Step 44995: loss = 2.67561
Step 45000: loss = 2.90805
Step 45005: loss = 2.64866
Step 45010: loss = 2.85004
Step 45015: loss = 2.74268
Step 45020: loss = 2.73161
Step 45025: loss = 2.76006
Step 45030: loss = 2.71001
Step 45035: loss = 2.88844
Step 45040: loss = 2.75437
Step 45045: loss = 2.89022
Step 45050: loss = 2.82930
Step 45055: loss = 2.85250
Step 45060: loss = 2.71033
Step 45065: loss = 2.55986
Step 45070: loss = 2.87676
Step 45075: loss = 3.01326
Step 45080: loss = 2.77959
Step 45085: loss = 2.69231
Step 45090: loss = 2.85393
Step 45095: loss = 2.82115
Step 45100: loss = 2.78723
Step 45105: loss = 2.68177
Step 45110: loss = 2.63178
Step 45115: loss = 2.88335
Step 45120: loss = 2.73399
Step 45125: loss = 2.64443
Step 45130: loss = 2.66354
Step 45135: loss = 2.55464
Step 45140: loss = 2.92640
Step 45145: loss = 2.56302
Step 45150: loss = 3.01714
Step 45155: loss = 2.78047
Step 45160: loss = 2.77531
Step 45165: loss = 3.00000
Step 45170: loss = 2.87921
Step 45175: loss = 2.69902
Step 45180: loss = 2.74468
Step 45185: loss = 2.73054
Step 45190: loss = 2.87070
Step 45195: loss = 2.54169
Step 45200: loss = 2.86038
Step 45205: loss = 2.72807
Step 45210: loss = 2.71865
Step 45215: loss = 3.05486
Step 45220: loss = 2.81265
Step 45225: loss = 2.77545
Step 45230: loss = 2.87109
Step 45235: loss = 2.78681
Step 45240: loss = 2.80439
Training Data Eval:
  Num examples: 49920, Num correct: 6738, Precision @ 1: 0.1350
('Testing Data Eval: EPOCH->', 117)
  Num examples: 9984, Num correct: 1248, Precision @ 1: 0.1250
Step 45245: loss = 2.49994
Step 45250: loss = 2.95033
Step 45255: loss = 2.81640
Step 45260: loss = 2.69604
Step 45265: loss = 2.75155
Step 45270: loss = 2.90960
Step 45275: loss = 2.87587
Step 45280: loss = 2.59791
Step 45285: loss = 2.89718
Step 45290: loss = 2.78025
Step 45295: loss = 2.79301
Step 45300: loss = 2.84660
Step 45305: loss = 2.69538
Step 45310: loss = 2.74396
Step 45315: loss = 2.70125
Step 45320: loss = 2.81077
Step 45325: loss = 2.75114
Step 45330: loss = 2.63291
Step 45335: loss = 2.76718
Step 45340: loss = 2.78738
Step 45345: loss = 2.72546
Step 45350: loss = 2.74102
Step 45355: loss = 2.61722
Step 45360: loss = 2.92634
Step 45365: loss = 2.78670
Step 45370: loss = 2.59396
Step 45375: loss = 2.86183
Step 45380: loss = 2.71057
Step 45385: loss = 2.83837
Step 45390: loss = 2.88294
Step 45395: loss = 2.82784
Step 45400: loss = 2.74021
Step 45405: loss = 2.73979
Step 45410: loss = 2.78490
Step 45415: loss = 2.81122
Step 45420: loss = 2.75793
Step 45425: loss = 2.73717
Step 45430: loss = 2.85186
Step 45435: loss = 2.85168
Step 45440: loss = 2.91914
Step 45445: loss = 2.77721
Step 45450: loss = 2.63337
Step 45455: loss = 2.88799
Step 45460: loss = 2.68756
Step 45465: loss = 2.98113
Step 45470: loss = 2.67725
Step 45475: loss = 2.76935
Step 45480: loss = 2.92531
Step 45485: loss = 2.79375
Step 45490: loss = 2.65405
Step 45495: loss = 2.83371
Step 45500: loss = 2.79351
Step 45505: loss = 2.76336
Step 45510: loss = 2.77295
Step 45515: loss = 2.85347
Step 45520: loss = 2.72752
Step 45525: loss = 2.80018
Step 45530: loss = 3.00439
Step 45535: loss = 2.72468
Step 45540: loss = 2.94702
Step 45545: loss = 3.06887
Step 45550: loss = 2.92419
Step 45555: loss = 2.88514
Step 45560: loss = 2.54927
Step 45565: loss = 2.54597
Step 45570: loss = 2.68003
Step 45575: loss = 2.80076
Step 45580: loss = 2.99340
Step 45585: loss = 2.89891
Step 45590: loss = 2.53871
Step 45595: loss = 2.71838
Step 45600: loss = 2.97947
Step 45605: loss = 2.84013
Step 45610: loss = 2.72983
Step 45615: loss = 2.78398
Step 45620: loss = 2.92575
Step 45625: loss = 2.85986
Step 45630: loss = 2.77520
Training Data Eval:
  Num examples: 49920, Num correct: 6836, Precision @ 1: 0.1369
('Testing Data Eval: EPOCH->', 118)
  Num examples: 9984, Num correct: 1414, Precision @ 1: 0.1416
Step 45635: loss = 2.83318
Step 45640: loss = 2.54816
Step 45645: loss = 2.90064
Step 45650: loss = 2.77332
Step 45655: loss = 2.83368
Step 45660: loss = 3.05915
Step 45665: loss = 2.95671
Step 45670: loss = 2.72364
Step 45675: loss = 2.78978
Step 45680: loss = 2.71355
Step 45685: loss = 2.72713
Step 45690: loss = 2.68414
Step 45695: loss = 2.64227
Step 45700: loss = 2.78762
Step 45705: loss = 2.69489
Step 45710: loss = 2.82254
Step 45715: loss = 2.92626
Step 45720: loss = 2.83548
Step 45725: loss = 2.88621
Step 45730: loss = 3.02569
Step 45735: loss = 3.01099
Step 45740: loss = 2.93658
Step 45745: loss = 2.95458
Step 45750: loss = 2.72963
Step 45755: loss = 2.72360
Step 45760: loss = 2.70232
Step 45765: loss = 2.87131
Step 45770: loss = 2.87161
Step 45775: loss = 2.79002
Step 45780: loss = 2.60811
Step 45785: loss = 2.84200
Step 45790: loss = 2.61824
Step 45795: loss = 2.98419
Step 45800: loss = 2.59650
Step 45805: loss = 2.75901
Step 45810: loss = 2.82671
Step 45815: loss = 2.83464
Step 45820: loss = 2.81146
Step 45825: loss = 2.47927
Step 45830: loss = 2.62950
Step 45835: loss = 2.55069
Step 45840: loss = 3.04941
Step 45845: loss = 2.88956
Step 45850: loss = 2.72633
Step 45855: loss = 2.73021
Step 45860: loss = 2.87004
Step 45865: loss = 2.90089
Step 45870: loss = 3.04431
Step 45875: loss = 2.70206
Step 45880: loss = 2.63865
Step 45885: loss = 2.81180
Step 45890: loss = 2.63076
Step 45895: loss = 2.82001
Step 45900: loss = 2.53868
Step 45905: loss = 2.58470
Step 45910: loss = 2.76671
Step 45915: loss = 2.94456
Step 45920: loss = 2.80294
Step 45925: loss = 2.98450
Step 45930: loss = 2.83918
Step 45935: loss = 2.74401
Step 45940: loss = 2.75664
Step 45945: loss = 2.70010
Step 45950: loss = 2.79055
Step 45955: loss = 2.83210
Step 45960: loss = 2.87223
Step 45965: loss = 2.71186
Step 45970: loss = 2.74964
Step 45975: loss = 2.69930
Step 45980: loss = 2.74762
Step 45985: loss = 2.70007
Step 45990: loss = 2.89596
Step 45995: loss = 2.83621
Step 46000: loss = 2.86678
Step 46005: loss = 2.51681
Step 46010: loss = 2.73762
Step 46015: loss = 2.75815
Step 46020: loss = 2.93118
Training Data Eval:
  Num examples: 49920, Num correct: 6919, Precision @ 1: 0.1386
('Testing Data Eval: EPOCH->', 119)
  Num examples: 9984, Num correct: 1397, Precision @ 1: 0.1399
Step 46025: loss = 2.87759
Step 46030: loss = 2.73458
Step 46035: loss = 2.76650
Step 46040: loss = 2.78020
Step 46045: loss = 2.66559
Step 46050: loss = 2.83646
Step 46055: loss = 3.00685
Step 46060: loss = 2.86060
Step 46065: loss = 2.64011
Step 46070: loss = 2.74197
Step 46075: loss = 2.92219
Step 46080: loss = 3.02972
Step 46085: loss = 2.81728
Step 46090: loss = 2.74135
Step 46095: loss = 3.05687
Step 46100: loss = 2.84142
Step 46105: loss = 2.79662
Step 46110: loss = 2.58822
Step 46115: loss = 2.68247
Step 46120: loss = 2.70741
Step 46125: loss = 2.68961
Step 46130: loss = 2.70507
Step 46135: loss = 2.81799
Step 46140: loss = 2.52645
Step 46145: loss = 2.86213
Step 46150: loss = 2.65881
Step 46155: loss = 2.83646
Step 46160: loss = 2.90471
Step 46165: loss = 2.78342
Step 46170: loss = 2.91932
Step 46175: loss = 2.96396
Step 46180: loss = 2.81984
Step 46185: loss = 2.95413
Step 46190: loss = 2.97914
Step 46195: loss = 2.88969
Step 46200: loss = 2.86164
Step 46205: loss = 2.80175
Step 46210: loss = 2.88999
Step 46215: loss = 2.75509
Step 46220: loss = 3.00590
Step 46225: loss = 3.04819
Step 46230: loss = 2.99643
Step 46235: loss = 2.88123
Step 46240: loss = 2.73377
Step 46245: loss = 2.68931
Step 46250: loss = 2.81782
Step 46255: loss = 2.87971
Step 46260: loss = 2.66628
Step 46265: loss = 2.94867
Step 46270: loss = 2.81053
Step 46275: loss = 2.99481
Step 46280: loss = 2.56618
Step 46285: loss = 2.92277
Step 46290: loss = 2.89301
Step 46295: loss = 2.80801
Step 46300: loss = 2.74473
Step 46305: loss = 2.69057
Step 46310: loss = 2.69954
Step 46315: loss = 2.76040
Step 46320: loss = 2.68163
Step 46325: loss = 2.84233
Step 46330: loss = 2.50267
Step 46335: loss = 3.00294
Step 46340: loss = 2.88503
Step 46345: loss = 2.78492
Step 46350: loss = 2.88594
Step 46355: loss = 2.90049
Step 46360: loss = 2.79168
Step 46365: loss = 2.81864
Step 46370: loss = 2.68867
Step 46375: loss = 2.97097
Step 46380: loss = 2.98586
Step 46385: loss = 2.81268
Step 46390: loss = 2.69837
Step 46395: loss = 2.83377
Step 46400: loss = 2.62568
Step 46405: loss = 2.68914
Step 46410: loss = 2.82168
Training Data Eval:
  Num examples: 49920, Num correct: 6678, Precision @ 1: 0.1338
('Testing Data Eval: EPOCH->', 120)
  Num examples: 9984, Num correct: 1414, Precision @ 1: 0.1416
Step 46415: loss = 2.88116
Step 46420: loss = 2.74870
Step 46425: loss = 2.75881
Step 46430: loss = 3.01455
Step 46435: loss = 2.71836
Step 46440: loss = 2.75005
Step 46445: loss = 2.77524
Step 46450: loss = 2.86610
Step 46455: loss = 2.56074
Step 46460: loss = 2.81370
Step 46465: loss = 2.71905
Step 46470: loss = 2.84207
Step 46475: loss = 2.82494
Step 46480: loss = 2.81943
Step 46485: loss = 2.74950
Step 46490: loss = 2.78052
Step 46495: loss = 2.94150
Step 46500: loss = 2.72920
Step 46505: loss = 2.95523
Step 46510: loss = 2.84700
Step 46515: loss = 2.74592
Step 46520: loss = 2.74013
Step 46525: loss = 2.62304
Step 46530: loss = 2.80965
Step 46535: loss = 2.77850
Step 46540: loss = 2.79823
Step 46545: loss = 2.75765
Step 46550: loss = 3.04390
Step 46555: loss = 2.87856
Step 46560: loss = 2.87785
Step 46565: loss = 2.64276
Step 46570: loss = 2.71189
Step 46575: loss = 2.78188
Step 46580: loss = 2.90562
Step 46585: loss = 2.65810
Step 46590: loss = 2.87109
Step 46595: loss = 2.78712
Step 46600: loss = 2.77833
Step 46605: loss = 2.89594
Step 46610: loss = 2.77292
Step 46615: loss = 2.77507
Step 46620: loss = 2.64948
Step 46625: loss = 2.77600
Step 46630: loss = 2.76105
Step 46635: loss = 3.04414
Step 46640: loss = 2.69931
Step 46645: loss = 2.73319
Step 46650: loss = 2.83452
Step 46655: loss = 2.91805
Step 46660: loss = 2.75743
Step 46665: loss = 2.67231
Step 46670: loss = 2.60425
Step 46675: loss = 2.87808
Step 46680: loss = 2.75192
Step 46685: loss = 2.68851
Step 46690: loss = 2.79369
Step 46695: loss = 2.70404
Step 46700: loss = 2.73482
Step 46705: loss = 2.82123
Step 46710: loss = 2.69687
Step 46715: loss = 2.59846
Step 46720: loss = 2.80687
Step 46725: loss = 2.73132
Step 46730: loss = 2.79462
Step 46735: loss = 3.08082
Step 46740: loss = 2.73838
Step 46745: loss = 2.75271
Step 46750: loss = 2.70451
Step 46755: loss = 2.79059
Step 46760: loss = 2.99362
Step 46765: loss = 2.77320
Step 46770: loss = 2.69761
Step 46775: loss = 2.77733
Step 46780: loss = 2.90046
Step 46785: loss = 2.64464
Step 46790: loss = 2.95532
Step 46795: loss = 2.90821
Step 46800: loss = 2.66344
Training Data Eval:
  Num examples: 49920, Num correct: 6752, Precision @ 1: 0.1353
('Testing Data Eval: EPOCH->', 121)
  Num examples: 9984, Num correct: 1285, Precision @ 1: 0.1287
Step 46805: loss = 2.75022
Step 46810: loss = 2.80082
Step 46815: loss = 2.63319
Step 46820: loss = 2.96621
Step 46825: loss = 2.59850
Step 46830: loss = 2.76201
Step 46835: loss = 2.71652
Step 46840: loss = 2.79583
Step 46845: loss = 2.80171
Step 46850: loss = 2.77400
Step 46855: loss = 2.90075
Step 46860: loss = 2.69518
Step 46865: loss = 2.76073
Step 46870: loss = 2.73099
Step 46875: loss = 2.74778
Step 46880: loss = 2.75638
Step 46885: loss = 2.87824
Step 46890: loss = 2.66629
Step 46895: loss = 2.76767
Step 46900: loss = 2.75153
Step 46905: loss = 2.86443
Step 46910: loss = 2.81530
Step 46915: loss = 2.82486
Step 46920: loss = 2.62056
Step 46925: loss = 3.05662
Step 46930: loss = 2.73054
Step 46935: loss = 2.68361
Step 46940: loss = 2.95159
Step 46945: loss = 2.65582
Step 46950: loss = 2.66347
Step 46955: loss = 2.88731
Step 46960: loss = 2.88037
Step 46965: loss = 2.80941
Step 46970: loss = 2.71573
Step 46975: loss = 2.86696
Step 46980: loss = 2.70697
Step 46985: loss = 2.99585
Step 46990: loss = 2.65284
Step 46995: loss = 2.85724
Step 47000: loss = 2.75131
Step 47005: loss = 2.63279
Step 47010: loss = 2.87887
Step 47015: loss = 2.76616
Step 47020: loss = 2.61896
Step 47025: loss = 2.80909
Step 47030: loss = 2.73707
Step 47035: loss = 2.71456
Step 47040: loss = 2.69161
Step 47045: loss = 2.80248
Step 47050: loss = 2.85778
Step 47055: loss = 2.74598
Step 47060: loss = 2.65810
Step 47065: loss = 2.63497
Step 47070: loss = 2.96184
Step 47075: loss = 2.70854
Step 47080: loss = 2.66524
Step 47085: loss = 2.71028
Step 47090: loss = 2.81051
Step 47095: loss = 2.84218
Step 47100: loss = 2.52814
Step 47105: loss = 2.66612
Step 47110: loss = 2.64857
Step 47115: loss = 2.41780
Step 47120: loss = 2.75755
Step 47125: loss = 2.70156
Step 47130: loss = 2.69102
Step 47135: loss = 2.83857
Step 47140: loss = 2.77258
Step 47145: loss = 2.93808
Step 47150: loss = 2.75009
Step 47155: loss = 2.94504
Step 47160: loss = 2.79813
Step 47165: loss = 2.64570
Step 47170: loss = 2.63788
Step 47175: loss = 2.89101
Step 47180: loss = 2.78490
Step 47185: loss = 2.56978
Step 47190: loss = 2.86486
Training Data Eval:
  Num examples: 49920, Num correct: 6509, Precision @ 1: 0.1304
('Testing Data Eval: EPOCH->', 122)
  Num examples: 9984, Num correct: 1311, Precision @ 1: 0.1313
Step 47195: loss = 2.87347
Step 47200: loss = 2.82605
Step 47205: loss = 2.76226
Step 47210: loss = 2.93968
Step 47215: loss = 2.82306
Step 47220: loss = 2.79681
Step 47225: loss = 2.83480
Step 47230: loss = 2.72934
Step 47235: loss = 2.72942
Step 47240: loss = 2.69828
Step 47245: loss = 2.71125
Step 47250: loss = 3.06110
Step 47255: loss = 2.77040
Step 47260: loss = 2.77803
Step 47265: loss = 2.77857
Step 47270: loss = 2.83172
Step 47275: loss = 3.00045
Step 47280: loss = 2.85183
Step 47285: loss = 2.64550
Step 47290: loss = 2.61804
Step 47295: loss = 2.91739
Step 47300: loss = 2.69404
Step 47305: loss = 2.99707
Step 47310: loss = 2.91153
Step 47315: loss = 2.78263
Step 47320: loss = 2.74840
Step 47325: loss = 2.85758
Step 47330: loss = 2.63538
Step 47335: loss = 2.76301
Step 47340: loss = 2.78334
Step 47345: loss = 3.00190
Step 47350: loss = 2.97942
Step 47355: loss = 2.79066
Step 47360: loss = 2.92240
Step 47365: loss = 2.62728
Step 47370: loss = 2.78426
Step 47375: loss = 2.78180
Step 47380: loss = 2.75830
Step 47385: loss = 2.96350
Step 47390: loss = 2.94916
Step 47395: loss = 2.94583
Step 47400: loss = 2.72273
Step 47405: loss = 2.88074
Step 47410: loss = 2.83438
Step 47415: loss = 2.79284
Step 47420: loss = 2.96110
Step 47425: loss = 2.57325
Step 47430: loss = 2.85537
Step 47435: loss = 2.62029
Step 47440: loss = 2.77461
Step 47445: loss = 2.77803
Step 47450: loss = 2.71848
Step 47455: loss = 2.58496
Step 47460: loss = 2.64959
Step 47465: loss = 2.90557
Step 47470: loss = 2.99908
Step 47475: loss = 2.98460
Step 47480: loss = 2.93704
Step 47485: loss = 2.85569
Step 47490: loss = 2.82947
Step 47495: loss = 2.69403
Step 47500: loss = 2.65987
Step 47505: loss = 2.91282
Step 47510: loss = 2.82520
Step 47515: loss = 2.83356
Step 47520: loss = 2.83532
Step 47525: loss = 2.66066
Step 47530: loss = 2.80784
Step 47535: loss = 2.74713
Step 47540: loss = 2.78737
Step 47545: loss = 2.88918
Step 47550: loss = 2.93438
Step 47555: loss = 2.91507
Step 47560: loss = 2.70830
Step 47565: loss = 2.71247
Step 47570: loss = 2.77149
Step 47575: loss = 2.72811
Step 47580: loss = 2.85803
Training Data Eval:
  Num examples: 49920, Num correct: 6517, Precision @ 1: 0.1305
('Testing Data Eval: EPOCH->', 123)
  Num examples: 9984, Num correct: 1400, Precision @ 1: 0.1402
Step 47585: loss = 3.06002
Step 47590: loss = 2.76849
Step 47595: loss = 2.96146
Step 47600: loss = 2.76406
Step 47605: loss = 2.72446
Step 47610: loss = 2.73248
Step 47615: loss = 2.84732
Step 47620: loss = 2.87587
Step 47625: loss = 2.69473
Step 47630: loss = 2.97502
Step 47635: loss = 2.65535
Step 47640: loss = 2.64100
Step 47645: loss = 3.00956
Step 47650: loss = 2.84393
Step 47655: loss = 2.73104
Step 47660: loss = 2.96903
Step 47665: loss = 2.80509
Step 47670: loss = 2.80227
Step 47675: loss = 2.99945
Step 47680: loss = 2.96684
Step 47685: loss = 2.68675
Step 47690: loss = 2.62353
Step 47695: loss = 2.75315
Step 47700: loss = 2.99109
Step 47705: loss = 2.72549
Step 47710: loss = 2.69481
Step 47715: loss = 2.67828
Step 47720: loss = 2.81002
Step 47725: loss = 2.65798
Step 47730: loss = 2.89466
Step 47735: loss = 2.77456
Step 47740: loss = 2.74050
Step 47745: loss = 2.69180
Step 47750: loss = 2.65414
Step 47755: loss = 3.03890
Step 47760: loss = 2.62719
Step 47765: loss = 2.78434
Step 47770: loss = 2.70026
Step 47775: loss = 2.65994
Step 47780: loss = 2.67967
Step 47785: loss = 2.87041
Step 47790: loss = 2.75738
Step 47795: loss = 2.67061
Step 47800: loss = 2.80406
Step 47805: loss = 2.83949
Step 47810: loss = 2.88600
Step 47815: loss = 2.97166
Step 47820: loss = 2.58920
Step 47825: loss = 2.96613
Step 47830: loss = 2.70491
Step 47835: loss = 2.99347
Step 47840: loss = 2.61109
Step 47845: loss = 2.65296
Step 47850: loss = 3.03831
Step 47855: loss = 3.06051
Step 47860: loss = 2.66987
Step 47865: loss = 2.88452
Step 47870: loss = 2.87941
Step 47875: loss = 2.95556
Step 47880: loss = 2.82246
Step 47885: loss = 2.70958
Step 47890: loss = 2.75029
Step 47895: loss = 2.93774
Step 47900: loss = 2.74596
Step 47905: loss = 2.80683
Step 47910: loss = 2.72060
Step 47915: loss = 2.66089
Step 47920: loss = 2.64793
Step 47925: loss = 2.79757
Step 47930: loss = 2.60540
Step 47935: loss = 2.86314
Step 47940: loss = 2.67889
Step 47945: loss = 2.79172
Step 47950: loss = 3.02893
Step 47955: loss = 2.89900
Step 47960: loss = 2.77554
Step 47965: loss = 2.73669
Step 47970: loss = 2.84672
Training Data Eval:
  Num examples: 49920, Num correct: 6632, Precision @ 1: 0.1329
('Testing Data Eval: EPOCH->', 124)
  Num examples: 9984, Num correct: 1301, Precision @ 1: 0.1303
Step 47975: loss = 2.84698
Step 47980: loss = 2.73566
Step 47985: loss = 2.64981
Step 47990: loss = 2.74858
Step 47995: loss = 2.85242
Step 48000: loss = 2.70954
Step 48005: loss = 2.66896
Step 48010: loss = 2.84153
Step 48015: loss = 2.68320
Step 48020: loss = 2.80343
Step 48025: loss = 2.83668
Step 48030: loss = 2.64179
Step 48035: loss = 2.81691
Step 48040: loss = 2.80852
Step 48045: loss = 2.74422
Step 48050: loss = 2.99641
Step 48055: loss = 2.65301
Step 48060: loss = 2.80723
Step 48065: loss = 2.84342
Step 48070: loss = 2.76759
Step 48075: loss = 2.85724
Step 48080: loss = 2.83849
Step 48085: loss = 2.60699
Step 48090: loss = 2.73733
Step 48095: loss = 2.70627
Step 48100: loss = 2.78330
Step 48105: loss = 2.76440
Step 48110: loss = 2.98140
Step 48115: loss = 2.69902
Step 48120: loss = 2.54890
Step 48125: loss = 2.86202
Step 48130: loss = 2.79592
Step 48135: loss = 2.92769
Step 48140: loss = 2.87231
Step 48145: loss = 2.80756
Step 48150: loss = 2.86898
Step 48155: loss = 2.93222
Step 48160: loss = 2.58416
Step 48165: loss = 2.70265
Step 48170: loss = 3.01451
Step 48175: loss = 2.90391
Step 48180: loss = 2.76446
Step 48185: loss = 2.90065
Step 48190: loss = 2.66518
Step 48195: loss = 2.90513
Step 48200: loss = 3.03807
Step 48205: loss = 2.84186
Step 48210: loss = 2.87942
Step 48215: loss = 2.84522
Step 48220: loss = 2.86572
Step 48225: loss = 2.92439
Step 48230: loss = 2.70331
Step 48235: loss = 2.72272
Step 48240: loss = 2.78380
Step 48245: loss = 2.87750
Step 48250: loss = 2.63121
Step 48255: loss = 2.83473
Step 48260: loss = 2.67100
Step 48265: loss = 2.74686
Step 48270: loss = 2.90432
Step 48275: loss = 2.66728
Step 48280: loss = 2.71689
Step 48285: loss = 2.67185
Step 48290: loss = 2.65420
Step 48295: loss = 2.73343
Step 48300: loss = 2.67601
Step 48305: loss = 2.75646
Step 48310: loss = 2.70554
Step 48315: loss = 2.85312
Step 48320: loss = 2.93770
Step 48325: loss = 2.65611
Step 48330: loss = 2.75785
Step 48335: loss = 2.87585
Step 48340: loss = 2.77954
Step 48345: loss = 2.72705
Step 48350: loss = 2.85893
Step 48355: loss = 2.84536
Step 48360: loss = 2.70117
Training Data Eval:
  Num examples: 49920, Num correct: 6546, Precision @ 1: 0.1311
('Testing Data Eval: EPOCH->', 125)
  Num examples: 9984, Num correct: 1358, Precision @ 1: 0.1360
Step 48365: loss = 2.61321
Step 48370: loss = 2.89960
Step 48375: loss = 2.91457
Step 48380: loss = 2.92889
Step 48385: loss = 2.87911
Step 48390: loss = 2.73922
Step 48395: loss = 2.79530
Step 48400: loss = 2.71053
Step 48405: loss = 2.73909
Step 48410: loss = 2.74817
Step 48415: loss = 2.87531
Step 48420: loss = 2.69072
Step 48425: loss = 2.67253
Step 48430: loss = 2.82372
Step 48435: loss = 2.72363
Step 48440: loss = 2.87090
Step 48445: loss = 2.77526
Step 48450: loss = 2.87320
Step 48455: loss = 2.65677
Step 48460: loss = 2.78728
Step 48465: loss = 2.61672
Step 48470: loss = 2.80892
Step 48475: loss = 3.05068
Step 48480: loss = 2.85772
Step 48485: loss = 2.83363
Step 48490: loss = 2.70586
Step 48495: loss = 2.73401
Step 48500: loss = 2.81213
Step 48505: loss = 2.91883
Step 48510: loss = 2.81181
Step 48515: loss = 2.65346
Step 48520: loss = 2.97138
Step 48525: loss = 2.78499
Step 48530: loss = 2.71662
Step 48535: loss = 2.71691
Step 48540: loss = 2.80806
Step 48545: loss = 2.90234
Step 48550: loss = 2.80611
Step 48555: loss = 2.79580
Step 48560: loss = 2.70187
Step 48565: loss = 2.61180
Step 48570: loss = 2.66995
Step 48575: loss = 2.88320
Step 48580: loss = 2.92138
Step 48585: loss = 2.81120
Step 48590: loss = 2.82274
Step 48595: loss = 2.92109
Step 48600: loss = 2.66168
Step 48605: loss = 2.69594
Step 48610: loss = 2.68871
Step 48615: loss = 2.52291
Step 48620: loss = 2.93085
Step 48625: loss = 2.68103
Step 48630: loss = 2.79815
Step 48635: loss = 2.88171
Step 48640: loss = 2.75473
Step 48645: loss = 2.87885
Step 48650: loss = 2.71479
Step 48655: loss = 2.79755
Step 48660: loss = 2.62826
Step 48665: loss = 2.66959
Step 48670: loss = 2.75305
Step 48675: loss = 2.87079
Step 48680: loss = 2.95188
Step 48685: loss = 2.75019
Step 48690: loss = 2.83445
Step 48695: loss = 2.87815
Step 48700: loss = 2.77050
Step 48705: loss = 2.89267
Step 48710: loss = 2.91146
Step 48715: loss = 3.08401
Step 48720: loss = 2.68264
Step 48725: loss = 2.79416
Step 48730: loss = 3.03566
Step 48735: loss = 2.82619
Step 48740: loss = 2.80099
Step 48745: loss = 2.76205
Step 48750: loss = 2.97425
Training Data Eval:
  Num examples: 49920, Num correct: 6652, Precision @ 1: 0.1333
('Testing Data Eval: EPOCH->', 126)
  Num examples: 9984, Num correct: 1301, Precision @ 1: 0.1303
Step 48755: loss = 2.85977
Step 48760: loss = 2.62509
Step 48765: loss = 2.99268
Step 48770: loss = 2.93609
Step 48775: loss = 2.89308
Step 48780: loss = 2.84559
Step 48785: loss = 2.97466
Step 48790: loss = 2.65243
Step 48795: loss = 2.56237
Step 48800: loss = 2.86267
Step 48805: loss = 2.79425
Step 48810: loss = 2.83523
Step 48815: loss = 2.94517
Step 48820: loss = 2.67739
Step 48825: loss = 2.82671
Step 48830: loss = 2.84415
Step 48835: loss = 2.78466
Step 48840: loss = 2.65779
Step 48845: loss = 3.05674
Step 48850: loss = 2.70212
Step 48855: loss = 2.76073
Step 48860: loss = 2.74493
Step 48865: loss = 2.71852
Step 48870: loss = 2.82620
Step 48875: loss = 2.86186
Step 48880: loss = 2.92151
Step 48885: loss = 2.90739
Step 48890: loss = 2.79934
Step 48895: loss = 2.88792
Step 48900: loss = 2.81888
Step 48905: loss = 2.67426
Step 48910: loss = 2.83311
Step 48915: loss = 2.75730
Step 48920: loss = 2.63350
Step 48925: loss = 2.78569
Step 48930: loss = 2.79752
Step 48935: loss = 2.80985
Step 48940: loss = 2.96938
Step 48945: loss = 2.69209
Step 48950: loss = 2.99456
Step 48955: loss = 3.03938
Step 48960: loss = 2.83055
Step 48965: loss = 2.93472
Step 48970: loss = 2.86024
Step 48975: loss = 2.90636
Step 48980: loss = 2.72473
Step 48985: loss = 2.61738
Step 48990: loss = 2.66027
Step 48995: loss = 2.81408
Step 49000: loss = 2.94936
Step 49005: loss = 2.92334
Step 49010: loss = 3.12130
Step 49015: loss = 2.67774
Step 49020: loss = 2.78970
Step 49025: loss = 2.82778
Step 49030: loss = 2.78319
Step 49035: loss = 2.66771
Step 49040: loss = 2.71293
Step 49045: loss = 2.87219
Step 49050: loss = 2.78388
Step 49055: loss = 2.70385
Step 49060: loss = 2.96551
Step 49065: loss = 2.83080
Step 49070: loss = 2.84525
Step 49075: loss = 2.90486
Step 49080: loss = 2.78968
Step 49085: loss = 2.65980
Step 49090: loss = 2.96952
Step 49095: loss = 2.97189
Step 49100: loss = 2.87886
Step 49105: loss = 2.61547
Step 49110: loss = 2.64367
Step 49115: loss = 2.68915
Step 49120: loss = 2.75412
Step 49125: loss = 2.65627
Step 49130: loss = 3.06420
Step 49135: loss = 2.77305
Step 49140: loss = 2.88413
Training Data Eval:
  Num examples: 49920, Num correct: 6933, Precision @ 1: 0.1389
('Testing Data Eval: EPOCH->', 127)
  Num examples: 9984, Num correct: 1339, Precision @ 1: 0.1341
Step 49145: loss = 2.79277
Step 49150: loss = 3.11743
Step 49155: loss = 2.78369
Step 49160: loss = 2.82404
Step 49165: loss = 2.88704
Step 49170: loss = 2.67479
Step 49175: loss = 2.70342
Step 49180: loss = 2.80139
Step 49185: loss = 2.98494
Step 49190: loss = 3.04366
Step 49195: loss = 2.81060
Step 49200: loss = 2.70437
Step 49205: loss = 3.00566
Step 49210: loss = 2.76083
Step 49215: loss = 2.79238
Step 49220: loss = 2.92894
Step 49225: loss = 2.57450
Step 49230: loss = 2.81581
Step 49235: loss = 2.82960
Step 49240: loss = 3.03986
Step 49245: loss = 2.81939
Step 49250: loss = 2.96109
Step 49255: loss = 2.96174
Step 49260: loss = 2.95133
Step 49265: loss = 2.77123
Step 49270: loss = 2.80475
Step 49275: loss = 2.68710
Step 49280: loss = 2.93831
Step 49285: loss = 2.81504
Step 49290: loss = 2.99295
Step 49295: loss = 2.69846
Step 49300: loss = 2.51356
Step 49305: loss = 2.61782
Step 49310: loss = 2.79740
Step 49315: loss = 2.70619
Step 49320: loss = 2.82506
Step 49325: loss = 2.88849
Step 49330: loss = 2.80280
Step 49335: loss = 2.92155
Step 49340: loss = 2.90249
Step 49345: loss = 2.78438
Step 49350: loss = 2.78433
Step 49355: loss = 2.82785
Step 49360: loss = 2.79455
Step 49365: loss = 2.69794
Step 49370: loss = 2.80451
Step 49375: loss = 2.57183
Step 49380: loss = 2.80750
Step 49385: loss = 3.05167
Step 49390: loss = 2.73077
Step 49395: loss = 2.66983
Step 49400: loss = 2.99410
Step 49405: loss = 2.92149
Step 49410: loss = 2.94174
Step 49415: loss = 2.73579
Step 49420: loss = 3.23078
Step 49425: loss = 2.81089
Step 49430: loss = 2.77746
Step 49435: loss = 2.78797
Step 49440: loss = 2.98805
Step 49445: loss = 2.77462
Step 49450: loss = 2.61010
Step 49455: loss = 2.79619
Step 49460: loss = 2.87518
Step 49465: loss = 2.69887
Step 49470: loss = 2.84121
Step 49475: loss = 2.92370
Step 49480: loss = 2.69343
Step 49485: loss = 2.66806
Step 49490: loss = 2.64899
Step 49495: loss = 2.87364
Step 49500: loss = 2.98040
Step 49505: loss = 2.83168
Step 49510: loss = 2.68229
Step 49515: loss = 2.96076
Step 49520: loss = 2.95485
Step 49525: loss = 2.72853
Step 49530: loss = 2.68338
Training Data Eval:
  Num examples: 49920, Num correct: 6681, Precision @ 1: 0.1338
('Testing Data Eval: EPOCH->', 128)
  Num examples: 9984, Num correct: 1372, Precision @ 1: 0.1374
Step 49535: loss = 2.55111
Step 49540: loss = 2.70127
Step 49545: loss = 2.85995
Step 49550: loss = 2.84394
Step 49555: loss = 2.86014
Step 49560: loss = 2.78301
Step 49565: loss = 2.61384
Step 49570: loss = 2.77308
Step 49575: loss = 2.69932
Step 49580: loss = 2.62990
Step 49585: loss = 2.68022
Step 49590: loss = 2.72677
Step 49595: loss = 2.67051
Step 49600: loss = 2.63639
Step 49605: loss = 2.92192
Step 49610: loss = 2.90259
Step 49615: loss = 2.92019
Step 49620: loss = 2.67900
Step 49625: loss = 2.88115
Step 49630: loss = 3.07508
Step 49635: loss = 2.77104
Step 49640: loss = 2.97951
Step 49645: loss = 2.90327
Step 49650: loss = 2.68787
Step 49655: loss = 2.85178
Step 49660: loss = 2.81137
Step 49665: loss = 2.75269
Step 49670: loss = 2.83770
Step 49675: loss = 2.71137
Step 49680: loss = 2.56693
Step 49685: loss = 2.80144
Step 49690: loss = 3.14189
Step 49695: loss = 3.00139
Step 49700: loss = 2.71289
Step 49705: loss = 2.85563
Step 49710: loss = 3.17684
Step 49715: loss = 2.82485
Step 49720: loss = 2.83025
Step 49725: loss = 2.79561
Step 49730: loss = 2.83074
Step 49735: loss = 2.91135
Step 49740: loss = 2.92402
Step 49745: loss = 2.65153
Step 49750: loss = 3.01512
Step 49755: loss = 2.94105
Step 49760: loss = 2.84503
Step 49765: loss = 2.75615
Step 49770: loss = 2.84303
Step 49775: loss = 2.66477
Step 49780: loss = 2.79132
Step 49785: loss = 2.82583
Step 49790: loss = 2.69733
Step 49795: loss = 2.76878
Step 49800: loss = 2.93595
Step 49805: loss = 2.69792
Step 49810: loss = 2.92459
Step 49815: loss = 2.50541
Step 49820: loss = 2.93477
Step 49825: loss = 2.92547
Step 49830: loss = 2.77654
Step 49835: loss = 2.87525
Step 49840: loss = 2.76378
Step 49845: loss = 2.75811
Step 49850: loss = 2.58366
Step 49855: loss = 2.81308
Step 49860: loss = 2.99314
Step 49865: loss = 2.49866
Step 49870: loss = 2.85117
Step 49875: loss = 2.81435
Step 49880: loss = 2.89653
Step 49885: loss = 2.62574
Step 49890: loss = 2.76371
Step 49895: loss = 3.07776
Step 49900: loss = 2.94416
Step 49905: loss = 2.87403
Step 49910: loss = 3.01856
Step 49915: loss = 2.85573
Step 49920: loss = 2.56479
Training Data Eval:
  Num examples: 49920, Num correct: 6651, Precision @ 1: 0.1332
('Testing Data Eval: EPOCH->', 129)
  Num examples: 9984, Num correct: 1246, Precision @ 1: 0.1248
Step 49925: loss = 2.84967
Step 49930: loss = 2.77000
Step 49935: loss = 3.04076
Step 49940: loss = 2.95885
Step 49945: loss = 3.04508
Step 49950: loss = 2.80921
Step 49955: loss = 2.91024
Step 49960: loss = 2.94583
Step 49965: loss = 2.81304
Step 49970: loss = 2.73653
Step 49975: loss = 2.86696
Step 49980: loss = 2.79213
Step 49985: loss = 2.88915
Step 49990: loss = 2.97622
Step 49995: loss = 2.98634
Step 50000: loss = 2.89252
Step 50005: loss = 2.77223
Step 50010: loss = 2.81611
Step 50015: loss = 2.53388
Step 50020: loss = 2.81848
Step 50025: loss = 2.80753
Step 50030: loss = 2.76473
Step 50035: loss = 2.81627
Step 50040: loss = 2.86471
Step 50045: loss = 2.68124
Step 50050: loss = 2.75884
Step 50055: loss = 2.92637
Step 50060: loss = 2.86927
Step 50065: loss = 2.73556
Step 50070: loss = 2.69307
Step 50075: loss = 2.96513
Step 50080: loss = 2.75845
Step 50085: loss = 2.83165
Step 50090: loss = 2.76531
Step 50095: loss = 2.83679
Step 50100: loss = 2.72436
Step 50105: loss = 2.73748
Step 50110: loss = 2.83623
Step 50115: loss = 2.61511
Step 50120: loss = 2.56863
Step 50125: loss = 2.74001
Step 50130: loss = 2.94761
Step 50135: loss = 2.69845
Step 50140: loss = 2.66540
Step 50145: loss = 2.71668
Step 50150: loss = 2.72630
Step 50155: loss = 2.59503
Step 50160: loss = 2.87221
Step 50165: loss = 2.74935
Step 50170: loss = 2.77519
Step 50175: loss = 2.66009
Step 50180: loss = 2.69793
Step 50185: loss = 3.04257
Step 50190: loss = 2.74478
Step 50195: loss = 2.78076
Step 50200: loss = 2.87018
Step 50205: loss = 2.84030
Step 50210: loss = 2.82797
Step 50215: loss = 3.01926
Step 50220: loss = 2.73941
Step 50225: loss = 2.86583
Step 50230: loss = 2.77914
Step 50235: loss = 2.76928
Step 50240: loss = 2.50370
Step 50245: loss = 2.95416
Step 50250: loss = 2.76840
Step 50255: loss = 2.79028
Step 50260: loss = 2.85147
Step 50265: loss = 3.03421
Step 50270: loss = 2.90342
Step 50275: loss = 2.98998
Step 50280: loss = 3.01417
Step 50285: loss = 2.73394
Step 50290: loss = 2.59518
Step 50295: loss = 2.71370
Step 50300: loss = 2.70583
Step 50305: loss = 2.79378
Step 50310: loss = 2.61574
Training Data Eval:
  Num examples: 49920, Num correct: 6661, Precision @ 1: 0.1334
('Testing Data Eval: EPOCH->', 130)
  Num examples: 9984, Num correct: 1355, Precision @ 1: 0.1357
Step 50315: loss = 2.64029
Step 50320: loss = 3.11172
Step 50325: loss = 2.83350
Step 50330: loss = 2.68225
Step 50335: loss = 2.77483
Step 50340: loss = 2.80993
Step 50345: loss = 2.69244
Step 50350: loss = 2.85878
Step 50355: loss = 2.67132
Step 50360: loss = 2.96519
Step 50365: loss = 2.89623
Step 50370: loss = 2.90446
Step 50375: loss = 2.74028
Step 50380: loss = 2.84728
Step 50385: loss = 2.70110
Step 50390: loss = 2.71850
Step 50395: loss = 2.82711
Step 50400: loss = 2.80016
Step 50405: loss = 2.97869
Step 50410: loss = 2.94658
Step 50415: loss = 2.76972
Step 50420: loss = 2.86776
Step 50425: loss = 2.94731
Step 50430: loss = 2.86106
Step 50435: loss = 2.66206
Step 50440: loss = 2.76126
Step 50445: loss = 2.77515
Step 50450: loss = 2.77178
Step 50455: loss = 2.81499
Step 50460: loss = 2.83245
Step 50465: loss = 2.79870
Step 50470: loss = 2.85093
Step 50475: loss = 2.96225
Step 50480: loss = 2.64794
Step 50485: loss = 2.94919
Step 50490: loss = 2.71040
Step 50495: loss = 2.82287
Step 50500: loss = 2.76427
Step 50505: loss = 2.81933
Step 50510: loss = 2.72583
Step 50515: loss = 3.01854
Step 50520: loss = 2.90188
Step 50525: loss = 2.80726
Step 50530: loss = 2.73341
Step 50535: loss = 2.62434
Step 50540: loss = 2.59994
Step 50545: loss = 2.87531
Step 50550: loss = 2.76076
Step 50555: loss = 2.98948
Step 50560: loss = 2.72143
Step 50565: loss = 2.91449
Step 50570: loss = 2.94914
Step 50575: loss = 2.66219
Step 50580: loss = 2.71332
Step 50585: loss = 2.73013
Step 50590: loss = 2.74220
Step 50595: loss = 2.76811
Step 50600: loss = 2.98263
Step 50605: loss = 2.79781
Step 50610: loss = 2.97112
Step 50615: loss = 2.84115
Step 50620: loss = 2.97410
Step 50625: loss = 2.91408
Step 50630: loss = 2.85923
Step 50635: loss = 2.87497
Step 50640: loss = 2.93473
Step 50645: loss = 2.84213
Step 50650: loss = 2.83957
Step 50655: loss = 2.73134
Step 50660: loss = 2.79793
Step 50665: loss = 2.68734
Step 50670: loss = 2.62321
Step 50675: loss = 2.78478
Step 50680: loss = 2.81109
Step 50685: loss = 2.82510
Step 50690: loss = 2.69834
Step 50695: loss = 2.58257
Step 50700: loss = 2.74173
Training Data Eval:
  Num examples: 49920, Num correct: 6586, Precision @ 1: 0.1319
('Testing Data Eval: EPOCH->', 131)
  Num examples: 9984, Num correct: 1393, Precision @ 1: 0.1395
Step 50705: loss = 2.76954
Step 50710: loss = 2.85457
Step 50715: loss = 2.68906
Step 50720: loss = 2.70584
Step 50725: loss = 3.10092
Step 50730: loss = 2.93577
Step 50735: loss = 2.82094
Step 50740: loss = 2.79510
Step 50745: loss = 2.67845
Step 50750: loss = 2.78959
Step 50755: loss = 2.90828
Step 50760: loss = 2.85084
Step 50765: loss = 3.05480
Step 50770: loss = 2.81156
Step 50775: loss = 2.59685
Step 50780: loss = 2.76187
Step 50785: loss = 3.03324
Step 50790: loss = 2.88105
Step 50795: loss = 2.94873
Step 50800: loss = 2.83980
Step 50805: loss = 2.92010
Step 50810: loss = 2.74486
Step 50815: loss = 2.75240
Step 50820: loss = 2.79037
Step 50825: loss = 2.48236
Step 50830: loss = 2.96468
Step 50835: loss = 2.73999
Step 50840: loss = 2.80488
Step 50845: loss = 2.80513
Step 50850: loss = 2.87176
Step 50855: loss = 2.85231
Step 50860: loss = 2.90459
Step 50865: loss = 2.91470
Step 50870: loss = 2.83686
Step 50875: loss = 2.97384
Step 50880: loss = 2.89907
Step 50885: loss = 2.62100
Step 50890: loss = 2.70547
Step 50895: loss = 2.88233
Step 50900: loss = 2.96933
Step 50905: loss = 2.69736
Step 50910: loss = 2.71490
Step 50915: loss = 2.64331
Step 50920: loss = 2.77132
Step 50925: loss = 2.76226
Step 50930: loss = 2.87784
Step 50935: loss = 2.79412
Step 50940: loss = 2.95552
Step 50945: loss = 2.81186
Step 50950: loss = 2.55944
Step 50955: loss = 2.66944
Step 50960: loss = 2.67128
Step 50965: loss = 2.65668
Step 50970: loss = 2.80944
Step 50975: loss = 2.69421
Step 50980: loss = 2.96320
Step 50985: loss = 2.90057
Step 50990: loss = 2.86941
Step 50995: loss = 2.87631
Step 51000: loss = 2.92554
Step 51005: loss = 2.82177
Step 51010: loss = 2.84909
Step 51015: loss = 2.62305
Step 51020: loss = 2.67931
Step 51025: loss = 2.55780
Step 51030: loss = 2.54504
Step 51035: loss = 2.71342
Step 51040: loss = 2.76697
Step 51045: loss = 2.75860
Step 51050: loss = 2.81362
Step 51055: loss = 2.79707
Step 51060: loss = 2.92514
Step 51065: loss = 2.85944
Step 51070: loss = 2.46004
Step 51075: loss = 2.70400
Step 51080: loss = 2.84621
Step 51085: loss = 2.82818
Step 51090: loss = 2.89854
Training Data Eval:
  Num examples: 49920, Num correct: 6758, Precision @ 1: 0.1354
('Testing Data Eval: EPOCH->', 132)
  Num examples: 9984, Num correct: 1303, Precision @ 1: 0.1305
Step 51095: loss = 2.70848
Step 51100: loss = 2.75350
Step 51105: loss = 2.92360
Step 51110: loss = 2.72983
Step 51115: loss = 2.85322
Step 51120: loss = 2.80416
Step 51125: loss = 2.74864
Step 51130: loss = 2.83558
Step 51135: loss = 2.88939
Step 51140: loss = 3.05515
Step 51145: loss = 2.73001
Step 51150: loss = 2.78273
Step 51155: loss = 2.68462
Step 51160: loss = 2.89060
Step 51165: loss = 2.83210
Step 51170: loss = 2.71406
Step 51175: loss = 2.74129
Step 51180: loss = 2.76787
Step 51185: loss = 2.87090
Step 51190: loss = 2.81551
Step 51195: loss = 2.80390
Step 51200: loss = 2.85446
Step 51205: loss = 2.69210
Step 51210: loss = 2.97453
Step 51215: loss = 2.79953
Step 51220: loss = 2.58930
Step 51225: loss = 2.74339
Step 51230: loss = 2.82474
Step 51235: loss = 2.98244
Step 51240: loss = 2.71548
Step 51245: loss = 2.63621
Step 51250: loss = 2.56842
Step 51255: loss = 2.96028
Step 51260: loss = 2.79606
Step 51265: loss = 2.86815
Step 51270: loss = 2.99489
Step 51275: loss = 2.95094
Step 51280: loss = 2.77283
Step 51285: loss = 2.95897
Step 51290: loss = 2.87490
Step 51295: loss = 2.88046
Step 51300: loss = 2.67212
Step 51305: loss = 2.71071
Step 51310: loss = 2.75029
Step 51315: loss = 2.81433
Step 51320: loss = 2.60623
Step 51325: loss = 2.88412
Step 51330: loss = 2.83084
Step 51335: loss = 2.75454
Step 51340: loss = 2.66911
Step 51345: loss = 2.93623
Step 51350: loss = 2.67966
Step 51355: loss = 2.77526
Step 51360: loss = 2.69974
Step 51365: loss = 2.95755
Step 51370: loss = 2.81050
Step 51375: loss = 2.78542
Step 51380: loss = 2.75831
Step 51385: loss = 2.78243
Step 51390: loss = 2.85405
Step 51395: loss = 2.74961
Step 51400: loss = 2.82725
Step 51405: loss = 2.82047
Step 51410: loss = 2.74042
Step 51415: loss = 2.87703
Step 51420: loss = 2.65452
Step 51425: loss = 2.67517
Step 51430: loss = 2.87405
Step 51435: loss = 2.88696
Step 51440: loss = 2.82122
Step 51445: loss = 2.72935
Step 51450: loss = 2.88597
Step 51455: loss = 2.82735
Step 51460: loss = 2.65259
Step 51465: loss = 2.86293
Step 51470: loss = 2.84075
Step 51475: loss = 2.76466
Step 51480: loss = 2.71720
Training Data Eval:
  Num examples: 49920, Num correct: 6877, Precision @ 1: 0.1378
('Testing Data Eval: EPOCH->', 133)
  Num examples: 9984, Num correct: 1414, Precision @ 1: 0.1416
Step 51485: loss = 2.66063
Step 51490: loss = 2.71430
Step 51495: loss = 2.65503
Step 51500: loss = 2.85904
Step 51505: loss = 2.74012
Step 51510: loss = 2.65585
Step 51515: loss = 2.76471
Step 51520: loss = 2.75443
Step 51525: loss = 2.75516
Step 51530: loss = 2.70197
Step 51535: loss = 3.07370
Step 51540: loss = 2.81128
Step 51545: loss = 2.85668
Step 51550: loss = 2.93718
Step 51555: loss = 2.73589
Step 51560: loss = 2.97416
Step 51565: loss = 2.95349
Step 51570: loss = 2.70289
Step 51575: loss = 3.06255
Step 51580: loss = 2.65255
Step 51585: loss = 2.62629
Step 51590: loss = 2.82955
Step 51595: loss = 2.83366
Step 51600: loss = 2.93089
Step 51605: loss = 2.62191
Step 51610: loss = 2.98048
Step 51615: loss = 2.68564
Step 51620: loss = 2.74745
Step 51625: loss = 2.90419
Step 51630: loss = 2.70637
Step 51635: loss = 2.82219
Step 51640: loss = 2.76534
Step 51645: loss = 2.82329
Step 51650: loss = 2.57541
Step 51655: loss = 2.65204
Step 51660: loss = 2.65528
Step 51665: loss = 2.89739
Step 51670: loss = 2.79324
Step 51675: loss = 2.89712
Step 51680: loss = 2.82677
Step 51685: loss = 2.77891
Step 51690: loss = 2.58198
Step 51695: loss = 2.48961
Step 51700: loss = 2.81045
Step 51705: loss = 2.98007
Step 51710: loss = 2.76130
Step 51715: loss = 2.90901
Step 51720: loss = 2.72626
Step 51725: loss = 2.74521
Step 51730: loss = 2.63674
Step 51735: loss = 2.77066
Step 51740: loss = 2.62000
Step 51745: loss = 2.71531
Step 51750: loss = 3.12635
Step 51755: loss = 2.90966
Step 51760: loss = 2.70452
Step 51765: loss = 2.74542
Step 51770: loss = 2.61991
Step 51775: loss = 2.86342
Step 51780: loss = 3.05592
Step 51785: loss = 2.59614
Step 51790: loss = 2.57677
Step 51795: loss = 2.83745
Step 51800: loss = 2.72287
Step 51805: loss = 2.89751
Step 51810: loss = 2.82953
Step 51815: loss = 2.86337
Step 51820: loss = 2.92484
Step 51825: loss = 2.96452
Step 51830: loss = 2.83978
Step 51835: loss = 2.69438
Step 51840: loss = 2.83284
Step 51845: loss = 2.92201
Step 51850: loss = 2.73895
Step 51855: loss = 3.00780
Step 51860: loss = 2.78307
Step 51865: loss = 2.78971
Step 51870: loss = 2.92029
Training Data Eval:
  Num examples: 49920, Num correct: 6823, Precision @ 1: 0.1367
('Testing Data Eval: EPOCH->', 134)
  Num examples: 9984, Num correct: 1374, Precision @ 1: 0.1376
Step 51875: loss = 2.95539
Step 51880: loss = 2.88273
Step 51885: loss = 2.80036
Step 51890: loss = 2.87523
Step 51895: loss = 3.02474
Step 51900: loss = 2.99183
Step 51905: loss = 2.70473
Step 51910: loss = 2.89399
Step 51915: loss = 2.77102
Step 51920: loss = 2.95850
Step 51925: loss = 2.79523
Step 51930: loss = 2.81782
Step 51935: loss = 2.98408
Step 51940: loss = 2.62473
Step 51945: loss = 2.79602
Step 51950: loss = 2.59832
Step 51955: loss = 2.92361
Step 51960: loss = 2.99897
Step 51965: loss = 2.74539
Step 51970: loss = 2.62902
Step 51975: loss = 2.65823
Step 51980: loss = 2.88339
Step 51985: loss = 2.64724
Step 51990: loss = 2.84147
Step 51995: loss = 2.74808
Step 52000: loss = 2.80747
Step 52005: loss = 2.75620
Step 52010: loss = 2.67383
Step 52015: loss = 2.76856
Step 52020: loss = 2.76146
Step 52025: loss = 2.96070
Step 52030: loss = 2.88791
Step 52035: loss = 2.89408
Step 52040: loss = 2.81723
Step 52045: loss = 2.83453
Step 52050: loss = 2.93319
Step 52055: loss = 2.95225
Step 52060: loss = 2.75000
Step 52065: loss = 2.68969
Step 52070: loss = 2.89908
Step 52075: loss = 2.79712
Step 52080: loss = 2.78023
Step 52085: loss = 2.72524
Step 52090: loss = 2.61070
Step 52095: loss = 2.71744
Step 52100: loss = 2.77506
Step 52105: loss = 2.84527
Step 52110: loss = 2.89066
Step 52115: loss = 2.82828
Step 52120: loss = 2.77661
Step 52125: loss = 2.65319
Step 52130: loss = 2.86935
Step 52135: loss = 2.80247
Step 52140: loss = 2.87594
Step 52145: loss = 2.92929
Step 52150: loss = 2.80386
Step 52155: loss = 2.64735
Step 52160: loss = 2.71761
Step 52165: loss = 2.94497
Step 52170: loss = 2.77454
Step 52175: loss = 2.80723
Step 52180: loss = 2.90229
Step 52185: loss = 2.46845
Step 52190: loss = 2.88425
Step 52195: loss = 2.70013
Step 52200: loss = 2.90509
Step 52205: loss = 2.62821
Step 52210: loss = 2.89402
Step 52215: loss = 2.83537
Step 52220: loss = 2.65839
Step 52225: loss = 2.89952
Step 52230: loss = 2.49574
Step 52235: loss = 2.86044
Step 52240: loss = 2.52368
Step 52245: loss = 2.75031
Step 52250: loss = 2.68111
Step 52255: loss = 3.08401
Step 52260: loss = 3.09124
Training Data Eval:
  Num examples: 49920, Num correct: 6879, Precision @ 1: 0.1378
('Testing Data Eval: EPOCH->', 135)
  Num examples: 9984, Num correct: 1365, Precision @ 1: 0.1367
Step 52265: loss = 2.78069
Step 52270: loss = 2.74686
Step 52275: loss = 2.93524
Step 52280: loss = 2.71774
Step 52285: loss = 2.93180
Step 52290: loss = 3.03422
Step 52295: loss = 2.92102
Step 52300: loss = 2.93894
Step 52305: loss = 2.88342
Step 52310: loss = 3.02794
Step 52315: loss = 2.81616
Step 52320: loss = 2.91594
Step 52325: loss = 2.82085
Step 52330: loss = 2.80183
Step 52335: loss = 2.66773
Step 52340: loss = 2.79110
Step 52345: loss = 2.66480
Step 52350: loss = 3.04949
Step 52355: loss = 2.53838
Step 52360: loss = 2.84721
Step 52365: loss = 2.67703
Step 52370: loss = 2.82931
Step 52375: loss = 2.71730
Step 52380: loss = 2.81312
Step 52385: loss = 2.84213
Step 52390: loss = 2.97818
Step 52395: loss = 2.73077
Step 52400: loss = 2.70275
Step 52405: loss = 2.94970
Step 52410: loss = 2.79043
Step 52415: loss = 2.85711
Step 52420: loss = 2.86637
Step 52425: loss = 2.96789
Step 52430: loss = 2.84051
Step 52435: loss = 2.81516
Step 52440: loss = 2.79952
Step 52445: loss = 2.78877
Step 52450: loss = 2.59505
Step 52455: loss = 2.81565
Step 52460: loss = 2.74611
Step 52465: loss = 2.94110
Step 52470: loss = 2.76764
Step 52475: loss = 2.82170
Step 52480: loss = 2.70135
Step 52485: loss = 2.76168
Step 52490: loss = 2.82675
Step 52495: loss = 2.70308
Step 52500: loss = 2.80940
Step 52505: loss = 2.72681
Step 52510: loss = 2.76085
Step 52515: loss = 2.64695
Step 52520: loss = 2.63872
Step 52525: loss = 3.15078
Step 52530: loss = 2.78443
Step 52535: loss = 2.61200
Step 52540: loss = 2.76600
Step 52545: loss = 2.70424
Step 52550: loss = 2.67659
Step 52555: loss = 2.73648
Step 52560: loss = 2.77087
Step 52565: loss = 3.06484
Step 52570: loss = 2.82574
Step 52575: loss = 2.75098
Step 52580: loss = 2.61996
Step 52585: loss = 2.82370
Step 52590: loss = 2.86119
Step 52595: loss = 2.75566
Step 52600: loss = 2.63663
Step 52605: loss = 2.86836
Step 52610: loss = 2.72616
Step 52615: loss = 2.82479
Step 52620: loss = 2.70272
Step 52625: loss = 2.85118
Step 52630: loss = 3.05220
Step 52635: loss = 2.94587
Step 52640: loss = 2.89382
Step 52645: loss = 2.92602
Step 52650: loss = 2.92609
Training Data Eval:
  Num examples: 49920, Num correct: 7134, Precision @ 1: 0.1429
('Testing Data Eval: EPOCH->', 136)
  Num examples: 9984, Num correct: 1418, Precision @ 1: 0.1420
Step 52655: loss = 2.55658
Step 52660: loss = 2.63389
Step 52665: loss = 2.72701
Step 52670: loss = 2.94061
Step 52675: loss = 2.86612
Step 52680: loss = 2.90888
Step 52685: loss = 2.91661
Step 52690: loss = 2.80346
Step 52695: loss = 2.74544
Step 52700: loss = 2.61836
Step 52705: loss = 2.97293
Step 52710: loss = 2.86499
Step 52715: loss = 2.73332
Step 52720: loss = 2.78228
Step 52725: loss = 2.81749
Step 52730: loss = 2.83443
Step 52735: loss = 2.80251
Step 52740: loss = 2.73112
Step 52745: loss = 2.60940
Step 52750: loss = 2.69216
Step 52755: loss = 2.85938
Step 52760: loss = 2.82045
Step 52765: loss = 3.00714
Step 52770: loss = 2.74177
Step 52775: loss = 2.91967
Step 52780: loss = 2.82817
Step 52785: loss = 2.94679
Step 52790: loss = 2.78147
Step 52795: loss = 2.88826
Step 52800: loss = 2.72404
Step 52805: loss = 2.83904
Step 52810: loss = 2.91344
Step 52815: loss = 3.04954
Step 52820: loss = 2.76640
Step 52825: loss = 2.63852
Step 52830: loss = 2.78037
Step 52835: loss = 2.66605
Step 52840: loss = 2.78401
Step 52845: loss = 2.83562
Step 52850: loss = 2.71380
Step 52855: loss = 2.73859
Step 52860: loss = 2.60202
Step 52865: loss = 2.77751
Step 52870: loss = 2.80738
Step 52875: loss = 2.97104
Step 52880: loss = 2.81367
Step 52885: loss = 2.81664
Step 52890: loss = 2.48068
Step 52895: loss = 2.80326
Step 52900: loss = 2.94539
Step 52905: loss = 2.70137
Step 52910: loss = 2.88243
Step 52915: loss = 2.63782
Step 52920: loss = 2.85571
Step 52925: loss = 2.82667
Step 52930: loss = 2.70525
Step 52935: loss = 2.82314
Step 52940: loss = 2.85635
Step 52945: loss = 3.09250
Step 52950: loss = 2.79352
Step 52955: loss = 2.68362
Step 52960: loss = 2.85369
Step 52965: loss = 2.62821
Step 52970: loss = 2.75947
Step 52975: loss = 2.88337
Step 52980: loss = 3.03671
Step 52985: loss = 2.86508
Step 52990: loss = 3.02993
Step 52995: loss = 3.07801
Step 53000: loss = 2.70433
Step 53005: loss = 2.74298
Step 53010: loss = 2.64981
Step 53015: loss = 2.96192
Step 53020: loss = 2.82833
Step 53025: loss = 2.92456
Step 53030: loss = 2.71761
Step 53035: loss = 2.59827
Step 53040: loss = 2.91574
Training Data Eval:
  Num examples: 49920, Num correct: 6836, Precision @ 1: 0.1369
('Testing Data Eval: EPOCH->', 137)
  Num examples: 9984, Num correct: 1431, Precision @ 1: 0.1433
Step 53045: loss = 2.86779
Step 53050: loss = 2.80703
Step 53055: loss = 2.76421
Step 53060: loss = 2.98905
Step 53065: loss = 2.87198
Step 53070: loss = 2.88915
Step 53075: loss = 2.66692
Step 53080: loss = 2.89978
Step 53085: loss = 2.81506
Step 53090: loss = 2.69943
Step 53095: loss = 2.71816
Step 53100: loss = 2.91867
Step 53105: loss = 2.98274
Step 53110: loss = 2.75988
Step 53115: loss = 2.69027
Step 53120: loss = 2.98862
Step 53125: loss = 2.78068
Step 53130: loss = 2.80923
Step 53135: loss = 2.84458
Step 53140: loss = 3.06492
Step 53145: loss = 2.99378
Step 53150: loss = 2.77070
Step 53155: loss = 2.61487
Step 53160: loss = 3.05683
Step 53165: loss = 2.58588
Step 53170: loss = 2.66327
Step 53175: loss = 2.66841
Step 53180: loss = 2.66170
Step 53185: loss = 2.92029
Step 53190: loss = 2.78109
Step 53195: loss = 2.83755
Step 53200: loss = 2.83015
Step 53205: loss = 2.77875
Step 53210: loss = 2.80399
Step 53215: loss = 2.73493
Step 53220: loss = 2.75259
Step 53225: loss = 2.88891
Step 53230: loss = 2.89358
Step 53235: loss = 2.67628
Step 53240: loss = 2.75274
Step 53245: loss = 2.95109
Step 53250: loss = 2.77143
Step 53255: loss = 2.75056
Step 53260: loss = 2.65091
Step 53265: loss = 2.71013
Step 53270: loss = 2.95359
Step 53275: loss = 2.84521
Step 53280: loss = 2.78076
Step 53285: loss = 2.77191
Step 53290: loss = 2.83729
Step 53295: loss = 2.92303
Step 53300: loss = 2.77854
Step 53305: loss = 2.75554
Step 53310: loss = 2.65325
Step 53315: loss = 2.78727
Step 53320: loss = 2.85365
Step 53325: loss = 2.63120
Step 53330: loss = 2.80054
Step 53335: loss = 2.93281
Step 53340: loss = 2.77191
Step 53345: loss = 2.82982
Step 53350: loss = 2.89745
Step 53355: loss = 2.88989
Step 53360: loss = 2.94081
Step 53365: loss = 2.82546
Step 53370: loss = 2.73740
Step 53375: loss = 2.78055
Step 53380: loss = 2.82817
Step 53385: loss = 2.80311
Step 53390: loss = 2.90868
Step 53395: loss = 2.63615
Step 53400: loss = 2.75647
Step 53405: loss = 2.87594
Step 53410: loss = 2.72811
Step 53415: loss = 2.90510
Step 53420: loss = 2.80910
Step 53425: loss = 2.77214
Step 53430: loss = 2.83008
Training Data Eval:
  Num examples: 49920, Num correct: 6655, Precision @ 1: 0.1333
('Testing Data Eval: EPOCH->', 138)
  Num examples: 9984, Num correct: 1300, Precision @ 1: 0.1302
Step 53435: loss = 2.80293
Step 53440: loss = 2.81047
Step 53445: loss = 2.85862
Step 53450: loss = 2.90243
Step 53455: loss = 2.76370
Step 53460: loss = 2.70364
Step 53465: loss = 2.92638
Step 53470: loss = 2.91559
Step 53475: loss = 2.68070
Step 53480: loss = 2.69442
Step 53485: loss = 2.76885
Step 53490: loss = 2.91848
Step 53495: loss = 2.90241
Step 53500: loss = 2.69210
Step 53505: loss = 2.74192
Step 53510: loss = 3.20963
Step 53515: loss = 2.75585
Step 53520: loss = 2.69831
Step 53525: loss = 2.84947
Step 53530: loss = 2.66929
Step 53535: loss = 2.62834
Step 53540: loss = 2.89031
Step 53545: loss = 2.92446
Step 53550: loss = 2.72917
Step 53555: loss = 2.86106
Step 53560: loss = 2.61271
Step 53565: loss = 2.77394
Step 53570: loss = 2.70700
Step 53575: loss = 2.72574
Step 53580: loss = 2.75027
Step 53585: loss = 2.74909
Step 53590: loss = 2.86792
Step 53595: loss = 2.74339
Step 53600: loss = 2.65323
Step 53605: loss = 2.98127
Step 53610: loss = 2.77594
Step 53615: loss = 2.75017
Step 53620: loss = 3.21518
Step 53625: loss = 2.60725
Step 53630: loss = 2.72749
Step 53635: loss = 2.89061
Step 53640: loss = 2.85567
Step 53645: loss = 2.99673
Step 53650: loss = 2.76745
Step 53655: loss = 2.89459
Step 53660: loss = 2.97125
Step 53665: loss = 2.95745
Step 53670: loss = 2.86168
Step 53675: loss = 2.85379
Step 53680: loss = 2.90927
Step 53685: loss = 2.87653
Step 53690: loss = 2.69746
Step 53695: loss = 2.77884
Step 53700: loss = 3.17229
Step 53705: loss = 2.75320
Step 53710: loss = 2.77287
Step 53715: loss = 2.86852
Step 53720: loss = 2.64137
Step 53725: loss = 2.65321
Step 53730: loss = 2.67578
Step 53735: loss = 2.66092
Step 53740: loss = 2.59599
Step 53745: loss = 2.64015
Step 53750: loss = 2.70679
Step 53755: loss = 2.57647
Step 53760: loss = 2.88451
Step 53765: loss = 2.63183
Step 53770: loss = 2.64309
Step 53775: loss = 2.86930
Step 53780: loss = 2.72783
Step 53785: loss = 2.77895
Step 53790: loss = 2.74893
Step 53795: loss = 2.87971
Step 53800: loss = 3.04506
Step 53805: loss = 2.72238
Step 53810: loss = 2.65545
Step 53815: loss = 2.80926
Step 53820: loss = 2.96674
Training Data Eval:
  Num examples: 49920, Num correct: 6921, Precision @ 1: 0.1386
('Testing Data Eval: EPOCH->', 139)
  Num examples: 9984, Num correct: 1423, Precision @ 1: 0.1425
Step 53825: loss = 2.91803
Step 53830: loss = 3.00562
Step 53835: loss = 3.03814
Step 53840: loss = 2.80975
Step 53845: loss = 2.62338
Step 53850: loss = 2.66343
Step 53855: loss = 2.61937
Step 53860: loss = 2.67569
Step 53865: loss = 2.89939
Step 53870: loss = 2.81310
Step 53875: loss = 2.70489
Step 53880: loss = 2.81469
Step 53885: loss = 2.73510
Step 53890: loss = 3.04680
Step 53895: loss = 2.88323
Step 53900: loss = 2.75917
Step 53905: loss = 2.63177
Step 53910: loss = 2.94847
Step 53915: loss = 2.83170
Step 53920: loss = 2.92222
Step 53925: loss = 2.96001
Step 53930: loss = 2.84708
Step 53935: loss = 2.77892
Step 53940: loss = 2.90673
Step 53945: loss = 2.80182
Step 53950: loss = 2.92443
Step 53955: loss = 2.74502
Step 53960: loss = 2.89264
Step 53965: loss = 2.80171
Step 53970: loss = 2.81301
Step 53975: loss = 2.83564
Step 53980: loss = 2.79047
Step 53985: loss = 2.86572
Step 53990: loss = 2.70142
Step 53995: loss = 2.73426
Step 54000: loss = 2.64418
Step 54005: loss = 2.67446
Step 54010: loss = 2.77931
Step 54015: loss = 2.76438
Step 54020: loss = 2.62459
Step 54025: loss = 2.82926
Step 54030: loss = 2.81894
Step 54035: loss = 2.75348
Step 54040: loss = 2.74900
Step 54045: loss = 2.89539
Step 54050: loss = 2.71808
Step 54055: loss = 2.83621
Step 54060: loss = 2.72677
Step 54065: loss = 2.82678
Step 54070: loss = 2.87721
Step 54075: loss = 2.71175
Step 54080: loss = 3.02602
Step 54085: loss = 2.59041
Step 54090: loss = 2.68536
Step 54095: loss = 2.74552
Step 54100: loss = 2.85048
Step 54105: loss = 2.78711
Step 54110: loss = 2.79238
Step 54115: loss = 2.93142
Step 54120: loss = 2.81972
Step 54125: loss = 2.71343
Step 54130: loss = 3.02419
Step 54135: loss = 2.79786
Step 54140: loss = 2.91346
Step 54145: loss = 2.78174
Step 54150: loss = 2.92033
Step 54155: loss = 2.51242
Step 54160: loss = 2.66988
Step 54165: loss = 2.71233
Step 54170: loss = 3.02746
Step 54175: loss = 2.89925
Step 54180: loss = 2.75949
Step 54185: loss = 2.75956
Step 54190: loss = 2.95170
Step 54195: loss = 2.73195
Step 54200: loss = 2.72969
Step 54205: loss = 3.09472
Step 54210: loss = 2.78496
Training Data Eval:
  Num examples: 49920, Num correct: 6937, Precision @ 1: 0.1390
('Testing Data Eval: EPOCH->', 140)
  Num examples: 9984, Num correct: 1319, Precision @ 1: 0.1321
Step 54215: loss = 2.67802
Step 54220: loss = 2.93673
Step 54225: loss = 2.83693
Step 54230: loss = 2.99960
Step 54235: loss = 2.92494
Step 54240: loss = 2.76780
Step 54245: loss = 2.95026
Step 54250: loss = 2.72788
Step 54255: loss = 2.89412
Step 54260: loss = 2.65165
Step 54265: loss = 2.69954
Step 54270: loss = 2.65625
Step 54275: loss = 3.00695
Step 54280: loss = 2.54358
Step 54285: loss = 2.63930
Step 54290: loss = 2.61997
Step 54295: loss = 2.97422
Step 54300: loss = 2.86638
Step 54305: loss = 2.77806
Step 54310: loss = 2.75510
Step 54315: loss = 2.82195
Step 54320: loss = 2.85686
Step 54325: loss = 2.98305
Step 54330: loss = 2.66517
Step 54335: loss = 2.89714
Step 54340: loss = 2.99696
Step 54345: loss = 2.89126
Step 54350: loss = 2.70800
Step 54355: loss = 2.94709
Step 54360: loss = 2.97850
Step 54365: loss = 2.88847
Step 54370: loss = 2.84804
Step 54375: loss = 3.05323
Step 54380: loss = 2.84848
Step 54385: loss = 2.90689
Step 54390: loss = 2.71722
Step 54395: loss = 2.61906
Step 54400: loss = 2.96004
Step 54405: loss = 3.07751
Step 54410: loss = 2.87140
Step 54415: loss = 2.69579
Step 54420: loss = 2.68493
Step 54425: loss = 2.80731
Step 54430: loss = 2.70733
Step 54435: loss = 2.92336
Step 54440: loss = 2.72022
Step 54445: loss = 2.60622
Step 54450: loss = 2.82614
Step 54455: loss = 2.81134
Step 54460: loss = 2.84805
Step 54465: loss = 2.74113
Step 54470: loss = 2.71059
Step 54475: loss = 2.85973
Step 54480: loss = 2.86953
Step 54485: loss = 2.75121
Step 54490: loss = 2.60855
Step 54495: loss = 2.90235
Step 54500: loss = 2.79423
Step 54505: loss = 2.59355
Step 54510: loss = 2.95400
Step 54515: loss = 2.88872
Step 54520: loss = 2.84446
Step 54525: loss = 2.93949
Step 54530: loss = 2.84395
Step 54535: loss = 2.70556
Step 54540: loss = 2.85775
Step 54545: loss = 3.03433
Step 54550: loss = 2.65508
Step 54555: loss = 3.07320
Step 54560: loss = 2.78483
Step 54565: loss = 2.86558
Step 54570: loss = 2.81060
Step 54575: loss = 2.78402
Step 54580: loss = 3.11859
Step 54585: loss = 2.70251
Step 54590: loss = 2.86923
Step 54595: loss = 2.77106
Step 54600: loss = 2.85996
Training Data Eval:
  Num examples: 49920, Num correct: 6604, Precision @ 1: 0.1323
('Testing Data Eval: EPOCH->', 141)
  Num examples: 9984, Num correct: 1353, Precision @ 1: 0.1355
Step 54605: loss = 2.83827
Step 54610: loss = 2.97298
Step 54615: loss = 2.82776
Step 54620: loss = 2.73098
Step 54625: loss = 2.79197
Step 54630: loss = 3.02677
Step 54635: loss = 2.79934
Step 54640: loss = 2.55149
Step 54645: loss = 2.62972
Step 54650: loss = 2.95989
Step 54655: loss = 3.04853
Step 54660: loss = 2.89719
Step 54665: loss = 2.79831
Step 54670: loss = 2.68082
Step 54675: loss = 2.81475
Step 54680: loss = 2.79607
Step 54685: loss = 2.95414
Step 54690: loss = 2.86995
Step 54695: loss = 2.67834
Step 54700: loss = 2.77000
Step 54705: loss = 2.78359
Step 54710: loss = 2.63918
Step 54715: loss = 2.60786
Step 54720: loss = 2.94301
Step 54725: loss = 2.52640
Step 54730: loss = 2.63766
Step 54735: loss = 2.90171
Step 54740: loss = 2.83312
Step 54745: loss = 2.81412
Step 54750: loss = 2.66129
Step 54755: loss = 2.78301
Step 54760: loss = 2.80377
Step 54765: loss = 2.83968
Step 54770: loss = 2.91840
Step 54775: loss = 2.78760
Step 54780: loss = 2.83132
Step 54785: loss = 2.74641
Step 54790: loss = 2.94782
Step 54795: loss = 2.77192
Step 54800: loss = 2.61161
Step 54805: loss = 2.83647
Step 54810: loss = 3.02991
Step 54815: loss = 2.82342
Step 54820: loss = 2.60230
Step 54825: loss = 2.93149
Step 54830: loss = 2.76142
Step 54835: loss = 2.80783
Step 54840: loss = 2.85452
Step 54845: loss = 2.57432
Step 54850: loss = 2.64679
Step 54855: loss = 2.74057
Step 54860: loss = 2.97852
Step 54865: loss = 2.93899
Step 54870: loss = 2.91300
Step 54875: loss = 2.77787
Step 54880: loss = 2.96332
Step 54885: loss = 2.77686
Step 54890: loss = 2.71416
Step 54895: loss = 3.02598
Step 54900: loss = 2.74885
Step 54905: loss = 2.86735
Step 54910: loss = 2.81906
Step 54915: loss = 3.19984
Step 54920: loss = 2.89482
Step 54925: loss = 2.73167
Step 54930: loss = 2.86990
Step 54935: loss = 2.81594
Step 54940: loss = 2.84595
Step 54945: loss = 2.74384
Step 54950: loss = 2.77285
Step 54955: loss = 2.72832
Step 54960: loss = 2.75607
Step 54965: loss = 2.76071
Step 54970: loss = 2.92427
Step 54975: loss = 2.94266
Step 54980: loss = 2.82925
Step 54985: loss = 2.85335
Step 54990: loss = 2.89219
Training Data Eval:
  Num examples: 49920, Num correct: 6897, Precision @ 1: 0.1382
('Testing Data Eval: EPOCH->', 142)
  Num examples: 9984, Num correct: 1393, Precision @ 1: 0.1395
Step 54995: loss = 2.88436
Step 55000: loss = 2.78285
Step 55005: loss = 2.75796
Step 55010: loss = 2.87279
Step 55015: loss = 3.06553
Step 55020: loss = 2.54983
Step 55025: loss = 2.87431
Step 55030: loss = 2.97071
Step 55035: loss = 2.97629
Step 55040: loss = 2.97338
Step 55045: loss = 2.81809
Step 55050: loss = 2.64588
Step 55055: loss = 2.69166
Step 55060: loss = 2.84979
Step 55065: loss = 2.88877
Step 55070: loss = 2.77120
Step 55075: loss = 2.64283
Step 55080: loss = 2.71970
Step 55085: loss = 2.78527
Step 55090: loss = 2.97227
Step 55095: loss = 2.82423
Step 55100: loss = 2.86814
Step 55105: loss = 2.87008
Step 55110: loss = 2.89394
Step 55115: loss = 2.67573
Step 55120: loss = 2.76342
Step 55125: loss = 3.04205
Step 55130: loss = 2.81693
Step 55135: loss = 2.64374
Step 55140: loss = 2.90877
Step 55145: loss = 2.84818
Step 55150: loss = 2.73495
Step 55155: loss = 2.80802
Step 55160: loss = 2.85735
Step 55165: loss = 2.68732
Step 55170: loss = 2.88066
Step 55175: loss = 2.89635
Step 55180: loss = 2.74263
Step 55185: loss = 2.74627
Step 55190: loss = 2.88241
Step 55195: loss = 2.81263
Step 55200: loss = 2.90141
Step 55205: loss = 2.91228
Step 55210: loss = 2.61325
Step 55215: loss = 2.73930
Step 55220: loss = 2.86527
Step 55225: loss = 2.89078
Step 55230: loss = 2.84766
Step 55235: loss = 2.73093
Step 55240: loss = 2.83286
Step 55245: loss = 2.79864
Step 55250: loss = 3.05900
Step 55255: loss = 2.85416
Step 55260: loss = 2.87653
Step 55265: loss = 2.89121
Step 55270: loss = 2.69408
Step 55275: loss = 2.81877
Step 55280: loss = 2.81894
Step 55285: loss = 2.75101
Step 55290: loss = 2.72125
Step 55295: loss = 2.63746
Step 55300: loss = 2.95308
Step 55305: loss = 2.61318
Step 55310: loss = 2.66690
Step 55315: loss = 2.83850
Step 55320: loss = 2.89690
Step 55325: loss = 2.99318
Step 55330: loss = 2.90445
Step 55335: loss = 2.78541
Step 55340: loss = 3.15617
Step 55345: loss = 2.77128
Step 55350: loss = 2.92543
Step 55355: loss = 2.76004
Step 55360: loss = 2.75935
Step 55365: loss = 2.82173
Step 55370: loss = 2.79049
Step 55375: loss = 3.01406
Step 55380: loss = 2.75017
Training Data Eval:
  Num examples: 49920, Num correct: 6477, Precision @ 1: 0.1297
('Testing Data Eval: EPOCH->', 143)
  Num examples: 9984, Num correct: 1341, Precision @ 1: 0.1343
Step 55385: loss = 2.85953
Step 55390: loss = 2.57447
Step 55395: loss = 2.91782
Step 55400: loss = 2.55673
Step 55405: loss = 2.75321
Step 55410: loss = 2.78025
Step 55415: loss = 2.62903
Step 55420: loss = 2.72638
Step 55425: loss = 2.78229
Step 55430: loss = 2.74882
Step 55435: loss = 2.77145
Step 55440: loss = 2.87621
Step 55445: loss = 2.80907
Step 55450: loss = 2.98893
Step 55455: loss = 2.86503
Step 55460: loss = 2.69631
Step 55465: loss = 2.96008
Step 55470: loss = 2.91019
Step 55475: loss = 2.84069
Step 55480: loss = 2.73445
Step 55485: loss = 2.78174
Step 55490: loss = 2.88675
Step 55495: loss = 2.71602
Step 55500: loss = 2.53288
Step 55505: loss = 2.85138
Step 55510: loss = 2.74725
Step 55515: loss = 2.78549
Step 55520: loss = 2.64233
Step 55525: loss = 2.94086
Step 55530: loss = 2.93810
Step 55535: loss = 2.84007
Step 55540: loss = 2.71902
Step 55545: loss = 2.88578
Step 55550: loss = 2.82333
Step 55555: loss = 2.77694
Step 55560: loss = 2.74409
Step 55565: loss = 2.73541
Step 55570: loss = 2.63243
Step 55575: loss = 2.75733
Step 55580: loss = 2.76940
Step 55585: loss = 2.89185
Step 55590: loss = 2.79228
Step 55595: loss = 2.99629
Step 55600: loss = 2.99671
Step 55605: loss = 2.78423
Step 55610: loss = 2.87928
Step 55615: loss = 2.84688
Step 55620: loss = 2.91713
Step 55625: loss = 2.66101
Step 55630: loss = 2.90332
Step 55635: loss = 2.91164
Step 55640: loss = 2.65633
Step 55645: loss = 2.72847
Step 55650: loss = 2.91710
Step 55655: loss = 2.52817
Step 55660: loss = 2.56798
Step 55665: loss = 2.81244
Step 55670: loss = 2.71667
Step 55675: loss = 2.62537
Step 55680: loss = 2.75657
Step 55685: loss = 2.75965
Step 55690: loss = 2.78398
Step 55695: loss = 2.87111
Step 55700: loss = 2.72888
Step 55705: loss = 2.67135
Step 55710: loss = 2.97123
Step 55715: loss = 2.64702
Step 55720: loss = 2.85512
Step 55725: loss = 2.80931
Step 55730: loss = 2.62823
Step 55735: loss = 2.77229
Step 55740: loss = 2.92463
Step 55745: loss = 2.99287
Step 55750: loss = 2.82357
Step 55755: loss = 2.90156
Step 55760: loss = 2.98611
Step 55765: loss = 2.74899
Step 55770: loss = 3.00715
Training Data Eval:
  Num examples: 49920, Num correct: 6939, Precision @ 1: 0.1390
('Testing Data Eval: EPOCH->', 144)
  Num examples: 9984, Num correct: 1432, Precision @ 1: 0.1434
Step 55775: loss = 2.75028
Step 55780: loss = 2.76128
Step 55785: loss = 2.65978
Step 55790: loss = 2.69159
Step 55795: loss = 2.63436
Step 55800: loss = 2.79554
Step 55805: loss = 2.69968
Step 55810: loss = 2.92453
Step 55815: loss = 2.68096
Step 55820: loss = 2.97203
Step 55825: loss = 2.56980
Step 55830: loss = 2.71780
Step 55835: loss = 2.82731
Step 55840: loss = 2.89961
Step 55845: loss = 3.05267
Step 55850: loss = 2.71273
Step 55855: loss = 2.94480
Step 55860: loss = 2.79085
Step 55865: loss = 2.98329
Step 55870: loss = 2.94633
Step 55875: loss = 2.84470
Step 55880: loss = 2.85503
Step 55885: loss = 2.82327
Step 55890: loss = 2.75281
Step 55895: loss = 2.76099
Step 55900: loss = 2.65593
Step 55905: loss = 2.92973
Step 55910: loss = 2.79325
Step 55915: loss = 2.78380
Step 55920: loss = 2.71901
Step 55925: loss = 2.82947
Step 55930: loss = 2.70409
Step 55935: loss = 2.71570
Step 55940: loss = 2.88338
Step 55945: loss = 2.98892
Step 55950: loss = 3.04763
Step 55955: loss = 2.77627
Step 55960: loss = 2.76775
Step 55965: loss = 2.75700
Step 55970: loss = 2.85248
Step 55975: loss = 3.00205
Step 55980: loss = 2.92452
Step 55985: loss = 2.69271
Step 55990: loss = 2.74004
Step 55995: loss = 2.65958
Step 56000: loss = 2.98921
Step 56005: loss = 2.62973
Step 56010: loss = 2.85793
Step 56015: loss = 3.03925
Step 56020: loss = 2.73957
Step 56025: loss = 2.84751
Step 56030: loss = 2.89091
Step 56035: loss = 2.59746
Step 56040: loss = 2.97520
Step 56045: loss = 2.85932
Step 56050: loss = 2.97122
Step 56055: loss = 2.77066
Step 56060: loss = 2.93359
Step 56065: loss = 2.82496
Step 56070: loss = 2.73820
Step 56075: loss = 2.70746
Step 56080: loss = 2.97091
Step 56085: loss = 2.91017
Step 56090: loss = 2.81894
Step 56095: loss = 2.49140
Step 56100: loss = 2.88750
Step 56105: loss = 2.86084
Step 56110: loss = 2.63562
Step 56115: loss = 3.26065
Step 56120: loss = 3.06072
Step 56125: loss = 3.25726
Step 56130: loss = 3.67006
Step 56135: loss = 2.73688
Step 56140: loss = 3.53622
Step 56145: loss = 2.70440
Step 56150: loss = 2.85596
Step 56155: loss = 2.93054
Step 56160: loss = 2.61106
Training Data Eval:
  Num examples: 49920, Num correct: 6691, Precision @ 1: 0.1340
('Testing Data Eval: EPOCH->', 145)
  Num examples: 9984, Num correct: 1306, Precision @ 1: 0.1308
Step 56165: loss = 2.78585
Step 56170: loss = 2.93569
Step 56175: loss = 2.78676
Step 56180: loss = 3.12506
Step 56185: loss = 2.85414
Step 56190: loss = 2.77330
Step 56195: loss = 2.80958
Step 56200: loss = 2.80191
Step 56205: loss = 2.59637
Step 56210: loss = 2.96764
Step 56215: loss = 2.64534
Step 56220: loss = 2.67972
Step 56225: loss = 3.02188
Step 56230: loss = 3.00656
Step 56235: loss = 2.80010
Step 56240: loss = 3.14214
Step 56245: loss = 2.74297
Step 56250: loss = 2.86621
Step 56255: loss = 2.78632
Step 56260: loss = 2.91884
Step 56265: loss = 2.83736
Step 56270: loss = 2.67614
Step 56275: loss = 2.64836
Step 56280: loss = 2.67792
Step 56285: loss = 2.86338
Step 56290: loss = 2.88056
Step 56295: loss = 2.80694
Step 56300: loss = 2.92574
Step 56305: loss = 2.77504
Step 56310: loss = 2.73298
Step 56315: loss = 2.74382
Step 56320: loss = 2.84705
Step 56325: loss = 2.78195
Step 56330: loss = 2.73955
Step 56335: loss = 2.85512
Step 56340: loss = 2.57554
Step 56345: loss = 2.71313
Step 56350: loss = 2.71951
Step 56355: loss = 2.80406
Step 56360: loss = 2.75135
Step 56365: loss = 2.79456
Step 56370: loss = 2.83985
Step 56375: loss = 2.65175
Step 56380: loss = 3.09261
Step 56385: loss = 2.80541
Step 56390: loss = 2.77981
Step 56395: loss = 2.82241
Step 56400: loss = 2.82611
Step 56405: loss = 2.80724
Step 56410: loss = 2.86572
Step 56415: loss = 2.73046
Step 56420: loss = 3.02050
Step 56425: loss = 2.77954
Step 56430: loss = 2.65276
Step 56435: loss = 2.74453
Step 56440: loss = 2.78754
Step 56445: loss = 2.78778
Step 56450: loss = 2.67752
Step 56455: loss = 2.73395
Step 56460: loss = 2.80812
Step 56465: loss = 2.86405
Step 56470: loss = 2.84591
Step 56475: loss = 2.97492
Step 56480: loss = 2.77342
Step 56485: loss = 3.07135
Step 56490: loss = 2.66713
Step 56495: loss = 2.86157
Step 56500: loss = 2.85746
Step 56505: loss = 2.89884
Step 56510: loss = 2.97816
Step 56515: loss = 2.89034
Step 56520: loss = 2.89245
Step 56525: loss = 3.03619
Step 56530: loss = 2.69384
Step 56535: loss = 2.92228
Step 56540: loss = 2.84086
Step 56545: loss = 3.07050
Step 56550: loss = 2.65954
Training Data Eval:
  Num examples: 49920, Num correct: 7047, Precision @ 1: 0.1412
('Testing Data Eval: EPOCH->', 146)
  Num examples: 9984, Num correct: 1396, Precision @ 1: 0.1398
Step 56555: loss = 2.91882
Step 56560: loss = 2.97309
Step 56565: loss = 2.94062
Step 56570: loss = 2.84004
Step 56575: loss = 2.70163
Step 56580: loss = 2.56384
Step 56585: loss = 3.05134
Step 56590: loss = 2.85182
Step 56595: loss = 3.01154
Step 56600: loss = 2.97055
Step 56605: loss = 2.75424
Step 56610: loss = 2.74957
Step 56615: loss = 2.79497
Step 56620: loss = 2.78165
Step 56625: loss = 3.02193
Step 56630: loss = 2.98973
Step 56635: loss = 2.69498
Step 56640: loss = 2.90095
Step 56645: loss = 2.81611
Step 56650: loss = 2.88810
Step 56655: loss = 2.99003
Step 56660: loss = 2.76373
Step 56665: loss = 2.86152
Step 56670: loss = 2.64876
Step 56675: loss = 2.80237
Step 56680: loss = 3.00671
Step 56685: loss = 2.93353
Step 56690: loss = 2.86020
Step 56695: loss = 2.71238
Step 56700: loss = 2.62209
Step 56705: loss = 2.56151
Step 56710: loss = 2.81371
Step 56715: loss = 2.87892
Step 56720: loss = 2.97477
Step 56725: loss = 2.72609
Step 56730: loss = 2.91649
Step 56735: loss = 2.80618
Step 56740: loss = 2.87755
Step 56745: loss = 2.95224
Step 56750: loss = 2.69974
Step 56755: loss = 2.76347
Step 56760: loss = 2.65590
Step 56765: loss = 2.81683
Step 56770: loss = 2.72991
Step 56775: loss = 2.82698
Step 56780: loss = 3.23077
Step 56785: loss = 2.93479
Step 56790: loss = 2.85098
Step 56795: loss = 2.81312
Step 56800: loss = 2.92642
Step 56805: loss = 2.62680
Step 56810: loss = 2.86592
Step 56815: loss = 3.00229
Step 56820: loss = 3.10975
Step 56825: loss = 2.78540
Step 56830: loss = 2.90892
Step 56835: loss = 2.78338
Step 56840: loss = 2.52937
Step 56845: loss = 2.88180
Step 56850: loss = 2.77048
Step 56855: loss = 2.92467
Step 56860: loss = 3.07164
Step 56865: loss = 2.73237
Step 56870: loss = 2.73081
Step 56875: loss = 2.76419
Step 56880: loss = 2.84181
Step 56885: loss = 2.87815
Step 56890: loss = 2.81437
Step 56895: loss = 2.76234
Step 56900: loss = 2.64683
Step 56905: loss = 2.87853
Step 56910: loss = 2.94440
Step 56915: loss = 2.59858
Step 56920: loss = 2.85412
Step 56925: loss = 2.87408
Step 56930: loss = 2.88896
Step 56935: loss = 2.73283
Step 56940: loss = 2.68307
Training Data Eval:
  Num examples: 49920, Num correct: 6538, Precision @ 1: 0.1310
('Testing Data Eval: EPOCH->', 147)
  Num examples: 9984, Num correct: 1334, Precision @ 1: 0.1336
Step 56945: loss = 2.92692
Step 56950: loss = 2.89589
Step 56955: loss = 3.00771
Step 56960: loss = 2.85792
Step 56965: loss = 2.73429
Step 56970: loss = 2.70251
Step 56975: loss = 2.93775
Step 56980: loss = 2.92081
Step 56985: loss = 3.18561
Step 56990: loss = 2.92050
Step 56995: loss = 2.79324
Step 57000: loss = 2.85945
Step 57005: loss = 2.91514
Step 57010: loss = 3.15732
Step 57015: loss = 2.97921
Step 57020: loss = 2.80062
Step 57025: loss = 2.98576
Step 57030: loss = 2.86077
Step 57035: loss = 2.82467
Step 57040: loss = 2.71815
Step 57045: loss = 2.94479
Step 57050: loss = 2.69611
Step 57055: loss = 2.73655
Step 57060: loss = 2.73987
Step 57065: loss = 2.66117
Step 57070: loss = 2.87369
Step 57075: loss = 2.81324
Step 57080: loss = 2.66761
Step 57085: loss = 2.61572
Step 57090: loss = 2.73875
Step 57095: loss = 2.91886
Step 57100: loss = 2.86905
Step 57105: loss = 2.99567
Step 57110: loss = 2.88038
Step 57115: loss = 2.81160
Step 57120: loss = 2.80439
Step 57125: loss = 2.83395
Step 57130: loss = 2.69510
Step 57135: loss = 2.78458
Step 57140: loss = 2.72780
Step 57145: loss = 2.99147
Step 57150: loss = 2.72089
Step 57155: loss = 2.55796
Step 57160: loss = 2.80163
Step 57165: loss = 2.90881
Step 57170: loss = 2.86973
Step 57175: loss = 2.73688
Step 57180: loss = 2.81661
Step 57185: loss = 2.85526
Step 57190: loss = 2.93583
Step 57195: loss = 2.52519
Step 57200: loss = 2.79612
Step 57205: loss = 2.71600
Step 57210: loss = 2.68923
Step 57215: loss = 2.73930
Step 57220: loss = 2.82019
Step 57225: loss = 2.81997
Step 57230: loss = 2.69251
Step 57235: loss = 2.94502
Step 57240: loss = 2.76448
Step 57245: loss = 2.93605
Step 57250: loss = 2.99931
Step 57255: loss = 2.82030
Step 57260: loss = 2.84293
Step 57265: loss = 2.77386
Step 57270: loss = 2.84370
Step 57275: loss = 2.73483
Step 57280: loss = 2.71431
Step 57285: loss = 2.79199
Step 57290: loss = 2.82664
Step 57295: loss = 2.97971
Step 57300: loss = 2.90170
Step 57305: loss = 2.80807
Step 57310: loss = 2.80170
Step 57315: loss = 2.93295
Step 57320: loss = 2.54693
Step 57325: loss = 2.90487
Step 57330: loss = 2.86867
Training Data Eval:
  Num examples: 49920, Num correct: 6411, Precision @ 1: 0.1284
('Testing Data Eval: EPOCH->', 148)
  Num examples: 9984, Num correct: 1349, Precision @ 1: 0.1351
Step 57335: loss = 2.74507
Step 57340: loss = 2.88591
Step 57345: loss = 2.96755
Step 57350: loss = 2.89700
Step 57355: loss = 2.70310
Step 57360: loss = 2.82698
Step 57365: loss = 2.95202
Step 57370: loss = 3.11151
Step 57375: loss = 2.74273
Step 57380: loss = 2.79427
Step 57385: loss = 2.79162
Step 57390: loss = 2.79236
Step 57395: loss = 2.67910
Step 57400: loss = 2.91169
Step 57405: loss = 2.76491
Step 57410: loss = 2.82599
Step 57415: loss = 2.75533
Step 57420: loss = 3.09892
Step 57425: loss = 2.74653
Step 57430: loss = 2.62385
Step 57435: loss = 2.60802
Step 57440: loss = 2.89683
Step 57445: loss = 2.78386
Step 57450: loss = 2.67164
Step 57455: loss = 2.82499
Step 57460: loss = 2.73639
Step 57465: loss = 2.67357
Step 57470: loss = 2.84208
Step 57475: loss = 3.05690
Step 57480: loss = 2.70260
Step 57485: loss = 3.02722
Step 57490: loss = 2.90104
Step 57495: loss = 2.72730
Step 57500: loss = 2.90908
Step 57505: loss = 2.85495
Step 57510: loss = 2.68891
Step 57515: loss = 2.80848
Step 57520: loss = 2.97131
Step 57525: loss = 2.85973
Step 57530: loss = 2.75472
Step 57535: loss = 2.75814
Step 57540: loss = 2.94960
Step 57545: loss = 2.90041
Step 57550: loss = 3.01383
Step 57555: loss = 2.88613
Step 57560: loss = 2.74271
Step 57565: loss = 2.78186
Step 57570: loss = 2.86714
Step 57575: loss = 2.73073
Step 57580: loss = 2.67056
Step 57585: loss = 2.77278
Step 57590: loss = 2.79146
Step 57595: loss = 2.84806
Step 57600: loss = 2.76514
Step 57605: loss = 3.02207
Step 57610: loss = 2.90565
Step 57615: loss = 2.94605
Step 57620: loss = 2.73789
Step 57625: loss = 3.06299
Step 57630: loss = 2.75234
Step 57635: loss = 2.82449
Step 57640: loss = 2.84270
Step 57645: loss = 2.92330
Step 57650: loss = 2.91284
Step 57655: loss = 2.95109
Step 57660: loss = 2.80192
Step 57665: loss = 2.76756
Step 57670: loss = 2.84958
Step 57675: loss = 2.78910
Step 57680: loss = 2.77442
Step 57685: loss = 2.78578
Step 57690: loss = 2.87160
Step 57695: loss = 2.90695
Step 57700: loss = 2.68720
Step 57705: loss = 2.84188
Step 57710: loss = 3.10827
Step 57715: loss = 2.80791
Step 57720: loss = 2.66816
Training Data Eval:
  Num examples: 49920, Num correct: 6717, Precision @ 1: 0.1346
('Testing Data Eval: EPOCH->', 149)
  Num examples: 9984, Num correct: 1361, Precision @ 1: 0.1363
Step 57725: loss = 2.86266
Step 57730: loss = 2.73916
Step 57735: loss = 2.85578
Step 57740: loss = 2.90272
Step 57745: loss = 2.86159
Step 57750: loss = 2.76166
Step 57755: loss = 2.75231
Step 57760: loss = 2.85500
Step 57765: loss = 2.87398
Step 57770: loss = 2.77237
Step 57775: loss = 3.09695
Step 57780: loss = 2.94970
Step 57785: loss = 2.70018
Step 57790: loss = 2.73325
Step 57795: loss = 2.64208
Step 57800: loss = 2.80960
Step 57805: loss = 2.61099
Step 57810: loss = 2.86046
Step 57815: loss = 2.69403
Step 57820: loss = 2.79528
Step 57825: loss = 2.85209
Step 57830: loss = 2.58410
Step 57835: loss = 2.73354
Step 57840: loss = 2.82593
Step 57845: loss = 2.89527
Step 57850: loss = 2.82489
Step 57855: loss = 2.74327
Step 57860: loss = 2.77501
Step 57865: loss = 2.91925
Step 57870: loss = 2.78388
Step 57875: loss = 2.66040
Step 57880: loss = 2.66030
Step 57885: loss = 2.76950
Step 57890: loss = 2.75643
Step 57895: loss = 2.79045
Step 57900: loss = 2.81339
Step 57905: loss = 2.71471
Step 57910: loss = 2.69309
Step 57915: loss = 2.68282
Step 57920: loss = 2.64618
Step 57925: loss = 2.76731
Step 57930: loss = 2.90070
Step 57935: loss = 2.68787
Step 57940: loss = 2.77475
Step 57945: loss = 2.87700
Step 57950: loss = 2.83088
Step 57955: loss = 2.79734
Step 57960: loss = 2.84439
Step 57965: loss = 2.65245
Step 57970: loss = 2.70421
Step 57975: loss = 2.96298
Step 57980: loss = 3.01633
Step 57985: loss = 2.70218
Step 57990: loss = 2.89458
Step 57995: loss = 2.64768
Step 58000: loss = 2.72421
Step 58005: loss = 2.78727
Step 58010: loss = 2.86939
Step 58015: loss = 2.83834
Step 58020: loss = 2.94184
Step 58025: loss = 2.86210
Step 58030: loss = 2.79515
Step 58035: loss = 2.81072
Step 58040: loss = 2.59820
Step 58045: loss = 2.75127
Step 58050: loss = 2.85770
Step 58055: loss = 2.78352
Step 58060: loss = 2.69418
Step 58065: loss = 2.87101
Step 58070: loss = 2.62017
Step 58075: loss = 2.80142
Step 58080: loss = 3.04526
Step 58085: loss = 2.98616
Step 58090: loss = 2.99111
Step 58095: loss = 2.77879
Step 58100: loss = 2.77127
Step 58105: loss = 2.84189
Step 58110: loss = 2.90156
Training Data Eval:
  Num examples: 49920, Num correct: 7081, Precision @ 1: 0.1418
('Testing Data Eval: EPOCH->', 150)
  Num examples: 9984, Num correct: 1480, Precision @ 1: 0.1482
Step 58115: loss = 2.96901
Step 58120: loss = 2.83404
Step 58125: loss = 2.73305
Step 58130: loss = 2.56812
Step 58135: loss = 3.04180
Step 58140: loss = 2.98099
Step 58145: loss = 2.85248
Step 58150: loss = 2.65435
Step 58155: loss = 2.50520
Step 58160: loss = 2.66784
Step 58165: loss = 2.74869
Step 58170: loss = 2.98111
Step 58175: loss = 2.83542
Step 58180: loss = 2.69236
Step 58185: loss = 2.89117
Step 58190: loss = 3.11122
Step 58195: loss = 2.93427
Step 58200: loss = 2.79125
Step 58205: loss = 3.18514
Step 58210: loss = 2.75923
Step 58215: loss = 2.82972
Step 58220: loss = 2.75912
Step 58225: loss = 2.58537
Step 58230: loss = 2.66772
Step 58235: loss = 2.81112
Step 58240: loss = 2.76744
Step 58245: loss = 2.87868
Step 58250: loss = 3.05319
Step 58255: loss = 2.85099
Step 58260: loss = 2.84667
Step 58265: loss = 2.93903
Step 58270: loss = 2.88568
Step 58275: loss = 2.90052
Step 58280: loss = 2.82602
Step 58285: loss = 2.78634
Step 58290: loss = 2.66354
Step 58295: loss = 2.74635
Step 58300: loss = 2.87420
Step 58305: loss = 2.93523
Step 58310: loss = 2.78779
Step 58315: loss = 2.82566
Step 58320: loss = 2.72827
Step 58325: loss = 2.84363
Step 58330: loss = 2.68688
Step 58335: loss = 2.90092
Step 58340: loss = 2.94548
Step 58345: loss = 2.75635
Step 58350: loss = 2.86430
Step 58355: loss = 2.80072
Step 58360: loss = 2.80484
Step 58365: loss = 2.84884
Step 58370: loss = 2.90288
Step 58375: loss = 3.02552
Step 58380: loss = 2.83716
Step 58385: loss = 2.88982
Step 58390: loss = 2.89159
Step 58395: loss = 2.69432
Step 58400: loss = 2.60337
Step 58405: loss = 2.92894
Step 58410: loss = 2.65838
Step 58415: loss = 2.48530
Step 58420: loss = 2.73386
Step 58425: loss = 2.86853
Step 58430: loss = 2.94803
Step 58435: loss = 2.81721
Step 58440: loss = 3.00022
Step 58445: loss = 2.89329
Step 58450: loss = 2.81443
Step 58455: loss = 2.93211
Step 58460: loss = 2.74210
Step 58465: loss = 2.86001
Step 58470: loss = 2.67031
Step 58475: loss = 2.93622
Step 58480: loss = 2.95337
Step 58485: loss = 2.70186
Step 58490: loss = 2.80426
Step 58495: loss = 2.65871
Step 58500: loss = 2.75698
Training Data Eval:
  Num examples: 49920, Num correct: 6981, Precision @ 1: 0.1398
('Testing Data Eval: EPOCH->', 151)
  Num examples: 9984, Num correct: 1411, Precision @ 1: 0.1413
Step 58505: loss = 2.74179
Step 58510: loss = 2.79939
Step 58515: loss = 2.64857
Step 58520: loss = 2.61898
Step 58525: loss = 3.12328
Step 58530: loss = 2.90918
Step 58535: loss = 3.14049
Step 58540: loss = 2.95431
Step 58545: loss = 2.84282
Step 58550: loss = 2.76957
Step 58555: loss = 2.96374
Step 58560: loss = 2.90641
Step 58565: loss = 2.98153
Step 58570: loss = 2.81329
Step 58575: loss = 2.65073
Step 58580: loss = 2.80556
Step 58585: loss = 2.97796
Step 58590: loss = 2.64019
Step 58595: loss = 2.65791
Step 58600: loss = 2.64482
Step 58605: loss = 2.79360
Step 58610: loss = 2.69172
Step 58615: loss = 2.94412
Step 58620: loss = 2.69696
Step 58625: loss = 2.80724
Step 58630: loss = 2.82288
Step 58635: loss = 2.62794
Step 58640: loss = 2.75546
Step 58645: loss = 2.79496
Step 58650: loss = 2.82758
Step 58655: loss = 2.80476
Step 58660: loss = 2.91367
Step 58665: loss = 2.84627
Step 58670: loss = 2.89526
Step 58675: loss = 2.75083
Step 58680: loss = 2.81353
Step 58685: loss = 2.87566
Step 58690: loss = 2.66755
Step 58695: loss = 3.07666
Step 58700: loss = 2.99927
Step 58705: loss = 3.05092
Step 58710: loss = 2.63835
Step 58715: loss = 2.81470
Step 58720: loss = 2.82578
Step 58725: loss = 2.86560
Step 58730: loss = 2.78414
Step 58735: loss = 2.92911
Step 58740: loss = 2.87096
Step 58745: loss = 3.07803
Step 58750: loss = 2.63640
Step 58755: loss = 2.84259
Step 58760: loss = 2.62135
Step 58765: loss = 2.79824
Step 58770: loss = 2.58348
Step 58775: loss = 2.80647
Step 58780: loss = 2.90761
Step 58785: loss = 2.73205
Step 58790: loss = 2.79460
Step 58795: loss = 2.78469
Step 58800: loss = 2.95791
Step 58805: loss = 2.76638
Step 58810: loss = 2.98373
Step 58815: loss = 2.81721
Step 58820: loss = 2.88424
Step 58825: loss = 2.79318
Step 58830: loss = 2.76928
Step 58835: loss = 2.95659
Step 58840: loss = 3.02039
Step 58845: loss = 2.78593
Step 58850: loss = 2.65088
Step 58855: loss = 2.71119
Step 58860: loss = 2.98091
Step 58865: loss = 2.92408
Step 58870: loss = 2.82835
Step 58875: loss = 2.74678
Step 58880: loss = 2.89032
Step 58885: loss = 2.80831
Step 58890: loss = 2.75319
Training Data Eval:
  Num examples: 49920, Num correct: 6937, Precision @ 1: 0.1390
('Testing Data Eval: EPOCH->', 152)
  Num examples: 9984, Num correct: 1378, Precision @ 1: 0.1380
Step 58895: loss = 2.69205
Step 58900: loss = 2.94202
Step 58905: loss = 2.76576
Step 58910: loss = 2.86111
Step 58915: loss = 2.72525
Step 58920: loss = 2.57632
Step 58925: loss = 2.97552
Step 58930: loss = 2.86989
Step 58935: loss = 2.75286
Step 58940: loss = 2.71263
Step 58945: loss = 2.78766
Step 58950: loss = 2.81190
Step 58955: loss = 2.85494
Step 58960: loss = 2.86168
Step 58965: loss = 2.52108
Step 58970: loss = 2.74060
Step 58975: loss = 2.78730
Step 58980: loss = 2.80090
Step 58985: loss = 2.79923
Step 58990: loss = 2.72286
Step 58995: loss = 3.04083
Step 59000: loss = 2.62208
Step 59005: loss = 2.81633
Step 59010: loss = 2.81807
Step 59015: loss = 2.82761
Step 59020: loss = 2.99288
Step 59025: loss = 2.96541
Step 59030: loss = 2.65601
Step 59035: loss = 2.80860
Step 59040: loss = 2.85759
Step 59045: loss = 2.80009
Step 59050: loss = 2.81918
Step 59055: loss = 2.80627
Step 59060: loss = 2.81045
Step 59065: loss = 2.74392
Step 59070: loss = 2.72156
Step 59075: loss = 2.67772
Step 59080: loss = 2.74315
Step 59085: loss = 3.17750
Step 59090: loss = 2.77216
Step 59095: loss = 2.88127
Step 59100: loss = 2.80135
Step 59105: loss = 2.73251
Step 59110: loss = 2.73877
Step 59115: loss = 2.75626
Step 59120: loss = 2.80224
Step 59125: loss = 2.74661
Step 59130: loss = 2.78842
Step 59135: loss = 2.87163
Step 59140: loss = 2.87601
Step 59145: loss = 2.80236
Step 59150: loss = 2.85212
Step 59155: loss = 2.74321
Step 59160: loss = 2.45782
Step 59165: loss = 2.94942
Step 59170: loss = 2.69001
Step 59175: loss = 2.42921
Step 59180: loss = 2.86934
Step 59185: loss = 2.67265
Step 59190: loss = 2.70086
Step 59195: loss = 2.61412
Step 59200: loss = 2.85676
Step 59205: loss = 2.61862
Step 59210: loss = 2.67294
Step 59215: loss = 2.75411
Step 59220: loss = 2.78644
Step 59225: loss = 2.74610
Step 59230: loss = 2.84028
Step 59235: loss = 2.93715
Step 59240: loss = 2.83405
Step 59245: loss = 2.99531
Step 59250: loss = 2.80651
Step 59255: loss = 2.93492
Step 59260: loss = 2.79153
Step 59265: loss = 3.25279
Step 59270: loss = 2.82692
Step 59275: loss = 2.68647
Step 59280: loss = 2.86375
Training Data Eval:
  Num examples: 49920, Num correct: 6716, Precision @ 1: 0.1345
('Testing Data Eval: EPOCH->', 153)
  Num examples: 9984, Num correct: 1362, Precision @ 1: 0.1364
Step 59285: loss = 2.88677
Step 59290: loss = 2.84860
Step 59295: loss = 2.78526
Step 59300: loss = 2.91186
Step 59305: loss = 2.73940
Step 59310: loss = 2.93483
Step 59315: loss = 2.78879
Step 59320: loss = 2.71828
Step 59325: loss = 2.91219
Step 59330: loss = 2.85037
Step 59335: loss = 2.81489
Step 59340: loss = 2.87024
Step 59345: loss = 2.76927
Step 59350: loss = 2.82106
Step 59355: loss = 2.56268
Step 59360: loss = 2.68664
Step 59365: loss = 2.97196
Step 59370: loss = 2.73768
Step 59375: loss = 2.77475
Step 59380: loss = 2.79201
Step 59385: loss = 2.67039
Step 59390: loss = 2.69517
Step 59395: loss = 2.76062
Step 59400: loss = 2.70248
Step 59405: loss = 2.70366
Step 59410: loss = 2.82923
Step 59415: loss = 2.74118
Step 59420: loss = 2.87123
Step 59425: loss = 2.91730
Step 59430: loss = 2.90710
Step 59435: loss = 2.62690
Step 59440: loss = 2.78300
Step 59445: loss = 2.88440
Step 59450: loss = 2.90131
Step 59455: loss = 2.64215
Step 59460: loss = 2.85800
Step 59465: loss = 2.73762
Step 59470: loss = 2.80026
Step 59475: loss = 2.98483
Step 59480: loss = 2.98977
Step 59485: loss = 2.82742
Step 59490: loss = 3.03578
Step 59495: loss = 2.99253
Step 59500: loss = 2.78917
Step 59505: loss = 2.86943
Step 59510: loss = 2.77465
Step 59515: loss = 2.78061
Step 59520: loss = 2.78696
Step 59525: loss = 2.82747
Step 59530: loss = 2.76155
Step 59535: loss = 2.77385
Step 59540: loss = 2.89100
Step 59545: loss = 2.60607
Step 59550: loss = 2.86198
Step 59555: loss = 2.74406
Step 59560: loss = 3.02793
Step 59565: loss = 2.97067
Step 59570: loss = 2.64145
Step 59575: loss = 2.85400
Step 59580: loss = 2.88198
Step 59585: loss = 2.70947
Step 59590: loss = 2.92861
Step 59595: loss = 2.74811
Step 59600: loss = 2.87397
Step 59605: loss = 2.70958
Step 59610: loss = 2.77107
Step 59615: loss = 2.59087
Step 59620: loss = 2.91184
Step 59625: loss = 2.91222
Step 59630: loss = 2.79128
Step 59635: loss = 2.74766
Step 59640: loss = 2.77084
Step 59645: loss = 2.96171
Step 59650: loss = 2.88632
Step 59655: loss = 2.92787
Step 59660: loss = 2.73663
Step 59665: loss = 2.70918
Step 59670: loss = 2.94174
Training Data Eval:
  Num examples: 49920, Num correct: 6710, Precision @ 1: 0.1344
('Testing Data Eval: EPOCH->', 154)
  Num examples: 9984, Num correct: 1425, Precision @ 1: 0.1427
Step 59675: loss = 2.77347
Step 59680: loss = 3.11073
Step 59685: loss = 2.77655
Step 59690: loss = 2.73241
Step 59695: loss = 2.76531
Step 59700: loss = 2.80841
Step 59705: loss = 2.73062
Step 59710: loss = 2.90746
Step 59715: loss = 2.76948
Step 59720: loss = 2.81027
Step 59725: loss = 2.76286
Step 59730: loss = 2.90838
Step 59735: loss = 2.64742
Step 59740: loss = 3.07360
Step 59745: loss = 2.94182
Step 59750: loss = 2.82157
Step 59755: loss = 2.83985
Step 59760: loss = 2.84453
Step 59765: loss = 2.95764
Step 59770: loss = 3.02965
Step 59775: loss = 3.08652
Step 59780: loss = 3.00472
Step 59785: loss = 2.98753
Step 59790: loss = 2.82984
Step 59795: loss = 2.72300
Step 59800: loss = 2.90936
Step 59805: loss = 2.62963
Step 59810: loss = 2.82097
Step 59815: loss = 3.04075
Step 59820: loss = 2.81013
Step 59825: loss = 2.55890
Step 59830: loss = 2.55911
Step 59835: loss = 2.68590
Step 59840: loss = 2.63911
Step 59845: loss = 2.77419
Step 59850: loss = 3.01613
Step 59855: loss = 2.65390
Step 59860: loss = 2.57137
Step 59865: loss = 2.64434
Step 59870: loss = 2.97184
Step 59875: loss = 2.83987
Step 59880: loss = 2.80740
Step 59885: loss = 2.85203
Step 59890: loss = 2.94995
Step 59895: loss = 3.03532
Step 59900: loss = 2.85054
Step 59905: loss = 2.74669
Step 59910: loss = 2.72351
Step 59915: loss = 3.16123
Step 59920: loss = 2.72238
Step 59925: loss = 2.86369
Step 59930: loss = 2.90515
Step 59935: loss = 2.80829
Step 59940: loss = 2.63177
Step 59945: loss = 2.89772
Step 59950: loss = 2.90825
Step 59955: loss = 2.80196
Step 59960: loss = 2.92817
Step 59965: loss = 2.85003
Step 59970: loss = 2.84027
Step 59975: loss = 2.93640
Step 59980: loss = 2.89153
Step 59985: loss = 2.88542
Step 59990: loss = 2.89364
Step 59995: loss = 2.99341
Step 60000: loss = 2.81104
Step 60005: loss = 3.00331
Step 60010: loss = 2.72666
Step 60015: loss = 2.75472
Step 60020: loss = 2.87047
Step 60025: loss = 2.77244
Step 60030: loss = 2.84563
Step 60035: loss = 2.82170
Step 60040: loss = 2.71843
Step 60045: loss = 2.76307
Step 60050: loss = 2.78944
Step 60055: loss = 2.99569
Step 60060: loss = 2.80854
Training Data Eval:
  Num examples: 49920, Num correct: 6695, Precision @ 1: 0.1341
('Testing Data Eval: EPOCH->', 155)
  Num examples: 9984, Num correct: 1470, Precision @ 1: 0.1472
Step 60065: loss = 2.66704
Step 60070: loss = 2.82079
Step 60075: loss = 2.94840
Step 60080: loss = 3.05699
Step 60085: loss = 3.09080
Step 60090: loss = 2.89658
Step 60095: loss = 2.89188
Step 60100: loss = 2.78471
Step 60105: loss = 2.73796
Step 60110: loss = 2.93903
Step 60115: loss = 2.98252
Step 60120: loss = 2.75862
Step 60125: loss = 2.85788
Step 60130: loss = 2.94850
Step 60135: loss = 2.69038
Step 60140: loss = 2.77325
Step 60145: loss = 2.74309
Step 60150: loss = 2.65459
Step 60155: loss = 2.97698
Step 60160: loss = 2.83945
Step 60165: loss = 2.86556
Step 60170: loss = 2.81068
Step 60175: loss = 2.91816
Step 60180: loss = 2.83757
Step 60185: loss = 2.51875
Step 60190: loss = 2.57080
Step 60195: loss = 2.97513
Step 60200: loss = 2.82155
Step 60205: loss = 2.64339
Step 60210: loss = 2.86345
Step 60215: loss = 2.87296
Step 60220: loss = 2.91257
Step 60225: loss = 2.92847
Step 60230: loss = 2.76414
Step 60235: loss = 2.94323
Step 60240: loss = 2.89317
Step 60245: loss = 2.98772
Step 60250: loss = 2.84999
Step 60255: loss = 2.78842
Step 60260: loss = 2.63010
Step 60265: loss = 3.06127
Step 60270: loss = 2.74792
Step 60275: loss = 2.77183
Step 60280: loss = 2.89857
Step 60285: loss = 2.90373
Step 60290: loss = 2.93783
Step 60295: loss = 2.58580
Step 60300: loss = 2.75259
Step 60305: loss = 2.85192
Step 60310: loss = 2.93247
Step 60315: loss = 2.62441
Step 60320: loss = 2.84681
Step 60325: loss = 3.02659
Step 60330: loss = 2.98404
Step 60335: loss = 2.69531
Step 60340: loss = 2.66624
Step 60345: loss = 2.82118
Step 60350: loss = 2.95688
Step 60355: loss = 2.96494
Step 60360: loss = 2.76439
Step 60365: loss = 2.95819
Step 60370: loss = 2.72336
Step 60375: loss = 2.68520
Step 60380: loss = 2.65946
Step 60385: loss = 2.78782
Step 60390: loss = 2.61661
Step 60395: loss = 2.76444
Step 60400: loss = 2.85423
Step 60405: loss = 2.82058
Step 60410: loss = 2.71470
Step 60415: loss = 2.69704
Step 60420: loss = 2.74448
Step 60425: loss = 2.96169
Step 60430: loss = 2.76937
Step 60435: loss = 2.76167
Step 60440: loss = 2.57071
Step 60445: loss = 2.67495
Step 60450: loss = 2.89185
Training Data Eval:
  Num examples: 49920, Num correct: 6659, Precision @ 1: 0.1334
('Testing Data Eval: EPOCH->', 156)
  Num examples: 9984, Num correct: 1301, Precision @ 1: 0.1303
Step 60455: loss = 2.68033
Step 60460: loss = 2.86083
Step 60465: loss = 2.90788
Step 60470: loss = 2.70667
Step 60475: loss = 2.81077
Step 60480: loss = 2.85957
Step 60485: loss = 2.86794
Step 60490: loss = 2.66195
Step 60495: loss = 2.92514
Step 60500: loss = 2.83773
Step 60505: loss = 2.70431
Step 60510: loss = 2.71489
Step 60515: loss = 2.93457
Step 60520: loss = 2.82842
Step 60525: loss = 2.66074
Step 60530: loss = 2.96518
Step 60535: loss = 2.93039
Step 60540: loss = 2.87416
Step 60545: loss = 2.80021
Step 60550: loss = 2.85326
Step 60555: loss = 2.88941
Step 60560: loss = 2.68437
Step 60565: loss = 2.79538
Step 60570: loss = 2.71285
Step 60575: loss = 2.52852
Step 60580: loss = 2.80410
Step 60585: loss = 2.77288
Step 60590: loss = 2.96464
Step 60595: loss = 2.86544
Step 60600: loss = 2.83662
Step 60605: loss = 2.83702
Step 60610: loss = 2.84980
Step 60615: loss = 2.99650
Step 60620: loss = 2.81949
Step 60625: loss = 2.68465
Step 60630: loss = 2.64423
Step 60635: loss = 2.82416
Step 60640: loss = 2.88143
Step 60645: loss = 3.05018
Step 60650: loss = 2.84793
Step 60655: loss = 2.65654
Step 60660: loss = 3.05998
Step 60665: loss = 2.82498
Step 60670: loss = 2.68134
Step 60675: loss = 2.85721
Step 60680: loss = 2.82724
Step 60685: loss = 2.82457
Step 60690: loss = 2.63464
Step 60695: loss = 2.81709
Step 60700: loss = 2.79543
Step 60705: loss = 2.93594
Step 60710: loss = 2.72466
Step 60715: loss = 2.79330
Step 60720: loss = 2.76175
Step 60725: loss = 2.73492
Step 60730: loss = 2.94669
Step 60735: loss = 2.97536
Step 60740: loss = 2.71721
Step 60745: loss = 2.95323
Step 60750: loss = 2.81741
Step 60755: loss = 2.95975
Step 60760: loss = 3.02445
Step 60765: loss = 3.02922
Step 60770: loss = 3.07401
Step 60775: loss = 2.96955
Step 60780: loss = 2.97326
Step 60785: loss = 2.88530
Step 60790: loss = 2.76587
Step 60795: loss = 2.71499
Step 60800: loss = 2.70856
Step 60805: loss = 2.78005
Step 60810: loss = 2.99080
Step 60815: loss = 2.82374
Step 60820: loss = 2.85894
Step 60825: loss = 2.86348
Step 60830: loss = 2.79637
Step 60835: loss = 2.83309
Step 60840: loss = 2.58640
Training Data Eval:
  Num examples: 49920, Num correct: 6538, Precision @ 1: 0.1310
('Testing Data Eval: EPOCH->', 157)
  Num examples: 9984, Num correct: 1380, Precision @ 1: 0.1382
Step 60845: loss = 3.02585
Step 60850: loss = 2.71691
Step 60855: loss = 3.01797
Step 60860: loss = 2.79302
Step 60865: loss = 2.67974
Step 60870: loss = 2.86842
Step 60875: loss = 2.92660
Step 60880: loss = 2.91151
Step 60885: loss = 2.75688
Step 60890: loss = 2.72685
Step 60895: loss = 2.68131
Step 60900: loss = 3.02147
Step 60905: loss = 2.61905
Step 60910: loss = 3.00626
Step 60915: loss = 2.92366
Step 60920: loss = 2.67954
Step 60925: loss = 2.59192
Step 60930: loss = 2.63968
Step 60935: loss = 2.57611
Step 60940: loss = 2.91184
Step 60945: loss = 2.65121
Step 60950: loss = 2.76196
Step 60955: loss = 2.96148
Step 60960: loss = 2.84577
Step 60965: loss = 2.93092
Step 60970: loss = 2.79250
Step 60975: loss = 2.69864
Step 60980: loss = 2.65061
Step 60985: loss = 2.83020
Step 60990: loss = 2.63388
Step 60995: loss = 2.78176
Step 61000: loss = 2.65945
Step 61005: loss = 2.93370
Step 61010: loss = 2.82402
Step 61015: loss = 2.75525
Step 61020: loss = 2.79991
Step 61025: loss = 2.64632
Step 61030: loss = 2.93253
Step 61035: loss = 2.75594
Step 61040: loss = 2.78379
Step 61045: loss = 2.94669
Step 61050: loss = 2.92855
Step 61055: loss = 2.97106
Step 61060: loss = 2.69234
Step 61065: loss = 2.86630
Step 61070: loss = 2.80130
Step 61075: loss = 2.78220
Step 61080: loss = 2.81387
Step 61085: loss = 2.77207
Step 61090: loss = 2.86856
Step 61095: loss = 2.88104
Step 61100: loss = 2.67870
Step 61105: loss = 2.62024
Step 61110: loss = 2.94074
Step 61115: loss = 2.97420
Step 61120: loss = 2.88117
Step 61125: loss = 2.81887
Step 61130: loss = 2.73499
Step 61135: loss = 3.00402
Step 61140: loss = 3.00653
Step 61145: loss = 2.86448
Step 61150: loss = 2.73236
Step 61155: loss = 2.90808
Step 61160: loss = 2.93372
Step 61165: loss = 2.73396
Step 61170: loss = 2.85442
Step 61175: loss = 2.96433
Step 61180: loss = 2.66612
Step 61185: loss = 3.08906
Step 61190: loss = 2.86223
Step 61195: loss = 2.81246
Step 61200: loss = 2.95895
Step 61205: loss = 2.65282
Step 61210: loss = 2.84690
Step 61215: loss = 2.90699
Step 61220: loss = 2.82013
Step 61225: loss = 2.94703
Step 61230: loss = 2.86465
Training Data Eval:
  Num examples: 49920, Num correct: 7015, Precision @ 1: 0.1405
('Testing Data Eval: EPOCH->', 158)
  Num examples: 9984, Num correct: 1461, Precision @ 1: 0.1463
Step 61235: loss = 2.90820
Step 61240: loss = 2.89122
Step 61245: loss = 2.81824
Step 61250: loss = 2.53458
Step 61255: loss = 2.80126
Step 61260: loss = 2.72390
Step 61265: loss = 2.77318
Step 61270: loss = 2.90587
Step 61275: loss = 2.79876
Step 61280: loss = 3.06168
Step 61285: loss = 2.98003
Step 61290: loss = 2.64045
Step 61295: loss = 2.74603
Step 61300: loss = 2.91920
Step 61305: loss = 2.81103
Step 61310: loss = 3.25815
Step 61315: loss = 2.86138
Step 61320: loss = 2.77346
Step 61325: loss = 2.73732
Step 61330: loss = 2.73413
Step 61335: loss = 2.62691
Step 61340: loss = 2.88917
Step 61345: loss = 2.90662
Step 61350: loss = 2.85401
Step 61355: loss = 2.83470
Step 61360: loss = 2.90283
Step 61365: loss = 2.90697
Step 61370: loss = 2.76431
Step 61375: loss = 2.73581
Step 61380: loss = 2.78595
Step 61385: loss = 2.61059
Step 61390: loss = 2.56891
Step 61395: loss = 3.21774
Step 61400: loss = 2.87336
Step 61405: loss = 3.02353
Step 61410: loss = 2.95604
Step 61415: loss = 2.76562
Step 61420: loss = 2.46154
Step 61425: loss = 2.73723
Step 61430: loss = 2.93712
Step 61435: loss = 2.75788
Step 61440: loss = 2.80961
Step 61445: loss = 2.87648
Step 61450: loss = 2.77592
Step 61455: loss = 2.82308
Step 61460: loss = 2.64656
Step 61465: loss = 2.69715
Step 61470: loss = 2.78749
Step 61475: loss = 3.05287
Step 61480: loss = 2.87042
Step 61485: loss = 2.69926
Step 61490: loss = 3.03665
Step 61495: loss = 2.55724
Step 61500: loss = 2.97689
Step 61505: loss = 2.84577
Step 61510: loss = 2.60621
Step 61515: loss = 2.85563
Step 61520: loss = 2.95741
Step 61525: loss = 2.66854
Step 61530: loss = 2.67440
Step 61535: loss = 2.81159
Step 61540: loss = 2.77309
Step 61545: loss = 2.74393
Step 61550: loss = 2.77810
Step 61555: loss = 2.44409
Step 61560: loss = 3.02620
Step 61565: loss = 3.05552
Step 61570: loss = 3.03287
Step 61575: loss = 2.89866
Step 61580: loss = 2.64081
Step 61585: loss = 2.79108
Step 61590: loss = 2.78738
Step 61595: loss = 2.91587
Step 61600: loss = 2.63761
Step 61605: loss = 2.82387
Step 61610: loss = 2.84676
Step 61615: loss = 2.91872
Step 61620: loss = 2.88626
Training Data Eval:
  Num examples: 49920, Num correct: 6809, Precision @ 1: 0.1364
('Testing Data Eval: EPOCH->', 159)
  Num examples: 9984, Num correct: 1399, Precision @ 1: 0.1401
Step 61625: loss = 3.02896
Step 61630: loss = 2.77198
Step 61635: loss = 2.90660
Step 61640: loss = 2.89837
Step 61645: loss = 2.85499
Step 61650: loss = 2.69704
Step 61655: loss = 2.80393
Step 61660: loss = 3.02675
Step 61665: loss = 2.70357
Step 61670: loss = 3.14297
Step 61675: loss = 2.71593
Step 61680: loss = 3.02285
Step 61685: loss = 2.93601
Step 61690: loss = 2.90034
Step 61695: loss = 2.74038
Step 61700: loss = 2.71896
Step 61705: loss = 2.87940
Step 61710: loss = 2.82371
Step 61715: loss = 2.98568
Step 61720: loss = 2.80731
Step 61725: loss = 2.76842
Step 61730: loss = 2.89034
Step 61735: loss = 2.93175
Step 61740: loss = 2.92340
Step 61745: loss = 2.91940
Step 61750: loss = 2.63828
Step 61755: loss = 2.91253
Step 61760: loss = 2.64677
Step 61765: loss = 2.69056
Step 61770: loss = 2.73200
Step 61775: loss = 2.79262
Step 61780: loss = 2.73022
Step 61785: loss = 2.52616
Step 61790: loss = 2.64730
Step 61795: loss = 2.73080
Step 61800: loss = 2.84282
Step 61805: loss = 3.06396
Step 61810: loss = 2.55624
Step 61815: loss = 2.94488
Step 61820: loss = 2.64279
Step 61825: loss = 2.64008
Step 61830: loss = 2.81358
Step 61835: loss = 2.76871
Step 61840: loss = 2.91760
Step 61845: loss = 2.87845
Step 61850: loss = 2.86878
Step 61855: loss = 2.73814
Step 61860: loss = 2.66139
Step 61865: loss = 2.68222
Step 61870: loss = 2.87485
Step 61875: loss = 2.88361
Step 61880: loss = 2.85394
Step 61885: loss = 2.94089
Step 61890: loss = 2.75037
Step 61895: loss = 2.86047
Step 61900: loss = 2.75632
Step 61905: loss = 2.83916
Step 61910: loss = 2.68625
Step 61915: loss = 2.68951
Step 61920: loss = 2.63843
Step 61925: loss = 2.57697
Step 61930: loss = 2.75211
Step 61935: loss = 2.65624
Step 61940: loss = 2.78774
Step 61945: loss = 2.64758
Step 61950: loss = 2.61115
Step 61955: loss = 2.74488
Step 61960: loss = 2.74024
Step 61965: loss = 2.79030
Step 61970: loss = 2.91024
Step 61975: loss = 2.71729
Step 61980: loss = 2.71476
Step 61985: loss = 2.74730
Step 61990: loss = 2.67505
Step 61995: loss = 2.76123
Step 62000: loss = 2.87114
Step 62005: loss = 2.90312
Step 62010: loss = 2.65766
Training Data Eval:
  Num examples: 49920, Num correct: 6963, Precision @ 1: 0.1395
('Testing Data Eval: EPOCH->', 160)
  Num examples: 9984, Num correct: 1461, Precision @ 1: 0.1463
Step 62015: loss = 2.72325
Step 62020: loss = 3.03600
Step 62025: loss = 2.73001
Step 62030: loss = 2.65975
Step 62035: loss = 3.08217
Step 62040: loss = 2.78977
Step 62045: loss = 2.68462
Step 62050: loss = 2.73298
Step 62055: loss = 2.70803
Step 62060: loss = 2.78873
Step 62065: loss = 2.84780
Step 62070: loss = 2.66325
Step 62075: loss = 2.78908
Step 62080: loss = 2.89240
Step 62085: loss = 2.76508
Step 62090: loss = 2.62096
Step 62095: loss = 3.00228
Step 62100: loss = 2.69442
Step 62105: loss = 2.68809
Step 62110: loss = 2.77880
Step 62115: loss = 2.80765
Step 62120: loss = 2.92107
Step 62125: loss = 2.68341
Step 62130: loss = 3.05923
Step 62135: loss = 2.73734
Step 62140: loss = 2.91468
Step 62145: loss = 2.87641
Step 62150: loss = 2.66748
Step 62155: loss = 2.60832
Step 62160: loss = 3.01070
Step 62165: loss = 2.69968
Step 62170: loss = 2.67219
Step 62175: loss = 2.88063
Step 62180: loss = 2.79168
Step 62185: loss = 2.65294
Step 62190: loss = 3.05805
Step 62195: loss = 2.74983
Step 62200: loss = 2.75553
Step 62205: loss = 3.01113
Step 62210: loss = 2.61814
Step 62215: loss = 2.81484
Step 62220: loss = 2.79354
Step 62225: loss = 2.75755
Step 62230: loss = 2.92058
Step 62235: loss = 2.65131
Step 62240: loss = 2.73803
Step 62245: loss = 2.68994
Step 62250: loss = 2.82852
Step 62255: loss = 2.80767
Step 62260: loss = 3.02626
Step 62265: loss = 2.67120
Step 62270: loss = 2.86973
Step 62275: loss = 2.81114
Step 62280: loss = 2.93250
Step 62285: loss = 2.83297
Step 62290: loss = 2.54823
Step 62295: loss = 2.60180
Step 62300: loss = 2.75622
Step 62305: loss = 2.80225
Step 62310: loss = 2.79804
Step 62315: loss = 2.77849
Step 62320: loss = 2.94797
Step 62325: loss = 2.81192
Step 62330: loss = 2.84162
Step 62335: loss = 2.65479
Step 62340: loss = 2.65986
Step 62345: loss = 2.79849
Step 62350: loss = 2.72580
Step 62355: loss = 2.86761
Step 62360: loss = 3.00724
Step 62365: loss = 2.88177
Step 62370: loss = 2.68159
Step 62375: loss = 2.73347
Step 62380: loss = 2.68353
Step 62385: loss = 2.97015
Step 62390: loss = 2.88071
Step 62395: loss = 2.91351
Step 62400: loss = 2.95408
Training Data Eval:
  Num examples: 49920, Num correct: 6426, Precision @ 1: 0.1287
('Testing Data Eval: EPOCH->', 161)
  Num examples: 9984, Num correct: 1283, Precision @ 1: 0.1285
Step 62405: loss = 2.70334
Step 62410: loss = 2.88776
Step 62415: loss = 2.66976
Step 62420: loss = 2.73477
Step 62425: loss = 2.75124
Step 62430: loss = 2.78657
Step 62435: loss = 2.97026
Step 62440: loss = 2.92491
Step 62445: loss = 2.86325
Step 62450: loss = 2.86743
Step 62455: loss = 2.83902
Step 62460: loss = 3.10238
Step 62465: loss = 2.81011
Step 62470: loss = 2.74780
Step 62475: loss = 2.67852
Step 62480: loss = 2.87162
Step 62485: loss = 2.86655
Step 62490: loss = 2.75897
Step 62495: loss = 2.83796
Step 62500: loss = 2.66121
Step 62505: loss = 3.02886
Step 62510: loss = 2.84321
Step 62515: loss = 3.00647
Step 62520: loss = 2.96855
Step 62525: loss = 2.72990
Step 62530: loss = 2.78350
Step 62535: loss = 2.61663
Step 62540: loss = 3.03514
Step 62545: loss = 2.72536
Step 62550: loss = 2.77726
Step 62555: loss = 2.80857
Step 62560: loss = 2.97292
Step 62565: loss = 2.66581
Step 62570: loss = 2.64915
Step 62575: loss = 2.67796
Step 62580: loss = 2.95794
Step 62585: loss = 2.88806
Step 62590: loss = 2.75362
Step 62595: loss = 2.83065
Step 62600: loss = 3.12031
Step 62605: loss = 2.70416
Step 62610: loss = 2.74567
Step 62615: loss = 2.77890
Step 62620: loss = 2.73427
Step 62625: loss = 2.67935
Step 62630: loss = 2.68036
Step 62635: loss = 2.57123
Step 62640: loss = 2.68921
Step 62645: loss = 2.75729
Step 62650: loss = 2.86503
Step 62655: loss = 2.73548
Step 62660: loss = 2.77885
Step 62665: loss = 3.24389
Step 62670: loss = 3.04077
Step 62675: loss = 3.02743
Step 62680: loss = 2.74321
Step 62685: loss = 2.63927
Step 62690: loss = 2.96310
Step 62695: loss = 2.74427
Step 62700: loss = 2.87799
Step 62705: loss = 2.97068
Step 62710: loss = 2.57801
Step 62715: loss = 2.66698
Step 62720: loss = 2.88701
Step 62725: loss = 2.69198
Step 62730: loss = 2.74879
Step 62735: loss = 2.90022
Step 62740: loss = 3.09709
Step 62745: loss = 2.95871
Step 62750: loss = 2.74399
Step 62755: loss = 3.03056
Step 62760: loss = 2.80002
Step 62765: loss = 2.64062
Step 62770: loss = 2.74784
Step 62775: loss = 2.71025
Step 62780: loss = 3.05133
Step 62785: loss = 2.81447
Step 62790: loss = 2.78128
Training Data Eval:
  Num examples: 49920, Num correct: 6774, Precision @ 1: 0.1357
('Testing Data Eval: EPOCH->', 162)
  Num examples: 9984, Num correct: 1418, Precision @ 1: 0.1420
Step 62795: loss = 2.77531
Step 62800: loss = 2.72662
Step 62805: loss = 2.94872
Step 62810: loss = 2.78821
Step 62815: loss = 2.89855
Step 62820: loss = 2.85028
Step 62825: loss = 3.09535
Step 62830: loss = 2.94559
Step 62835: loss = 2.79260
Step 62840: loss = 2.81415
Step 62845: loss = 2.80534
Step 62850: loss = 2.84761
Step 62855: loss = 2.60226
Step 62860: loss = 2.50638
Step 62865: loss = 2.68570
Step 62870: loss = 2.85510
Step 62875: loss = 2.83654
Step 62880: loss = 2.67971
Step 62885: loss = 2.90297
Step 62890: loss = 2.85056
Step 62895: loss = 2.73270
Step 62900: loss = 2.85929
Step 62905: loss = 2.92388
Step 62910: loss = 2.82776
Step 62915: loss = 2.90466
Step 62920: loss = 2.76978
Step 62925: loss = 2.60610
Step 62930: loss = 2.70944
Step 62935: loss = 2.65230
Step 62940: loss = 2.78945
Step 62945: loss = 2.57641
Step 62950: loss = 2.99521
Step 62955: loss = 2.99795
Step 62960: loss = 2.56514
Step 62965: loss = 2.90455
Step 62970: loss = 2.80926
Step 62975: loss = 2.78312
Step 62980: loss = 2.92242
Step 62985: loss = 2.77417
Step 62990: loss = 3.00544
Step 62995: loss = 3.01925
Step 63000: loss = 2.81925
Step 63005: loss = 2.80397
Step 63010: loss = 2.89059
Step 63015: loss = 2.82406
Step 63020: loss = 2.71349
Step 63025: loss = 2.67945
Step 63030: loss = 2.91806
Step 63035: loss = 2.92766
Step 63040: loss = 2.77123
Step 63045: loss = 2.87612
Step 63050: loss = 2.71634
Step 63055: loss = 2.80255
Step 63060: loss = 2.72532
Step 63065: loss = 2.79270
Step 63070: loss = 2.94463
Step 63075: loss = 2.83719
Step 63080: loss = 2.71496
Step 63085: loss = 2.92207
Step 63090: loss = 2.84187
Step 63095: loss = 2.83652
Step 63100: loss = 2.71056
Step 63105: loss = 2.88096
Step 63110: loss = 3.01916
Step 63115: loss = 2.68172
Step 63120: loss = 2.87080
Step 63125: loss = 2.88307
Step 63130: loss = 2.90113
Step 63135: loss = 2.76180
Step 63140: loss = 2.89828
Step 63145: loss = 3.04382
Step 63150: loss = 2.89390
Step 63155: loss = 3.09183
Step 63160: loss = 3.06735
Step 63165: loss = 3.00339
Step 63170: loss = 3.04239
Step 63175: loss = 2.89239
Step 63180: loss = 2.90547
Training Data Eval:
  Num examples: 49920, Num correct: 6855, Precision @ 1: 0.1373
('Testing Data Eval: EPOCH->', 163)
  Num examples: 9984, Num correct: 1337, Precision @ 1: 0.1339
Step 63185: loss = 2.84330
Step 63190: loss = 2.77188
Step 63195: loss = 2.84982
Step 63200: loss = 2.87118
Step 63205: loss = 2.84619
Step 63210: loss = 2.59071
Step 63215: loss = 2.88834
Step 63220: loss = 2.90654
Step 63225: loss = 2.80464
Step 63230: loss = 3.24539
Step 63235: loss = 2.63300
Step 63240: loss = 2.76080
Step 63245: loss = 2.80809
Step 63250: loss = 2.75532
Step 63255: loss = 2.72345
Step 63260: loss = 2.85112
Step 63265: loss = 2.86592
Step 63270: loss = 2.77233
Step 63275: loss = 2.97346
Step 63280: loss = 2.89888
Step 63285: loss = 2.80862
Step 63290: loss = 2.91624
Step 63295: loss = 2.97199
Step 63300: loss = 2.79079
Step 63305: loss = 2.73217
Step 63310: loss = 2.78969
Step 63315: loss = 2.87693
Step 63320: loss = 2.54488
Step 63325: loss = 2.67812
Step 63330: loss = 2.74505
Step 63335: loss = 2.78182
Step 63340: loss = 2.71835
Step 63345: loss = 2.89151
Step 63350: loss = 2.91677
Step 63355: loss = 2.75315
Step 63360: loss = 2.84888
Step 63365: loss = 2.78181
Step 63370: loss = 2.67155
Step 63375: loss = 2.70052
Step 63380: loss = 2.60104
Step 63385: loss = 2.63725
Step 63390: loss = 2.77486
Step 63395: loss = 2.72763
Step 63400: loss = 2.84820
Step 63405: loss = 2.59082
Step 63410: loss = 2.74594
Step 63415: loss = 2.74197
Step 63420: loss = 2.89969
Step 63425: loss = 2.70049
Step 63430: loss = 2.81633
Step 63435: loss = 2.69427
Step 63440: loss = 2.59317
Step 63445: loss = 2.79054
Step 63450: loss = 2.85612
Step 63455: loss = 2.79421
Step 63460: loss = 2.70452
Step 63465: loss = 2.97981
Step 63470: loss = 3.07647
Step 63475: loss = 3.14720
Step 63480: loss = 2.79225
Step 63485: loss = 2.72712
Step 63490: loss = 2.82559
Step 63495: loss = 2.87096
Step 63500: loss = 2.75561
Step 63505: loss = 2.96526
Step 63510: loss = 2.85001
Step 63515: loss = 2.75876
Step 63520: loss = 2.81847
Step 63525: loss = 2.71087
Step 63530: loss = 2.72053
Step 63535: loss = 2.62232
Step 63540: loss = 3.09264
Step 63545: loss = 2.68737
Step 63550: loss = 2.59207
Step 63555: loss = 2.76151
Step 63560: loss = 2.60879
Step 63565: loss = 2.62691
Step 63570: loss = 2.90149
Training Data Eval:
  Num examples: 49920, Num correct: 7030, Precision @ 1: 0.1408
('Testing Data Eval: EPOCH->', 164)
  Num examples: 9984, Num correct: 1389, Precision @ 1: 0.1391
Step 63575: loss = 2.92662
Step 63580: loss = 2.78617
Step 63585: loss = 2.70948
Step 63590: loss = 2.81364
Step 63595: loss = 2.82978
Step 63600: loss = 2.88741
Step 63605: loss = 2.60715
Step 63610: loss = 2.80514
Step 63615: loss = 2.74476
Step 63620: loss = 2.73655
Step 63625: loss = 2.77934
Step 63630: loss = 3.05502
Step 63635: loss = 2.70322
Step 63640: loss = 2.68603
Step 63645: loss = 3.07712
Step 63650: loss = 2.80427
Step 63655: loss = 2.86357
Step 63660: loss = 2.80879
Step 63665: loss = 2.88856
Step 63670: loss = 2.90188
Step 63675: loss = 2.68746
Step 63680: loss = 3.01005
Step 63685: loss = 2.99876
Step 63690: loss = 2.77445
Step 63695: loss = 2.91651
Step 63700: loss = 2.85096
Step 63705: loss = 2.95519
Step 63710: loss = 2.70347
Step 63715: loss = 2.98580
Step 63720: loss = 2.85539
Step 63725: loss = 2.73199
Step 63730: loss = 2.82379
Step 63735: loss = 2.79178
Step 63740: loss = 2.90087
Step 63745: loss = 2.83128
Step 63750: loss = 2.61544
Step 63755: loss = 2.72408
Step 63760: loss = 2.72190
Step 63765: loss = 2.55523
Step 63770: loss = 2.76472
Step 63775: loss = 3.02014
Step 63780: loss = 3.00637
Step 63785: loss = 2.61744
Step 63790: loss = 2.66658
Step 63795: loss = 2.69478
Step 63800: loss = 2.81513
Step 63805: loss = 2.67833
Step 63810: loss = 2.72624
Step 63815: loss = 2.72555
Step 63820: loss = 2.80845
Step 63825: loss = 2.92422
Step 63830: loss = 2.84178
Step 63835: loss = 2.86782
Step 63840: loss = 2.65234
Step 63845: loss = 2.83644
Step 63850: loss = 2.95460
Step 63855: loss = 2.74634
Step 63860: loss = 2.64266
Step 63865: loss = 2.75675
Step 63870: loss = 2.72262
Step 63875: loss = 2.71466
Step 63880: loss = 2.63372
Step 63885: loss = 2.66920
Step 63890: loss = 2.78035
Step 63895: loss = 2.78966
Step 63900: loss = 2.70312
Step 63905: loss = 2.55198
Step 63910: loss = 2.92665
Step 63915: loss = 2.76853
Step 63920: loss = 2.87399
Step 63925: loss = 2.90838
Step 63930: loss = 2.72413
Step 63935: loss = 2.87239
Step 63940: loss = 2.52412
Step 63945: loss = 2.75479
Step 63950: loss = 2.75863
Step 63955: loss = 2.70359
Step 63960: loss = 2.76809
Training Data Eval:
  Num examples: 49920, Num correct: 6777, Precision @ 1: 0.1358
('Testing Data Eval: EPOCH->', 165)
  Num examples: 9984, Num correct: 1402, Precision @ 1: 0.1404
Step 63965: loss = 2.81005
Step 63970: loss = 2.85890
Step 63975: loss = 2.83742
Step 63980: loss = 2.79620
Step 63985: loss = 2.86010
Step 63990: loss = 2.72116
Step 63995: loss = 2.61703
Step 64000: loss = 3.10556
Step 64005: loss = 2.51873
Step 64010: loss = 3.01004
Step 64015: loss = 2.84747
Step 64020: loss = 2.82489
Step 64025: loss = 2.71265
Step 64030: loss = 2.80670
Step 64035: loss = 2.75424
Step 64040: loss = 2.61743
Step 64045: loss = 2.67773
Step 64050: loss = 2.82332
Step 64055: loss = 2.67791
Step 64060: loss = 2.76578
Step 64065: loss = 2.81293
Step 64070: loss = 2.77752
Step 64075: loss = 2.68518
Step 64080: loss = 2.87352
Step 64085: loss = 2.78555
Step 64090: loss = 2.86573
Step 64095: loss = 2.70148
Step 64100: loss = 2.57868
Step 64105: loss = 2.90530
Step 64110: loss = 2.81339
Step 64115: loss = 2.69865
Step 64120: loss = 2.92403
Step 64125: loss = 2.65055
Step 64130: loss = 2.76505
Step 64135: loss = 2.64191
Step 64140: loss = 2.65412
Step 64145: loss = 2.95742
Step 64150: loss = 2.76475
Step 64155: loss = 2.73887
Step 64160: loss = 2.79275
Step 64165: loss = 2.74797
Step 64170: loss = 2.74988
Step 64175: loss = 2.83294
Step 64180: loss = 2.69510
Step 64185: loss = 2.84464
Step 64190: loss = 2.88833
Step 64195: loss = 2.96082
Step 64200: loss = 2.84923
Step 64205: loss = 2.98782
Step 64210: loss = 2.78732
Step 64215: loss = 2.86270
Step 64220: loss = 2.70447
Step 64225: loss = 2.93110
Step 64230: loss = 2.83526
Step 64235: loss = 2.83330
Step 64240: loss = 2.78325
Step 64245: loss = 2.83901
Step 64250: loss = 2.61502
Step 64255: loss = 2.71142
Step 64260: loss = 2.64345
Step 64265: loss = 2.73086
Step 64270: loss = 2.89627
Step 64275: loss = 3.10141
Step 64280: loss = 2.87526
Step 64285: loss = 2.62084
Step 64290: loss = 2.71838
Step 64295: loss = 3.03089
Step 64300: loss = 2.63486
Step 64305: loss = 2.66271
Step 64310: loss = 2.93372
Step 64315: loss = 2.79988
Step 64320: loss = 2.68416
Step 64325: loss = 2.90138
Step 64330: loss = 2.74205
Step 64335: loss = 2.76919
Step 64340: loss = 2.96747
Step 64345: loss = 2.87359
Step 64350: loss = 3.02527
Training Data Eval:
  Num examples: 49920, Num correct: 6779, Precision @ 1: 0.1358
('Testing Data Eval: EPOCH->', 166)
  Num examples: 9984, Num correct: 1349, Precision @ 1: 0.1351
Step 64355: loss = 2.89216
Step 64360: loss = 2.83426
Step 64365: loss = 2.60385
Step 64370: loss = 3.09974
Step 64375: loss = 2.71672
Step 64380: loss = 3.03199
Step 64385: loss = 2.79119
Step 64390: loss = 2.95970
Step 64395: loss = 2.61733
Step 64400: loss = 2.67517
Step 64405: loss = 3.05798
Step 64410: loss = 2.86212
Step 64415: loss = 2.93152
Step 64420: loss = 2.84242
Step 64425: loss = 2.74641
Step 64430: loss = 2.57266
Step 64435: loss = 2.79577
Step 64440: loss = 2.65613
Step 64445: loss = 2.72835
Step 64450: loss = 2.73541
Step 64455: loss = 2.88504
Step 64460: loss = 2.98019
Step 64465: loss = 2.72779
Step 64470: loss = 2.90321
Step 64475: loss = 2.86137
Step 64480: loss = 2.81518
Step 64485: loss = 2.62756
Step 64490: loss = 2.66405
Step 64495: loss = 2.76060
Step 64500: loss = 2.88550
Step 64505: loss = 3.04396
Step 64510: loss = 2.99885
Step 64515: loss = 2.90380
Step 64520: loss = 2.86880
Step 64525: loss = 2.75482
Step 64530: loss = 2.55470
Step 64535: loss = 2.94699
Step 64540: loss = 2.82253
Step 64545: loss = 2.81228
Step 64550: loss = 2.76770
Step 64555: loss = 2.90473
Step 64560: loss = 2.85157
Step 64565: loss = 2.91890
Step 64570: loss = 2.78242
Step 64575: loss = 2.49687
Step 64580: loss = 2.93629
Step 64585: loss = 2.77735
Step 64590: loss = 2.68703
Step 64595: loss = 2.82689
Step 64600: loss = 2.82451
Step 64605: loss = 2.81289
Step 64610: loss = 2.99428
Step 64615: loss = 2.77127
Step 64620: loss = 2.91311
Step 64625: loss = 2.89845
Step 64630: loss = 2.74943
Step 64635: loss = 2.89012
Step 64640: loss = 2.91035
Step 64645: loss = 2.64925
Step 64650: loss = 2.69387
Step 64655: loss = 2.72725
Step 64660: loss = 2.85550
Step 64665: loss = 2.92047
Step 64670: loss = 2.70260
Step 64675: loss = 2.59593
Step 64680: loss = 2.78783
Step 64685: loss = 3.04212
Step 64690: loss = 2.74450
Step 64695: loss = 2.64851
Step 64700: loss = 2.82212
Step 64705: loss = 2.83551
Step 64710: loss = 2.69569
Step 64715: loss = 2.87233
Step 64720: loss = 2.95235
Step 64725: loss = 3.01757
Step 64730: loss = 3.17810
Step 64735: loss = 2.89643
Step 64740: loss = 2.84762
Training Data Eval:
  Num examples: 49920, Num correct: 6607, Precision @ 1: 0.1324
('Testing Data Eval: EPOCH->', 167)
  Num examples: 9984, Num correct: 1312, Precision @ 1: 0.1314
Step 64745: loss = 2.77171
Step 64750: loss = 2.79576
Step 64755: loss = 2.88879
Step 64760: loss = 2.97678
Step 64765: loss = 2.90683
Step 64770: loss = 2.74747
Step 64775: loss = 2.84980
Step 64780: loss = 2.91300
Step 64785: loss = 2.90354
Step 64790: loss = 2.91464
Step 64795: loss = 2.80708
Step 64800: loss = 3.12743
Step 64805: loss = 2.70559
Step 64810: loss = 2.82693
Step 64815: loss = 2.70091
Step 64820: loss = 2.92503
Step 64825: loss = 2.87886
Step 64830: loss = 2.79401
Step 64835: loss = 2.87350
Step 64840: loss = 2.89637
Step 64845: loss = 2.75588
Step 64850: loss = 2.86437
Step 64855: loss = 2.58069
Step 64860: loss = 2.92550
Step 64865: loss = 2.72699
Step 64870: loss = 2.79093
Step 64875: loss = 2.76722
Step 64880: loss = 3.06253
Step 64885: loss = 2.80156
Step 64890: loss = 2.91894
Step 64895: loss = 2.73534
Step 64900: loss = 2.80739
Step 64905: loss = 2.54270
Step 64910: loss = 2.65219
Step 64915: loss = 2.83281
Step 64920: loss = 2.87406
Step 64925: loss = 2.71142
Step 64930: loss = 2.77769
Step 64935: loss = 2.65470
Step 64940: loss = 2.76089
Step 64945: loss = 2.93427
Step 64950: loss = 2.68547
Step 64955: loss = 2.93219
Step 64960: loss = 2.86872
Step 64965: loss = 2.83256
Step 64970: loss = 2.89731
Step 64975: loss = 2.78087
Step 64980: loss = 2.76720
Step 64985: loss = 2.72297
Step 64990: loss = 2.75558
Step 64995: loss = 2.84766
Step 65000: loss = 2.56693
Step 65005: loss = 2.95479
Step 65010: loss = 2.80212
Step 65015: loss = 2.74903
Step 65020: loss = 2.73444
Step 65025: loss = 2.85595
Step 65030: loss = 2.75931
Step 65035: loss = 2.63944
Step 65040: loss = 2.93752
Step 65045: loss = 2.80738
Step 65050: loss = 2.85236
Step 65055: loss = 2.89828
Step 65060: loss = 2.80982
Step 65065: loss = 2.81904
Step 65070: loss = 2.77947
Step 65075: loss = 2.75444
Step 65080: loss = 2.53913
Step 65085: loss = 2.85103
Step 65090: loss = 2.75865
Step 65095: loss = 2.68191
Step 65100: loss = 2.86808
Step 65105: loss = 2.86212
Step 65110: loss = 2.77914
Step 65115: loss = 2.84350
Step 65120: loss = 3.02742
Step 65125: loss = 2.97541
Step 65130: loss = 2.72223
Training Data Eval:
  Num examples: 49920, Num correct: 6875, Precision @ 1: 0.1377
('Testing Data Eval: EPOCH->', 168)
  Num examples: 9984, Num correct: 1361, Precision @ 1: 0.1363
Step 65135: loss = 2.68417
Step 65140: loss = 2.79090
Step 65145: loss = 2.77392
Step 65150: loss = 2.64308
Step 65155: loss = 2.69828
Step 65160: loss = 2.65045
Step 65165: loss = 2.67861
Step 65170: loss = 2.84394
Step 65175: loss = 2.82374
Step 65180: loss = 2.80728
Step 65185: loss = 2.80870
Step 65190: loss = 2.75319
Step 65195: loss = 2.80273
Step 65200: loss = 2.82327
Step 65205: loss = 2.60792
Step 65210: loss = 2.70459
Step 65215: loss = 2.80003
Step 65220: loss = 2.70216
Step 65225: loss = 2.78133
Step 65230: loss = 2.69005
Step 65235: loss = 3.01618
Step 65240: loss = 2.87130
Step 65245: loss = 2.95370
Step 65250: loss = 3.07007
Step 65255: loss = 2.74904
Step 65260: loss = 2.61017
Step 65265: loss = 2.78571
Step 65270: loss = 2.67231
Step 65275: loss = 2.85738
Step 65280: loss = 2.72833
Step 65285: loss = 2.82954
Step 65290: loss = 2.72230
Step 65295: loss = 2.81173
Step 65300: loss = 2.80545
Step 65305: loss = 2.79003
Step 65310: loss = 2.79457
Step 65315: loss = 2.88072
Step 65320: loss = 2.74826
Step 65325: loss = 2.66590
Step 65330: loss = 2.79134
Step 65335: loss = 2.70194
Step 65340: loss = 2.89080
Step 65345: loss = 2.77693
Step 65350: loss = 2.70979
Step 65355: loss = 2.89224
Step 65360: loss = 2.76193
Step 65365: loss = 2.69596
Step 65370: loss = 2.71324
Step 65375: loss = 2.89857
Step 65380: loss = 2.96022
Step 65385: loss = 2.76930
Step 65390: loss = 2.78234
Step 65395: loss = 2.66000
Step 65400: loss = 2.81664
Step 65405: loss = 2.76101
Step 65410: loss = 2.70566
Step 65415: loss = 2.93731
Step 65420: loss = 2.73230
Step 65425: loss = 2.80530
Step 65430: loss = 2.78723
Step 65435: loss = 2.89437
Step 65440: loss = 2.70477
Step 65445: loss = 2.70064
Step 65450: loss = 2.62370
Step 65455: loss = 2.64638
Step 65460: loss = 2.76866
Step 65465: loss = 2.95160
Step 65470: loss = 2.74748
Step 65475: loss = 3.07057
Step 65480: loss = 2.73200
Step 65485: loss = 2.83066
Step 65490: loss = 2.63949
Step 65495: loss = 2.88469
Step 65500: loss = 2.93469
Step 65505: loss = 2.72222
Step 65510: loss = 2.66952
Step 65515: loss = 2.74453
Step 65520: loss = 2.79137
Training Data Eval:
  Num examples: 49920, Num correct: 6711, Precision @ 1: 0.1344
('Testing Data Eval: EPOCH->', 169)
  Num examples: 9984, Num correct: 1350, Precision @ 1: 0.1352
Step 65525: loss = 2.96426
Step 65530: loss = 2.66904
Step 65535: loss = 2.95277
Step 65540: loss = 2.95495
Step 65545: loss = 2.76270
Step 65550: loss = 2.89597
Step 65555: loss = 2.80298
Step 65560: loss = 2.75644
Step 65565: loss = 2.62476
Step 65570: loss = 2.81166
Step 65575: loss = 2.74902
Step 65580: loss = 2.71336
Step 65585: loss = 2.70702
Step 65590: loss = 2.75158
Step 65595: loss = 2.57020
Step 65600: loss = 2.85814
Step 65605: loss = 2.97107
Step 65610: loss = 2.85755
Step 65615: loss = 2.91714
Step 65620: loss = 2.62265
Step 65625: loss = 2.91925
Step 65630: loss = 2.78316
Step 65635: loss = 2.62231
Step 65640: loss = 2.81932
Step 65645: loss = 2.86518
Step 65650: loss = 2.82211
Step 65655: loss = 2.90632
Step 65660: loss = 2.93375
Step 65665: loss = 2.69636
Step 65670: loss = 2.74089
Step 65675: loss = 2.85123
Step 65680: loss = 2.70517
Step 65685: loss = 3.03464
Step 65690: loss = 2.73486
Step 65695: loss = 2.88784
Step 65700: loss = 2.89784
Step 65705: loss = 2.84454
Step 65710: loss = 2.84266
Step 65715: loss = 2.72710
Step 65720: loss = 2.73291
Step 65725: loss = 2.67995
Step 65730: loss = 2.70781
Step 65735: loss = 2.90254
Step 65740: loss = 2.96343
Step 65745: loss = 2.64492
Step 65750: loss = 3.00382
Step 65755: loss = 2.81815
Step 65760: loss = 2.66068
Step 65765: loss = 2.78164
Step 65770: loss = 2.82391
Step 65775: loss = 2.95140
Step 65780: loss = 2.94706
Step 65785: loss = 2.82289
Step 65790: loss = 2.91119
Step 65795: loss = 2.91148
Step 65800: loss = 2.75049
Step 65805: loss = 2.92378
Step 65810: loss = 2.89325
Step 65815: loss = 2.81667
Step 65820: loss = 2.82689
Step 65825: loss = 3.12304
Step 65830: loss = 2.98118
Step 65835: loss = 2.83483
Step 65840: loss = 2.79693
Step 65845: loss = 2.85239
Step 65850: loss = 2.77047
Step 65855: loss = 2.81548
Step 65860: loss = 2.97437
Step 65865: loss = 2.94224
Step 65870: loss = 2.80530
Step 65875: loss = 2.81707
Step 65880: loss = 2.80835
Step 65885: loss = 2.77850
Step 65890: loss = 2.82198
Step 65895: loss = 2.85057
Step 65900: loss = 2.72574
Step 65905: loss = 2.84041
Step 65910: loss = 3.10423
Training Data Eval:
  Num examples: 49920, Num correct: 6720, Precision @ 1: 0.1346
('Testing Data Eval: EPOCH->', 170)
  Num examples: 9984, Num correct: 1346, Precision @ 1: 0.1348
Step 65915: loss = 2.88496
Step 65920: loss = 2.85788
Step 65925: loss = 2.91100
Step 65930: loss = 2.69032
Step 65935: loss = 2.93609
Step 65940: loss = 2.69276
Step 65945: loss = 2.97696
Step 65950: loss = 2.59435
Step 65955: loss = 2.85775
Step 65960: loss = 2.76119
Step 65965: loss = 2.88982
Step 65970: loss = 2.93972
Step 65975: loss = 2.76800
Step 65980: loss = 3.00736
Step 65985: loss = 3.13290
Step 65990: loss = 2.89178
Step 65995: loss = 2.82558
Step 66000: loss = 3.10498
Step 66005: loss = 2.54329
Step 66010: loss = 2.66765
Step 66015: loss = 2.71020
Step 66020: loss = 2.76603
Step 66025: loss = 2.79954
Step 66030: loss = 3.13132
Step 66035: loss = 2.95767
Step 66040: loss = 2.86970
Step 66045: loss = 2.58224
Step 66050: loss = 2.91525
Step 66055: loss = 2.63768
Step 66060: loss = 2.77213
Step 66065: loss = 2.96144
Step 66070: loss = 2.74029
Step 66075: loss = 2.90018
Step 66080: loss = 3.10688
Step 66085: loss = 2.95663
Step 66090: loss = 2.69439
Step 66095: loss = 2.78853
Step 66100: loss = 2.74721
Step 66105: loss = 2.68509
Step 66110: loss = 2.75264
Step 66115: loss = 2.85912
Step 66120: loss = 2.88323
Step 66125: loss = 2.68432
Step 66130: loss = 2.75556
Step 66135: loss = 2.75562
Step 66140: loss = 2.81374
Step 66145: loss = 2.95229
Step 66150: loss = 2.77456
Step 66155: loss = 2.79823
Step 66160: loss = 2.72345
Step 66165: loss = 2.68393
Step 66170: loss = 2.68290
Step 66175: loss = 2.81494
Step 66180: loss = 3.01954
Step 66185: loss = 2.86664
Step 66190: loss = 2.90217
Step 66195: loss = 2.81352
Step 66200: loss = 2.76469
Step 66205: loss = 2.88311
Step 66210: loss = 2.96596
Step 66215: loss = 2.78613
Step 66220: loss = 2.83038
Step 66225: loss = 2.94748
Step 66230: loss = 3.13160
Step 66235: loss = 2.73284
Step 66240: loss = 2.93812
Step 66245: loss = 2.88257
Step 66250: loss = 2.82621
Step 66255: loss = 2.75790
Step 66260: loss = 2.65930
Step 66265: loss = 2.51238
Step 66270: loss = 2.77542
Step 66275: loss = 2.76369
Step 66280: loss = 2.78746
Step 66285: loss = 2.87810
Step 66290: loss = 2.59704
Step 66295: loss = 2.72575
Step 66300: loss = 2.81538
Training Data Eval:
  Num examples: 49920, Num correct: 6660, Precision @ 1: 0.1334
('Testing Data Eval: EPOCH->', 171)
  Num examples: 9984, Num correct: 1372, Precision @ 1: 0.1374
Step 66305: loss = 2.85342
Step 66310: loss = 2.64718
Step 66315: loss = 2.80481
Step 66320: loss = 2.80510
Step 66325: loss = 2.72610
Step 66330: loss = 2.83529
Step 66335: loss = 3.13231
Step 66340: loss = 2.88769
Step 66345: loss = 2.81470
Step 66350: loss = 3.01443
Step 66355: loss = 2.72351
Step 66360: loss = 2.73866
Step 66365: loss = 2.74572
Step 66370: loss = 3.00213
Step 66375: loss = 2.73444
Step 66380: loss = 2.83440
Step 66385: loss = 2.72833
Step 66390: loss = 2.95631
Step 66395: loss = 2.89112
Step 66400: loss = 2.72563
Step 66405: loss = 2.64892
Step 66410: loss = 2.85525
Step 66415: loss = 2.99045
Step 66420: loss = 2.69389
Step 66425: loss = 2.92506
Step 66430: loss = 2.98084
Step 66435: loss = 2.60890
Step 66440: loss = 2.65821
Step 66445: loss = 2.76941
Step 66450: loss = 2.77346
Step 66455: loss = 2.77053
Step 66460: loss = 2.76317
Step 66465: loss = 2.85419
Step 66470: loss = 2.77585
Step 66475: loss = 2.80395
Step 66480: loss = 2.68031
Step 66485: loss = 2.81673
Step 66490: loss = 2.91236
Step 66495: loss = 3.14577
Step 66500: loss = 2.98333
Step 66505: loss = 2.79029
Step 66510: loss = 2.99083
Step 66515: loss = 2.78559
Step 66520: loss = 2.74750
Step 66525: loss = 2.78933
Step 66530: loss = 2.98691
Step 66535: loss = 2.85502
Step 66540: loss = 2.74305
Step 66545: loss = 2.77682
Step 66550: loss = 2.98889
Step 66555: loss = 2.88288
Step 66560: loss = 3.03792
Step 66565: loss = 2.75939
Step 66570: loss = 2.74131
Step 66575: loss = 2.81200
Step 66580: loss = 2.58395
Step 66585: loss = 2.59538
Step 66590: loss = 2.67600
Step 66595: loss = 2.79385
Step 66600: loss = 2.73530
Step 66605: loss = 2.70919
Step 66610: loss = 2.86493
Step 66615: loss = 2.73982
Step 66620: loss = 3.06162
Step 66625: loss = 2.75321
Step 66630: loss = 2.69509
Step 66635: loss = 2.94475
Step 66640: loss = 2.82105
Step 66645: loss = 2.76169
Step 66650: loss = 2.70029
Step 66655: loss = 2.68199
Step 66660: loss = 2.70506
Step 66665: loss = 2.97518
Step 66670: loss = 2.72681
Step 66675: loss = 2.92244
Step 66680: loss = 2.93024
Step 66685: loss = 2.73943
Step 66690: loss = 2.88094
Training Data Eval:
  Num examples: 49920, Num correct: 6962, Precision @ 1: 0.1395
('Testing Data Eval: EPOCH->', 172)
  Num examples: 9984, Num correct: 1425, Precision @ 1: 0.1427
Step 66695: loss = 2.69403
Step 66700: loss = 2.86891
Step 66705: loss = 2.95060
Step 66710: loss = 2.89526
Step 66715: loss = 2.86750
Step 66720: loss = 2.75205
Step 66725: loss = 3.03536
Step 66730: loss = 2.71573
Step 66735: loss = 2.99495
Step 66740: loss = 2.94009
Step 66745: loss = 3.12682
Step 66750: loss = 3.00483
Step 66755: loss = 2.66570
Step 66760: loss = 2.94747
Step 66765: loss = 2.92236
Step 66770: loss = 2.75058
Step 66775: loss = 2.82899
Step 66780: loss = 2.66457
Step 66785: loss = 2.77760
Step 66790: loss = 2.80298
Step 66795: loss = 2.77859
Step 66800: loss = 2.92961
Step 66805: loss = 2.76308
Step 66810: loss = 2.91327
Step 66815: loss = 2.64033
Step 66820: loss = 2.64423
Step 66825: loss = 2.86649
Step 66830: loss = 2.75168
Step 66835: loss = 2.59647
Step 66840: loss = 2.81473
Step 66845: loss = 2.74649
Step 66850: loss = 2.86542
Step 66855: loss = 2.58119
Step 66860: loss = 2.80665
Step 66865: loss = 2.95345
Step 66870: loss = 2.88703
Step 66875: loss = 2.75296
Step 66880: loss = 2.86173
Step 66885: loss = 2.92854
Step 66890: loss = 2.65428
Step 66895: loss = 2.88305
Step 66900: loss = 2.88769
Step 66905: loss = 2.68098
Step 66910: loss = 2.59755
Step 66915: loss = 2.76029
Step 66920: loss = 2.55102
Step 66925: loss = 2.71081
Step 66930: loss = 2.86653
Step 66935: loss = 2.61248
Step 66940: loss = 2.78772
Step 66945: loss = 2.87869
Step 66950: loss = 2.76447
Step 66955: loss = 2.76981
Step 66960: loss = 3.06978
Step 66965: loss = 3.13384
Step 66970: loss = 2.85184
Step 66975: loss = 2.90133
Step 66980: loss = 2.73211
Step 66985: loss = 2.84538
Step 66990: loss = 2.76910
Step 66995: loss = 2.79159
Step 67000: loss = 2.61400
Step 67005: loss = 2.91481
Step 67010: loss = 2.87071
Step 67015: loss = 2.77909
Step 67020: loss = 2.92741
Step 67025: loss = 3.00570
Step 67030: loss = 2.91937
Step 67035: loss = 2.99027
Step 67040: loss = 2.58259
Step 67045: loss = 2.78078
Step 67050: loss = 2.71514
Step 67055: loss = 2.81683
Step 67060: loss = 3.10595
Step 67065: loss = 3.06739
Step 67070: loss = 2.71848
Step 67075: loss = 2.74478
Step 67080: loss = 2.71112
Training Data Eval:
  Num examples: 49920, Num correct: 6651, Precision @ 1: 0.1332
('Testing Data Eval: EPOCH->', 173)
  Num examples: 9984, Num correct: 1372, Precision @ 1: 0.1374
Step 67085: loss = 2.74192
Step 67090: loss = 2.65954
Step 67095: loss = 2.94307
Step 67100: loss = 2.73722
Step 67105: loss = 2.97698
Step 67110: loss = 2.87076
Step 67115: loss = 2.76295
Step 67120: loss = 2.77150
Step 67125: loss = 2.69778
Step 67130: loss = 3.08498
Step 67135: loss = 2.80240
Step 67140: loss = 2.83904
Step 67145: loss = 2.94739
Step 67150: loss = 2.84711
Step 67155: loss = 2.97192
Step 67160: loss = 2.84947
Step 67165: loss = 2.74818
Step 67170: loss = 2.76167
Step 67175: loss = 2.95693
Step 67180: loss = 3.00100
Step 67185: loss = 2.77982
Step 67190: loss = 2.81666
Step 67195: loss = 2.74086
Step 67200: loss = 2.89402
Step 67205: loss = 2.80632
Step 67210: loss = 2.93264
Step 67215: loss = 2.53846
Step 67220: loss = 2.86431
Step 67225: loss = 2.76440
Step 67230: loss = 2.82282
Step 67235: loss = 2.79622
Step 67240: loss = 2.79574
Step 67245: loss = 2.74538
Step 67250: loss = 2.93989
Step 67255: loss = 2.82233
Step 67260: loss = 2.69833
Step 67265: loss = 2.72886
Step 67270: loss = 2.75308
Step 67275: loss = 2.81643
Step 67280: loss = 2.83210
Step 67285: loss = 2.95230
Step 67290: loss = 2.79332
Step 67295: loss = 3.12223
Step 67300: loss = 2.86710
Step 67305: loss = 3.04949
Step 67310: loss = 2.68530
Step 67315: loss = 2.76747
Step 67320: loss = 3.10472
Step 67325: loss = 2.68165
Step 67330: loss = 2.59693
Step 67335: loss = 2.88364
Step 67340: loss = 2.79510
Step 67345: loss = 2.71171
Step 67350: loss = 2.81787
Step 67355: loss = 2.83642
Step 67360: loss = 2.93987
Step 67365: loss = 3.02052
Step 67370: loss = 3.06626
Step 67375: loss = 2.87002
Step 67380: loss = 2.67468
Step 67385: loss = 2.96776
Step 67390: loss = 2.75462
Step 67395: loss = 2.87552
Step 67400: loss = 2.96294
Step 67405: loss = 2.98533
Step 67410: loss = 2.83739
Step 67415: loss = 2.88087
Step 67420: loss = 2.57516
Step 67425: loss = 2.80677
Step 67430: loss = 2.69231
Step 67435: loss = 2.83524
Step 67440: loss = 2.69315
Step 67445: loss = 2.78668
Step 67450: loss = 2.85641
Step 67455: loss = 2.93987
Step 67460: loss = 2.83285
Step 67465: loss = 2.72874
Step 67470: loss = 2.78241
Training Data Eval:
  Num examples: 49920, Num correct: 6738, Precision @ 1: 0.1350
('Testing Data Eval: EPOCH->', 174)
  Num examples: 9984, Num correct: 1345, Precision @ 1: 0.1347
Step 67475: loss = 3.03797
Step 67480: loss = 3.03182
Step 67485: loss = 2.79290
Step 67490: loss = 2.83277
Step 67495: loss = 2.76118
Step 67500: loss = 2.88834
Step 67505: loss = 2.80919
Step 67510: loss = 2.60959
Step 67515: loss = 2.76852
Step 67520: loss = 2.89720
Step 67525: loss = 3.00479
Step 67530: loss = 2.77638
Step 67535: loss = 2.97989
Step 67540: loss = 2.80018
Step 67545: loss = 2.83496
Step 67550: loss = 2.85478
Step 67555: loss = 2.76745
Step 67560: loss = 2.84883
Step 67565: loss = 2.80227
Step 67570: loss = 2.75650
Step 67575: loss = 2.79174
Step 67580: loss = 2.89445
Step 67585: loss = 2.92440
Step 67590: loss = 2.86143
Step 67595: loss = 2.95535
Step 67600: loss = 2.97523
Step 67605: loss = 2.80545
Step 67610: loss = 2.93642
Step 67615: loss = 3.06655
Step 67620: loss = 2.56661
Step 67625: loss = 2.87074
Step 67630: loss = 2.89191
Step 67635: loss = 3.11742
Step 67640: loss = 3.09598
Step 67645: loss = 3.02517
Step 67650: loss = 2.63254
Step 67655: loss = 2.92748
Step 67660: loss = 2.80326
Step 67665: loss = 2.86560
Step 67670: loss = 2.91401
Step 67675: loss = 2.72182
Step 67680: loss = 2.67697
Step 67685: loss = 2.73086
Step 67690: loss = 2.80632
Step 67695: loss = 2.83559
Step 67700: loss = 2.94884
Step 67705: loss = 2.91059
Step 67710: loss = 2.99567
Step 67715: loss = 2.81545
Step 67720: loss = 2.84137
Step 67725: loss = 2.72784
Step 67730: loss = 2.98243
Step 67735: loss = 2.94458
Step 67740: loss = 2.65477
Step 67745: loss = 2.89481
Step 67750: loss = 2.92741
Step 67755: loss = 2.96191
Step 67760: loss = 3.01267
Step 67765: loss = 3.13247
Step 67770: loss = 3.03711
Step 67775: loss = 2.65506
Step 67780: loss = 2.82501
Step 67785: loss = 2.70688
Step 67790: loss = 2.82373
Step 67795: loss = 2.84052
Step 67800: loss = 2.67687
Step 67805: loss = 2.77271
Step 67810: loss = 2.59442
Step 67815: loss = 2.79771
Step 67820: loss = 2.82029
Step 67825: loss = 2.84929
Step 67830: loss = 3.05092
Step 67835: loss = 2.83974
Step 67840: loss = 2.68464
Step 67845: loss = 2.69701
Step 67850: loss = 2.92352
Step 67855: loss = 2.90976
Step 67860: loss = 2.91654
Training Data Eval:
  Num examples: 49920, Num correct: 6834, Precision @ 1: 0.1369
('Testing Data Eval: EPOCH->', 175)
  Num examples: 9984, Num correct: 1416, Precision @ 1: 0.1418
Step 67865: loss = 2.91431
Step 67870: loss = 2.82187
Step 67875: loss = 2.77470
Step 67880: loss = 2.94754
Step 67885: loss = 2.73388
Step 67890: loss = 2.76424
Step 67895: loss = 2.90017
Step 67900: loss = 2.88962
Step 67905: loss = 2.75271
Step 67910: loss = 2.75204
Step 67915: loss = 3.06079
Step 67920: loss = 2.77929
Step 67925: loss = 2.91567
Step 67930: loss = 2.70464
Step 67935: loss = 2.97336
Step 67940: loss = 2.83548
Step 67945: loss = 2.80538
Step 67950: loss = 2.89451
Step 67955: loss = 2.90125
Step 67960: loss = 2.90448
Step 67965: loss = 3.04031
Step 67970: loss = 2.97589
Step 67975: loss = 2.86164
Step 67980: loss = 2.93842
Step 67985: loss = 2.96576
Step 67990: loss = 2.93449
Step 67995: loss = 2.90543
Step 68000: loss = 2.85148
Step 68005: loss = 2.57967
Step 68010: loss = 2.87059
Step 68015: loss = 2.87864
Step 68020: loss = 2.91542
Step 68025: loss = 2.69036
Step 68030: loss = 2.87533
Step 68035: loss = 3.05207
Step 68040: loss = 2.77210
Step 68045: loss = 2.75089
Step 68050: loss = 2.81608
Step 68055: loss = 2.87841
Step 68060: loss = 2.89382
Step 68065: loss = 2.64263
Step 68070: loss = 2.45989
Step 68075: loss = 2.66213
Step 68080: loss = 2.73669
Step 68085: loss = 2.79895
Step 68090: loss = 2.89468
Step 68095: loss = 2.81881
Step 68100: loss = 2.84193
Step 68105: loss = 2.66408
Step 68110: loss = 2.70526
Step 68115: loss = 2.66138
Step 68120: loss = 2.88688
Step 68125: loss = 2.97366
Step 68130: loss = 2.82489
Step 68135: loss = 2.82182
Step 68140: loss = 2.83276
Step 68145: loss = 2.85830
Step 68150: loss = 2.70232
Step 68155: loss = 2.68672
Step 68160: loss = 2.62838
Step 68165: loss = 2.68179
Step 68170: loss = 2.78049
Step 68175: loss = 2.64983
Step 68180: loss = 2.84872
Step 68185: loss = 2.80520
Step 68190: loss = 2.63343
Step 68195: loss = 2.70345
Step 68200: loss = 2.90130
Step 68205: loss = 2.85316
Step 68210: loss = 2.64470
Step 68215: loss = 2.70857
Step 68220: loss = 2.77517
Step 68225: loss = 2.80386
Step 68230: loss = 2.92182
Step 68235: loss = 2.72625
Step 68240: loss = 2.89238
Step 68245: loss = 2.57070
Step 68250: loss = 2.89938
Training Data Eval:
  Num examples: 49920, Num correct: 6893, Precision @ 1: 0.1381
('Testing Data Eval: EPOCH->', 176)
  Num examples: 9984, Num correct: 1443, Precision @ 1: 0.1445
Step 68255: loss = 2.83618
Step 68260: loss = 3.02146
Step 68265: loss = 2.85449
Step 68270: loss = 2.88248
Step 68275: loss = 2.87389
Step 68280: loss = 2.74546
Step 68285: loss = 2.94957
Step 68290: loss = 2.62063
Step 68295: loss = 2.70914
Step 68300: loss = 3.05926
Step 68305: loss = 2.76847
Step 68310: loss = 2.78601
Step 68315: loss = 2.82775
Step 68320: loss = 2.74260
Step 68325: loss = 2.82727
Step 68330: loss = 2.84658
Step 68335: loss = 2.76160
Step 68340: loss = 2.86474
Step 68345: loss = 2.80501
Step 68350: loss = 2.93492
Step 68355: loss = 2.94987
Step 68360: loss = 2.82776
Step 68365: loss = 2.84121
Step 68370: loss = 2.67566
Step 68375: loss = 2.91007
Step 68380: loss = 2.83602
Step 68385: loss = 2.76295
Step 68390: loss = 2.71286
Step 68395: loss = 2.77786
Step 68400: loss = 2.63688
Step 68405: loss = 3.03378
Step 68410: loss = 2.73027
Step 68415: loss = 2.69414
Step 68420: loss = 2.96976
Step 68425: loss = 2.93970
Step 68430: loss = 3.03859
Step 68435: loss = 2.97620
Step 68440: loss = 2.78530
Step 68445: loss = 2.54911
Step 68450: loss = 2.88478
Step 68455: loss = 2.85890
Step 68460: loss = 2.65866
Step 68465: loss = 2.82382
Step 68470: loss = 2.59513
Step 68475: loss = 2.76839
Step 68480: loss = 2.64297
Step 68485: loss = 2.66666
Step 68490: loss = 2.87008
Step 68495: loss = 2.94957
Step 68500: loss = 2.98418
Step 68505: loss = 2.78206
Step 68510: loss = 2.80508
Step 68515: loss = 2.67028
Step 68520: loss = 3.00924
Step 68525: loss = 2.75934
Step 68530: loss = 2.55642
Step 68535: loss = 2.81885
Step 68540: loss = 2.76376
Step 68545: loss = 2.76432
Step 68550: loss = 2.83656
Step 68555: loss = 2.90921
Step 68560: loss = 2.75121
Step 68565: loss = 2.70413
Step 68570: loss = 2.69759
Step 68575: loss = 2.86082
Step 68580: loss = 2.68302
Step 68585: loss = 2.62062
Step 68590: loss = 2.56624
Step 68595: loss = 2.65216
Step 68600: loss = 2.75115
Step 68605: loss = 3.03336
Step 68610: loss = 2.86221
Step 68615: loss = 2.79138
Step 68620: loss = 2.87230
Step 68625: loss = 2.66507
Step 68630: loss = 2.83314
Step 68635: loss = 2.70309
Step 68640: loss = 2.61055
Training Data Eval:
  Num examples: 49920, Num correct: 6970, Precision @ 1: 0.1396
('Testing Data Eval: EPOCH->', 177)
  Num examples: 9984, Num correct: 1433, Precision @ 1: 0.1435
Step 68645: loss = 2.84138
Step 68650: loss = 2.55560
Step 68655: loss = 2.94099
Step 68660: loss = 3.12414
Step 68665: loss = 2.55176
Step 68670: loss = 2.90538
Step 68675: loss = 2.74297
Step 68680: loss = 3.03196
Step 68685: loss = 2.68390
Step 68690: loss = 2.76440
Step 68695: loss = 2.69755
Step 68700: loss = 2.88697
Step 68705: loss = 2.71747
Step 68710: loss = 3.03054
Step 68715: loss = 2.59706
Step 68720: loss = 2.80467
Step 68725: loss = 2.91073
Step 68730: loss = 2.85685
Step 68735: loss = 3.06715
Step 68740: loss = 2.87181
Step 68745: loss = 2.91772
Step 68750: loss = 2.87365
Step 68755: loss = 2.90932
Step 68760: loss = 2.79331
Step 68765: loss = 3.07283
Step 68770: loss = 3.05775
Step 68775: loss = 2.75830
Step 68780: loss = 2.72634
Step 68785: loss = 2.69736
Step 68790: loss = 2.92463
Step 68795: loss = 2.69874
Step 68800: loss = 2.84848
Step 68805: loss = 2.81914
Step 68810: loss = 2.97285
Step 68815: loss = 2.72628
Step 68820: loss = 2.81502
Step 68825: loss = 2.62534
Step 68830: loss = 2.62681
Step 68835: loss = 2.73861
Step 68840: loss = 2.98140
Step 68845: loss = 2.78110
Step 68850: loss = 3.05417
Step 68855: loss = 2.88901
Step 68860: loss = 2.96184
Step 68865: loss = 2.88304
Step 68870: loss = 2.83604
Step 68875: loss = 2.82954
Step 68880: loss = 2.86617
Step 68885: loss = 2.66031
Step 68890: loss = 2.70545
Step 68895: loss = 2.76264
Step 68900: loss = 2.75269
Step 68905: loss = 2.87517
Step 68910: loss = 2.89213
Step 68915: loss = 2.79336
Step 68920: loss = 2.90933
Step 68925: loss = 2.77493
Step 68930: loss = 2.93990
Step 68935: loss = 2.70570
Step 68940: loss = 2.71464
Step 68945: loss = 2.74452
Step 68950: loss = 2.79832
Step 68955: loss = 2.72557
Step 68960: loss = 2.90708
Step 68965: loss = 2.83958
Step 68970: loss = 2.82280
Step 68975: loss = 2.64894
Step 68980: loss = 2.85201
Step 68985: loss = 2.73616
Step 68990: loss = 3.13806
Step 68995: loss = 2.66976
Step 69000: loss = 2.97575
Step 69005: loss = 2.56830
Step 69010: loss = 3.00221
Step 69015: loss = 2.91803
Step 69020: loss = 2.82619
Step 69025: loss = 2.91524
Step 69030: loss = 2.81626
Training Data Eval:
  Num examples: 49920, Num correct: 6606, Precision @ 1: 0.1323
('Testing Data Eval: EPOCH->', 178)
  Num examples: 9984, Num correct: 1333, Precision @ 1: 0.1335
Step 69035: loss = 2.96426
Step 69040: loss = 2.82947
Step 69045: loss = 2.77894
Step 69050: loss = 2.64918
Step 69055: loss = 2.89822
Step 69060: loss = 2.82504
Step 69065: loss = 2.71561
Step 69070: loss = 2.58156
Step 69075: loss = 2.86818
Step 69080: loss = 2.74505
Step 69085: loss = 2.65826
Step 69090: loss = 2.66541
Step 69095: loss = 3.02242
Step 69100: loss = 2.77782
Step 69105: loss = 3.04360
Step 69110: loss = 2.79803
Step 69115: loss = 2.73193
Step 69120: loss = 2.85510
Step 69125: loss = 3.10638
Step 69130: loss = 2.79698
Step 69135: loss = 2.86751
Step 69140: loss = 3.11410
Step 69145: loss = 2.70681
Step 69150: loss = 3.06241
Step 69155: loss = 2.83206
Step 69160: loss = 2.72592
Step 69165: loss = 2.89373
Step 69170: loss = 2.70426
Step 69175: loss = 2.81469
Step 69180: loss = 2.91123
Step 69185: loss = 2.76435
Step 69190: loss = 2.81500
Step 69195: loss = 2.75044
Step 69200: loss = 2.76834
Step 69205: loss = 3.00002
Step 69210: loss = 2.72587
Step 69215: loss = 2.73416
Step 69220: loss = 2.87526
Step 69225: loss = 2.80114
Step 69230: loss = 2.70535
Step 69235: loss = 2.72174
Step 69240: loss = 2.62805
Step 69245: loss = 2.99420
Step 69250: loss = 2.74542
Step 69255: loss = 2.76418
Step 69260: loss = 2.78755
Step 69265: loss = 2.98747
Step 69270: loss = 2.77385
Step 69275: loss = 2.78134
Step 69280: loss = 2.59110
Step 69285: loss = 2.89734
Step 69290: loss = 2.67176
Step 69295: loss = 2.64719
Step 69300: loss = 2.82528
Step 69305: loss = 2.79526
Step 69310: loss = 2.78511
Step 69315: loss = 2.71598
Step 69320: loss = 2.97188
Step 69325: loss = 2.92111
Step 69330: loss = 2.91399
Step 69335: loss = 2.67183
Step 69340: loss = 2.53547
Step 69345: loss = 2.89414
Step 69350: loss = 2.93525
Step 69355: loss = 2.96434
Step 69360: loss = 2.75259
Step 69365: loss = 2.93955
Step 69370: loss = 2.89932
Step 69375: loss = 2.88691
Step 69380: loss = 2.82351
Step 69385: loss = 2.78450
Step 69390: loss = 3.05508
Step 69395: loss = 2.97693
Step 69400: loss = 2.76888
Step 69405: loss = 2.91727
Step 69410: loss = 3.01805
Step 69415: loss = 2.56124
Step 69420: loss = 2.70240
Training Data Eval:
  Num examples: 49920, Num correct: 6778, Precision @ 1: 0.1358
('Testing Data Eval: EPOCH->', 179)
  Num examples: 9984, Num correct: 1364, Precision @ 1: 0.1366
Step 69425: loss = 2.68222
Step 69430: loss = 2.74264
Step 69435: loss = 2.77201
Step 69440: loss = 2.76627
Step 69445: loss = 3.11712
Step 69450: loss = 2.76890
Step 69455: loss = 3.02596
Step 69460: loss = 2.84904
Step 69465: loss = 2.88855
Step 69470: loss = 3.02735
Step 69475: loss = 2.84583
Step 69480: loss = 2.82854
Step 69485: loss = 2.84531
Step 69490: loss = 2.89048
Step 69495: loss = 2.92263
Step 69500: loss = 2.88995
Step 69505: loss = 2.68969
Step 69510: loss = 2.81776
Step 69515: loss = 2.83436
Step 69520: loss = 3.01693
Step 69525: loss = 2.80417
Step 69530: loss = 2.90965
Step 69535: loss = 2.98362
Step 69540: loss = 2.67269
Step 69545: loss = 2.84400
Step 69550: loss = 2.78241
Step 69555: loss = 2.71576
Step 69560: loss = 2.67716
Step 69565: loss = 2.67969
Step 69570: loss = 2.86983
Step 69575: loss = 2.89107
Step 69580: loss = 2.87621
Step 69585: loss = 2.90526
Step 69590: loss = 2.84863
Step 69595: loss = 2.91733
Step 69600: loss = 2.69280
Step 69605: loss = 3.04366
Step 69610: loss = 2.75153
Step 69615: loss = 2.84139
Step 69620: loss = 2.64679
Step 69625: loss = 2.99492
Step 69630: loss = 2.69702
Step 69635: loss = 2.72110
Step 69640: loss = 2.86979
Step 69645: loss = 2.64082
Step 69650: loss = 2.85084
Step 69655: loss = 2.70238
Step 69660: loss = 2.74244
Step 69665: loss = 2.75373
Step 69670: loss = 2.93248
Step 69675: loss = 2.90811
Step 69680: loss = 2.87576
Step 69685: loss = 2.75570
Step 69690: loss = 2.64009
Step 69695: loss = 2.90418
Step 69700: loss = 2.96875
Step 69705: loss = 2.54611
Step 69710: loss = 3.03543
Step 69715: loss = 2.80138
Step 69720: loss = 2.74912
Step 69725: loss = 2.94772
Step 69730: loss = 2.99593
Step 69735: loss = 2.84174
Step 69740: loss = 2.90556
Step 69745: loss = 2.88801
Step 69750: loss = 2.59638
Step 69755: loss = 2.59281
Step 69760: loss = 2.84000
Step 69765: loss = 2.86113
Step 69770: loss = 2.77952
Step 69775: loss = 2.90045
Step 69780: loss = 2.97393
Step 69785: loss = 2.77196
Step 69790: loss = 2.58737
Step 69795: loss = 2.88222
Step 69800: loss = 2.68280
Step 69805: loss = 2.63157
Step 69810: loss = 3.20663
Training Data Eval:
  Num examples: 49920, Num correct: 6722, Precision @ 1: 0.1347
('Testing Data Eval: EPOCH->', 180)
  Num examples: 9984, Num correct: 1450, Precision @ 1: 0.1452
Step 69815: loss = 2.97947
Step 69820: loss = 3.06920
Step 69825: loss = 2.72448
Step 69830: loss = 2.98228
Step 69835: loss = 2.94425
Step 69840: loss = 3.02233
Step 69845: loss = 2.74416
Step 69850: loss = 2.98767
Step 69855: loss = 2.95076
Step 69860: loss = 2.85767
Step 69865: loss = 2.86044
Step 69870: loss = 2.95933
Step 69875: loss = 2.88235
Step 69880: loss = 2.72250
Step 69885: loss = 2.86995
Step 69890: loss = 2.76706
Step 69895: loss = 2.88511
Step 69900: loss = 2.70218
Step 69905: loss = 2.73073
Step 69910: loss = 2.96740
Step 69915: loss = 2.78880
Step 69920: loss = 2.78247
Step 69925: loss = 2.79523
Step 69930: loss = 3.05258
Step 69935: loss = 2.95535
Step 69940: loss = 2.96704
Step 69945: loss = 2.75476
Step 69950: loss = 2.70454
Step 69955: loss = 2.93032
Step 69960: loss = 2.91549
Step 69965: loss = 2.64070
Step 69970: loss = 2.77061
Step 69975: loss = 2.88160
Step 69980: loss = 2.95865
Step 69985: loss = 2.76877
Step 69990: loss = 2.96484
Step 69995: loss = 2.66064
Step 70000: loss = 2.78145
Step 70005: loss = 2.80436
Step 70010: loss = 2.74473
Step 70015: loss = 2.96696
Step 70020: loss = 2.75997
Step 70025: loss = 2.51021
Step 70030: loss = 2.71385
Step 70035: loss = 2.85793
Step 70040: loss = 3.00706
Step 70045: loss = 3.04674
Step 70050: loss = 2.83928
Step 70055: loss = 2.86795
Step 70060: loss = 2.83036
Step 70065: loss = 2.77836
Step 70070: loss = 2.74576
Step 70075: loss = 2.93057
Step 70080: loss = 2.72525
Step 70085: loss = 3.12606
Step 70090: loss = 3.06855
Step 70095: loss = 2.83703
Step 70100: loss = 2.87554
Step 70105: loss = 2.74880
Step 70110: loss = 3.11968
Step 70115: loss = 2.71553
Step 70120: loss = 2.94639
Step 70125: loss = 2.70897
Step 70130: loss = 2.69278
Step 70135: loss = 2.76415
Step 70140: loss = 2.82568
Step 70145: loss = 2.97774
Step 70150: loss = 2.85843
Step 70155: loss = 2.93001
Step 70160: loss = 2.67288
Step 70165: loss = 2.75512
Step 70170: loss = 2.73414
Step 70175: loss = 2.76555
Step 70180: loss = 2.90381
Step 70185: loss = 2.62351
Step 70190: loss = 2.78143
Step 70195: loss = 2.76995
Step 70200: loss = 2.85350
Training Data Eval:
  Num examples: 49920, Num correct: 6783, Precision @ 1: 0.1359
('Testing Data Eval: EPOCH->', 181)
  Num examples: 9984, Num correct: 1426, Precision @ 1: 0.1428
Step 70205: loss = 2.78890
Step 70210: loss = 2.91120
Step 70215: loss = 2.76140
Step 70220: loss = 2.77476
Step 70225: loss = 2.96503
Step 70230: loss = 2.75912
Step 70235: loss = 2.75265
Step 70240: loss = 2.97194
Step 70245: loss = 2.71238
Step 70250: loss = 2.74002
Step 70255: loss = 2.79616
Step 70260: loss = 2.64798
Step 70265: loss = 2.79316
Step 70270: loss = 2.77049
Step 70275: loss = 2.82288
Step 70280: loss = 2.79589
Step 70285: loss = 2.67635
Step 70290: loss = 2.84179
Step 70295: loss = 2.70927
Step 70300: loss = 2.74065
Step 70305: loss = 2.91243
Step 70310: loss = 2.98664
Step 70315: loss = 2.79291
Step 70320: loss = 2.81843
Step 70325: loss = 2.78109
Step 70330: loss = 2.81995
Step 70335: loss = 2.85308
Step 70340: loss = 2.71153
Step 70345: loss = 2.69472
Step 70350: loss = 2.79279
Step 70355: loss = 2.88724
Step 70360: loss = 2.91866
Step 70365: loss = 2.79205
Step 70370: loss = 2.65120
Step 70375: loss = 2.80145
Step 70380: loss = 2.77208
Step 70385: loss = 3.18790
Step 70390: loss = 2.76570
Step 70395: loss = 2.95068
Step 70400: loss = 2.77330
Step 70405: loss = 2.94223
Step 70410: loss = 2.78876
Step 70415: loss = 2.75334
Step 70420: loss = 2.84235
Step 70425: loss = 2.68756
Step 70430: loss = 2.75617
Step 70435: loss = 2.82454
Step 70440: loss = 2.70764
Step 70445: loss = 2.76544
Step 70450: loss = 2.93320
Step 70455: loss = 3.01846
Step 70460: loss = 2.72614
Step 70465: loss = 2.73340
Step 70470: loss = 3.21365
Step 70475: loss = 2.93138
Step 70480: loss = 2.91333
Step 70485: loss = 2.91964
Step 70490: loss = 2.98909
Step 70495: loss = 2.98280
Step 70500: loss = 2.79569
Step 70505: loss = 2.87594
Step 70510: loss = 2.88923
Step 70515: loss = 2.82464
Step 70520: loss = 2.77839
Step 70525: loss = 2.94359
Step 70530: loss = 3.03612
Step 70535: loss = 2.66153
Step 70540: loss = 2.58740
Step 70545: loss = 2.84787
Step 70550: loss = 2.86196
Step 70555: loss = 2.80066
Step 70560: loss = 2.73257
Step 70565: loss = 2.97332
Step 70570: loss = 2.91668
Step 70575: loss = 3.00446
Step 70580: loss = 2.56279
Step 70585: loss = 2.82888
Step 70590: loss = 2.71554
Training Data Eval:
  Num examples: 49920, Num correct: 6709, Precision @ 1: 0.1344
('Testing Data Eval: EPOCH->', 182)
  Num examples: 9984, Num correct: 1353, Precision @ 1: 0.1355
Step 70595: loss = 2.75034
Step 70600: loss = 2.79620
Step 70605: loss = 2.75654
Step 70610: loss = 2.74953
Step 70615: loss = 2.57713
Step 70620: loss = 2.87082
Step 70625: loss = 2.85765
Step 70630: loss = 2.77741
Step 70635: loss = 2.97714
Step 70640: loss = 2.83268
Step 70645: loss = 2.73555
Step 70650: loss = 2.76964
Step 70655: loss = 2.81757
Step 70660: loss = 2.98762
Step 70665: loss = 2.68414
Step 70670: loss = 2.89625
Step 70675: loss = 2.88247
Step 70680: loss = 2.97334
Step 70685: loss = 2.90128
Step 70690: loss = 2.71844
Step 70695: loss = 2.79707
Step 70700: loss = 2.90958
Step 70705: loss = 2.73806
Step 70710: loss = 2.94457
Step 70715: loss = 2.72439
Step 70720: loss = 2.83616
Step 70725: loss = 2.67948
Step 70730: loss = 2.94919
Step 70735: loss = 2.86113
Step 70740: loss = 3.05231
Step 70745: loss = 2.99133
Step 70750: loss = 2.89162
Step 70755: loss = 2.94578
Step 70760: loss = 2.82533
Step 70765: loss = 2.71644
Step 70770: loss = 2.68879
Step 70775: loss = 2.98770
Step 70780: loss = 2.82963
Step 70785: loss = 2.81639
Step 70790: loss = 2.61612
Step 70795: loss = 2.81064
Step 70800: loss = 2.70350
Step 70805: loss = 2.78904
Step 70810: loss = 2.84414
Step 70815: loss = 3.15901
Step 70820: loss = 3.02111
Step 70825: loss = 2.96775
Step 70830: loss = 2.79814
Step 70835: loss = 2.82993
Step 70840: loss = 2.83276
Step 70845: loss = 2.82367
Step 70850: loss = 2.93702
Step 70855: loss = 2.68063
Step 70860: loss = 2.85467
Step 70865: loss = 2.94719
Step 70870: loss = 2.55250
Step 70875: loss = 2.64819
Step 70880: loss = 2.93175
Step 70885: loss = 2.68054
Step 70890: loss = 2.81615
Step 70895: loss = 2.66213
Step 70900: loss = 2.63484
Step 70905: loss = 2.81124
Step 70910: loss = 2.91138
Step 70915: loss = 2.74835
Step 70920: loss = 2.91907
Step 70925: loss = 3.08996
Step 70930: loss = 2.79756
Step 70935: loss = 2.97114
Step 70940: loss = 2.92617
Step 70945: loss = 2.80571
Step 70950: loss = 2.62000
Step 70955: loss = 2.83886
Step 70960: loss = 2.73217
Step 70965: loss = 2.73812
Step 70970: loss = 2.84403
Step 70975: loss = 2.70401
Step 70980: loss = 2.72420
Training Data Eval:
  Num examples: 49920, Num correct: 6703, Precision @ 1: 0.1343
('Testing Data Eval: EPOCH->', 183)
  Num examples: 9984, Num correct: 1348, Precision @ 1: 0.1350
Step 70985: loss = 2.69169
Step 70990: loss = 2.93099
Step 70995: loss = 2.73289
Step 71000: loss = 2.75909
Step 71005: loss = 2.78822
Step 71010: loss = 2.90611
Step 71015: loss = 2.76973
Step 71020: loss = 2.89944
Step 71025: loss = 2.70932
Step 71030: loss = 2.95553
Step 71035: loss = 2.81392
Step 71040: loss = 2.88473
Step 71045: loss = 2.88899
Step 71050: loss = 2.74634
Step 71055: loss = 2.96132
Step 71060: loss = 2.74280
Step 71065: loss = 2.80338
Step 71070: loss = 2.84716
Step 71075: loss = 2.98052
Step 71080: loss = 2.92311
Step 71085: loss = 3.14382
Step 71090: loss = 2.74005
Step 71095: loss = 2.70848
Step 71100: loss = 2.75413
Step 71105: loss = 2.91285
Step 71110: loss = 2.81184
Step 71115: loss = 2.87139
Step 71120: loss = 3.01311
Step 71125: loss = 2.83435
Step 71130: loss = 2.70054
Step 71135: loss = 2.88926
Step 71140: loss = 2.84702
Step 71145: loss = 2.98692
Step 71150: loss = 2.90599
Step 71155: loss = 2.89471
Step 71160: loss = 2.83594
Step 71165: loss = 2.85321
Step 71170: loss = 2.80073
Step 71175: loss = 2.89539
Step 71180: loss = 2.95364
Step 71185: loss = 2.64202
Step 71190: loss = 2.84962
Step 71195: loss = 2.83916
Step 71200: loss = 2.45367
Step 71205: loss = 2.75013
Step 71210: loss = 2.89718
Step 71215: loss = 2.68067
Step 71220: loss = 2.62460
Step 71225: loss = 2.89968
Step 71230: loss = 2.82879
Step 71235: loss = 2.94593
Step 71240: loss = 2.87776
Step 71245: loss = 3.05197
Step 71250: loss = 2.73764
Step 71255: loss = 2.73494
Step 71260: loss = 2.58654
Step 71265: loss = 2.85828
Step 71270: loss = 3.08982
Step 71275: loss = 2.86693
Step 71280: loss = 2.91454
Step 71285: loss = 2.78238
Step 71290: loss = 2.95377
Step 71295: loss = 2.58318
Step 71300: loss = 2.88521
Step 71305: loss = 2.68412
Step 71310: loss = 2.83062
Step 71315: loss = 2.78171
Step 71320: loss = 2.94225
Step 71325: loss = 2.79923
Step 71330: loss = 2.86733
Step 71335: loss = 2.73890
Step 71340: loss = 2.84786
Step 71345: loss = 2.79928
Step 71350: loss = 3.00130
Step 71355: loss = 2.91613
Step 71360: loss = 2.75276
Step 71365: loss = 2.81233
Step 71370: loss = 2.95463
Training Data Eval:
  Num examples: 49920, Num correct: 7018, Precision @ 1: 0.1406
('Testing Data Eval: EPOCH->', 184)
  Num examples: 9984, Num correct: 1496, Precision @ 1: 0.1498
Step 71375: loss = 3.05019
Step 71380: loss = 2.69788
Step 71385: loss = 2.81804
Step 71390: loss = 2.88515
Step 71395: loss = 3.08553
Step 71400: loss = 2.95777
Step 71405: loss = 2.65139
Step 71410: loss = 2.86129
Step 71415: loss = 2.93223
Step 71420: loss = 3.16522
Step 71425: loss = 2.94129
Step 71430: loss = 3.05392
Step 71435: loss = 2.95168
Step 71440: loss = 2.81667
Step 71445: loss = 2.66928
Step 71450: loss = 3.01764
Step 71455: loss = 2.69242
Step 71460: loss = 2.77030
Step 71465: loss = 2.75734
Step 71470: loss = 2.81090
Step 71475: loss = 2.93334
Step 71480: loss = 2.73601
Step 71485: loss = 3.01498
Step 71490: loss = 2.92642
Step 71495: loss = 2.82431
Step 71500: loss = 3.03857
Step 71505: loss = 3.01691
Step 71510: loss = 2.81019
Step 71515: loss = 2.79815
Step 71520: loss = 2.89534
Step 71525: loss = 2.76754
Step 71530: loss = 2.92469
Step 71535: loss = 2.75680
Step 71540: loss = 2.88940
Step 71545: loss = 2.80144
Step 71550: loss = 2.94557
Step 71555: loss = 2.55432
Step 71560: loss = 2.81657
Step 71565: loss = 2.77149
Step 71570: loss = 2.91079
Step 71575: loss = 2.60875
Step 71580: loss = 3.02344
Step 71585: loss = 3.03601
Step 71590: loss = 2.90195
Step 71595: loss = 2.90055
Step 71600: loss = 2.76612
Step 71605: loss = 2.88270
Step 71610: loss = 2.96175
Step 71615: loss = 2.90719
Step 71620: loss = 2.91939
Step 71625: loss = 2.81565
Step 71630: loss = 2.89013
Step 71635: loss = 2.89896
Step 71640: loss = 2.72720
Step 71645: loss = 2.68913
Step 71650: loss = 2.74451
Step 71655: loss = 2.76016
Step 71660: loss = 2.67637
Step 71665: loss = 2.98005
Step 71670: loss = 2.85649
Step 71675: loss = 2.85484
Step 71680: loss = 2.86111
Step 71685: loss = 2.85265
Step 71690: loss = 2.87891
Step 71695: loss = 2.87063
Step 71700: loss = 2.65755
Step 71705: loss = 2.94446
Step 71710: loss = 2.70946
Step 71715: loss = 2.75052
Step 71720: loss = 2.86103
Step 71725: loss = 2.87586
Step 71730: loss = 3.09197
Step 71735: loss = 2.77793
Step 71740: loss = 2.98606
Step 71745: loss = 2.74469
Step 71750: loss = 2.88536
Step 71755: loss = 2.89727
Step 71760: loss = 2.76649
Training Data Eval:
  Num examples: 49920, Num correct: 6695, Precision @ 1: 0.1341
('Testing Data Eval: EPOCH->', 185)
  Num examples: 9984, Num correct: 1327, Precision @ 1: 0.1329
Step 71765: loss = 2.87067
Step 71770: loss = 2.82668
Step 71775: loss = 2.84694
Step 71780: loss = 2.80812
Step 71785: loss = 2.82330
Step 71790: loss = 2.70311
Step 71795: loss = 2.76909
Step 71800: loss = 2.68231
Step 71805: loss = 2.64848
Step 71810: loss = 2.71048
Step 71815: loss = 2.79671
Step 71820: loss = 2.91016
Step 71825: loss = 2.68928
Step 71830: loss = 2.91883
Step 71835: loss = 2.88510
Step 71840: loss = 2.86203
Step 71845: loss = 2.81395
Step 71850: loss = 2.59029
Step 71855: loss = 2.77702
Step 71860: loss = 3.15215
Step 71865: loss = 2.76848
Step 71870: loss = 2.95761
Step 71875: loss = 2.75929
Step 71880: loss = 2.62391
Step 71885: loss = 2.83411
Step 71890: loss = 2.90718
Step 71895: loss = 2.77185
Step 71900: loss = 2.87511
Step 71905: loss = 2.87376
Step 71910: loss = 3.09619
Step 71915: loss = 2.79058
Step 71920: loss = 2.97122
Step 71925: loss = 2.87128
Step 71930: loss = 2.73501
Step 71935: loss = 2.92801
Step 71940: loss = 2.84599
Step 71945: loss = 2.57721
Step 71950: loss = 2.81988
Step 71955: loss = 2.83307
Step 71960: loss = 2.79222
Step 71965: loss = 2.69246
Step 71970: loss = 2.74430
Step 71975: loss = 2.79543
Step 71980: loss = 2.76604
Step 71985: loss = 2.92457
Step 71990: loss = 2.71005
Step 71995: loss = 2.85161
Step 72000: loss = 2.83620
Step 72005: loss = 2.87200
Step 72010: loss = 2.95447
Step 72015: loss = 2.88298
Step 72020: loss = 2.74681
Step 72025: loss = 2.70003
Step 72030: loss = 3.00874
Step 72035: loss = 2.77872
Step 72040: loss = 2.82406
Step 72045: loss = 2.77663
Step 72050: loss = 2.75220
Step 72055: loss = 2.77031
Step 72060: loss = 2.78812
Step 72065: loss = 2.63491
Step 72070: loss = 2.76411
Step 72075: loss = 3.05889
Step 72080: loss = 2.86555
Step 72085: loss = 2.71359
Step 72090: loss = 2.77853
Step 72095: loss = 2.77495
Step 72100: loss = 2.69625
Step 72105: loss = 2.74480
Step 72110: loss = 2.99086
Step 72115: loss = 2.90933
Step 72120: loss = 2.70213
Step 72125: loss = 2.88608
Step 72130: loss = 2.98286
Step 72135: loss = 3.01036
Step 72140: loss = 2.85301
Step 72145: loss = 2.84392
Step 72150: loss = 3.13784
Training Data Eval:
  Num examples: 49920, Num correct: 6736, Precision @ 1: 0.1349
('Testing Data Eval: EPOCH->', 186)
  Num examples: 9984, Num correct: 1408, Precision @ 1: 0.1410
Step 72155: loss = 2.85116
Step 72160: loss = 2.78201
Step 72165: loss = 2.79819
Step 72170: loss = 2.80687
Step 72175: loss = 2.75413
Step 72180: loss = 2.80143
Step 72185: loss = 2.95381
Step 72190: loss = 3.08115
Step 72195: loss = 2.85560
Step 72200: loss = 2.99596
Step 72205: loss = 2.84620
Step 72210: loss = 2.78214
Step 72215: loss = 2.84921
Step 72220: loss = 2.83439
Step 72225: loss = 2.54018
Step 72230: loss = 2.84778
Step 72235: loss = 2.64021
Step 72240: loss = 3.12801
Step 72245: loss = 2.77363
Step 72250: loss = 2.77949
Step 72255: loss = 2.99714
Step 72260: loss = 2.77528
Step 72265: loss = 2.70572
Step 72270: loss = 2.97487
Step 72275: loss = 2.87559
Step 72280: loss = 2.86599
Step 72285: loss = 3.01588
Step 72290: loss = 2.69393
Step 72295: loss = 2.82986
Step 72300: loss = 2.69376
Step 72305: loss = 2.87706
Step 72310: loss = 2.88076
Step 72315: loss = 2.80791
Step 72320: loss = 2.90002
Step 72325: loss = 3.01672
Step 72330: loss = 2.73256
Step 72335: loss = 2.76936
Step 72340: loss = 2.64248
Step 72345: loss = 2.54488
Step 72350: loss = 2.92665
Step 72355: loss = 2.78409
Step 72360: loss = 2.85475
Step 72365: loss = 3.11622
Step 72370: loss = 2.89737
Step 72375: loss = 2.64072
Step 72380: loss = 2.77221
Step 72385: loss = 2.84038
Step 72390: loss = 2.66109
Step 72395: loss = 2.80569
Step 72400: loss = 2.95137
Step 72405: loss = 2.94179
Step 72410: loss = 2.76131
Step 72415: loss = 2.82745
Step 72420: loss = 2.95545
Step 72425: loss = 2.60061
Step 72430: loss = 2.97173
Step 72435: loss = 2.83711
Step 72440: loss = 2.94290
Step 72445: loss = 2.88226
Step 72450: loss = 2.66960
Step 72455: loss = 2.89176
Step 72460: loss = 2.72837
Step 72465: loss = 2.95608
Step 72470: loss = 2.66795
Step 72475: loss = 2.87113
Step 72480: loss = 2.54661
Step 72485: loss = 2.72879
Step 72490: loss = 2.68512
Step 72495: loss = 2.88036
Step 72500: loss = 2.81332
Step 72505: loss = 2.83974
Step 72510: loss = 2.73342
Step 72515: loss = 3.08842
Step 72520: loss = 2.84844
Step 72525: loss = 2.79676
Step 72530: loss = 2.86124
Step 72535: loss = 2.71885
Step 72540: loss = 3.14209
Training Data Eval:
  Num examples: 49920, Num correct: 6723, Precision @ 1: 0.1347
('Testing Data Eval: EPOCH->', 187)
  Num examples: 9984, Num correct: 1361, Precision @ 1: 0.1363
Step 72545: loss = 2.82285
Step 72550: loss = 2.76985
Step 72555: loss = 2.89851
Step 72560: loss = 3.13332
Step 72565: loss = 2.83466
Step 72570: loss = 2.76653
Step 72575: loss = 2.70029
Step 72580: loss = 2.69609
Step 72585: loss = 2.79602
Step 72590: loss = 2.76476
Step 72595: loss = 2.83475
Step 72600: loss = 2.79485
Step 72605: loss = 2.77693
Step 72610: loss = 2.63991
Step 72615: loss = 2.95251
Step 72620: loss = 2.91017
Step 72625: loss = 2.81853
Step 72630: loss = 2.85981
Step 72635: loss = 2.80648
Step 72640: loss = 2.86294
Step 72645: loss = 3.06444
Step 72650: loss = 2.62017
Step 72655: loss = 3.11464
Step 72660: loss = 2.91612
Step 72665: loss = 2.85818
Step 72670: loss = 2.71813
Step 72675: loss = 2.81450
Step 72680: loss = 2.91455
Step 72685: loss = 2.84303
Step 72690: loss = 2.68363
Step 72695: loss = 3.16225
Step 72700: loss = 2.89376
Step 72705: loss = 2.74055
Step 72710: loss = 2.92132
Step 72715: loss = 2.84511
Step 72720: loss = 2.88853
Step 72725: loss = 2.99550
Step 72730: loss = 2.94670
Step 72735: loss = 2.78582
Step 72740: loss = 3.00842
Step 72745: loss = 2.94901
Step 72750: loss = 2.91667
Step 72755: loss = 2.89393
Step 72760: loss = 2.85426
Step 72765: loss = 2.93318
Step 72770: loss = 2.61383
Step 72775: loss = 2.94902
Step 72780: loss = 2.84534
Step 72785: loss = 2.90527
Step 72790: loss = 2.89920
Step 72795: loss = 2.77995
Step 72800: loss = 2.84989
Step 72805: loss = 2.91926
Step 72810: loss = 2.62297
Step 72815: loss = 3.00703
Step 72820: loss = 2.86703
Step 72825: loss = 2.81815
Step 72830: loss = 2.64814
Step 72835: loss = 2.72954
Step 72840: loss = 2.90209
Step 72845: loss = 2.65956
Step 72850: loss = 2.95107
Step 72855: loss = 2.69032
Step 72860: loss = 2.91452
Step 72865: loss = 2.82753
Step 72870: loss = 2.94083
Step 72875: loss = 3.03251
Step 72880: loss = 2.98713
Step 72885: loss = 2.86286
Step 72890: loss = 2.94398
Step 72895: loss = 2.94300
Step 72900: loss = 2.90861
Step 72905: loss = 2.82631
Step 72910: loss = 2.73302
Step 72915: loss = 2.89145
Step 72920: loss = 2.68604
Step 72925: loss = 2.64140
Step 72930: loss = 2.61562
Training Data Eval:
  Num examples: 49920, Num correct: 6662, Precision @ 1: 0.1335
('Testing Data Eval: EPOCH->', 188)
  Num examples: 9984, Num correct: 1394, Precision @ 1: 0.1396
Step 72935: loss = 2.71737
Step 72940: loss = 2.78187
Step 72945: loss = 2.78788
Step 72950: loss = 2.70260
Step 72955: loss = 2.93748
Step 72960: loss = 2.70594
Step 72965: loss = 2.74885
Step 72970: loss = 2.93276
Step 72975: loss = 2.76978
Step 72980: loss = 2.64880
Step 72985: loss = 3.01368
Step 72990: loss = 2.75229
Step 72995: loss = 2.80274
Step 73000: loss = 2.82129
Step 73005: loss = 2.85943
Step 73010: loss = 2.90936
Step 73015: loss = 3.03049
Step 73020: loss = 2.81739
Step 73025: loss = 2.48204
Step 73030: loss = 2.73733
Step 73035: loss = 2.84013
Step 73040: loss = 2.89881
Step 73045: loss = 2.69299
Step 73050: loss = 2.91274
Step 73055: loss = 2.94971
Step 73060: loss = 2.88241
Step 73065: loss = 2.74737
Step 73070: loss = 2.70067
Step 73075: loss = 2.71437
Step 73080: loss = 2.57367
Step 73085: loss = 2.84692
Step 73090: loss = 2.87445
Step 73095: loss = 2.59711
Step 73100: loss = 3.03792
Step 73105: loss = 2.93221
Step 73110: loss = 2.83405
Step 73115: loss = 2.87259
Step 73120: loss = 3.04789
Step 73125: loss = 2.73300
Step 73130: loss = 2.71938
Step 73135: loss = 2.78910
Step 73140: loss = 2.89733
Step 73145: loss = 2.80664
Step 73150: loss = 2.92929
Step 73155: loss = 2.84848
Step 73160: loss = 2.76713
Step 73165: loss = 2.83153
Step 73170: loss = 2.70268
Step 73175: loss = 2.77589
Step 73180: loss = 2.84328
Step 73185: loss = 2.88527
Step 73190: loss = 2.68958
Step 73195: loss = 2.55424
Step 73200: loss = 2.86395
Step 73205: loss = 2.92495
Step 73210: loss = 2.87842
Step 73215: loss = 3.13265
Step 73220: loss = 2.75044
Step 73225: loss = 2.66562
Step 73230: loss = 2.78062
Step 73235: loss = 3.02186
Step 73240: loss = 2.76181
Step 73245: loss = 2.58107
Step 73250: loss = 2.80341
Step 73255: loss = 2.68794
Step 73260: loss = 2.61658
Step 73265: loss = 2.86087
Step 73270: loss = 2.76188
Step 73275: loss = 2.96643
Step 73280: loss = 2.82191
Step 73285: loss = 2.83447
Step 73290: loss = 2.86742
Step 73295: loss = 2.83240
Step 73300: loss = 2.62011
Step 73305: loss = 2.90457
Step 73310: loss = 2.67604
Step 73315: loss = 3.11718
Step 73320: loss = 2.73455
Training Data Eval:
  Num examples: 49920, Num correct: 6928, Precision @ 1: 0.1388
('Testing Data Eval: EPOCH->', 189)
  Num examples: 9984, Num correct: 1375, Precision @ 1: 0.1377
Step 73325: loss = 2.86261
Step 73330: loss = 2.93194
Step 73335: loss = 2.76987
Step 73340: loss = 2.91434
Step 73345: loss = 2.80670
Step 73350: loss = 2.76409
Step 73355: loss = 2.87042
Step 73360: loss = 3.10788
Step 73365: loss = 3.02360
Step 73370: loss = 2.87027
Step 73375: loss = 3.12120
Step 73380: loss = 2.93692
Step 73385: loss = 2.68472
Step 73390: loss = 2.65905
Step 73395: loss = 2.69779
Step 73400: loss = 2.85648
Step 73405: loss = 2.79688
Step 73410: loss = 2.74382
Step 73415: loss = 3.00746
Step 73420: loss = 2.85198
Step 73425: loss = 2.60630
Step 73430: loss = 2.83511
Step 73435: loss = 3.01898
Step 73440: loss = 2.77324
Step 73445: loss = 2.80473
Step 73450: loss = 2.71854
Step 73455: loss = 2.66057
Step 73460: loss = 2.90250
Step 73465: loss = 2.73824
Step 73470: loss = 2.78903
Step 73475: loss = 2.81500
Step 73480: loss = 2.91332
Step 73485: loss = 2.76383
Step 73490: loss = 2.76955
Step 73495: loss = 2.88200
Step 73500: loss = 2.87664
Step 73505: loss = 3.05015
Step 73510: loss = 2.80564
Step 73515: loss = 2.80592
Step 73520: loss = 3.15230
Step 73525: loss = 2.93115
Step 73530: loss = 2.83984
Step 73535: loss = 2.97414
Step 73540: loss = 2.82538
Step 73545: loss = 2.76816
Step 73550: loss = 2.94345
Step 73555: loss = 2.79033
Step 73560: loss = 2.83957
Step 73565: loss = 2.86321
Step 73570: loss = 2.70159
Step 73575: loss = 2.98397
Step 73580: loss = 2.78082
Step 73585: loss = 2.79811
Step 73590: loss = 2.99713
Step 73595: loss = 2.66128
Step 73600: loss = 2.71253
Step 73605: loss = 2.64090
Step 73610: loss = 2.78623
Step 73615: loss = 2.70701
Step 73620: loss = 2.66197
Step 73625: loss = 2.76531
Step 73630: loss = 2.84173
Step 73635: loss = 2.57060
Step 73640: loss = 2.56361
Step 73645: loss = 2.72891
Step 73650: loss = 2.98291
Step 73655: loss = 2.85968
Step 73660: loss = 2.83972
Step 73665: loss = 2.73580
Step 73670: loss = 2.52873
Step 73675: loss = 3.00016
Step 73680: loss = 2.78228
Step 73685: loss = 2.85255
Step 73690: loss = 2.90508
Step 73695: loss = 2.75853
Step 73700: loss = 2.88911
Step 73705: loss = 2.62155
Step 73710: loss = 2.93868
Training Data Eval:
  Num examples: 49920, Num correct: 6854, Precision @ 1: 0.1373
('Testing Data Eval: EPOCH->', 190)
  Num examples: 9984, Num correct: 1371, Precision @ 1: 0.1373
Step 73715: loss = 2.76647
Step 73720: loss = 2.64979
Step 73725: loss = 2.92279
Step 73730: loss = 2.91456
Step 73735: loss = 2.80961
Step 73740: loss = 2.77368
Step 73745: loss = 2.90650
Step 73750: loss = 2.80817
Step 73755: loss = 2.95425
Step 73760: loss = 2.67013
Step 73765: loss = 2.96929
Step 73770: loss = 2.67561
Step 73775: loss = 2.71595
Step 73780: loss = 2.73622
Step 73785: loss = 2.92359
Step 73790: loss = 2.85700
Step 73795: loss = 2.80621
Step 73800: loss = 2.83344
Step 73805: loss = 2.87735
Step 73810: loss = 2.86425
Step 73815: loss = 2.73650
Step 73820: loss = 2.91350
Step 73825: loss = 2.81224
Step 73830: loss = 2.61164
Step 73835: loss = 2.98853
Step 73840: loss = 2.70822
Step 73845: loss = 2.76273
Step 73850: loss = 2.79061
Step 73855: loss = 2.71112
Step 73860: loss = 2.63561
Step 73865: loss = 2.83311
Step 73870: loss = 2.79486
Step 73875: loss = 2.77462
Step 73880: loss = 2.70581
Step 73885: loss = 2.97215
Step 73890: loss = 2.63374
Step 73895: loss = 2.52959
Step 73900: loss = 2.81532
Step 73905: loss = 2.69950
Step 73910: loss = 2.76093
Step 73915: loss = 2.70696
Step 73920: loss = 2.82581
Step 73925: loss = 2.79505
Step 73930: loss = 2.75727
Step 73935: loss = 2.81033
Step 73940: loss = 2.59685
Step 73945: loss = 2.96041
Step 73950: loss = 2.87723
Step 73955: loss = 3.03323
Step 73960: loss = 2.92307
Step 73965: loss = 2.98509
Step 73970: loss = 2.93401
Step 73975: loss = 2.81920
Step 73980: loss = 2.71881
Step 73985: loss = 2.77569
Step 73990: loss = 3.11405
Step 73995: loss = 3.01946
Step 74000: loss = 3.11267
Step 74005: loss = 2.89004
Step 74010: loss = 2.66274
Step 74015: loss = 2.64995
Step 74020: loss = 2.86852
Step 74025: loss = 2.79878
Step 74030: loss = 2.88550
Step 74035: loss = 2.72674
Step 74040: loss = 2.59687
Step 74045: loss = 2.67345
Step 74050: loss = 2.68364
Step 74055: loss = 2.75132
Step 74060: loss = 2.82990
Step 74065: loss = 2.91487
Step 74070: loss = 2.95053
Step 74075: loss = 2.78079
Step 74080: loss = 2.84203
Step 74085: loss = 2.93191
Step 74090: loss = 2.88935
Step 74095: loss = 2.73238
Step 74100: loss = 2.61541
Training Data Eval:
  Num examples: 49920, Num correct: 6913, Precision @ 1: 0.1385
('Testing Data Eval: EPOCH->', 191)
  Num examples: 9984, Num correct: 1366, Precision @ 1: 0.1368
Step 74105: loss = 2.79216
Step 74110: loss = 3.12904
Step 74115: loss = 2.97353
Step 74120: loss = 2.96656
Step 74125: loss = 2.98663
Step 74130: loss = 2.79938
Step 74135: loss = 2.93499
Step 74140: loss = 2.87056
Step 74145: loss = 2.85319
Step 74150: loss = 2.73516
Step 74155: loss = 2.99031
Step 74160: loss = 2.58530
Step 74165: loss = 2.80572
Step 74170: loss = 2.89854
Step 74175: loss = 2.75695
Step 74180: loss = 2.99270
Step 74185: loss = 2.96315
Step 74190: loss = 2.94143
Step 74195: loss = 2.84348
Step 74200: loss = 3.01054
Step 74205: loss = 2.80319
Step 74210: loss = 3.02773
Step 74215: loss = 2.99125
Step 74220: loss = 2.76087
Step 74225: loss = 2.82068
Step 74230: loss = 2.81957
Step 74235: loss = 2.91350
Step 74240: loss = 2.81069
Step 74245: loss = 2.78981
Step 74250: loss = 2.91931
Step 74255: loss = 2.89989
Step 74260: loss = 2.84531
Step 74265: loss = 2.66088
Step 74270: loss = 2.80479
Step 74275: loss = 2.67826
Step 74280: loss = 2.61000
Step 74285: loss = 2.60281
Step 74290: loss = 2.71887
Step 74295: loss = 2.80182
Step 74300: loss = 2.60900
Step 74305: loss = 2.84159
Step 74310: loss = 2.66529
Step 74315: loss = 2.81051
Step 74320: loss = 2.92221
Step 74325: loss = 2.69336
Step 74330: loss = 2.69684
Step 74335: loss = 2.69445
Step 74340: loss = 2.90774
Step 74345: loss = 2.82677
Step 74350: loss = 2.93727
Step 74355: loss = 2.81544
Step 74360: loss = 2.91812
Step 74365: loss = 2.67994
Step 74370: loss = 2.79964
Step 74375: loss = 2.87698
Step 74380: loss = 2.91376
Step 74385: loss = 3.00621
Step 74390: loss = 2.85752
Step 74395: loss = 2.78943
Step 74400: loss = 2.74600
Step 74405: loss = 2.97898
Step 74410: loss = 3.07328
Step 74415: loss = 2.68162
Step 74420: loss = 2.66225
Step 74425: loss = 2.77779
Step 74430: loss = 3.04123
Step 74435: loss = 3.01603
Step 74440: loss = 2.87201
Step 74445: loss = 2.61185
Step 74450: loss = 2.61526
Step 74455: loss = 2.87157
Step 74460: loss = 2.96881
Step 74465: loss = 2.95206
Step 74470: loss = 2.94436
Step 74475: loss = 2.87938
Step 74480: loss = 2.92171
Step 74485: loss = 2.77550
Step 74490: loss = 2.90479
Training Data Eval:
  Num examples: 49920, Num correct: 6713, Precision @ 1: 0.1345
('Testing Data Eval: EPOCH->', 192)
  Num examples: 9984, Num correct: 1391, Precision @ 1: 0.1393
Step 74495: loss = 2.98316
Step 74500: loss = 3.13189
Step 74505: loss = 2.65770
Step 74510: loss = 2.80683
Step 74515: loss = 3.07341
Step 74520: loss = 2.84938
Step 74525: loss = 2.87276
Step 74530: loss = 2.77019
Step 74535: loss = 2.72013
Step 74540: loss = 2.64744
Step 74545: loss = 2.81834
Step 74550: loss = 2.81920
Step 74555: loss = 2.78851
Step 74560: loss = 2.78290
Step 74565: loss = 2.93767
Step 74570: loss = 3.05225
Step 74575: loss = 2.93614
Step 74580: loss = 2.95032
Step 74585: loss = 2.74664
Step 74590: loss = 2.91608
Step 74595: loss = 2.71192
Step 74600: loss = 2.90498
Step 74605: loss = 2.62292
Step 74610: loss = 2.93457
Step 74615: loss = 2.82185
Step 74620: loss = 2.89510
Step 74625: loss = 2.93744
Step 74630: loss = 2.79941
Step 74635: loss = 3.16056
Step 74640: loss = 3.01223
Step 74645: loss = 2.83918
Step 74650: loss = 2.99665
Step 74655: loss = 2.86793
Step 74660: loss = 2.87275
Step 74665: loss = 2.71716
Step 74670: loss = 2.70189
Step 74675: loss = 2.84688
Step 74680: loss = 2.78512
Step 74685: loss = 3.03379
Step 74690: loss = 2.76217
Step 74695: loss = 2.87324
Step 74700: loss = 2.81123
Step 74705: loss = 2.90028
Step 74710: loss = 3.08613
Step 74715: loss = 2.64252
Step 74720: loss = 2.76946
Step 74725: loss = 2.63416
Step 74730: loss = 2.91709
Step 74735: loss = 2.69390
Step 74740: loss = 2.70515
Step 74745: loss = 2.71241
Step 74750: loss = 2.92404
Step 74755: loss = 2.84543
Step 74760: loss = 2.81737
Step 74765: loss = 2.86892
Step 74770: loss = 2.91117
Step 74775: loss = 2.80331
Step 74780: loss = 2.79390
Step 74785: loss = 2.85041
Step 74790: loss = 2.88238
Step 74795: loss = 2.85486
Step 74800: loss = 2.92179
Step 74805: loss = 2.76027
Step 74810: loss = 3.05357
Step 74815: loss = 2.78194
Step 74820: loss = 2.71637
Step 74825: loss = 2.71396
Step 74830: loss = 2.55384
Step 74835: loss = 2.71678
Step 74840: loss = 2.97655
Step 74845: loss = 2.66071
Step 74850: loss = 2.74703
Step 74855: loss = 2.60119
Step 74860: loss = 2.75586
Step 74865: loss = 2.91597
Step 74870: loss = 2.76634
Step 74875: loss = 2.74406
Step 74880: loss = 2.98590
Training Data Eval:
  Num examples: 49920, Num correct: 6860, Precision @ 1: 0.1374
('Testing Data Eval: EPOCH->', 193)
  Num examples: 9984, Num correct: 1359, Precision @ 1: 0.1361
Step 74885: loss = 2.78029
Step 74890: loss = 2.83907
Step 74895: loss = 2.73562
Step 74900: loss = 2.93745
Step 74905: loss = 2.87719
Step 74910: loss = 2.69883
Step 74915: loss = 2.66339
Step 74920: loss = 2.86403
Step 74925: loss = 3.00289
Step 74930: loss = 2.76871
Step 74935: loss = 2.92661
Step 74940: loss = 2.94613
Step 74945: loss = 3.21474
Step 74950: loss = 2.67255
Step 74955: loss = 2.90879
Step 74960: loss = 2.77743
Step 74965: loss = 2.78846
Step 74970: loss = 2.71193
Step 74975: loss = 2.79726
Step 74980: loss = 2.97053
Step 74985: loss = 3.03092
Step 74990: loss = 2.83778
Step 74995: loss = 2.94495
Step 75000: loss = 2.92012
Step 75005: loss = 2.66456
Step 75010: loss = 2.82651
Step 75015: loss = 3.02121
Step 75020: loss = 2.85214
Step 75025: loss = 2.80865
Step 75030: loss = 2.87685
Step 75035: loss = 2.77443
Step 75040: loss = 2.85032
Step 75045: loss = 2.94274
Step 75050: loss = 2.81816
Step 75055: loss = 2.80496
Step 75060: loss = 2.84154
Step 75065: loss = 3.03934
Step 75070: loss = 2.88918
Step 75075: loss = 2.84172
Step 75080: loss = 2.89565
Step 75085: loss = 2.91905
Step 75090: loss = 2.69982
Step 75095: loss = 2.98177
Step 75100: loss = 2.84020
Step 75105: loss = 2.84631
Step 75110: loss = 2.79618
Step 75115: loss = 3.03902
Step 75120: loss = 2.84019
Step 75125: loss = 2.92434
Step 75130: loss = 2.57638
Step 75135: loss = 2.89140
Step 75140: loss = 2.79202
Step 75145: loss = 2.83488
Step 75150: loss = 2.76524
Step 75155: loss = 3.12570
Step 75160: loss = 2.89887
Step 75165: loss = 2.87633
Step 75170: loss = 2.77953
Step 75175: loss = 2.89260
Step 75180: loss = 2.89137
Step 75185: loss = 3.06264
Step 75190: loss = 2.96920
Step 75195: loss = 2.84367
Step 75200: loss = 2.81248
Step 75205: loss = 2.81515
Step 75210: loss = 2.65422
Step 75215: loss = 2.58604
Step 75220: loss = 2.73462
Step 75225: loss = 2.58170
Step 75230: loss = 2.89758
Step 75235: loss = 2.87113
Step 75240: loss = 2.79394
Step 75245: loss = 2.85419
Step 75250: loss = 3.04407
Step 75255: loss = 2.63039
Step 75260: loss = 2.98599
Step 75265: loss = 2.88559
Step 75270: loss = 2.92584
Training Data Eval:
  Num examples: 49920, Num correct: 6731, Precision @ 1: 0.1348
('Testing Data Eval: EPOCH->', 194)
  Num examples: 9984, Num correct: 1407, Precision @ 1: 0.1409
Step 75275: loss = 2.56426
Step 75280: loss = 2.77619
Step 75285: loss = 2.92090
Step 75290: loss = 2.79656
Step 75295: loss = 2.74682
Step 75300: loss = 2.98756
Step 75305: loss = 2.88808
Step 75310: loss = 2.75792
Step 75315: loss = 2.54251
Step 75320: loss = 2.75985
Step 75325: loss = 2.91960
Step 75330: loss = 2.99960
Step 75335: loss = 2.76449
Step 75340: loss = 2.97885
Step 75345: loss = 2.90135
Step 75350: loss = 2.79653
Step 75355: loss = 2.91430
Step 75360: loss = 2.72459
Step 75365: loss = 2.86601
Step 75370: loss = 2.93675
Step 75375: loss = 2.97265
Step 75380: loss = 2.72558
Step 75385: loss = 2.92042
Step 75390: loss = 2.79731
Step 75395: loss = 2.74807
Step 75400: loss = 2.88698
Step 75405: loss = 2.82451
Step 75410: loss = 2.89168
Step 75415: loss = 2.86069
Step 75420: loss = 2.98792
Step 75425: loss = 2.75291
Step 75430: loss = 2.73903
Step 75435: loss = 2.58108
Step 75440: loss = 2.64257
Step 75445: loss = 2.93876
Step 75450: loss = 2.79325
Step 75455: loss = 2.88481
Step 75460: loss = 2.91013
Step 75465: loss = 2.85478
Step 75470: loss = 2.84041
Step 75475: loss = 2.78916
Step 75480: loss = 2.91200
Step 75485: loss = 2.87449
Step 75490: loss = 2.89962
Step 75495: loss = 2.87673
Step 75500: loss = 2.84436
Step 75505: loss = 2.75776
Step 75510: loss = 2.98334
Step 75515: loss = 2.95459
Step 75520: loss = 2.99352
Step 75525: loss = 2.73528
Step 75530: loss = 2.85858
Step 75535: loss = 2.71749
Step 75540: loss = 2.88795
Step 75545: loss = 2.67319
Step 75550: loss = 2.72482
Step 75555: loss = 2.75381
Step 75560: loss = 2.90542
Step 75565: loss = 2.85952
Step 75570: loss = 2.84752
Step 75575: loss = 2.76754
Step 75580: loss = 2.70856
Step 75585: loss = 2.77942
Step 75590: loss = 2.83888
Step 75595: loss = 2.80672
Step 75600: loss = 2.70990
Step 75605: loss = 2.90533
Step 75610: loss = 2.81732
Step 75615: loss = 2.87923
Step 75620: loss = 2.66210
Step 75625: loss = 2.60304
Step 75630: loss = 2.78549
Step 75635: loss = 2.82701
Step 75640: loss = 2.95727
Step 75645: loss = 2.77489
Step 75650: loss = 2.87591
Step 75655: loss = 2.73269
Step 75660: loss = 2.80552
Training Data Eval:
  Num examples: 49920, Num correct: 6882, Precision @ 1: 0.1379
('Testing Data Eval: EPOCH->', 195)
  Num examples: 9984, Num correct: 1437, Precision @ 1: 0.1439
Step 75665: loss = 2.76243
Step 75670: loss = 2.94759
Step 75675: loss = 2.87213
Step 75680: loss = 2.80431
Step 75685: loss = 2.58248
Step 75690: loss = 2.80364
Step 75695: loss = 2.80790
Step 75700: loss = 2.75947
Step 75705: loss = 3.56830
Step 75710: loss = 2.73933
Step 75715: loss = 2.86729
Step 75720: loss = 3.05974
Step 75725: loss = 2.77056
Step 75730: loss = 2.82261
Step 75735: loss = 2.82319
Step 75740: loss = 3.06983
Step 75745: loss = 2.99450
Step 75750: loss = 2.81930
Step 75755: loss = 3.07260
Step 75760: loss = 2.96637
Step 75765: loss = 2.90369
Step 75770: loss = 2.88531
Step 75775: loss = 2.93146
Step 75780: loss = 2.94252
Step 75785: loss = 2.88911
Step 75790: loss = 2.92800
Step 75795: loss = 2.89519
Step 75800: loss = 2.75970
Step 75805: loss = 2.85626
Step 75810: loss = 2.80247
Step 75815: loss = 2.74977
Step 75820: loss = 2.60668
Step 75825: loss = 2.73659
Step 75830: loss = 2.62315
Step 75835: loss = 2.67814
Step 75840: loss = 2.77843
Step 75845: loss = 2.70744
Step 75850: loss = 2.92424
Step 75855: loss = 2.74095
Step 75860: loss = 2.95662
Step 75865: loss = 2.89617
Step 75870: loss = 2.76388
Step 75875: loss = 2.86839
Step 75880: loss = 2.83573
Step 75885: loss = 3.01039
Step 75890: loss = 2.79431
Step 75895: loss = 2.73899
Step 75900: loss = 2.81114
Step 75905: loss = 2.80334
Step 75910: loss = 2.84141
Step 75915: loss = 2.75691
Step 75920: loss = 2.77971
Step 75925: loss = 2.56885
Step 75930: loss = 3.03696
Step 75935: loss = 2.93539
Step 75940: loss = 3.00232
Step 75945: loss = 2.97319
Step 75950: loss = 2.71453
Step 75955: loss = 2.85324
Step 75960: loss = 2.90344
Step 75965: loss = 3.01062
Step 75970: loss = 2.70359
Step 75975: loss = 2.82405
Step 75980: loss = 2.96864
Step 75985: loss = 2.96228
Step 75990: loss = 3.00913
Step 75995: loss = 2.71830
Step 76000: loss = 2.84026
Step 76005: loss = 2.72412
Step 76010: loss = 2.81700
Step 76015: loss = 2.69299
Step 76020: loss = 2.74932
Step 76025: loss = 2.88051
Step 76030: loss = 2.90845
Step 76035: loss = 2.67740
Step 76040: loss = 2.89659
Step 76045: loss = 2.81608
Step 76050: loss = 3.06442
Training Data Eval:
  Num examples: 49920, Num correct: 6817, Precision @ 1: 0.1366
('Testing Data Eval: EPOCH->', 196)
  Num examples: 9984, Num correct: 1387, Precision @ 1: 0.1389
Step 76055: loss = 2.85691
Step 76060: loss = 2.68020
Step 76065: loss = 2.77184
Step 76070: loss = 2.84414
Step 76075: loss = 3.13088
Step 76080: loss = 2.80823
Step 76085: loss = 2.93809
Step 76090: loss = 2.94704
Step 76095: loss = 2.83014
Step 76100: loss = 2.98262
Step 76105: loss = 2.73116
Step 76110: loss = 2.76821
Step 76115: loss = 2.75647
Step 76120: loss = 3.13316
Step 76125: loss = 2.92631
Step 76130: loss = 2.93169
Step 76135: loss = 2.85046
Step 76140: loss = 2.63692
Step 76145: loss = 2.55620
Step 76150: loss = 2.99748
Step 76155: loss = 2.80992
Step 76160: loss = 3.11840
Step 76165: loss = 3.13452
Step 76170: loss = 2.71165
Step 76175: loss = 2.69274
Step 76180: loss = 2.76945
Step 76185: loss = 3.06657
Step 76190: loss = 2.83103
Step 76195: loss = 2.86292
Step 76200: loss = 2.82316
Step 76205: loss = 2.94175
Step 76210: loss = 2.89323
Step 76215: loss = 2.74174
Step 76220: loss = 2.74409
Step 76225: loss = 2.68442
Step 76230: loss = 2.72621
Step 76235: loss = 2.92469
Step 76240: loss = 2.86420
Step 76245: loss = 2.94399
Step 76250: loss = 2.93045
Step 76255: loss = 2.69470
Step 76260: loss = 2.80536
Step 76265: loss = 2.83575
Step 76270: loss = 2.93894
Step 76275: loss = 2.84734
Step 76280: loss = 2.78842
Step 76285: loss = 2.74422
Step 76290: loss = 2.72568
Step 76295: loss = 3.29245
Step 76300: loss = 2.86761
Step 76305: loss = 2.70141
Step 76310: loss = 2.94970
Step 76315: loss = 2.82078
Step 76320: loss = 3.04766
Step 76325: loss = 2.74842
Step 76330: loss = 2.95477
Step 76335: loss = 2.74765
Step 76340: loss = 2.88373
Step 76345: loss = 2.55178
Step 76350: loss = 2.67588
Step 76355: loss = 2.68236
Step 76360: loss = 2.70913
Step 76365: loss = 2.69469
Step 76370: loss = 2.90617
Step 76375: loss = 3.10000
Step 76380: loss = 2.93577
Step 76385: loss = 2.87554
Step 76390: loss = 2.64853
Step 76395: loss = 2.75136
Step 76400: loss = 2.76587
Step 76405: loss = 2.94020
Step 76410: loss = 2.75140
Step 76415: loss = 3.01711
Step 76420: loss = 2.97854
Step 76425: loss = 2.78097
Step 76430: loss = 2.76399
Step 76435: loss = 2.63852
Step 76440: loss = 3.03035
Training Data Eval:
  Num examples: 49920, Num correct: 6750, Precision @ 1: 0.1352
('Testing Data Eval: EPOCH->', 197)
  Num examples: 9984, Num correct: 1389, Precision @ 1: 0.1391
Step 76445: loss = 3.12542
Step 76450: loss = 2.81029
Step 76455: loss = 3.01554
Step 76460: loss = 3.05308
Step 76465: loss = 2.98991
Step 76470: loss = 3.13450
Step 76475: loss = 2.96277
Step 76480: loss = 2.97132
Step 76485: loss = 2.79074
Step 76490: loss = 3.01664
Step 76495: loss = 2.81261
Step 76500: loss = 2.73233
Step 76505: loss = 2.91756
Step 76510: loss = 2.90233
Step 76515: loss = 2.75842
Step 76520: loss = 2.91395
Step 76525: loss = 2.76093
Step 76530: loss = 2.84421
Step 76535: loss = 2.72360
Step 76540: loss = 2.68492
Step 76545: loss = 2.75119
Step 76550: loss = 2.83141
Step 76555: loss = 2.81016
Step 76560: loss = 2.85112
Step 76565: loss = 2.85215
Step 76570: loss = 2.70332
Step 76575: loss = 2.73932
Step 76580: loss = 2.75663
Step 76585: loss = 2.96032
Step 76590: loss = 2.70126
Step 76595: loss = 3.01577
Step 76600: loss = 2.90978
Step 76605: loss = 2.98519
Step 76610: loss = 2.66518
Step 76615: loss = 2.93974
Step 76620: loss = 3.04200
Step 76625: loss = 2.98791
Step 76630: loss = 2.87969
Step 76635: loss = 2.76854
Step 76640: loss = 2.90634
Step 76645: loss = 2.91059
Step 76650: loss = 2.76194
Step 76655: loss = 3.03977
Step 76660: loss = 2.98541
Step 76665: loss = 2.62893
Step 76670: loss = 2.81082
Step 76675: loss = 2.91502
Step 76680: loss = 3.13594
Step 76685: loss = 2.99074
Step 76690: loss = 2.93579
Step 76695: loss = 2.87319
Step 76700: loss = 2.78081
Step 76705: loss = 2.89830
Step 76710: loss = 2.88557
Step 76715: loss = 3.15676
Step 76720: loss = 2.80653
Step 76725: loss = 2.71593
Step 76730: loss = 2.89018
Step 76735: loss = 2.77817
Step 76740: loss = 3.10650
Step 76745: loss = 2.99227
Step 76750: loss = 2.98906
Step 76755: loss = 2.85990
Step 76760: loss = 3.03002
Step 76765: loss = 2.77862
Step 76770: loss = 2.80407
Step 76775: loss = 2.69545
Step 76780: loss = 2.84990
Step 76785: loss = 2.94187
Step 76790: loss = 2.91840
Step 76795: loss = 2.71096
Step 76800: loss = 2.79107
Step 76805: loss = 2.86487
Step 76810: loss = 2.93301
Step 76815: loss = 2.72610
Step 76820: loss = 2.99357
Step 76825: loss = 2.85837
Step 76830: loss = 3.00430
Training Data Eval:
  Num examples: 49920, Num correct: 7053, Precision @ 1: 0.1413
('Testing Data Eval: EPOCH->', 198)
  Num examples: 9984, Num correct: 1457, Precision @ 1: 0.1459
Step 76835: loss = 2.75334
Step 76840: loss = 2.87483
Step 76845: loss = 3.00898
Step 76850: loss = 2.66097
Step 76855: loss = 3.13836
Step 76860: loss = 2.79200
Step 76865: loss = 2.83438
Step 76870: loss = 3.02947
Step 76875: loss = 2.85187
Step 76880: loss = 2.89363
Step 76885: loss = 2.74761
Step 76890: loss = 2.73895
Step 76895: loss = 2.87605
Step 76900: loss = 2.85457
Step 76905: loss = 2.90845
Step 76910: loss = 2.80388
Step 76915: loss = 2.78472
Step 76920: loss = 2.80703
Step 76925: loss = 2.90565
Step 76930: loss = 2.62622
Step 76935: loss = 2.91209
Step 76940: loss = 2.96499
Step 76945: loss = 3.02332
Step 76950: loss = 2.59537
Step 76955: loss = 2.77090
Step 76960: loss = 2.90546
Step 76965: loss = 2.78636
Step 76970: loss = 2.59661
Step 76975: loss = 2.79025
Step 76980: loss = 2.84539
Step 76985: loss = 2.76535
Step 76990: loss = 3.00711
Step 76995: loss = 3.03570
Step 77000: loss = 2.92400
Step 77005: loss = 2.94484
Step 77010: loss = 2.81122
Step 77015: loss = 2.64266
Step 77020: loss = 2.88708
Step 77025: loss = 3.09607
Step 77030: loss = 2.81587
Step 77035: loss = 2.95014
Step 77040: loss = 2.86731
Step 77045: loss = 3.02574
Step 77050: loss = 2.65146
Step 77055: loss = 2.96697
Step 77060: loss = 2.87317
Step 77065: loss = 2.80393
Step 77070: loss = 2.65353
Step 77075: loss = 2.66381
Step 77080: loss = 2.59356
Step 77085: loss = 2.88432
Step 77090: loss = 3.05595
Step 77095: loss = 2.68654
Step 77100: loss = 2.92737
Step 77105: loss = 2.78896
Step 77110: loss = 2.93280
Step 77115: loss = 3.08535
Step 77120: loss = 2.61210
Step 77125: loss = 2.87904
Step 77130: loss = 2.63746
Step 77135: loss = 2.85085
Step 77140: loss = 2.92155
Step 77145: loss = 2.85249
Step 77150: loss = 2.92499
Step 77155: loss = 2.70020
Step 77160: loss = 2.94712
Step 77165: loss = 2.77623
Step 77170: loss = 2.93078
Step 77175: loss = 2.90358
Step 77180: loss = 2.94625
Step 77185: loss = 2.80340
Step 77190: loss = 2.84676
Step 77195: loss = 2.84752
Step 77200: loss = 2.85120
Step 77205: loss = 2.73568
Step 77210: loss = 2.87943
Step 77215: loss = 2.62639
Step 77220: loss = 2.92631
Training Data Eval:
  Num examples: 49920, Num correct: 6856, Precision @ 1: 0.1373
('Testing Data Eval: EPOCH->', 199)
  Num examples: 9984, Num correct: 1409, Precision @ 1: 0.1411
Step 77225: loss = 2.78474
Step 77230: loss = 2.88677
Step 77235: loss = 2.81436
Step 77240: loss = 2.89159
Step 77245: loss = 2.77836
Step 77250: loss = 2.86732
Step 77255: loss = 2.82654
Step 77260: loss = 3.02173
Step 77265: loss = 2.83955
Step 77270: loss = 2.67061
Step 77275: loss = 2.85465
Step 77280: loss = 3.05066
Step 77285: loss = 2.90021
Step 77290: loss = 2.78861
Step 77295: loss = 2.78008
Step 77300: loss = 2.88475
Step 77305: loss = 2.72894
Step 77310: loss = 2.97082
Step 77315: loss = 3.11104
Step 77320: loss = 2.87890
Step 77325: loss = 2.89493
Step 77330: loss = 2.87345
Step 77335: loss = 2.74947
Step 77340: loss = 2.95757
Step 77345: loss = 2.98036
Step 77350: loss = 3.06832
Step 77355: loss = 2.81952
Step 77360: loss = 2.70558
Step 77365: loss = 2.87873
Step 77370: loss = 2.68979
Step 77375: loss = 2.77219
Step 77380: loss = 2.80988
Step 77385: loss = 2.80814
Step 77390: loss = 2.76500
Step 77395: loss = 2.80172
Step 77400: loss = 3.00247
Step 77405: loss = 2.86038
Step 77410: loss = 2.98080
Step 77415: loss = 2.72209
Step 77420: loss = 2.81388
Step 77425: loss = 3.01247
Step 77430: loss = 2.88356
Step 77435: loss = 2.71041
Step 77440: loss = 2.74234
Step 77445: loss = 3.16032
Step 77450: loss = 2.80300
Step 77455: loss = 2.65181
Step 77460: loss = 2.68021
Step 77465: loss = 2.66356
Step 77470: loss = 2.92984
Step 77475: loss = 2.74096
Step 77480: loss = 2.75811
Step 77485: loss = 2.69354
Step 77490: loss = 2.99255
Step 77495: loss = 2.86245
Step 77500: loss = 2.70365
Step 77505: loss = 2.78240
Step 77510: loss = 2.77357
Step 77515: loss = 2.74935
Step 77520: loss = 2.85133
Step 77525: loss = 2.87878
Step 77530: loss = 2.74382
Step 77535: loss = 2.71319
Step 77540: loss = 2.67847
Step 77545: loss = 3.05667
Step 77550: loss = 2.89601
Step 77555: loss = 2.93341
Step 77560: loss = 2.79383
Step 77565: loss = 2.79206
Step 77570: loss = 2.84444
Step 77575: loss = 2.84391
Step 77580: loss = 3.10586
Step 77585: loss = 2.79180
Step 77590: loss = 2.86215
Step 77595: loss = 2.73710
Step 77600: loss = 2.91739
Step 77605: loss = 2.86162
Step 77610: loss = 2.87059
Training Data Eval:
  Num examples: 49920, Num correct: 6946, Precision @ 1: 0.1391
('Testing Data Eval: EPOCH->', 200)
  Num examples: 9984, Num correct: 1480, Precision @ 1: 0.1482
Step 77615: loss = 2.79365
Step 77620: loss = 2.83817
Step 77625: loss = 2.53696
Step 77630: loss = 2.75749
Step 77635: loss = 2.96179
Step 77640: loss = 3.01488
Step 77645: loss = 3.03972
Step 77650: loss = 2.79020
Step 77655: loss = 2.82148
Step 77660: loss = 2.96283
Step 77665: loss = 3.06939
Step 77670: loss = 2.94692
Step 77675: loss = 2.80189
Step 77680: loss = 2.87315
Step 77685: loss = 2.86330
Step 77690: loss = 2.96764
Step 77695: loss = 2.86714
Step 77700: loss = 2.83276
Step 77705: loss = 2.91540
Step 77710: loss = 2.87164
Step 77715: loss = 2.98064
Step 77720: loss = 2.91654
Step 77725: loss = 2.72160
Step 77730: loss = 2.68956
Step 77735: loss = 2.80891
Step 77740: loss = 2.80673
Step 77745: loss = 2.66722
Step 77750: loss = 2.84270
Step 77755: loss = 2.99559
Step 77760: loss = 2.76048
Step 77765: loss = 2.84916
Step 77770: loss = 2.88488
Step 77775: loss = 2.78872
Step 77780: loss = 2.76824
Step 77785: loss = 2.82260
Step 77790: loss = 2.81375
Step 77795: loss = 2.73357
Step 77800: loss = 2.79494
Step 77805: loss = 2.73008
Step 77810: loss = 2.80391
Step 77815: loss = 3.00784
Step 77820: loss = 3.24077
Step 77825: loss = 2.87534
Step 77830: loss = 2.68697
Step 77835: loss = 2.71001
Step 77840: loss = 2.85169
Step 77845: loss = 2.73706
Step 77850: loss = 2.75509
Step 77855: loss = 2.57778
Step 77860: loss = 2.74403
Step 77865: loss = 2.73089
Step 77870: loss = 2.82441
Step 77875: loss = 2.84488
Step 77880: loss = 2.94111
Step 77885: loss = 2.87159
Step 77890: loss = 2.79531
Step 77895: loss = 2.85895
Step 77900: loss = 2.83244
Step 77905: loss = 2.65383
Step 77910: loss = 2.68283
Step 77915: loss = 2.76265
Step 77920: loss = 2.89849
Step 77925: loss = 2.95755
Step 77930: loss = 2.65520
Step 77935: loss = 3.00931
Step 77940: loss = 2.68851
Step 77945: loss = 2.93094
Step 77950: loss = 2.86108
Step 77955: loss = 2.73758
Step 77960: loss = 2.91905
Step 77965: loss = 2.96739
Step 77970: loss = 3.02118
Step 77975: loss = 2.82948
Step 77980: loss = 3.10041
Step 77985: loss = 2.76704
Step 77990: loss = 3.04759
Step 77995: loss = 2.80592
Step 78000: loss = 3.02144
Training Data Eval:
  Num examples: 49920, Num correct: 6960, Precision @ 1: 0.1394
('Testing Data Eval: EPOCH->', 201)
  Num examples: 9984, Num correct: 1358, Precision @ 1: 0.1360
Step 78005: loss = 2.79751
Step 78010: loss = 2.95783
Step 78015: loss = 2.76589
Step 78020: loss = 3.16621
Step 78025: loss = 3.00754
Step 78030: loss = 2.68691
Step 78035: loss = 2.55190
Step 78040: loss = 2.90249
Step 78045: loss = 2.81827
Step 78050: loss = 2.77752
Step 78055: loss = 2.79849
Step 78060: loss = 2.93441
Step 78065: loss = 2.65636
Step 78070: loss = 2.82891
Step 78075: loss = 2.54791
Step 78080: loss = 3.03450
Step 78085: loss = 3.02996
Step 78090: loss = 2.87151
Step 78095: loss = 2.58278
Step 78100: loss = 2.84485
Step 78105: loss = 2.77306
Step 78110: loss = 2.92408
Step 78115: loss = 3.18620
Step 78120: loss = 3.02877
Step 78125: loss = 2.90428
Step 78130: loss = 2.79479
Step 78135: loss = 2.90501
Step 78140: loss = 2.96342
Step 78145: loss = 2.67952
Step 78150: loss = 3.13065
Step 78155: loss = 3.13927
Step 78160: loss = 2.75905
Step 78165: loss = 2.97612
Step 78170: loss = 2.89027
Step 78175: loss = 2.73466
Step 78180: loss = 2.90426
Step 78185: loss = 2.61206
Step 78190: loss = 2.83668
Step 78195: loss = 2.50479
Step 78200: loss = 2.68305
Step 78205: loss = 3.02301
Step 78210: loss = 2.71043
Step 78215: loss = 2.82168
Step 78220: loss = 2.65749
Step 78225: loss = 2.71653
Step 78230: loss = 2.90344
Step 78235: loss = 2.63140
Step 78240: loss = 2.82063
Step 78245: loss = 2.81613
Step 78250: loss = 2.98804
Step 78255: loss = 2.86181
Step 78260: loss = 2.97053
Step 78265: loss = 2.58899
Step 78270: loss = 2.71081
Step 78275: loss = 2.64803
Step 78280: loss = 2.86494
Step 78285: loss = 2.92798
Step 78290: loss = 2.88379
Step 78295: loss = 3.03260
Step 78300: loss = 2.96872
Step 78305: loss = 2.83830
Step 78310: loss = 2.95703
Step 78315: loss = 2.85450
Step 78320: loss = 2.92880
Step 78325: loss = 2.79779
Step 78330: loss = 2.98931
Step 78335: loss = 3.02414
Step 78340: loss = 2.85850
Step 78345: loss = 2.95196
Step 78350: loss = 2.78570
Step 78355: loss = 2.56876
Step 78360: loss = 2.79845
Step 78365: loss = 2.45205
Step 78370: loss = 3.00910
Step 78375: loss = 2.70970
Step 78380: loss = 2.87561
Step 78385: loss = 2.85673
Step 78390: loss = 3.21841
Training Data Eval:
  Num examples: 49920, Num correct: 6821, Precision @ 1: 0.1366
('Testing Data Eval: EPOCH->', 202)
  Num examples: 9984, Num correct: 1410, Precision @ 1: 0.1412
Step 78395: loss = 2.64342
Step 78400: loss = 2.83828
Step 78405: loss = 2.91357
Step 78410: loss = 2.67528
Step 78415: loss = 2.90782
Step 78420: loss = 2.80784
Step 78425: loss = 2.77966
Step 78430: loss = 2.70896
Step 78435: loss = 2.87952
Step 78440: loss = 2.83523
Step 78445: loss = 2.78368
Step 78450: loss = 2.90608
Step 78455: loss = 2.79710
Step 78460: loss = 2.71551
Step 78465: loss = 2.93368
Step 78470: loss = 2.81337
Step 78475: loss = 2.74912
Step 78480: loss = 2.94616
Step 78485: loss = 2.55377
Step 78490: loss = 2.95594
Step 78495: loss = 2.93241
Step 78500: loss = 2.85206
Step 78505: loss = 2.90071
Step 78510: loss = 2.74355
Step 78515: loss = 2.90126
Step 78520: loss = 2.71566
Step 78525: loss = 2.80453
Step 78530: loss = 2.78312
Step 78535: loss = 2.72234
Step 78540: loss = 2.95223
Step 78545: loss = 2.74807
Step 78550: loss = 2.76008
Step 78555: loss = 3.02005
Step 78560: loss = 2.81583
Step 78565: loss = 2.63130
Step 78570: loss = 2.88944
Step 78575: loss = 3.04182
Step 78580: loss = 3.02587
Step 78585: loss = 2.96653
Step 78590: loss = 2.91030
Step 78595: loss = 2.77138
Step 78600: loss = 2.96890
Step 78605: loss = 2.74215
Step 78610: loss = 2.88063
Step 78615: loss = 2.79461
Step 78620: loss = 2.81381
Step 78625: loss = 2.70967
Step 78630: loss = 2.88744
Step 78635: loss = 2.84954
Step 78640: loss = 2.98071
Step 78645: loss = 2.50656
Step 78650: loss = 2.79433
Step 78655: loss = 2.62568
Step 78660: loss = 2.81088
Step 78665: loss = 2.65195
Step 78670: loss = 2.82760
Step 78675: loss = 2.73137
Step 78680: loss = 3.14814
Step 78685: loss = 2.90755
Step 78690: loss = 2.99424
Step 78695: loss = 2.81758
Step 78700: loss = 2.82565
Step 78705: loss = 2.74865
Step 78710: loss = 2.75231
Step 78715: loss = 2.70634
Step 78720: loss = 2.77659
Step 78725: loss = 2.95090
Step 78730: loss = 3.04236
Step 78735: loss = 2.85365
Step 78740: loss = 2.80461
Step 78745: loss = 3.06618
Step 78750: loss = 2.98345
Step 78755: loss = 3.11316
Step 78760: loss = 2.73320
Step 78765: loss = 2.81794
Step 78770: loss = 2.86979
Step 78775: loss = 2.86046
Step 78780: loss = 3.12709
Training Data Eval:
  Num examples: 49920, Num correct: 6835, Precision @ 1: 0.1369
('Testing Data Eval: EPOCH->', 203)
  Num examples: 9984, Num correct: 1462, Precision @ 1: 0.1464
Step 78785: loss = 2.87087
Step 78790: loss = 2.88016
Step 78795: loss = 2.84360
Step 78800: loss = 2.77696
Step 78805: loss = 2.70973
Step 78810: loss = 2.87783
Step 78815: loss = 3.03862
Step 78820: loss = 2.97570
Step 78825: loss = 2.83453
Step 78830: loss = 3.04887
Step 78835: loss = 2.69892
Step 78840: loss = 2.85005
Step 78845: loss = 2.74408
Step 78850: loss = 2.81640
Step 78855: loss = 2.75917
Step 78860: loss = 2.78951
Step 78865: loss = 2.71513
Step 78870: loss = 2.85525
Step 78875: loss = 3.02192
Step 78880: loss = 3.03143
Step 78885: loss = 2.94135
Step 78890: loss = 2.90987
Step 78895: loss = 2.79456
Step 78900: loss = 2.67882
Step 78905: loss = 2.85627
Step 78910: loss = 2.63918
Step 78915: loss = 2.86726
Step 78920: loss = 2.75700
Step 78925: loss = 2.87100
Step 78930: loss = 2.84711
Step 78935: loss = 2.70003
Step 78940: loss = 3.08207
Step 78945: loss = 2.52635
Step 78950: loss = 2.70355
Step 78955: loss = 2.78628
Step 78960: loss = 2.77891
Step 78965: loss = 3.02683
Step 78970: loss = 2.83824
Step 78975: loss = 2.91137
Step 78980: loss = 3.22580
Step 78985: loss = 2.98659
Step 78990: loss = 2.67398
Step 78995: loss = 2.79328
Step 79000: loss = 2.85462
Step 79005: loss = 2.88419
Step 79010: loss = 3.02597
Step 79015: loss = 2.84194
Step 79020: loss = 2.92070
Step 79025: loss = 2.65989
Step 79030: loss = 2.75615
Step 79035: loss = 2.71079
Step 79040: loss = 2.84406
Step 79045: loss = 2.76218
Step 79050: loss = 2.87135
Step 79055: loss = 2.91482
Step 79060: loss = 2.76613
Step 79065: loss = 3.20497
Step 79070: loss = 2.94211
Step 79075: loss = 2.97140
Step 79080: loss = 3.10734
Step 79085: loss = 2.87580
Step 79090: loss = 2.77204
Step 79095: loss = 2.67561
Step 79100: loss = 2.93767
Step 79105: loss = 2.70659
Step 79110: loss = 2.67191
Step 79115: loss = 2.82551
Step 79120: loss = 2.90147
Step 79125: loss = 2.83839
Step 79130: loss = 2.77894
Step 79135: loss = 2.98304
Step 79140: loss = 2.82129
Step 79145: loss = 2.66141
Step 79150: loss = 2.89939
Step 79155: loss = 2.84202
Step 79160: loss = 2.81550
Step 79165: loss = 2.86143
Step 79170: loss = 2.83277
Training Data Eval:
  Num examples: 49920, Num correct: 6744, Precision @ 1: 0.1351
('Testing Data Eval: EPOCH->', 204)
  Num examples: 9984, Num correct: 1453, Precision @ 1: 0.1455
Step 79175: loss = 2.73252
Step 79180: loss = 2.97112
Step 79185: loss = 3.15562
Step 79190: loss = 2.93360
Step 79195: loss = 2.90015
Step 79200: loss = 2.99025
Step 79205: loss = 3.00946
Step 79210: loss = 2.89329
Step 79215: loss = 3.03611
Step 79220: loss = 3.06057
Step 79225: loss = 2.80166
Step 79230: loss = 2.84896
Step 79235: loss = 2.87363
Step 79240: loss = 3.08187
Step 79245: loss = 2.81233
Step 79250: loss = 2.83074
Step 79255: loss = 2.86644
Step 79260: loss = 2.71245
Step 79265: loss = 2.76409
Step 79270: loss = 3.04093
Step 79275: loss = 2.83427
Step 79280: loss = 2.88971
Step 79285: loss = 2.70579
Step 79290: loss = 2.78590
Step 79295: loss = 3.28653
Step 79300: loss = 2.85143
Step 79305: loss = 2.77691
Step 79310: loss = 2.82870
Step 79315: loss = 2.81735
Step 79320: loss = 2.95935
Step 79325: loss = 3.03302
Step 79330: loss = 3.04486
Step 79335: loss = 2.92227
Step 79340: loss = 2.78489
Step 79345: loss = 2.91390
Step 79350: loss = 2.73524
Step 79355: loss = 3.07310
Step 79360: loss = 2.85580
Step 79365: loss = 2.72870
Step 79370: loss = 2.79772
Step 79375: loss = 2.81573
Step 79380: loss = 2.88607
Step 79385: loss = 2.78995
Step 79390: loss = 2.95974
Step 79395: loss = 2.80892
Step 79400: loss = 2.83170
Step 79405: loss = 2.62854
Step 79410: loss = 2.80489
Step 79415: loss = 2.64074
Step 79420: loss = 2.70016
Step 79425: loss = 3.01155
Step 79430: loss = 2.81565
Step 79435: loss = 2.83596
Step 79440: loss = 2.54333
Step 79445: loss = 2.82732
Step 79450: loss = 2.84116
Step 79455: loss = 2.88952
Step 79460: loss = 2.62767
Step 79465: loss = 2.83418
Step 79470: loss = 2.69422
Step 79475: loss = 2.70621
Step 79480: loss = 2.79461
Step 79485: loss = 2.79791
Step 79490: loss = 2.80313
Step 79495: loss = 2.78354
Step 79500: loss = 2.62576
Step 79505: loss = 2.50353
Step 79510: loss = 2.73093
Step 79515: loss = 2.78740
Step 79520: loss = 2.76006
Step 79525: loss = 2.82482
Step 79530: loss = 2.95580
Step 79535: loss = 2.67419
Step 79540: loss = 2.78433
Step 79545: loss = 2.69851
Step 79550: loss = 2.79494
Step 79555: loss = 2.46754
Step 79560: loss = 2.63178
Training Data Eval:
  Num examples: 49920, Num correct: 6789, Precision @ 1: 0.1360
('Testing Data Eval: EPOCH->', 205)
  Num examples: 9984, Num correct: 1388, Precision @ 1: 0.1390
Step 79565: loss = 2.76942
Step 79570: loss = 3.08203
Step 79575: loss = 2.94116
Step 79580: loss = 2.87530
Step 79585: loss = 2.80193
Step 79590: loss = 2.88899
Step 79595: loss = 2.90483
Step 79600: loss = 2.60402
Step 79605: loss = 2.60565
Step 79610: loss = 3.17191
Step 79615: loss = 2.80471
Step 79620: loss = 2.86647
Step 79625: loss = 2.76918
Step 79630: loss = 2.96049
Step 79635: loss = 2.84569
Step 79640: loss = 2.93297
Step 79645: loss = 2.81197
Step 79650: loss = 2.82535
Step 79655: loss = 2.78924
Step 79660: loss = 2.85399
Step 79665: loss = 2.90607
Step 79670: loss = 3.01387
Step 79675: loss = 3.03221
Step 79680: loss = 3.09432
Step 79685: loss = 2.87368
Step 79690: loss = 2.79081
Step 79695: loss = 2.75938
Step 79700: loss = 2.69641
Step 79705: loss = 2.79589
Step 79710: loss = 2.62040
Step 79715: loss = 2.82445
Step 79720: loss = 3.03333
Step 79725: loss = 2.81171
Step 79730: loss = 2.66407
Step 79735: loss = 3.07116
Step 79740: loss = 2.96795
Step 79745: loss = 2.73723
Step 79750: loss = 2.86851
Step 79755: loss = 2.86237
Step 79760: loss = 2.88034
Step 79765: loss = 3.01363
Step 79770: loss = 2.67528
Step 79775: loss = 2.77618
Step 79780: loss = 2.91255
Step 79785: loss = 2.94200
Step 79790: loss = 2.72348
Step 79795: loss = 2.93481
Step 79800: loss = 2.79236
Step 79805: loss = 2.67836
Step 79810: loss = 2.85743
Step 79815: loss = 2.94801
Step 79820: loss = 2.74238
Step 79825: loss = 3.16638
Step 79830: loss = 2.75746
Step 79835: loss = 2.81653
Step 79840: loss = 2.83389
Step 79845: loss = 2.69327
Step 79850: loss = 2.82390
Step 79855: loss = 2.77781
Step 79860: loss = 2.98642
Step 79865: loss = 2.91053
Step 79870: loss = 2.96034
Step 79875: loss = 2.91334
Step 79880: loss = 2.97344
Step 79885: loss = 2.82075
Step 79890: loss = 2.82431
Step 79895: loss = 2.82229
Step 79900: loss = 2.74958
Step 79905: loss = 2.80198
Step 79910: loss = 2.88471
Step 79915: loss = 2.84412
Step 79920: loss = 2.94678
Step 79925: loss = 3.01423
Step 79930: loss = 2.85284
Step 79935: loss = 3.01837
Step 79940: loss = 2.69331
Step 79945: loss = 3.04117
Step 79950: loss = 2.74858
Training Data Eval:
  Num examples: 49920, Num correct: 6729, Precision @ 1: 0.1348
('Testing Data Eval: EPOCH->', 206)
  Num examples: 9984, Num correct: 1421, Precision @ 1: 0.1423
Step 79955: loss = 2.62400
Step 79960: loss = 2.70721
Step 79965: loss = 2.80509
Step 79970: loss = 2.84777
Step 79975: loss = 2.65057
Step 79980: loss = 3.01303
Step 79985: loss = 2.90344
Step 79990: loss = 2.88740
Step 79995: loss = 3.20836
Step 80000: loss = 2.89787
Step 80005: loss = 2.81576
Step 80010: loss = 2.80231
Step 80015: loss = 2.87655
Step 80020: loss = 2.75042
Step 80025: loss = 2.80381
Step 80030: loss = 2.94056
Step 80035: loss = 2.94036
Step 80040: loss = 3.00930
Step 80045: loss = 2.97885
Step 80050: loss = 2.73446
Step 80055: loss = 2.82705
Step 80060: loss = 2.94443
Step 80065: loss = 2.83759
Step 80070: loss = 2.62299
Step 80075: loss = 2.84402
Step 80080: loss = 2.93230
Step 80085: loss = 2.92711
Step 80090: loss = 2.76011
Step 80095: loss = 3.02467
Step 80100: loss = 2.87110
Step 80105: loss = 2.86588
Step 80110: loss = 2.68963
Step 80115: loss = 2.78572
Step 80120: loss = 2.94619
Step 80125: loss = 2.68841
Step 80130: loss = 2.68463
Step 80135: loss = 2.81222
Step 80140: loss = 2.62804
Step 80145: loss = 2.82670
Step 80150: loss = 2.71993
Step 80155: loss = 2.88471
Step 80160: loss = 2.82458
Step 80165: loss = 2.96833
Step 80170: loss = 2.78447
Step 80175: loss = 2.77064
Step 80180: loss = 3.13698
Step 80185: loss = 2.81056
Step 80190: loss = 3.06355
Step 80195: loss = 2.83553
Step 80200: loss = 2.85766
Step 80205: loss = 3.03033
Step 80210: loss = 2.81685
Step 80215: loss = 2.84488
Step 80220: loss = 2.90814
Step 80225: loss = 2.84211
Step 80230: loss = 2.75310
Step 80235: loss = 2.89272
Step 80240: loss = 2.88521
Step 80245: loss = 2.90567
Step 80250: loss = 2.89720
Step 80255: loss = 2.69144
Step 80260: loss = 3.07878
Step 80265: loss = 2.95593
Step 80270: loss = 2.74399
Step 80275: loss = 2.86262
Step 80280: loss = 3.30375
Step 80285: loss = 2.82054
Step 80290: loss = 2.69966
Step 80295: loss = 2.66337
Step 80300: loss = 2.76355
Step 80305: loss = 2.87977
Step 80310: loss = 2.94181
Step 80315: loss = 2.77218
Step 80320: loss = 3.06650
Step 80325: loss = 2.90596
Step 80330: loss = 2.76885
Step 80335: loss = 2.78243
Step 80340: loss = 2.88614
Training Data Eval:
  Num examples: 49920, Num correct: 6764, Precision @ 1: 0.1355
('Testing Data Eval: EPOCH->', 207)
  Num examples: 9984, Num correct: 1405, Precision @ 1: 0.1407
Step 80345: loss = 2.75693
Step 80350: loss = 3.05078
Step 80355: loss = 3.02819
Step 80360: loss = 2.71753
Step 80365: loss = 2.84890
Step 80370: loss = 2.75777
Step 80375: loss = 2.76857
Step 80380: loss = 2.82831
Step 80385: loss = 2.72067
Step 80390: loss = 2.86290
Step 80395: loss = 3.01810
Step 80400: loss = 2.98527
Step 80405: loss = 2.95918
Step 80410: loss = 2.79287
Step 80415: loss = 2.79492
Step 80420: loss = 2.81678
Step 80425: loss = 2.90400
Step 80430: loss = 2.99586
Step 80435: loss = 2.89716
Step 80440: loss = 2.82055
Step 80445: loss = 2.85532
Step 80450: loss = 2.70174
Step 80455: loss = 2.96500
Step 80460: loss = 2.84322
Step 80465: loss = 2.79786
Step 80470: loss = 2.85349
Step 80475: loss = 2.74970
Step 80480: loss = 2.72666
Step 80485: loss = 2.57192
Step 80490: loss = 2.85927
Step 80495: loss = 2.80763
Step 80500: loss = 2.75637
Step 80505: loss = 2.99741
Step 80510: loss = 2.90351
Step 80515: loss = 3.07007
Step 80520: loss = 2.96249
Step 80525: loss = 2.91792
Step 80530: loss = 2.94382
Step 80535: loss = 2.89774
Step 80540: loss = 2.65701
Step 80545: loss = 2.72833
Step 80550: loss = 2.76041
Step 80555: loss = 2.79120
Step 80560: loss = 2.82193
Step 80565: loss = 2.69537
Step 80570: loss = 2.78765
Step 80575: loss = 2.78462
Step 80580: loss = 2.48812
Step 80585: loss = 2.88086
Step 80590: loss = 2.74521
Step 80595: loss = 2.44240
Step 80600: loss = 2.66985
Step 80605: loss = 3.17786
Step 80610: loss = 2.94966
Step 80615: loss = 3.29332
Step 80620: loss = 2.97006
Step 80625: loss = 2.84587
Step 80630: loss = 2.84522
Step 80635: loss = 2.83290
Step 80640: loss = 2.75848
Step 80645: loss = 2.81131
Step 80650: loss = 2.90029
Step 80655: loss = 2.79676
Step 80660: loss = 2.87673
Step 80665: loss = 2.81323
Step 80670: loss = 2.84039
Step 80675: loss = 2.90914
Step 80680: loss = 2.84299
Step 80685: loss = 3.05302
Step 80690: loss = 2.98532
Step 80695: loss = 2.58206
Step 80700: loss = 3.08666
Step 80705: loss = 2.77046
Step 80710: loss = 2.62382
Step 80715: loss = 2.94428
Step 80720: loss = 2.91794
Step 80725: loss = 2.73534
Step 80730: loss = 2.83920
Training Data Eval:
  Num examples: 49920, Num correct: 6902, Precision @ 1: 0.1383
('Testing Data Eval: EPOCH->', 208)
  Num examples: 9984, Num correct: 1384, Precision @ 1: 0.1386
Step 80735: loss = 2.86833
Step 80740: loss = 3.08945
Step 80745: loss = 3.00582
Step 80750: loss = 2.54870
Step 80755: loss = 3.09848
Step 80760: loss = 3.17747
Step 80765: loss = 2.76364
Step 80770: loss = 2.87773
Step 80775: loss = 2.88027
Step 80780: loss = 2.69067
Step 80785: loss = 2.71061
Step 80790: loss = 3.08381
Step 80795: loss = 2.73219
Step 80800: loss = 2.86550
Step 80805: loss = 2.90503
Step 80810: loss = 2.82912
Step 80815: loss = 2.93516
Step 80820: loss = 2.80829
Step 80825: loss = 2.91041
Step 80830: loss = 2.84111
Step 80835: loss = 2.95659
Step 80840: loss = 3.00794
Step 80845: loss = 3.04762
Step 80850: loss = 2.92239
Step 80855: loss = 3.00466
Step 80860: loss = 2.86298
Step 80865: loss = 2.89469
Step 80870: loss = 2.75583
Step 80875: loss = 3.02929
Step 80880: loss = 3.02314
Step 80885: loss = 2.89748
Step 80890: loss = 2.83742
Step 80895: loss = 2.96184
Step 80900: loss = 2.86244
Step 80905: loss = 3.10764
Step 80910: loss = 2.81534
Step 80915: loss = 2.84519
Step 80920: loss = 2.95160
Step 80925: loss = 2.91264
Step 80930: loss = 2.94500
Step 80935: loss = 2.94245
Step 80940: loss = 2.67149
Step 80945: loss = 2.74906
Step 80950: loss = 2.81003
Step 80955: loss = 2.88271
Step 80960: loss = 2.81100
Step 80965: loss = 3.03806
Step 80970: loss = 2.81534
Step 80975: loss = 2.90798
Step 80980: loss = 2.78956
Step 80985: loss = 2.76548
Step 80990: loss = 2.76434
Step 80995: loss = 2.75954
Step 81000: loss = 2.74330
Step 81005: loss = 2.82889
Step 81010: loss = 2.80446
Step 81015: loss = 3.08187
Step 81020: loss = 2.68997
Step 81025: loss = 2.62253
Step 81030: loss = 2.76790
Step 81035: loss = 2.85355
Step 81040: loss = 2.89357
Step 81045: loss = 2.64852
Step 81050: loss = 2.76443
Step 81055: loss = 2.94561
Step 81060: loss = 2.69315
Step 81065: loss = 2.70944
Step 81070: loss = 2.85681
Step 81075: loss = 2.75151
Step 81080: loss = 3.03260
Step 81085: loss = 2.89441
Step 81090: loss = 2.55503
Step 81095: loss = 2.90862
Step 81100: loss = 2.77602
Step 81105: loss = 2.68007
Step 81110: loss = 2.82864
Step 81115: loss = 2.65633
Step 81120: loss = 2.78963
Training Data Eval:
  Num examples: 49920, Num correct: 6620, Precision @ 1: 0.1326
('Testing Data Eval: EPOCH->', 209)
  Num examples: 9984, Num correct: 1326, Precision @ 1: 0.1328
Step 81125: loss = 3.13502
Step 81130: loss = 2.96298
Step 81135: loss = 2.73824
Step 81140: loss = 2.87395
Step 81145: loss = 2.94038
Step 81150: loss = 2.77055
Step 81155: loss = 2.85563
Step 81160: loss = 3.02478
Step 81165: loss = 3.04149
Step 81170: loss = 2.79881
Step 81175: loss = 2.99065
Step 81180: loss = 2.84107
Step 81185: loss = 2.88498
Step 81190: loss = 2.87560
Step 81195: loss = 2.96038
Step 81200: loss = 2.69100
Step 81205: loss = 2.83443
Step 81210: loss = 2.69285
Step 81215: loss = 2.81778
Step 81220: loss = 2.83153
Step 81225: loss = 2.77247
Step 81230: loss = 2.71996
Step 81235: loss = 2.93178
Step 81240: loss = 3.05318
Step 81245: loss = 2.91433
Step 81250: loss = 2.87988
Step 81255: loss = 2.87306
Step 81260: loss = 2.86386
Step 81265: loss = 2.85566
Step 81270: loss = 2.84286
Step 81275: loss = 2.85078
Step 81280: loss = 3.02447
Step 81285: loss = 3.15299
Step 81290: loss = 3.06320
Step 81295: loss = 2.83450
Step 81300: loss = 2.83988
Step 81305: loss = 2.76521
Step 81310: loss = 2.90351
Step 81315: loss = 2.84813
Step 81320: loss = 2.96446
Step 81325: loss = 2.99274
Step 81330: loss = 2.84225
Step 81335: loss = 2.88959
Step 81340: loss = 2.72989
Step 81345: loss = 2.69871
Step 81350: loss = 3.07153
Step 81355: loss = 2.78673
Step 81360: loss = 2.79057
Step 81365: loss = 2.78785
Step 81370: loss = 2.99250
Step 81375: loss = 2.83307
Step 81380: loss = 2.81015
Step 81385: loss = 2.60702
Step 81390: loss = 2.69263
Step 81395: loss = 3.06327
Step 81400: loss = 2.62114
Step 81405: loss = 2.86960
Step 81410: loss = 2.98012
Step 81415: loss = 2.73391
Step 81420: loss = 2.99088
Step 81425: loss = 2.97623
Step 81430: loss = 2.73307
Step 81435: loss = 2.87521
Step 81440: loss = 3.27247
Step 81445: loss = 2.91040
Step 81450: loss = 2.89261
Step 81455: loss = 2.90112
Step 81460: loss = 2.88091
Step 81465: loss = 2.79507
Step 81470: loss = 2.73029
Step 81475: loss = 3.05688
Step 81480: loss = 2.89704
Step 81485: loss = 2.69924
Step 81490: loss = 2.83034
Step 81495: loss = 2.79434
Step 81500: loss = 2.72324
Step 81505: loss = 2.79025
Step 81510: loss = 2.87652
Training Data Eval:
  Num examples: 49920, Num correct: 6718, Precision @ 1: 0.1346
('Testing Data Eval: EPOCH->', 210)
  Num examples: 9984, Num correct: 1431, Precision @ 1: 0.1433
Step 81515: loss = 2.90127
Step 81520: loss = 2.72853
Step 81525: loss = 3.23631
Step 81530: loss = 2.89040
Step 81535: loss = 2.88058
Step 81540: loss = 2.74734
Step 81545: loss = 2.80077
Step 81550: loss = 2.80395
Step 81555: loss = 2.54131
Step 81560: loss = 2.65072
Step 81565: loss = 2.71985
Step 81570: loss = 2.79989
Step 81575: loss = 2.81343
Step 81580: loss = 2.72656
Step 81585: loss = 2.57512
Step 81590: loss = 3.04978
Step 81595: loss = 3.04939
Step 81600: loss = 2.72606
Step 81605: loss = 2.71850
Step 81610: loss = 2.70404
Step 81615: loss = 3.23106
Step 81620: loss = 2.87833
Step 81625: loss = 2.90815
Step 81630: loss = 2.93842
Step 81635: loss = 2.59973
Step 81640: loss = 2.80790
Step 81645: loss = 2.90881
Step 81650: loss = 2.82536
Step 81655: loss = 2.79902
Step 81660: loss = 2.97974
Step 81665: loss = 2.95375
Step 81670: loss = 3.01824
Step 81675: loss = 2.85081
Step 81680: loss = 2.62646
Step 81685: loss = 2.76715
Step 81690: loss = 2.79099
Step 81695: loss = 2.83596
Step 81700: loss = 2.85468
Step 81705: loss = 2.98492
Step 81710: loss = 2.69309
Step 81715: loss = 2.80860
Step 81720: loss = 2.83665
Step 81725: loss = 2.87464
Step 81730: loss = 2.79678
Step 81735: loss = 2.84529
Step 81740: loss = 2.86827
Step 81745: loss = 2.77010
Step 81750: loss = 2.99747
Step 81755: loss = 2.86850
Step 81760: loss = 2.86694
Step 81765: loss = 3.03621
Step 81770: loss = 2.71600
Step 81775: loss = 2.75210
Step 81780: loss = 2.84353
Step 81785: loss = 2.78012
Step 81790: loss = 2.69122
Step 81795: loss = 2.87049
Step 81800: loss = 2.93644
Step 81805: loss = 2.67183
Step 81810: loss = 2.90054
Step 81815: loss = 2.58207
Step 81820: loss = 3.00288
Step 81825: loss = 3.09438
Step 81830: loss = 2.69052
Step 81835: loss = 2.92749
Step 81840: loss = 2.68923
Step 81845: loss = 2.87009
Step 81850: loss = 2.80652
Step 81855: loss = 2.98645
Step 81860: loss = 2.78212
Step 81865: loss = 2.98899
Step 81870: loss = 2.88570
Step 81875: loss = 2.86688
Step 81880: loss = 2.91460
Step 81885: loss = 2.95582
Step 81890: loss = 2.71261
Step 81895: loss = 3.05858
Step 81900: loss = 3.00158
Training Data Eval:
  Num examples: 49920, Num correct: 6653, Precision @ 1: 0.1333
('Testing Data Eval: EPOCH->', 211)
  Num examples: 9984, Num correct: 1341, Precision @ 1: 0.1343
Step 81905: loss = 2.80682
Step 81910: loss = 2.88841
Step 81915: loss = 2.76667
Step 81920: loss = 2.97689
Step 81925: loss = 2.83694
Step 81930: loss = 2.93832
Step 81935: loss = 2.94630
Step 81940: loss = 2.84545
Step 81945: loss = 2.81596
Step 81950: loss = 2.87561
Step 81955: loss = 2.67462
Step 81960: loss = 3.06815
Step 81965: loss = 2.73738
Step 81970: loss = 2.81275
Step 81975: loss = 2.95337
Step 81980: loss = 2.83931
Step 81985: loss = 2.89550
Step 81990: loss = 2.84949
Step 81995: loss = 2.81421
Step 82000: loss = 2.89935
Step 82005: loss = 2.94295
Step 82010: loss = 3.02960
Step 82015: loss = 2.77840
Step 82020: loss = 3.12868
Step 82025: loss = 2.78841
Step 82030: loss = 2.75438
Step 82035: loss = 2.84500
Step 82040: loss = 2.85773
Step 82045: loss = 3.03891
Step 82050: loss = 3.07384
Step 82055: loss = 3.16442
Step 82060: loss = 2.89149
Step 82065: loss = 2.91567
Step 82070: loss = 3.05341
Step 82075: loss = 2.93234
Step 82080: loss = 3.04465
Step 82085: loss = 2.74220
Step 82090: loss = 3.05281
Step 82095: loss = 2.83269
Step 82100: loss = 2.70925
Step 82105: loss = 2.68367
Step 82110: loss = 2.72735
Step 82115: loss = 2.92180
Step 82120: loss = 3.02303
Step 82125: loss = 2.84722
Step 82130: loss = 3.16927
Step 82135: loss = 2.99727
Step 82140: loss = 2.91371
Step 82145: loss = 2.92842
Step 82150: loss = 2.91456
Step 82155: loss = 2.89075
Step 82160: loss = 2.85926
Step 82165: loss = 2.68897
Step 82170: loss = 2.93156
Step 82175: loss = 2.90392
Step 82180: loss = 2.73192
Step 82185: loss = 2.93098
Step 82190: loss = 2.78158
Step 82195: loss = 2.94352
Step 82200: loss = 3.09233
Step 82205: loss = 3.09083
Step 82210: loss = 3.00053
Step 82215: loss = 2.67853
Step 82220: loss = 2.85448
Step 82225: loss = 2.80019
Step 82230: loss = 2.75747
Step 82235: loss = 3.03559
Step 82240: loss = 2.74886
Step 82245: loss = 2.89772
Step 82250: loss = 2.99965
Step 82255: loss = 2.99133
Step 82260: loss = 2.89684
Step 82265: loss = 2.84487
Step 82270: loss = 2.88494
Step 82275: loss = 2.88088
Step 82280: loss = 2.88314
Step 82285: loss = 3.09489
Step 82290: loss = 3.21356
Training Data Eval:
  Num examples: 49920, Num correct: 6767, Precision @ 1: 0.1356
('Testing Data Eval: EPOCH->', 212)
  Num examples: 9984, Num correct: 1441, Precision @ 1: 0.1443
Step 82295: loss = 2.83384
Step 82300: loss = 2.72908
Step 82305: loss = 2.93100
Step 82310: loss = 2.93554
Step 82315: loss = 2.81844
Step 82320: loss = 2.71623
Step 82325: loss = 3.02391
Step 82330: loss = 2.74339
Step 82335: loss = 2.73745
Step 82340: loss = 2.86519
Step 82345: loss = 2.86672
Step 82350: loss = 2.89210
Step 82355: loss = 2.77344
Step 82360: loss = 2.89268
Step 82365: loss = 2.81949
Step 82370: loss = 2.78515
Step 82375: loss = 2.98144
Step 82380: loss = 2.97038
Step 82385: loss = 2.85683
Step 82390: loss = 2.85327
Step 82395: loss = 2.84628
Step 82400: loss = 2.87097
Step 82405: loss = 2.80381
Step 82410: loss = 2.73103
Step 82415: loss = 2.64753
Step 82420: loss = 3.06801
Step 82425: loss = 3.05403
Step 82430: loss = 2.90611
Step 82435: loss = 2.83415
Step 82440: loss = 2.79681
Step 82445: loss = 2.90250
Step 82450: loss = 2.85454
Step 82455: loss = 2.91113
Step 82460: loss = 2.99370
Step 82465: loss = 2.89289
Step 82470: loss = 2.76020
Step 82475: loss = 2.97672
Step 82480: loss = 2.98497
Step 82485: loss = 3.07268
Step 82490: loss = 3.20017
Step 82495: loss = 2.47695
Step 82500: loss = 2.79768
Step 82505: loss = 2.99892
Step 82510: loss = 2.81368
Step 82515: loss = 2.83952
Step 82520: loss = 2.91498
Step 82525: loss = 2.80476
Step 82530: loss = 2.66855
Step 82535: loss = 2.96937
Step 82540: loss = 2.78025
Step 82545: loss = 3.01199
Step 82550: loss = 2.96341
Step 82555: loss = 2.92613
Step 82560: loss = 2.81476
Step 82565: loss = 2.95955
Step 82570: loss = 2.82066
Step 82575: loss = 2.82714
Step 82580: loss = 2.99988
Step 82585: loss = 2.76650
Step 82590: loss = 3.04286
Step 82595: loss = 2.58590
Step 82600: loss = 3.03022
Step 82605: loss = 3.04701
Step 82610: loss = 3.05463
Step 82615: loss = 2.81524
Step 82620: loss = 3.17538
Step 82625: loss = 2.96608
Step 82630: loss = 2.91023
Step 82635: loss = 2.84751
Step 82640: loss = 3.02171
Step 82645: loss = 2.72035
Step 82650: loss = 2.91845
Step 82655: loss = 2.86500
Step 82660: loss = 2.88235
Step 82665: loss = 2.78836
Step 82670: loss = 2.87271
Step 82675: loss = 2.71887
Step 82680: loss = 2.80585
Training Data Eval:
  Num examples: 49920, Num correct: 6844, Precision @ 1: 0.1371
('Testing Data Eval: EPOCH->', 213)
  Num examples: 9984, Num correct: 1376, Precision @ 1: 0.1378
Step 82685: loss = 2.66837
Step 82690: loss = 2.91807
Step 82695: loss = 2.89589
Step 82700: loss = 2.91724
Step 82705: loss = 2.70445
Step 82710: loss = 2.87722
Step 82715: loss = 2.93865
Step 82720: loss = 2.98622
Step 82725: loss = 2.92756
Step 82730: loss = 2.93870
Step 82735: loss = 2.87587
Step 82740: loss = 2.85826
Step 82745: loss = 2.76308
Step 82750: loss = 2.66689
Step 82755: loss = 3.03317
Step 82760: loss = 3.03476
Step 82765: loss = 3.10917
Step 82770: loss = 3.06206
Step 82775: loss = 2.95395
Step 82780: loss = 2.85592
Step 82785: loss = 2.85523
Step 82790: loss = 2.81640
Step 82795: loss = 2.76959
Step 82800: loss = 2.80814
Step 82805: loss = 2.95143
Step 82810: loss = 2.52924
Step 82815: loss = 2.87180
Step 82820: loss = 3.05122
Step 82825: loss = 2.87381
Step 82830: loss = 3.00667
Step 82835: loss = 2.93492
Step 82840: loss = 2.76208
Step 82845: loss = 2.66862
Step 82850: loss = 2.71627
Step 82855: loss = 2.84480
Step 82860: loss = 2.63627
Step 82865: loss = 2.81038
Step 82870: loss = 2.72898
Step 82875: loss = 2.71897
Step 82880: loss = 3.01480
Step 82885: loss = 2.83960
Step 82890: loss = 2.71826
Step 82895: loss = 2.89578
Step 82900: loss = 2.68995
Step 82905: loss = 2.96170
Step 82910: loss = 2.99509
Step 82915: loss = 2.70417
Step 82920: loss = 2.79685
Step 82925: loss = 3.09180
Step 82930: loss = 2.84449
Step 82935: loss = 2.90017
Step 82940: loss = 2.89321
Step 82945: loss = 3.08108
Step 82950: loss = 2.92974
Step 82955: loss = 2.57544
Step 82960: loss = 2.76897
Step 82965: loss = 2.73803
Step 82970: loss = 2.77413
Step 82975: loss = 2.84625
Step 82980: loss = 2.83094
Step 82985: loss = 2.65350
Step 82990: loss = 3.19054
Step 82995: loss = 2.81182
Step 83000: loss = 2.85674
Step 83005: loss = 3.01106
Step 83010: loss = 2.97882
Step 83015: loss = 2.84194
Step 83020: loss = 3.02311
Step 83025: loss = 2.74672
Step 83030: loss = 2.75781
Step 83035: loss = 3.00535
Step 83040: loss = 2.89059
Step 83045: loss = 2.87147
Step 83050: loss = 2.72563
Step 83055: loss = 2.89783
Step 83060: loss = 2.94795
Step 83065: loss = 2.93473
Step 83070: loss = 2.82878
Training Data Eval:
  Num examples: 49920, Num correct: 6766, Precision @ 1: 0.1355
('Testing Data Eval: EPOCH->', 214)
  Num examples: 9984, Num correct: 1438, Precision @ 1: 0.1440
Step 83075: loss = 2.80920
Step 83080: loss = 2.78512
Step 83085: loss = 2.76456
Step 83090: loss = 2.58301
Step 83095: loss = 2.66130
Step 83100: loss = 2.68878
Step 83105: loss = 2.81576
Step 83110: loss = 3.02441
Step 83115: loss = 3.01576
Step 83120: loss = 2.96936
Step 83125: loss = 3.09904
Step 83130: loss = 2.78672
Step 83135: loss = 3.09944
Step 83140: loss = 2.68077
Step 83145: loss = 2.67149
Step 83150: loss = 2.82888
Step 83155: loss = 2.91326
Step 83160: loss = 2.73385
Step 83165: loss = 2.97732
Step 83170: loss = 2.86786
Step 83175: loss = 2.94858
Step 83180: loss = 2.98489
Step 83185: loss = 2.85979
Step 83190: loss = 2.79084
Step 83195: loss = 2.84797
Step 83200: loss = 2.84920
Step 83205: loss = 2.57136
Step 83210: loss = 2.94838
Step 83215: loss = 2.87875
Step 83220: loss = 2.99299
Step 83225: loss = 2.54812
Step 83230: loss = 2.93750
Step 83235: loss = 2.87645
Step 83240: loss = 2.87340
Step 83245: loss = 2.64027
Step 83250: loss = 2.86899
Step 83255: loss = 2.79995
Step 83260: loss = 3.05991
Step 83265: loss = 2.81180
Step 83270: loss = 2.72627
Step 83275: loss = 2.67728
Step 83280: loss = 2.64647
Step 83285: loss = 2.80774
Step 83290: loss = 2.95280
Step 83295: loss = 2.93541
Step 83300: loss = 2.90352
Step 83305: loss = 2.90068
Step 83310: loss = 2.92753
Step 83315: loss = 3.10101
Step 83320: loss = 3.02151
Step 83325: loss = 2.93644
Step 83330: loss = 2.86405
Step 83335: loss = 2.82093
Step 83340: loss = 2.97625
Step 83345: loss = 2.87994
Step 83350: loss = 2.87175
Step 83355: loss = 2.91623
Step 83360: loss = 2.76500
Step 83365: loss = 2.75947
Step 83370: loss = 2.84691
Step 83375: loss = 2.95198
Step 83380: loss = 2.72471
Step 83385: loss = 3.03740
Step 83390: loss = 2.67735
Step 83395: loss = 2.98300
Step 83400: loss = 2.90359
Step 83405: loss = 2.92893
Step 83410: loss = 2.97471
Step 83415: loss = 2.79364
Step 83420: loss = 2.82299
Step 83425: loss = 2.90860
Step 83430: loss = 2.94398
Step 83435: loss = 2.90050
Step 83440: loss = 3.06396
Step 83445: loss = 2.97861
Step 83450: loss = 2.80389
Step 83455: loss = 2.83851
Step 83460: loss = 2.61998
Training Data Eval:
  Num examples: 49920, Num correct: 6980, Precision @ 1: 0.1398
('Testing Data Eval: EPOCH->', 215)
  Num examples: 9984, Num correct: 1420, Precision @ 1: 0.1422
Step 83465: loss = 2.81866
Step 83470: loss = 2.89372
Step 83475: loss = 3.02180
Step 83480: loss = 2.84331
Step 83485: loss = 2.87334
Step 83490: loss = 2.57876
Step 83495: loss = 2.99891
Step 83500: loss = 2.58131
Step 83505: loss = 2.91862
Step 83510: loss = 2.86930
Step 83515: loss = 2.88142
Step 83520: loss = 3.02933
Step 83525: loss = 2.91715
Step 83530: loss = 3.06615
Step 83535: loss = 3.23575
Step 83540: loss = 2.81088
Step 83545: loss = 3.14766
Step 83550: loss = 2.89313
Step 83555: loss = 3.02124
Step 83560: loss = 2.89610
Step 83565: loss = 2.81472
Step 83570: loss = 2.87817
Step 83575: loss = 2.74274
Step 83580: loss = 2.91895
Step 83585: loss = 2.90867
Step 83590: loss = 2.73034
Step 83595: loss = 2.85664
Step 83600: loss = 2.86009
Step 83605: loss = 2.72024
Step 83610: loss = 2.74858
Step 83615: loss = 2.72277
Step 83620: loss = 2.93939
Step 83625: loss = 3.04267
Step 83630: loss = 2.68606
Step 83635: loss = 2.83131
Step 83640: loss = 2.99564
Step 83645: loss = 2.73012
Step 83650: loss = 2.87026
Step 83655: loss = 2.87327
Step 83660: loss = 3.04888
Step 83665: loss = 2.88686
Step 83670: loss = 2.98968
Step 83675: loss = 2.80616
Step 83680: loss = 2.68287
Step 83685: loss = 2.74355
Step 83690: loss = 2.53608
Step 83695: loss = 3.00593
Step 83700: loss = 2.74468
Step 83705: loss = 2.77821
Step 83710: loss = 2.76204
Step 83715: loss = 2.90019
Step 83720: loss = 2.77804
Step 83725: loss = 2.96410
Step 83730: loss = 2.62554
Step 83735: loss = 2.73931
Step 83740: loss = 2.73336
Step 83745: loss = 2.58397
Step 83750: loss = 2.63319
Step 83755: loss = 2.75447
Step 83760: loss = 3.05446
Step 83765: loss = 2.91437
Step 83770: loss = 2.81721
Step 83775: loss = 2.82635
Step 83780: loss = 2.87227
Step 83785: loss = 3.01622
Step 83790: loss = 3.02523
Step 83795: loss = 2.81697
Step 83800: loss = 2.85626
Step 83805: loss = 2.68543
Step 83810: loss = 2.70592
Step 83815: loss = 2.60416
Step 83820: loss = 2.73047
Step 83825: loss = 2.72294
Step 83830: loss = 2.92211
Step 83835: loss = 3.13528
Step 83840: loss = 2.93085
Step 83845: loss = 2.90783
Step 83850: loss = 2.78621
Training Data Eval:
  Num examples: 49920, Num correct: 6701, Precision @ 1: 0.1342
('Testing Data Eval: EPOCH->', 216)
  Num examples: 9984, Num correct: 1384, Precision @ 1: 0.1386
Step 83855: loss = 2.89741
Step 83860: loss = 2.76420
Step 83865: loss = 2.57015
Step 83870: loss = 2.87067
Step 83875: loss = 2.96072
Step 83880: loss = 2.92821
Step 83885: loss = 3.01990
Step 83890: loss = 2.80401
Step 83895: loss = 2.88625
Step 83900: loss = 2.82703
Step 83905: loss = 2.82123
Step 83910: loss = 2.81105
Step 83915: loss = 2.84174
Step 83920: loss = 2.95635
Step 83925: loss = 2.81843
Step 83930: loss = 2.78343
Step 83935: loss = 3.09078
Step 83940: loss = 3.03111
Step 83945: loss = 2.93979
Step 83950: loss = 2.89982
Step 83955: loss = 2.98060
Step 83960: loss = 3.11442
Step 83965: loss = 2.98350
Step 83970: loss = 2.88319
Step 83975: loss = 2.88914
Step 83980: loss = 2.67361
Step 83985: loss = 2.81819
Step 83990: loss = 2.82989
Step 83995: loss = 2.92141
Step 84000: loss = 2.91628
Step 84005: loss = 2.92683
Step 84010: loss = 3.02946
Step 84015: loss = 2.97535
Step 84020: loss = 2.89580
Step 84025: loss = 3.05728
Step 84030: loss = 2.65859
Step 84035: loss = 2.84982
Step 84040: loss = 2.70613
Step 84045: loss = 3.03619
Step 84050: loss = 2.83101
Step 84055: loss = 2.74346
Step 84060: loss = 2.78515
Step 84065: loss = 3.12101
Step 84070: loss = 2.79856
Step 84075: loss = 2.92331
Step 84080: loss = 2.85865
Step 84085: loss = 3.07944
Step 84090: loss = 2.99282
Step 84095: loss = 2.91728
Step 84100: loss = 2.91648
Step 84105: loss = 2.84540
Step 84110: loss = 2.82706
Step 84115: loss = 3.02229
Step 84120: loss = 2.98902
Step 84125: loss = 2.72311
Step 84130: loss = 2.99714
Step 84135: loss = 2.72848
Step 84140: loss = 2.63997
Step 84145: loss = 2.86509
Step 84150: loss = 2.92937
Step 84155: loss = 2.77922
Step 84160: loss = 2.75989
Step 84165: loss = 2.77558
Step 84170: loss = 3.04979
Step 84175: loss = 2.71720
Step 84180: loss = 2.96518
Step 84185: loss = 2.97549
Step 84190: loss = 2.76244
Step 84195: loss = 3.06729
Step 84200: loss = 2.97309
Step 84205: loss = 2.71688
Step 84210: loss = 2.97657
Step 84215: loss = 2.79637
Step 84220: loss = 2.96377
Step 84225: loss = 2.62568
Step 84230: loss = 2.89416
Step 84235: loss = 2.86106
Step 84240: loss = 3.01926
Training Data Eval:
  Num examples: 49920, Num correct: 6884, Precision @ 1: 0.1379
('Testing Data Eval: EPOCH->', 217)
  Num examples: 9984, Num correct: 1462, Precision @ 1: 0.1464
Step 84245: loss = 2.93631
Step 84250: loss = 2.86284
Step 84255: loss = 2.65241
Step 84260: loss = 2.86801
Step 84265: loss = 2.92086
Step 84270: loss = 3.08459
Step 84275: loss = 3.04094
Step 84280: loss = 2.98639
Step 84285: loss = 2.72374
Step 84290: loss = 2.87407
Step 84295: loss = 2.61396
Step 84300: loss = 2.99643
Step 84305: loss = 2.67719
Step 84310: loss = 3.00059
Step 84315: loss = 2.79008
Step 84320: loss = 2.68629
Step 84325: loss = 2.88865
Step 84330: loss = 2.78231
Step 84335: loss = 2.75539
Step 84340: loss = 2.76664
Step 84345: loss = 2.65399
Step 84350: loss = 2.89276
Step 84355: loss = 3.07931
Step 84360: loss = 2.90861
Step 84365: loss = 2.77425
Step 84370: loss = 2.97206
Step 84375: loss = 2.95433
Step 84380: loss = 2.81305
Step 84385: loss = 2.57149
Step 84390: loss = 2.87053
Step 84395: loss = 2.71791
Step 84400: loss = 2.84628
Step 84405: loss = 2.93571
Step 84410: loss = 3.19580
Step 84415: loss = 2.79664
Step 84420: loss = 2.64529
Step 84425: loss = 2.93298
Step 84430: loss = 2.79938
Step 84435: loss = 2.71805
Step 84440: loss = 2.95205
Step 84445: loss = 2.79677
Step 84450: loss = 3.08532
Step 84455: loss = 2.84634
Step 84460: loss = 2.65420
Step 84465: loss = 2.83507
Step 84470: loss = 2.89796
Step 84475: loss = 2.68186
Step 84480: loss = 2.78924
Step 84485: loss = 2.92754
Step 84490: loss = 2.82752
Step 84495: loss = 2.97273
Step 84500: loss = 2.87073
Step 84505: loss = 2.94282
Step 84510: loss = 2.73425
Step 84515: loss = 2.71572
Step 84520: loss = 3.09519
Step 84525: loss = 2.76712
Step 84530: loss = 2.77872
Step 84535: loss = 2.95774
Step 84540: loss = 3.00111
Step 84545: loss = 2.76388
Step 84550: loss = 2.75544
Step 84555: loss = 2.74497
Step 84560: loss = 2.71920
Step 84565: loss = 2.72635
Step 84570: loss = 2.83411
Step 84575: loss = 3.03257
Step 84580: loss = 2.97144
Step 84585: loss = 3.13815
Step 84590: loss = 2.84930
Step 84595: loss = 2.78489
Step 84600: loss = 2.94858
Step 84605: loss = 2.84773
Step 84610: loss = 2.97253
Step 84615: loss = 2.72157
Step 84620: loss = 3.23902
Step 84625: loss = 2.88949
Step 84630: loss = 2.80619
Training Data Eval:
  Num examples: 49920, Num correct: 6708, Precision @ 1: 0.1344
('Testing Data Eval: EPOCH->', 218)
  Num examples: 9984, Num correct: 1425, Precision @ 1: 0.1427
Step 84635: loss = 2.88734
Step 84640: loss = 2.62717
Step 84645: loss = 2.74548
Step 84650: loss = 2.78977
Step 84655: loss = 2.88480
Step 84660: loss = 2.74358
Step 84665: loss = 2.87243
Step 84670: loss = 2.70043
Step 84675: loss = 2.92392
Step 84680: loss = 2.73898
Step 84685: loss = 2.90863
Step 84690: loss = 2.97577
Step 84695: loss = 2.70075
Step 84700: loss = 2.77759
Step 84705: loss = 2.95338
Step 84710: loss = 3.04022
Step 84715: loss = 2.70304
Step 84720: loss = 2.80575
Step 84725: loss = 2.65930
Step 84730: loss = 2.60776
Step 84735: loss = 2.87220
Step 84740: loss = 2.70452
Step 84745: loss = 2.88817
Step 84750: loss = 2.72006
Step 84755: loss = 2.96688
Step 84760: loss = 2.88573
Step 84765: loss = 2.87500
Step 84770: loss = 3.18956
Step 84775: loss = 2.99768
Step 84780: loss = 3.27211
Step 84785: loss = 2.80532
Step 84790: loss = 2.78864
Step 84795: loss = 2.70131
Step 84800: loss = 2.88122
Step 84805: loss = 2.82881
Step 84810: loss = 2.79038
Step 84815: loss = 2.87678
Step 84820: loss = 2.85459
Step 84825: loss = 2.94788
Step 84830: loss = 3.04486
Step 84835: loss = 2.86797
Step 84840: loss = 2.82270
Step 84845: loss = 2.92127
Step 84850: loss = 2.83620
Step 84855: loss = 2.79690
Step 84860: loss = 2.74660
Step 84865: loss = 2.98882
Step 84870: loss = 2.76505
Step 84875: loss = 2.63120
Step 84880: loss = 2.65958
Step 84885: loss = 3.16057
Step 84890: loss = 2.88040
Step 84895: loss = 2.88184
Step 84900: loss = 2.77196
Step 84905: loss = 2.68902
Step 84910: loss = 2.61943
Step 84915: loss = 2.87523
Step 84920: loss = 2.84100
Step 84925: loss = 2.97266
Step 84930: loss = 2.97069
Step 84935: loss = 3.18720
Step 84940: loss = 2.68603
Step 84945: loss = 2.86552
Step 84950: loss = 2.67083
Step 84955: loss = 2.86447
Step 84960: loss = 2.86352
Step 84965: loss = 2.78460
Step 84970: loss = 2.74495
Step 84975: loss = 2.89653
Step 84980: loss = 2.89840
Step 84985: loss = 2.95733
Step 84990: loss = 3.12036
Step 84995: loss = 3.02618
Step 85000: loss = 3.05058
Step 85005: loss = 2.84790
Step 85010: loss = 2.73978
Step 85015: loss = 2.81683
Step 85020: loss = 2.85827
Training Data Eval:
  Num examples: 49920, Num correct: 6756, Precision @ 1: 0.1353
('Testing Data Eval: EPOCH->', 219)
  Num examples: 9984, Num correct: 1391, Precision @ 1: 0.1393
Step 85025: loss = 2.80673
Step 85030: loss = 2.92753
Step 85035: loss = 2.99511
Step 85040: loss = 2.87932
Step 85045: loss = 2.78215
Step 85050: loss = 3.05874
Step 85055: loss = 2.58810
Step 85060: loss = 2.72332
Step 85065: loss = 2.55385
Step 85070: loss = 3.00908
Step 85075: loss = 2.81254
Step 85080: loss = 2.73541
Step 85085: loss = 2.89100
Step 85090: loss = 2.89181
Step 85095: loss = 3.10320
Step 85100: loss = 2.97059
Step 85105: loss = 2.80856
Step 85110: loss = 2.96736
Step 85115: loss = 2.84103
Step 85120: loss = 3.02591
Step 85125: loss = 3.01129
Step 85130: loss = 2.97867
Step 85135: loss = 2.98264
Step 85140: loss = 3.11684
Step 85145: loss = 2.97976
Step 85150: loss = 2.93842
Step 85155: loss = 2.97859
Step 85160: loss = 2.69621
Step 85165: loss = 2.62059
Step 85170: loss = 2.92554
Step 85175: loss = 3.14803
Step 85180: loss = 2.78480
Step 85185: loss = 2.66599
Step 85190: loss = 2.68219
Step 85195: loss = 2.69696
Step 85200: loss = 3.10024
Step 85205: loss = 2.92569
Step 85210: loss = 2.83528
Step 85215: loss = 2.79166
Step 85220: loss = 2.71968
Step 85225: loss = 2.85286
Step 85230: loss = 2.98406
Step 85235: loss = 2.75654
Step 85240: loss = 2.80173
Step 85245: loss = 2.84866
Step 85250: loss = 2.77748
Step 85255: loss = 2.93510
Step 85260: loss = 2.81034
Step 85265: loss = 2.93317
Step 85270: loss = 2.93739
Step 85275: loss = 2.69030
Step 85280: loss = 2.91026
Step 85285: loss = 2.74558
Step 85290: loss = 3.05041
Step 85295: loss = 2.75613
Step 85300: loss = 2.83138
Step 85305: loss = 2.90551
Step 85310: loss = 2.93839
Step 85315: loss = 2.81513
Step 85320: loss = 2.66719
Step 85325: loss = 2.70014
Step 85330: loss = 2.87820
Step 85335: loss = 2.92867
Step 85340: loss = 2.76180
Step 85345: loss = 3.13320
Step 85350: loss = 2.91787
Step 85355: loss = 2.78113
Step 85360: loss = 2.86587
Step 85365: loss = 2.73869
Step 85370: loss = 2.90698
Step 85375: loss = 2.96997
Step 85380: loss = 2.70512
Step 85385: loss = 2.83442
Step 85390: loss = 3.07277
Step 85395: loss = 2.89229
Step 85400: loss = 2.73720
Step 85405: loss = 2.74937
Step 85410: loss = 3.00470
Training Data Eval:
  Num examples: 49920, Num correct: 6759, Precision @ 1: 0.1354
('Testing Data Eval: EPOCH->', 220)
  Num examples: 9984, Num correct: 1396, Precision @ 1: 0.1398
Step 85415: loss = 2.81148
Step 85420: loss = 2.83317
Step 85425: loss = 2.86523
Step 85430: loss = 2.68396
Step 85435: loss = 3.07874
Step 85440: loss = 3.01611
Step 85445: loss = 2.87229
Step 85450: loss = 2.85143
Step 85455: loss = 2.76709
Step 85460: loss = 2.72547
Step 85465: loss = 2.99547
Step 85470: loss = 2.90787
Step 85475: loss = 2.96045
Step 85480: loss = 2.81224
Step 85485: loss = 2.79962
Step 85490: loss = 2.94003
Step 85495: loss = 2.75409
Step 85500: loss = 2.93674
Step 85505: loss = 2.63335
Step 85510: loss = 3.27232
Step 85515: loss = 2.92058
Step 85520: loss = 2.80899
Step 85525: loss = 2.95572
Step 85530: loss = 2.90963
Step 85535: loss = 2.88778
Step 85540: loss = 2.63416
Step 85545: loss = 2.94666
Step 85550: loss = 3.06790
Step 85555: loss = 2.87616
Step 85560: loss = 3.04784
Step 85565: loss = 2.80988
Step 85570: loss = 2.93292
Step 85575: loss = 3.04555
Step 85580: loss = 2.96217
Step 85585: loss = 3.14755
Step 85590: loss = 2.90137
Step 85595: loss = 3.07573
Step 85600: loss = 2.83189
Step 85605: loss = 2.84472
Step 85610: loss = 2.77618
Step 85615: loss = 2.99650
Step 85620: loss = 2.89708
Step 85625: loss = 2.77728
Step 85630: loss = 2.99319
Step 85635: loss = 2.79066
Step 85640: loss = 2.63345
Step 85645: loss = 3.03897
Step 85650: loss = 2.85780
Step 85655: loss = 2.84559
Step 85660: loss = 2.86672
Step 85665: loss = 2.51453
Step 85670: loss = 2.92168
Step 85675: loss = 2.97177
Step 85680: loss = 2.70371
Step 85685: loss = 2.72586
Step 85690: loss = 2.76170
Step 85695: loss = 2.67031
Step 85700: loss = 2.91147
Step 85705: loss = 2.64838
Step 85710: loss = 2.92646
Step 85715: loss = 2.90908
Step 85720: loss = 2.90969
Step 85725: loss = 2.99238
Step 85730: loss = 2.99703
Step 85735: loss = 2.97518
Step 85740: loss = 2.81532
Step 85745: loss = 2.81981
Step 85750: loss = 3.04245
Step 85755: loss = 2.90273
Step 85760: loss = 2.78433
Step 85765: loss = 2.78191
Step 85770: loss = 2.84957
Step 85775: loss = 3.00300
Step 85780: loss = 2.80644
Step 85785: loss = 2.78881
Step 85790: loss = 2.82635
Step 85795: loss = 2.99844
Step 85800: loss = 2.86396
Training Data Eval:
  Num examples: 49920, Num correct: 6751, Precision @ 1: 0.1352
('Testing Data Eval: EPOCH->', 221)
  Num examples: 9984, Num correct: 1421, Precision @ 1: 0.1423
Step 85805: loss = 3.00790
Step 85810: loss = 2.73633
Step 85815: loss = 3.01462
Step 85820: loss = 2.62913
Step 85825: loss = 3.09323
Step 85830: loss = 2.74235
Step 85835: loss = 2.87128
Step 85840: loss = 2.90601
Step 85845: loss = 2.71639
Step 85850: loss = 2.78216
Step 85855: loss = 2.84916
Step 85860: loss = 3.04531
Step 85865: loss = 2.84820
Step 85870: loss = 2.52108
Step 85875: loss = 2.92563
Step 85880: loss = 2.93831
Step 85885: loss = 2.81960
Step 85890: loss = 2.86313
Step 85895: loss = 2.80399
Step 85900: loss = 2.79006
Step 85905: loss = 2.96013
Step 85910: loss = 3.08597
Step 85915: loss = 2.89444
Step 85920: loss = 2.55625
Step 85925: loss = 2.95870
Step 85930: loss = 2.81308
Step 85935: loss = 2.92408
Step 85940: loss = 2.92004
Step 85945: loss = 2.83406
Step 85950: loss = 2.84535
Step 85955: loss = 2.85065
Step 85960: loss = 2.89398
Step 85965: loss = 2.91823
Step 85970: loss = 2.99391
Step 85975: loss = 2.66205
Step 85980: loss = 3.03793
Step 85985: loss = 2.69206
Step 85990: loss = 2.87797
Step 85995: loss = 2.55499
Step 86000: loss = 2.93065
Step 86005: loss = 2.77909
Step 86010: loss = 2.75092
Step 86015: loss = 2.92615
Step 86020: loss = 2.74865
Step 86025: loss = 2.86231
Step 86030: loss = 3.06772
Step 86035: loss = 2.97518
Step 86040: loss = 2.92472
Step 86045: loss = 2.67490
Step 86050: loss = 2.89489
Step 86055: loss = 2.83106
Step 86060: loss = 2.75882
Step 86065: loss = 2.91132
Step 86070: loss = 3.19964
Step 86075: loss = 3.00950
Step 86080: loss = 2.98706
Step 86085: loss = 2.81862
Step 86090: loss = 2.78442
Step 86095: loss = 2.91155
Step 86100: loss = 2.73836
Step 86105: loss = 2.99043
Step 86110: loss = 3.13065
Step 86115: loss = 2.90429
Step 86120: loss = 2.75293
Step 86125: loss = 2.87147
Step 86130: loss = 3.12536
Step 86135: loss = 2.98004
Step 86140: loss = 2.87358
Step 86145: loss = 3.00552
Step 86150: loss = 2.75671
Step 86155: loss = 2.69920
Step 86160: loss = 2.95848
Step 86165: loss = 2.79452
Step 86170: loss = 2.93855
Step 86175: loss = 2.89591
Step 86180: loss = 2.86419
Step 86185: loss = 2.96678
Step 86190: loss = 2.62880
Training Data Eval:
  Num examples: 49920, Num correct: 6804, Precision @ 1: 0.1363
('Testing Data Eval: EPOCH->', 222)
  Num examples: 9984, Num correct: 1413, Precision @ 1: 0.1415
Step 86195: loss = 2.82556
Step 86200: loss = 2.71547
Step 86205: loss = 2.84361
Step 86210: loss = 2.79593
Step 86215: loss = 2.81629
Step 86220: loss = 2.83109
Step 86225: loss = 2.90659
Step 86230: loss = 2.94745
Step 86235: loss = 2.83899
Step 86240: loss = 3.06838
Step 86245: loss = 2.76367
Step 86250: loss = 3.04309
Step 86255: loss = 2.89353
Step 86260: loss = 2.83898
Step 86265: loss = 2.96994
Step 86270: loss = 2.55430
Step 86275: loss = 2.92287
Step 86280: loss = 2.93514
Step 86285: loss = 3.00776
Step 86290: loss = 2.93312
Step 86295: loss = 2.87631
Step 86300: loss = 2.82975
Step 86305: loss = 2.82331
Step 86310: loss = 2.82344
Step 86315: loss = 2.99206
Step 86320: loss = 2.84904
Step 86325: loss = 2.96237
Step 86330: loss = 2.72421
Step 86335: loss = 2.81062
Step 86340: loss = 2.61365
Step 86345: loss = 2.83756
Step 86350: loss = 2.89781
Step 86355: loss = 2.75084
Step 86360: loss = 2.93303
Step 86365: loss = 2.77972
Step 86370: loss = 2.85463
Step 86375: loss = 2.83458
Step 86380: loss = 3.08313
Step 86385: loss = 2.80435
Step 86390: loss = 3.00247
Step 86395: loss = 2.82588
Step 86400: loss = 2.76992
Step 86405: loss = 2.90961
Step 86410: loss = 2.87490
Step 86415: loss = 3.00230
Step 86420: loss = 2.79803
Step 86425: loss = 2.71034
Step 86430: loss = 3.02380
Step 86435: loss = 2.78808
Step 86440: loss = 2.84420
Step 86445: loss = 2.71978
Step 86450: loss = 2.91433
Step 86455: loss = 2.91349
Step 86460: loss = 2.87976
Step 86465: loss = 2.93732
Step 86470: loss = 2.83591
Step 86475: loss = 3.01799
Step 86480: loss = 2.94000
Step 86485: loss = 2.66492
Step 86490: loss = 2.84369
Step 86495: loss = 2.92506
Step 86500: loss = 2.73383
Step 86505: loss = 2.74453
Step 86510: loss = 2.84010
Step 86515: loss = 2.78149
Step 86520: loss = 2.79061
Step 86525: loss = 2.92603
Step 86530: loss = 2.96652
Step 86535: loss = 2.82577
Step 86540: loss = 2.91173
Step 86545: loss = 2.79542
Step 86550: loss = 3.03598
Step 86555: loss = 2.85130
Step 86560: loss = 3.03809
Step 86565: loss = 2.89565
Step 86570: loss = 2.81489
Step 86575: loss = 2.84553
Step 86580: loss = 2.85788
Training Data Eval:
  Num examples: 49920, Num correct: 6792, Precision @ 1: 0.1361
('Testing Data Eval: EPOCH->', 223)
  Num examples: 9984, Num correct: 1398, Precision @ 1: 0.1400
Step 86585: loss = 2.77194
Step 86590: loss = 2.83800
Step 86595: loss = 2.81583
Step 86600: loss = 2.84660
Step 86605: loss = 2.94001
Step 86610: loss = 2.82967
Step 86615: loss = 2.88358
Step 86620: loss = 2.82686
Step 86625: loss = 2.92782
Step 86630: loss = 2.99246
Step 86635: loss = 2.81106
Step 86640: loss = 2.93195
Step 86645: loss = 2.62460
Step 86650: loss = 2.86684
Step 86655: loss = 3.06493
Step 86660: loss = 2.70939
Step 86665: loss = 2.88602
Step 86670: loss = 2.70682
Step 86675: loss = 2.90792
Step 86680: loss = 2.82137
Step 86685: loss = 2.88635
Step 86690: loss = 2.75416
Step 86695: loss = 3.02196
Step 86700: loss = 2.90596
Step 86705: loss = 3.07865
Step 86710: loss = 3.14815
Step 86715: loss = 3.10333
Step 86720: loss = 2.89494
Step 86725: loss = 2.92845
Step 86730: loss = 2.71790
Step 86735: loss = 2.90976
Step 86740: loss = 3.03247
Step 86745: loss = 2.86135
Step 86750: loss = 2.90002
Step 86755: loss = 2.73604
Step 86760: loss = 3.11020
Step 86765: loss = 2.99325
Step 86770: loss = 3.01551
Step 86775: loss = 2.87078
Step 86780: loss = 2.65177
Step 86785: loss = 2.78350
Step 86790: loss = 2.73543
Step 86795: loss = 2.87998
Step 86800: loss = 2.87133
Step 86805: loss = 2.92546
Step 86810: loss = 2.93000
Step 86815: loss = 2.90182
Step 86820: loss = 2.75993
Step 86825: loss = 3.04420
Step 86830: loss = 2.98543
Step 86835: loss = 2.59936
Step 86840: loss = 2.71765
Step 86845: loss = 2.86870
Step 86850: loss = 2.75877
Step 86855: loss = 2.65401
Step 86860: loss = 2.90472
Step 86865: loss = 2.93746
Step 86870: loss = 2.90571
Step 86875: loss = 2.92315
Step 86880: loss = 2.88067
Step 86885: loss = 2.99563
Step 86890: loss = 2.84944
Step 86895: loss = 3.00067
Step 86900: loss = 2.83360
Step 86905: loss = 2.99793
Step 86910: loss = 2.67336
Step 86915: loss = 2.91326
Step 86920: loss = 2.66561
Step 86925: loss = 2.88452
Step 86930: loss = 3.18649
Step 86935: loss = 2.86813
Step 86940: loss = 2.70250
Step 86945: loss = 2.81506
Step 86950: loss = 2.83391
Step 86955: loss = 2.99780
Step 86960: loss = 2.91880
Step 86965: loss = 2.98771
Step 86970: loss = 2.72823
Training Data Eval:
  Num examples: 49920, Num correct: 6810, Precision @ 1: 0.1364
('Testing Data Eval: EPOCH->', 224)
  Num examples: 9984, Num correct: 1391, Precision @ 1: 0.1393
Step 86975: loss = 2.99199
Step 86980: loss = 2.94769
Step 86985: loss = 2.71654
Step 86990: loss = 2.81021
Step 86995: loss = 2.80593
Step 87000: loss = 2.64855
Step 87005: loss = 2.88465
Step 87010: loss = 3.15080
Step 87015: loss = 3.05739
Step 87020: loss = 2.58464
Step 87025: loss = 2.70735
Step 87030: loss = 2.80129
Step 87035: loss = 2.74523
Step 87040: loss = 2.88765
Step 87045: loss = 3.04894
Step 87050: loss = 3.04045
Step 87055: loss = 2.87040
Step 87060: loss = 2.80228
Step 87065: loss = 2.92064
Step 87070: loss = 2.78711
Step 87075: loss = 2.85406
Step 87080: loss = 2.96263
Step 87085: loss = 2.73033
Step 87090: loss = 2.75730
Step 87095: loss = 2.84114
Step 87100: loss = 2.56982
Step 87105: loss = 2.88526
Step 87110: loss = 2.81418
Step 87115: loss = 2.77837
Step 87120: loss = 2.62172
Step 87125: loss = 2.82094
Step 87130: loss = 2.84447
Step 87135: loss = 2.77415
Step 87140: loss = 3.03902
Step 87145: loss = 3.03320
Step 87150: loss = 2.99046
Step 87155: loss = 2.66930
Step 87160: loss = 2.99207
Step 87165: loss = 2.79467
Step 87170: loss = 3.02981
Step 87175: loss = 2.85239
Step 87180: loss = 2.92207
Step 87185: loss = 2.86311
Step 87190: loss = 2.73136
Step 87195: loss = 2.85110
Step 87200: loss = 2.52266
Step 87205: loss = 2.80692
Step 87210: loss = 2.80825
Step 87215: loss = 2.85299
Step 87220: loss = 2.94791
Step 87225: loss = 2.95308
Step 87230: loss = 2.78007
Step 87235: loss = 2.75833
Step 87240: loss = 2.57448
Step 87245: loss = 2.80072
Step 87250: loss = 3.20474
Step 87255: loss = 2.77250
Step 87260: loss = 2.93450
Step 87265: loss = 2.64725
Step 87270: loss = 2.77264
Step 87275: loss = 2.76247
Step 87280: loss = 2.93007
Step 87285: loss = 2.84444
Step 87290: loss = 2.79409
Step 87295: loss = 3.01772
Step 87300: loss = 2.82183
Step 87305: loss = 2.99231
Step 87310: loss = 2.89437
Step 87315: loss = 2.90394
Step 87320: loss = 2.87856
Step 87325: loss = 2.89476
Step 87330: loss = 3.00481
Step 87335: loss = 2.72887
Step 87340: loss = 2.63196
Step 87345: loss = 2.81943
Step 87350: loss = 2.74717
Step 87355: loss = 2.58327
Step 87360: loss = 2.84149
Training Data Eval:
  Num examples: 49920, Num correct: 6696, Precision @ 1: 0.1341
('Testing Data Eval: EPOCH->', 225)
  Num examples: 9984, Num correct: 1408, Precision @ 1: 0.1410
Step 87365: loss = 3.06690
Step 87370: loss = 2.88779
Step 87375: loss = 2.93465
Step 87380: loss = 2.92283
Step 87385: loss = 2.86448
Step 87390: loss = 2.76692
Step 87395: loss = 2.85603
Step 87400: loss = 2.80441
Step 87405: loss = 2.68968
Step 87410: loss = 2.91264
Step 87415: loss = 3.10690
Step 87420: loss = 3.04154
Step 87425: loss = 2.77093
Step 87430: loss = 2.75879
Step 87435: loss = 2.70559
Step 87440: loss = 2.88878
Step 87445: loss = 3.02897
Step 87450: loss = 2.52599
Step 87455: loss = 3.01730
Step 87460: loss = 3.04122
Step 87465: loss = 2.73621
Step 87470: loss = 2.85657
Step 87475: loss = 3.14192
Step 87480: loss = 2.82691
Step 87485: loss = 2.87417
Step 87490: loss = 2.86855
Step 87495: loss = 2.99083
Step 87500: loss = 2.98886
Step 87505: loss = 2.92654
Step 87510: loss = 2.84408
Step 87515: loss = 2.83056
Step 87520: loss = 2.85852
Step 87525: loss = 2.89756
Step 87530: loss = 2.87865
Step 87535: loss = 2.87593
Step 87540: loss = 2.94644
Step 87545: loss = 2.78786
Step 87550: loss = 3.01358
Step 87555: loss = 2.88471
Step 87560: loss = 2.70560
Step 87565: loss = 2.72704
Step 87570: loss = 2.58976
Step 87575: loss = 2.91800
Step 87580: loss = 3.01871
Step 87585: loss = 2.81874
Step 87590: loss = 3.07251
Step 87595: loss = 2.94257
Step 87600: loss = 2.58861
Step 87605: loss = 2.73513
Step 87610: loss = 2.51577
Step 87615: loss = 2.85536
Step 87620: loss = 2.73746
Step 87625: loss = 2.72480
Step 87630: loss = 2.83809
Step 87635: loss = 2.81249
Step 87640: loss = 2.67562
Step 87645: loss = 2.91452
Step 87650: loss = 2.73409
Step 87655: loss = 2.80347
Step 87660: loss = 2.96102
Step 87665: loss = 2.70172
Step 87670: loss = 2.82395
Step 87675: loss = 2.73066
Step 87680: loss = 2.78455
Step 87685: loss = 2.99776
Step 87690: loss = 2.89483
Step 87695: loss = 2.79039
Step 87700: loss = 2.81354
Step 87705: loss = 2.75104
Step 87710: loss = 3.07063
Step 87715: loss = 2.72713
Step 87720: loss = 3.10465
Step 87725: loss = 2.73619
Step 87730: loss = 2.96889
Step 87735: loss = 2.80159
Step 87740: loss = 2.93847
Step 87745: loss = 2.87947
Step 87750: loss = 2.79777
Training Data Eval:
  Num examples: 49920, Num correct: 6720, Precision @ 1: 0.1346
('Testing Data Eval: EPOCH->', 226)
  Num examples: 9984, Num correct: 1382, Precision @ 1: 0.1384
Step 87755: loss = 2.69159
Step 87760: loss = 3.22440
Step 87765: loss = 2.67986
Step 87770: loss = 2.99043
Step 87775: loss = 3.03437
Step 87780: loss = 2.89739
Step 87785: loss = 2.81798
Step 87790: loss = 2.91474
Step 87795: loss = 2.79833
Step 87800: loss = 2.98386
Step 87805: loss = 3.04664
Step 87810: loss = 3.23923
Step 87815: loss = 3.18859
Step 87820: loss = 2.97478
Step 87825: loss = 2.99963
Step 87830: loss = 3.10588
Step 87835: loss = 3.09453
Step 87840: loss = 3.11878
Step 87845: loss = 2.97889
Step 87850: loss = 2.97455
Step 87855: loss = 2.93403
Step 87860: loss = 2.92202
Step 87865: loss = 2.94413
Step 87870: loss = 3.03230
Step 87875: loss = 2.77219
Step 87880: loss = 2.62974
Step 87885: loss = 2.73617
Step 87890: loss = 2.89593
Step 87895: loss = 3.05601
Step 87900: loss = 2.93639
Step 87905: loss = 2.86766
Step 87910: loss = 2.79741
Step 87915: loss = 2.64386
Step 87920: loss = 2.80066
Step 87925: loss = 2.86508
Step 87930: loss = 2.97631
Step 87935: loss = 2.87629
Step 87940: loss = 2.73635
Step 87945: loss = 2.92042
Step 87950: loss = 2.99592
Step 87955: loss = 2.97446
Step 87960: loss = 2.97243
Step 87965: loss = 2.87519
Step 87970: loss = 2.89572
Step 87975: loss = 2.90111
Step 87980: loss = 2.87013
Step 87985: loss = 2.74892
Step 87990: loss = 3.00953
Step 87995: loss = 2.97731
Step 88000: loss = 2.88932
Step 88005: loss = 2.89102
Step 88010: loss = 2.91685
Step 88015: loss = 3.15565
Step 88020: loss = 2.91761
Step 88025: loss = 2.95550
Step 88030: loss = 2.82216
Step 88035: loss = 2.77755
Step 88040: loss = 2.87805
Step 88045: loss = 2.71030
Step 88050: loss = 2.95158
Step 88055: loss = 2.98317
Step 88060: loss = 2.73071
Step 88065: loss = 2.76683
Step 88070: loss = 2.96116
Step 88075: loss = 2.96305
Step 88080: loss = 2.79705
Step 88085: loss = 2.96216
Step 88090: loss = 2.89929
Step 88095: loss = 3.02349
Step 88100: loss = 2.80673
Step 88105: loss = 2.73380
Step 88110: loss = 2.79974
Step 88115: loss = 2.71782
Step 88120: loss = 2.76080
Step 88125: loss = 2.99099
Step 88130: loss = 3.10222
Step 88135: loss = 2.79004
Step 88140: loss = 2.77102
Training Data Eval:
  Num examples: 49920, Num correct: 6693, Precision @ 1: 0.1341
('Testing Data Eval: EPOCH->', 227)
  Num examples: 9984, Num correct: 1399, Precision @ 1: 0.1401
Step 88145: loss = 2.67406
Step 88150: loss = 2.69624
Step 88155: loss = 2.64974
Step 88160: loss = 2.86438
Step 88165: loss = 3.09632
Step 88170: loss = 2.89438
Step 88175: loss = 2.99038
Step 88180: loss = 2.99368
Step 88185: loss = 2.66334
Step 88190: loss = 2.96031
Step 88195: loss = 2.89802
Step 88200: loss = 2.98489
Step 88205: loss = 2.78931
Step 88210: loss = 2.90669
Step 88215: loss = 2.97992
Step 88220: loss = 2.97317
Step 88225: loss = 2.92962
Step 88230: loss = 2.88264
Step 88235: loss = 2.92133
Step 88240: loss = 2.79207
Step 88245: loss = 2.78429
Step 88250: loss = 2.74750
Step 88255: loss = 2.79478
Step 88260: loss = 2.60383
Step 88265: loss = 2.85271
Step 88270: loss = 2.87410
Step 88275: loss = 2.68730
Step 88280: loss = 2.91420
Step 88285: loss = 2.98283
Step 88290: loss = 2.86645
Step 88295: loss = 2.74992
Step 88300: loss = 2.76295
Step 88305: loss = 2.99370
Step 88310: loss = 2.72084
Step 88315: loss = 2.70588
Step 88320: loss = 2.94015
Step 88325: loss = 2.96315
Step 88330: loss = 2.94058
Step 88335: loss = 2.91994
Step 88340: loss = 2.91762
Step 88345: loss = 3.02580
Step 88350: loss = 2.84054
Step 88355: loss = 2.95529
Step 88360: loss = 2.77352
Step 88365: loss = 3.04715
Step 88370: loss = 2.81353
Step 88375: loss = 2.86869
Step 88380: loss = 3.18188
Step 88385: loss = 3.03312
Step 88390: loss = 2.83721
Step 88395: loss = 2.86568
Step 88400: loss = 2.53665
Step 88405: loss = 2.74777
Step 88410: loss = 2.99188
Step 88415: loss = 2.58840
Step 88420: loss = 2.98554
Step 88425: loss = 2.69542
Step 88430: loss = 2.88933
Step 88435: loss = 3.03917
Step 88440: loss = 2.85790
Step 88445: loss = 2.75095
Step 88450: loss = 2.86922
Step 88455: loss = 2.90586
Step 88460: loss = 2.91008
Step 88465: loss = 2.96927
Step 88470: loss = 2.96145
Step 88475: loss = 2.95897
Step 88480: loss = 2.85795
Step 88485: loss = 2.67948
Step 88490: loss = 2.93656
Step 88495: loss = 3.02585
Step 88500: loss = 3.06961
Step 88505: loss = 2.69946
Step 88510: loss = 2.67288
Step 88515: loss = 2.86590
Step 88520: loss = 2.88615
Step 88525: loss = 3.01175
Step 88530: loss = 2.75924
Training Data Eval:
  Num examples: 49920, Num correct: 6525, Precision @ 1: 0.1307
('Testing Data Eval: EPOCH->', 228)
  Num examples: 9984, Num correct: 1244, Precision @ 1: 0.1246
Step 88535: loss = 2.87993
Step 88540: loss = 3.27532
Step 88545: loss = 2.79545
Step 88550: loss = 2.95331
Step 88555: loss = 2.82197
Step 88560: loss = 3.03980
Step 88565: loss = 2.75697
Step 88570: loss = 2.92661
Step 88575: loss = 3.08396
Step 88580: loss = 2.89968
Step 88585: loss = 2.86139
Step 88590: loss = 2.89224
Step 88595: loss = 2.81222
Step 88600: loss = 2.84129
Step 88605: loss = 2.81432
Step 88610: loss = 2.99300
Step 88615: loss = 3.22186
Step 88620: loss = 2.79578
Step 88625: loss = 2.98656
Step 88630: loss = 2.80895
Step 88635: loss = 2.75306
Step 88640: loss = 2.84718
Step 88645: loss = 2.89166
Step 88650: loss = 2.88857
Step 88655: loss = 3.23724
Step 88660: loss = 2.98310
Step 88665: loss = 2.96628
Step 88670: loss = 2.74845
Step 88675: loss = 2.95778
Step 88680: loss = 2.97525
Step 88685: loss = 2.83650
Step 88690: loss = 2.81248
Step 88695: loss = 2.90167
Step 88700: loss = 3.03537
Step 88705: loss = 2.75005
Step 88710: loss = 2.96180
Step 88715: loss = 2.69730
Step 88720: loss = 2.78724
Step 88725: loss = 2.82308
Step 88730: loss = 2.82996
Step 88735: loss = 2.79154
Step 88740: loss = 2.79379
Step 88745: loss = 2.92967
Step 88750: loss = 2.84278
Step 88755: loss = 2.75841
Step 88760: loss = 3.16493
Step 88765: loss = 2.99402
Step 88770: loss = 2.68518
Step 88775: loss = 3.05203
Step 88780: loss = 2.84921
Step 88785: loss = 3.07864
Step 88790: loss = 2.82818
Step 88795: loss = 2.96349
Step 88800: loss = 2.89878
Step 88805: loss = 2.81533
Step 88810: loss = 2.72465
Step 88815: loss = 2.76060
Step 88820: loss = 3.01462
Step 88825: loss = 2.56183
Step 88830: loss = 2.96091
Step 88835: loss = 2.90137
Step 88840: loss = 3.03453
Step 88845: loss = 3.00207
Step 88850: loss = 2.82513
Step 88855: loss = 2.79736
Step 88860: loss = 2.73571
Step 88865: loss = 2.88594
Step 88870: loss = 2.78006
Step 88875: loss = 2.92491
Step 88880: loss = 2.60701
Step 88885: loss = 2.98921
Step 88890: loss = 2.80812
Step 88895: loss = 2.88296
Step 88900: loss = 2.83281
Step 88905: loss = 2.76439
Step 88910: loss = 2.79961
Step 88915: loss = 2.68258
Step 88920: loss = 2.96620
Training Data Eval:
  Num examples: 49920, Num correct: 6879, Precision @ 1: 0.1378
('Testing Data Eval: EPOCH->', 229)
  Num examples: 9984, Num correct: 1464, Precision @ 1: 0.1466
Step 88925: loss = 2.80666
Step 88930: loss = 2.59376
Step 88935: loss = 2.98611
Step 88940: loss = 2.90976
Step 88945: loss = 2.79924
Step 88950: loss = 3.11225
Step 88955: loss = 2.83955
Step 88960: loss = 3.04732
Step 88965: loss = 2.91534
Step 88970: loss = 3.00696
Step 88975: loss = 2.73449
Step 88980: loss = 3.01557
Step 88985: loss = 2.58500
Step 88990: loss = 3.08688
Step 88995: loss = 2.92260
Step 89000: loss = 2.71904
Step 89005: loss = 3.09462
Step 89010: loss = 2.86876
Step 89015: loss = 2.72422
Step 89020: loss = 2.89435
Step 89025: loss = 2.89870
Step 89030: loss = 2.86174
Step 89035: loss = 2.68609
Step 89040: loss = 3.06284
Step 89045: loss = 2.87489
Step 89050: loss = 3.03154
Step 89055: loss = 2.83739
Step 89060: loss = 2.83918
Step 89065: loss = 2.70056
Step 89070: loss = 2.97823
Step 89075: loss = 2.74543
Step 89080: loss = 2.87531
Step 89085: loss = 2.86901
Step 89090: loss = 2.90115
Step 89095: loss = 2.69241
Step 89100: loss = 2.78671
Step 89105: loss = 2.69769
Step 89110: loss = 2.88864
Step 89115: loss = 2.88861
Step 89120: loss = 2.84175
Step 89125: loss = 2.85702
Step 89130: loss = 2.97995
Step 89135: loss = 2.82199
Step 89140: loss = 2.86478
Step 89145: loss = 2.73227
Step 89150: loss = 2.84916
Step 89155: loss = 2.88353
Step 89160: loss = 2.96393
Step 89165: loss = 2.74302
Step 89170: loss = 2.56002
Step 89175: loss = 3.06166
Step 89180: loss = 2.69819
Step 89185: loss = 3.10677
Step 89190: loss = 2.77058
Step 89195: loss = 2.91000
Step 89200: loss = 2.92180
Step 89205: loss = 3.00180
Step 89210: loss = 2.92808
Step 89215: loss = 2.77432
Step 89220: loss = 2.83988
Step 89225: loss = 2.86320
Step 89230: loss = 2.80553
Step 89235: loss = 2.72476
Step 89240: loss = 2.96608
Step 89245: loss = 2.95895
Step 89250: loss = 2.78004
Step 89255: loss = 2.99448
Step 89260: loss = 2.91020
Step 89265: loss = 2.94651
Step 89270: loss = 2.91762
Step 89275: loss = 3.03416
Step 89280: loss = 2.79637
Step 89285: loss = 2.77848
Step 89290: loss = 2.84787
Step 89295: loss = 2.94224
Step 89300: loss = 2.81316
Step 89305: loss = 2.97683
Step 89310: loss = 2.94081
Training Data Eval:
  Num examples: 49920, Num correct: 6794, Precision @ 1: 0.1361
('Testing Data Eval: EPOCH->', 230)
  Num examples: 9984, Num correct: 1377, Precision @ 1: 0.1379
Step 89315: loss = 2.99131
Step 89320: loss = 3.08035
Step 89325: loss = 2.82566
Step 89330: loss = 2.75462
Step 89335: loss = 2.75640
Step 89340: loss = 2.92920
Step 89345: loss = 2.85690
Step 89350: loss = 2.69514
Step 89355: loss = 2.94658
Step 89360: loss = 2.91227
Step 89365: loss = 3.19945
Step 89370: loss = 2.86274
Step 89375: loss = 2.96404
Step 89380: loss = 2.86298
Step 89385: loss = 3.00237
Step 89390: loss = 2.65770
Step 89395: loss = 2.91114
Step 89400: loss = 2.67672
Step 89405: loss = 3.02590
Step 89410: loss = 2.66908
Step 89415: loss = 2.83747
Step 89420: loss = 2.82974
Step 89425: loss = 3.01742
Step 89430: loss = 2.87364
Step 89435: loss = 2.98371
Step 89440: loss = 3.21840
Step 89445: loss = 2.73783
Step 89450: loss = 3.28490
Step 89455: loss = 2.91032
Step 89460: loss = 3.02911
Step 89465: loss = 2.82862
Step 89470: loss = 3.07914
Step 89475: loss = 2.86475
Step 89480: loss = 2.78581
Step 89485: loss = 2.77643
Step 89490: loss = 3.23737
Step 89495: loss = 2.86370
Step 89500: loss = 2.91956
Step 89505: loss = 2.85368
Step 89510: loss = 2.95984
Step 89515: loss = 2.93529
Step 89520: loss = 2.77441
Step 89525: loss = 2.62544
Step 89530: loss = 3.24235
Step 89535: loss = 2.69084
Step 89540: loss = 2.92885
Step 89545: loss = 2.95624
Step 89550: loss = 2.80054
Step 89555: loss = 3.25045
Step 89560: loss = 3.08931
Step 89565: loss = 2.88535
Step 89570: loss = 2.83807
Step 89575: loss = 2.72527
Step 89580: loss = 3.01225
Step 89585: loss = 2.89415
Step 89590: loss = 2.84939
Step 89595: loss = 2.74462
Step 89600: loss = 2.97264
Step 89605: loss = 2.87172
Step 89610: loss = 3.11189
Step 89615: loss = 3.13352
Step 89620: loss = 2.96341
Step 89625: loss = 2.88731
Step 89630: loss = 3.01385
Step 89635: loss = 2.78860
Step 89640: loss = 2.82120
Step 89645: loss = 2.83129
Step 89650: loss = 2.79885
Step 89655: loss = 2.86735
Step 89660: loss = 2.75087
Step 89665: loss = 2.70676
Step 89670: loss = 2.90721
Step 89675: loss = 2.89903
Step 89680: loss = 2.85226
Step 89685: loss = 3.09259
Step 89690: loss = 3.01568
Step 89695: loss = 2.89482
Step 89700: loss = 2.76952
Training Data Eval:
  Num examples: 49920, Num correct: 6694, Precision @ 1: 0.1341
('Testing Data Eval: EPOCH->', 231)
  Num examples: 9984, Num correct: 1366, Precision @ 1: 0.1368
Step 89705: loss = 2.77626
Step 89710: loss = 3.04154
Step 89715: loss = 2.84132
Step 89720: loss = 2.90736
Step 89725: loss = 2.80833
Step 89730: loss = 2.86400
Step 89735: loss = 2.92433
Step 89740: loss = 2.96634
Step 89745: loss = 3.02929
Step 89750: loss = 2.81604
Step 89755: loss = 2.86616
Step 89760: loss = 2.78440
Step 89765: loss = 2.83941
Step 89770: loss = 2.71286
Step 89775: loss = 2.71104
Step 89780: loss = 2.81466
Step 89785: loss = 2.90530
Step 89790: loss = 2.90903
Step 89795: loss = 3.11013
Step 89800: loss = 2.96339
Step 89805: loss = 2.96314
Step 89810: loss = 2.93101
Step 89815: loss = 3.00338
Step 89820: loss = 2.70296
Step 89825: loss = 2.96540
Step 89830: loss = 2.70250
Step 89835: loss = 3.01814
Step 89840: loss = 3.20869
Step 89845: loss = 2.81579
Step 89850: loss = 2.95864
Step 89855: loss = 2.82702
Step 89860: loss = 2.89663
Step 89865: loss = 2.63526
Step 89870: loss = 2.78023
Step 89875: loss = 2.83940
Step 89880: loss = 2.90299
Step 89885: loss = 3.00503
Step 89890: loss = 2.91324
Step 89895: loss = 2.71507
Step 89900: loss = 2.86017
Step 89905: loss = 2.97520
Step 89910: loss = 2.82740
Step 89915: loss = 2.99310
Step 89920: loss = 2.86813
Step 89925: loss = 2.86230
Step 89930: loss = 2.62698
Step 89935: loss = 2.81785
Step 89940: loss = 2.92502
Step 89945: loss = 2.83233
Step 89950: loss = 3.05561
Step 89955: loss = 3.07638
Step 89960: loss = 2.83679
Step 89965: loss = 2.92962
Step 89970: loss = 2.95833
Step 89975: loss = 2.90908
Step 89980: loss = 2.84301
Step 89985: loss = 2.91218
Step 89990: loss = 3.02866
Step 89995: loss = 2.81488
Step 90000: loss = 2.81682
Step 90005: loss = 2.77605
Step 90010: loss = 2.83883
Step 90015: loss = 2.76603
Step 90020: loss = 2.98078
Step 90025: loss = 2.81315
Step 90030: loss = 2.60590
Step 90035: loss = 2.86279
Step 90040: loss = 2.62877
Step 90045: loss = 2.59275
Step 90050: loss = 2.98153
Step 90055: loss = 2.96028
Step 90060: loss = 3.05037
Step 90065: loss = 3.03107
Step 90070: loss = 2.77875
Step 90075: loss = 2.70133
Step 90080: loss = 2.83324
Step 90085: loss = 2.98807
Step 90090: loss = 2.94643
Training Data Eval:
  Num examples: 49920, Num correct: 6935, Precision @ 1: 0.1389
('Testing Data Eval: EPOCH->', 232)
  Num examples: 9984, Num correct: 1449, Precision @ 1: 0.1451
Step 90095: loss = 2.89051
Step 90100: loss = 2.56104
Step 90105: loss = 2.83997
Step 90110: loss = 2.89055
Step 90115: loss = 2.89446
Step 90120: loss = 3.03001
Step 90125: loss = 3.12633
Step 90130: loss = 2.90883
Step 90135: loss = 3.17194
Step 90140: loss = 2.95645
Step 90145: loss = 2.95199
Step 90150: loss = 3.02671
Step 90155: loss = 2.87798
Step 90160: loss = 2.92555
Step 90165: loss = 2.85021
Step 90170: loss = 2.82473
Step 90175: loss = 2.78503
Step 90180: loss = 2.93039
Step 90185: loss = 3.09233
Step 90190: loss = 2.85314
Step 90195: loss = 2.74154
Step 90200: loss = 2.72103
Step 90205: loss = 3.09246
Step 90210: loss = 3.06812
Step 90215: loss = 2.97924
Step 90220: loss = 2.89524
Step 90225: loss = 3.06170
Step 90230: loss = 3.06673
Step 90235: loss = 2.94179
Step 90240: loss = 2.91633
Step 90245: loss = 2.81866
Step 90250: loss = 3.04937
Step 90255: loss = 2.83682
Step 90260: loss = 3.03911
Step 90265: loss = 2.88104
Step 90270: loss = 2.78300
Step 90275: loss = 2.87768
Step 90280: loss = 2.89833
Step 90285: loss = 3.01967
Step 90290: loss = 3.04476
Step 90295: loss = 2.90700
Step 90300: loss = 2.68134
Step 90305: loss = 2.86440
Step 90310: loss = 2.87551
Step 90315: loss = 2.85994
Step 90320: loss = 2.91208
Step 90325: loss = 3.01393
Step 90330: loss = 2.95753
Step 90335: loss = 2.67921
Step 90340: loss = 2.89839
Step 90345: loss = 3.27563
Step 90350: loss = 2.94010
Step 90355: loss = 2.84348
Step 90360: loss = 2.72614
Step 90365: loss = 2.79642
Step 90370: loss = 2.76880
Step 90375: loss = 2.75346
Step 90380: loss = 2.99497
Step 90385: loss = 2.83170
Step 90390: loss = 2.52287
Step 90395: loss = 3.10585
Step 90400: loss = 2.99468
Step 90405: loss = 2.64331
Step 90410: loss = 2.69698
Step 90415: loss = 2.87625
Step 90420: loss = 2.85609
Step 90425: loss = 2.89181
Step 90430: loss = 2.63674
Step 90435: loss = 3.01563
Step 90440: loss = 2.65818
Step 90445: loss = 2.83328
Step 90450: loss = 2.80294
Step 90455: loss = 2.78733
Step 90460: loss = 2.93138
Step 90465: loss = 2.80885
Step 90470: loss = 2.82238
Step 90475: loss = 2.62390
Step 90480: loss = 2.98659
Training Data Eval:
  Num examples: 49920, Num correct: 6977, Precision @ 1: 0.1398
('Testing Data Eval: EPOCH->', 233)
  Num examples: 9984, Num correct: 1456, Precision @ 1: 0.1458
Step 90485: loss = 2.54159
Step 90490: loss = 2.92013
Step 90495: loss = 2.78576
Step 90500: loss = 2.80811
Step 90505: loss = 2.99993
Step 90510: loss = 2.96788
Step 90515: loss = 2.73845
Step 90520: loss = 2.81740
Step 90525: loss = 2.90537
Step 90530: loss = 2.87842
Step 90535: loss = 2.98221
Step 90540: loss = 2.92422
Step 90545: loss = 2.87920
Step 90550: loss = 3.02128
Step 90555: loss = 3.02022
Step 90560: loss = 2.78088
Step 90565: loss = 3.01237
Step 90570: loss = 2.84823
Step 90575: loss = 2.84388
Step 90580: loss = 2.89855
Step 90585: loss = 3.10605
Step 90590: loss = 3.03110
Step 90595: loss = 2.79230
Step 90600: loss = 2.63527
Step 90605: loss = 2.80325
Step 90610: loss = 3.10028
Step 90615: loss = 2.89918
Step 90620: loss = 2.99871
Step 90625: loss = 2.73761
Step 90630: loss = 3.01853
Step 90635: loss = 3.05169
Step 90640: loss = 2.63624
Step 90645: loss = 2.72330
Step 90650: loss = 2.99339
Step 90655: loss = 2.88717
Step 90660: loss = 2.78400
Step 90665: loss = 2.68982
Step 90670: loss = 2.98980
Step 90675: loss = 2.94905
Step 90680: loss = 2.72832
Step 90685: loss = 2.72973
Step 90690: loss = 2.95217
Step 90695: loss = 2.83585
Step 90700: loss = 2.89366
Step 90705: loss = 2.86749
Step 90710: loss = 2.86482
Step 90715: loss = 2.87567
Step 90720: loss = 2.93449
Step 90725: loss = 2.87320
Step 90730: loss = 2.81158
Step 90735: loss = 2.97696
Step 90740: loss = 3.03619
Step 90745: loss = 3.16119
Step 90750: loss = 2.60461
Step 90755: loss = 2.64986
Step 90760: loss = 2.99106
Step 90765: loss = 3.00544
Step 90770: loss = 2.89891
Step 90775: loss = 3.24151
Step 90780: loss = 3.59622
Step 90785: loss = 2.83040
Step 90790: loss = 3.13462
Step 90795: loss = 2.81669
Step 90800: loss = 2.84872
Step 90805: loss = 2.89366
Step 90810: loss = 2.87100
Step 90815: loss = 2.97123
Step 90820: loss = 2.86118
Step 90825: loss = 2.68719
Step 90830: loss = 2.84279
Step 90835: loss = 2.82626
Step 90840: loss = 3.01253
Step 90845: loss = 2.80000
Step 90850: loss = 3.01325
Step 90855: loss = 2.76517
Step 90860: loss = 2.63127
Step 90865: loss = 2.82152
Step 90870: loss = 2.90132
Training Data Eval:
  Num examples: 49920, Num correct: 6940, Precision @ 1: 0.1390
('Testing Data Eval: EPOCH->', 234)
  Num examples: 9984, Num correct: 1399, Precision @ 1: 0.1401
Step 90875: loss = 2.98178
Step 90880: loss = 2.77963
Step 90885: loss = 2.63913
Step 90890: loss = 3.05230
Step 90895: loss = 2.84667
Step 90900: loss = 2.75174
Step 90905: loss = 2.76500
Step 90910: loss = 3.12678
Step 90915: loss = 2.76297
Step 90920: loss = 2.91363
Step 90925: loss = 2.82157
Step 90930: loss = 2.72130
Step 90935: loss = 2.97340
Step 90940: loss = 3.07974
Step 90945: loss = 2.96480
Step 90950: loss = 2.84561
Step 90955: loss = 2.86562
Step 90960: loss = 2.85745
Step 90965: loss = 2.92722
Step 90970: loss = 2.75359
Step 90975: loss = 2.92954
Step 90980: loss = 2.92252
Step 90985: loss = 2.75800
Step 90990: loss = 2.93898
Step 90995: loss = 2.76280
Step 91000: loss = 2.86528
Step 91005: loss = 2.81788
Step 91010: loss = 2.76910
Step 91015: loss = 2.92491
Step 91020: loss = 2.88733
Step 91025: loss = 2.89091
Step 91030: loss = 2.82181
Step 91035: loss = 2.89400
Step 91040: loss = 3.05260
Step 91045: loss = 2.91402
Step 91050: loss = 3.05720
Step 91055: loss = 2.73602
Step 91060: loss = 2.82588
Step 91065: loss = 2.96687
Step 91070: loss = 2.91303
Step 91075: loss = 2.87596
Step 91080: loss = 3.08951
Step 91085: loss = 2.83204
Step 91090: loss = 3.03652
Step 91095: loss = 2.72355
Step 91100: loss = 2.73775
Step 91105: loss = 2.76443
Step 91110: loss = 2.72320
Step 91115: loss = 2.72254
Step 91120: loss = 2.85833
Step 91125: loss = 2.75719
Step 91130: loss = 2.69423
Step 91135: loss = 2.75371
Step 91140: loss = 2.92392
Step 91145: loss = 2.85619
Step 91150: loss = 2.98920
Step 91155: loss = 2.79408
Step 91160: loss = 3.08591
Step 91165: loss = 2.75139
Step 91170: loss = 2.80905
Step 91175: loss = 2.82909
Step 91180: loss = 3.00073
Step 91185: loss = 2.75213
Step 91190: loss = 3.03068
Step 91195: loss = 2.92747
Step 91200: loss = 3.18517
Step 91205: loss = 3.05766
Step 91210: loss = 2.66791
Step 91215: loss = 2.89003
Step 91220: loss = 3.04149
Step 91225: loss = 3.03563
Step 91230: loss = 2.94441
Step 91235: loss = 2.87905
Step 91240: loss = 2.77472
Step 91245: loss = 2.73099
Step 91250: loss = 2.70933
Step 91255: loss = 2.82177
Step 91260: loss = 2.85783
Training Data Eval:
  Num examples: 49920, Num correct: 6909, Precision @ 1: 0.1384
('Testing Data Eval: EPOCH->', 235)
  Num examples: 9984, Num correct: 1448, Precision @ 1: 0.1450
Step 91265: loss = 2.97052
Step 91270: loss = 2.82985
Step 91275: loss = 2.80375
Step 91280: loss = 2.82209
Step 91285: loss = 2.80785
Step 91290: loss = 3.19198
Step 91295: loss = 2.89940
Step 91300: loss = 3.10282
Step 91305: loss = 2.92809
Step 91310: loss = 2.78187
Step 91315: loss = 2.65316
Step 91320: loss = 2.60057
Step 91325: loss = 2.71680
Step 91330: loss = 2.78771
Step 91335: loss = 2.93249
Step 91340: loss = 2.70496
Step 91345: loss = 2.89099
Step 91350: loss = 3.05303
Step 91355: loss = 2.92147
Step 91360: loss = 2.82614
Step 91365: loss = 2.93791
Step 91370: loss = 2.82392
Step 91375: loss = 2.80374
Step 91380: loss = 2.96524
Step 91385: loss = 3.08505
Step 91390: loss = 3.04908
Step 91395: loss = 2.78176
Step 91400: loss = 2.75797
Step 91405: loss = 2.98988
Step 91410: loss = 2.99841
Step 91415: loss = 2.96951
Step 91420: loss = 2.75316
Step 91425: loss = 2.84527
Step 91430: loss = 2.97889
Step 91435: loss = 2.67661
Step 91440: loss = 3.02512
Step 91445: loss = 2.94310
Step 91450: loss = 3.13358
Step 91455: loss = 2.73452
Step 91460: loss = 3.00458
Step 91465: loss = 2.81859
Step 91470: loss = 3.00609
Step 91475: loss = 2.97787
Step 91480: loss = 3.13839
Step 91485: loss = 2.92735
Step 91490: loss = 2.86064
Step 91495: loss = 2.72623
Step 91500: loss = 2.81206
Step 91505: loss = 2.71673
Step 91510: loss = 2.81792
Step 91515: loss = 2.82888
Step 91520: loss = 3.11428
Step 91525: loss = 3.14748
Step 91530: loss = 2.89408
Step 91535: loss = 2.88457
Step 91540: loss = 2.76191
Step 91545: loss = 2.91270
Step 91550: loss = 2.83397
Step 91555: loss = 3.04998
Step 91560: loss = 2.72308
Step 91565: loss = 2.89460
Step 91570: loss = 2.86812
Step 91575: loss = 2.72016
Step 91580: loss = 2.99118
Step 91585: loss = 2.76863
Step 91590: loss = 3.14727
Step 91595: loss = 2.79962
Step 91600: loss = 2.84100
Step 91605: loss = 2.92515
Step 91610: loss = 2.92511
Step 91615: loss = 2.74723
Step 91620: loss = 2.76852
Step 91625: loss = 2.83237
Step 91630: loss = 2.71233
Step 91635: loss = 2.74337
Step 91640: loss = 2.92922
Step 91645: loss = 2.91267
Step 91650: loss = 2.74587
Training Data Eval:
  Num examples: 49920, Num correct: 6967, Precision @ 1: 0.1396
('Testing Data Eval: EPOCH->', 236)
  Num examples: 9984, Num correct: 1470, Precision @ 1: 0.1472
Step 91655: loss = 2.48240
Step 91660: loss = 2.86797
Step 91665: loss = 2.81332
Step 91670: loss = 2.81222
Step 91675: loss = 2.89882
Step 91680: loss = 2.86683
Step 91685: loss = 2.83310
Step 91690: loss = 2.91925
Step 91695: loss = 2.94516
Step 91700: loss = 2.91118
Step 91705: loss = 2.93010
Step 91710: loss = 3.00004
Step 91715: loss = 2.93618
Step 91720: loss = 2.97810
Step 91725: loss = 2.99949
Step 91730: loss = 2.89779
Step 91735: loss = 2.91455
Step 91740: loss = 2.78763
Step 91745: loss = 2.83192
Step 91750: loss = 3.10182
Step 91755: loss = 2.80035
Step 91760: loss = 3.01845
Step 91765: loss = 3.11820
Step 91770: loss = 3.29529
Step 91775: loss = 3.02143
Step 91780: loss = 2.85290
Step 91785: loss = 2.85506
Step 91790: loss = 2.80822
Step 91795: loss = 2.84546
Step 91800: loss = 3.03611
Step 91805: loss = 2.85794
Step 91810: loss = 2.72597
Step 91815: loss = 2.99650
Step 91820: loss = 2.89818
Step 91825: loss = 2.90400
Step 91830: loss = 2.83000
Step 91835: loss = 2.89800
Step 91840: loss = 2.80192
Step 91845: loss = 3.09554
Step 91850: loss = 2.74839
Step 91855: loss = 2.93858
Step 91860: loss = 2.69931
Step 91865: loss = 3.04041
Step 91870: loss = 2.63763
Step 91875: loss = 2.90580
Step 91880: loss = 3.01014
Step 91885: loss = 2.93094
Step 91890: loss = 2.87658
Step 91895: loss = 3.03102
Step 91900: loss = 2.83409
Step 91905: loss = 2.66957
Step 91910: loss = 2.93354
Step 91915: loss = 2.76708
Step 91920: loss = 2.94937
Step 91925: loss = 2.74908
Step 91930: loss = 2.71083
Step 91935: loss = 2.88159
Step 91940: loss = 2.75691
Step 91945: loss = 2.82302
Step 91950: loss = 2.93608
Step 91955: loss = 2.73606
Step 91960: loss = 3.06411
Step 91965: loss = 2.98963
Step 91970: loss = 2.97927
Step 91975: loss = 2.91384
Step 91980: loss = 2.97366
Step 91985: loss = 3.03512
Step 91990: loss = 2.70512
Step 91995: loss = 3.01517
Step 92000: loss = 2.81270
Step 92005: loss = 2.85144
Step 92010: loss = 3.00064
Step 92015: loss = 2.93902
Step 92020: loss = 3.11521
Step 92025: loss = 2.95563
Step 92030: loss = 2.88167
Step 92035: loss = 2.78644
Step 92040: loss = 3.15800
Training Data Eval:
  Num examples: 49920, Num correct: 6813, Precision @ 1: 0.1365
('Testing Data Eval: EPOCH->', 237)
  Num examples: 9984, Num correct: 1412, Precision @ 1: 0.1414
Step 92045: loss = 2.87358
Step 92050: loss = 2.86236
Step 92055: loss = 2.96181
Step 92060: loss = 2.94364
Step 92065: loss = 2.79108
Step 92070: loss = 2.86858
Step 92075: loss = 2.74289
Step 92080: loss = 2.83565
Step 92085: loss = 2.94817
Step 92090: loss = 2.82416
Step 92095: loss = 3.05357
Step 92100: loss = 2.88647
Step 92105: loss = 2.98240
Step 92110: loss = 2.93819
Step 92115: loss = 3.09183
Step 92120: loss = 3.00674
Step 92125: loss = 3.10875
Step 92130: loss = 2.97287
Step 92135: loss = 2.78929
Step 92140: loss = 2.97946
Step 92145: loss = 2.92812
Step 92150: loss = 2.78210
Step 92155: loss = 2.73113
Step 92160: loss = 2.92088
Step 92165: loss = 2.79094
Step 92170: loss = 2.97058
Step 92175: loss = 2.97322
Step 92180: loss = 2.94654
Step 92185: loss = 3.02770
Step 92190: loss = 2.83599
Step 92195: loss = 2.92040
Step 92200: loss = 2.89615
Step 92205: loss = 3.01532
Step 92210: loss = 2.80196
Step 92215: loss = 2.86667
Step 92220: loss = 2.86114
Step 92225: loss = 3.09906
Step 92230: loss = 2.83865
Step 92235: loss = 2.95600
Step 92240: loss = 3.05088
Step 92245: loss = 2.64669
Step 92250: loss = 2.77508
Step 92255: loss = 2.88969
Step 92260: loss = 2.85981
Step 92265: loss = 2.77893
Step 92270: loss = 2.70664
Step 92275: loss = 2.77698
Step 92280: loss = 2.86452
Step 92285: loss = 2.78369
Step 92290: loss = 2.98525
Step 92295: loss = 3.20574
Step 92300: loss = 2.98386
Step 92305: loss = 2.85992
Step 92310: loss = 2.81363
Step 92315: loss = 2.79940
Step 92320: loss = 2.95657
Step 92325: loss = 2.95390
Step 92330: loss = 2.94917
Step 92335: loss = 2.81814
Step 92340: loss = 2.91200
Step 92345: loss = 2.74319
Step 92350: loss = 2.86679
Step 92355: loss = 2.86287
Step 92360: loss = 2.99657
Step 92365: loss = 3.01240
Step 92370: loss = 2.76589
Step 92375: loss = 2.84089
Step 92380: loss = 3.04617
Step 92385: loss = 2.96255
Step 92390: loss = 3.02044
Step 92395: loss = 2.92286
Step 92400: loss = 2.87729
Step 92405: loss = 2.88912
Step 92410: loss = 2.86857
Step 92415: loss = 2.97917
Step 92420: loss = 3.00488
Step 92425: loss = 2.99310
Step 92430: loss = 3.01455
Training Data Eval:
  Num examples: 49920, Num correct: 6508, Precision @ 1: 0.1304
('Testing Data Eval: EPOCH->', 238)
  Num examples: 9984, Num correct: 1306, Precision @ 1: 0.1308
Step 92435: loss = 3.14641
Step 92440: loss = 2.92989
Step 92445: loss = 2.97558
Step 92450: loss = 3.01814
Step 92455: loss = 2.81769
Step 92460: loss = 2.77927
Step 92465: loss = 2.84218
Step 92470: loss = 2.94534
Step 92475: loss = 2.82305
Step 92480: loss = 2.73645
Step 92485: loss = 3.02679
Step 92490: loss = 2.83311
Step 92495: loss = 2.92263
Step 92500: loss = 2.87143
Step 92505: loss = 2.92024
Step 92510: loss = 2.99721
Step 92515: loss = 2.99331
Step 92520: loss = 2.98062
Step 92525: loss = 2.69595
Step 92530: loss = 3.10800
Step 92535: loss = 3.00785
Step 92540: loss = 2.84827
Step 92545: loss = 2.98368
Step 92550: loss = 3.00922
Step 92555: loss = 2.87439
Step 92560: loss = 3.07880
Step 92565: loss = 2.83097
Step 92570: loss = 2.70608
Step 92575: loss = 3.02142
Step 92580: loss = 2.97315
Step 92585: loss = 2.91279
Step 92590: loss = 2.77082
Step 92595: loss = 2.75947
Step 92600: loss = 2.96233
Step 92605: loss = 3.09490
Step 92610: loss = 2.75321
Step 92615: loss = 3.00888
Step 92620: loss = 2.88601
Step 92625: loss = 3.04362
Step 92630: loss = 2.99415
Step 92635: loss = 2.67985
Step 92640: loss = 3.06128
Step 92645: loss = 2.89736
Step 92650: loss = 2.77521
Step 92655: loss = 2.99558
Step 92660: loss = 2.89601
Step 92665: loss = 2.80488
Step 92670: loss = 3.01899
Step 92675: loss = 2.75246
Step 92680: loss = 2.79405
Step 92685: loss = 2.83629
Step 92690: loss = 3.04360
Step 92695: loss = 2.99141
Step 92700: loss = 3.18424
Step 92705: loss = 2.85359
Step 92710: loss = 2.87371
Step 92715: loss = 2.86203
Step 92720: loss = 2.70029
Step 92725: loss = 2.88100
Step 92730: loss = 2.85802
Step 92735: loss = 3.14260
Step 92740: loss = 2.91782
Step 92745: loss = 3.09205
Step 92750: loss = 2.95516
Step 92755: loss = 3.02645
Step 92760: loss = 2.74174
Step 92765: loss = 2.73467
Step 92770: loss = 3.00384
Step 92775: loss = 3.17390
Step 92780: loss = 2.81753
Step 92785: loss = 2.72769
Step 92790: loss = 2.80690
Step 92795: loss = 2.97401
Step 92800: loss = 2.80665
Step 92805: loss = 2.73340
Step 92810: loss = 2.96612
Step 92815: loss = 2.98906
Step 92820: loss = 3.02537
Training Data Eval:
  Num examples: 49920, Num correct: 6758, Precision @ 1: 0.1354
('Testing Data Eval: EPOCH->', 239)
  Num examples: 9984, Num correct: 1415, Precision @ 1: 0.1417
Step 92825: loss = 2.83146
Step 92830: loss = 2.89926
Step 92835: loss = 2.99231
Step 92840: loss = 2.93309
Step 92845: loss = 2.69612
Step 92850: loss = 2.70916
Step 92855: loss = 2.72720
Step 92860: loss = 2.77376
Step 92865: loss = 2.95589
Step 92870: loss = 2.87131
Step 92875: loss = 3.00385
Step 92880: loss = 3.03268
Step 92885: loss = 2.78398
Step 92890: loss = 2.78286
Step 92895: loss = 2.96728
Step 92900: loss = 3.00754
Step 92905: loss = 2.75387
Step 92910: loss = 2.75026
Step 92915: loss = 3.05339
Step 92920: loss = 2.68882
Step 92925: loss = 2.79384
Step 92930: loss = 2.75058
Step 92935: loss = 2.71148
Step 92940: loss = 2.92253
Step 92945: loss = 3.37967
Step 92950: loss = 2.75111
Step 92955: loss = 2.70627
Step 92960: loss = 2.83605
Step 92965: loss = 2.99491
Step 92970: loss = 2.85074
Step 92975: loss = 2.75333
Step 92980: loss = 2.89396
Step 92985: loss = 3.16749
Step 92990: loss = 3.00873
Step 92995: loss = 3.04129
Step 93000: loss = 2.87280
Step 93005: loss = 3.09812
Step 93010: loss = 2.83022
Step 93015: loss = 2.88890
Step 93020: loss = 2.89779
Step 93025: loss = 2.93919
Step 93030: loss = 2.68018
Step 93035: loss = 2.68585
Step 93040: loss = 2.71210
Step 93045: loss = 2.85960
Step 93050: loss = 3.05907
Step 93055: loss = 2.78228
Step 93060: loss = 2.92509
Step 93065: loss = 2.90548
Step 93070: loss = 3.01061
Step 93075: loss = 2.95612
Step 93080: loss = 2.87527
Step 93085: loss = 2.88722
Step 93090: loss = 2.79563
Step 93095: loss = 2.86365
Step 93100: loss = 3.03826
Step 93105: loss = 3.05602
Step 93110: loss = 2.89493
Step 93115: loss = 2.99217
Step 93120: loss = 2.76307
Step 93125: loss = 2.97265
Step 93130: loss = 3.06316
Step 93135: loss = 3.03224
Step 93140: loss = 2.88650
Step 93145: loss = 2.86404
Step 93150: loss = 2.89726
Step 93155: loss = 2.62412
Step 93160: loss = 2.82732
Step 93165: loss = 2.89055
Step 93170: loss = 3.22021
Step 93175: loss = 2.80491
Step 93180: loss = 2.95219
Step 93185: loss = 3.10688
Step 93190: loss = 3.07983
Step 93195: loss = 2.90296
Step 93200: loss = 2.93250
Step 93205: loss = 2.81045
Step 93210: loss = 2.59868
Training Data Eval:
  Num examples: 49920, Num correct: 6711, Precision @ 1: 0.1344
('Testing Data Eval: EPOCH->', 240)
  Num examples: 9984, Num correct: 1366, Precision @ 1: 0.1368
Step 93215: loss = 2.92514
Step 93220: loss = 2.67742
Step 93225: loss = 3.08362
Step 93230: loss = 2.50979
Step 93235: loss = 2.84788
Step 93240: loss = 2.77154
Step 93245: loss = 2.83803
Step 93250: loss = 2.74563
Step 93255: loss = 3.09670
Step 93260: loss = 2.95541
Step 93265: loss = 3.03487
Step 93270: loss = 2.85088
Step 93275: loss = 2.89117
Step 93280: loss = 2.71256
Step 93285: loss = 2.92216
Step 93290: loss = 2.71841
Step 93295: loss = 2.54835
Step 93300: loss = 3.14292
Step 93305: loss = 2.83116
Step 93310: loss = 2.68458
Step 93315: loss = 2.93659
Step 93320: loss = 2.65811
Step 93325: loss = 2.83737
Step 93330: loss = 3.02588
Step 93335: loss = 2.95807
Step 93340: loss = 2.92373
Step 93345: loss = 2.81109
Step 93350: loss = 2.90454
Step 93355: loss = 2.88253
Step 93360: loss = 2.65251
Step 93365: loss = 2.73162
Step 93370: loss = 2.92985
Step 93375: loss = 2.85198
Step 93380: loss = 3.01555
Step 93385: loss = 2.62152
Step 93390: loss = 3.14990
Step 93395: loss = 3.14332
Step 93400: loss = 3.15242
Step 93405: loss = 2.90854
Step 93410: loss = 2.70928
Step 93415: loss = 2.74391
Step 93420: loss = 2.88764
Step 93425: loss = 2.80504
Step 93430: loss = 2.81518
Step 93435: loss = 3.06873
Step 93440: loss = 2.93875
Step 93445: loss = 2.97670
Step 93450: loss = 2.74371
Step 93455: loss = 2.67103
Step 93460: loss = 2.80003
Step 93465: loss = 2.90778
Step 93470: loss = 2.87655
Step 93475: loss = 2.65782
Step 93480: loss = 2.97819
Step 93485: loss = 2.98002
Step 93490: loss = 2.65808
Step 93495: loss = 3.13215
Step 93500: loss = 2.97944
Step 93505: loss = 3.02387
Step 93510: loss = 2.80284
Step 93515: loss = 2.83038
Step 93520: loss = 2.84817
Step 93525: loss = 2.67315
Step 93530: loss = 2.84689
Step 93535: loss = 2.61051
Step 93540: loss = 3.08814
Step 93545: loss = 2.78509
Step 93550: loss = 2.72831
Step 93555: loss = 2.94397
Step 93560: loss = 2.67213
Step 93565: loss = 2.95819
Step 93570: loss = 3.16586
Step 93575: loss = 2.99568
Step 93580: loss = 3.21121
Step 93585: loss = 2.94501
Step 93590: loss = 2.90540
Step 93595: loss = 2.75942
Step 93600: loss = 2.89329
Training Data Eval:
  Num examples: 49920, Num correct: 6760, Precision @ 1: 0.1354
('Testing Data Eval: EPOCH->', 241)
  Num examples: 9984, Num correct: 1393, Precision @ 1: 0.1395
Step 93605: loss = 2.90393
Step 93610: loss = 2.87121
Step 93615: loss = 3.18631
Step 93620: loss = 2.94207
Step 93625: loss = 2.86993
Step 93630: loss = 2.75157
Step 93635: loss = 3.02291
Step 93640: loss = 3.00682
Step 93645: loss = 2.86609
Step 93650: loss = 2.84558
Step 93655: loss = 2.77803
Step 93660: loss = 2.92777
Step 93665: loss = 3.03805
Step 93670: loss = 2.92245
Step 93675: loss = 2.73612
Step 93680: loss = 2.87826
Step 93685: loss = 2.88078
Step 93690: loss = 2.84978
Step 93695: loss = 2.72210
Step 93700: loss = 3.18082
Step 93705: loss = 3.04492
Step 93710: loss = 3.07462
Step 93715: loss = 2.78674
Step 93720: loss = 2.71665
Step 93725: loss = 3.18987
Step 93730: loss = 2.89992
Step 93735: loss = 2.87178
Step 93740: loss = 3.13612
Step 93745: loss = 2.84630
Step 93750: loss = 3.14430
Step 93755: loss = 2.83393
Step 93760: loss = 3.11126
Step 93765: loss = 2.60244
Step 93770: loss = 3.06355
Step 93775: loss = 3.13208
Step 93780: loss = 3.07918
Step 93785: loss = 2.77695
Step 93790: loss = 2.98055
Step 93795: loss = 2.90288
Step 93800: loss = 3.07638
Step 93805: loss = 2.84876
Step 93810: loss = 2.66720
Step 93815: loss = 2.70829
Step 93820: loss = 3.03868
Step 93825: loss = 3.00194
Step 93830: loss = 2.84522
Step 93835: loss = 2.93619
Step 93840: loss = 2.72251
Step 93845: loss = 2.98659
Step 93850: loss = 3.04111
Step 93855: loss = 2.98544
Step 93860: loss = 2.66404
Step 93865: loss = 3.11701
Step 93870: loss = 2.93745
Step 93875: loss = 2.72837
Step 93880: loss = 2.94847
Step 93885: loss = 2.98711
Step 93890: loss = 3.05464
Step 93895: loss = 2.71045
Step 93900: loss = 2.97030
Step 93905: loss = 2.86035
Step 93910: loss = 2.72081
Step 93915: loss = 2.74371
Step 93920: loss = 2.86609
Step 93925: loss = 2.57292
Step 93930: loss = 3.03772
Step 93935: loss = 3.10425
Step 93940: loss = 2.89031
Step 93945: loss = 2.94595
Step 93950: loss = 2.83134
Step 93955: loss = 2.80689
Step 93960: loss = 3.04162
Step 93965: loss = 3.03151
Step 93970: loss = 2.91693
Step 93975: loss = 3.00728
Step 93980: loss = 3.20683
Step 93985: loss = 2.96877
Step 93990: loss = 2.67683
Training Data Eval:
  Num examples: 49920, Num correct: 6597, Precision @ 1: 0.1322
('Testing Data Eval: EPOCH->', 242)
  Num examples: 9984, Num correct: 1337, Precision @ 1: 0.1339
Step 93995: loss = 2.76249
Step 94000: loss = 2.81052
Step 94005: loss = 2.85419
Step 94010: loss = 2.99983
Step 94015: loss = 3.04851
Step 94020: loss = 2.89032
Step 94025: loss = 2.99713
Step 94030: loss = 2.69826
Step 94035: loss = 2.80526
Step 94040: loss = 2.71946
Step 94045: loss = 2.85022
Step 94050: loss = 2.88116
Step 94055: loss = 2.81114
Step 94060: loss = 2.91391
Step 94065: loss = 2.93200
Step 94070: loss = 2.83475
Step 94075: loss = 2.94276
Step 94080: loss = 2.87859
Step 94085: loss = 2.81796
Step 94090: loss = 3.02347
Step 94095: loss = 2.92078
Step 94100: loss = 2.81497
Step 94105: loss = 2.86716
Step 94110: loss = 2.93970
Step 94115: loss = 3.00614
Step 94120: loss = 2.76865
Step 94125: loss = 2.78292
Step 94130: loss = 2.87188
Step 94135: loss = 2.85801
Step 94140: loss = 2.81031
Step 94145: loss = 2.96456
Step 94150: loss = 2.75434
Step 94155: loss = 2.74257
Step 94160: loss = 2.79928
Step 94165: loss = 2.93153
Step 94170: loss = 3.03269
Step 94175: loss = 3.11349
Step 94180: loss = 2.90170
Step 94185: loss = 2.98002
Step 94190: loss = 2.75752
Step 94195: loss = 2.95083
Step 94200: loss = 2.72545
Step 94205: loss = 2.78147
Step 94210: loss = 2.98132
Step 94215: loss = 3.04207
Step 94220: loss = 2.77401
Step 94225: loss = 2.71258
Step 94230: loss = 2.76294
Step 94235: loss = 3.08473
Step 94240: loss = 2.76301
Step 94245: loss = 2.87139
Step 94250: loss = 2.82709
Step 94255: loss = 2.94709
Step 94260: loss = 2.80257
Step 94265: loss = 2.81983
Step 94270: loss = 2.72258
Step 94275: loss = 2.79721
Step 94280: loss = 2.91820
Step 94285: loss = 2.76705
Step 94290: loss = 3.12926
Step 94295: loss = 3.02862
Step 94300: loss = 3.03619
Step 94305: loss = 2.82659
Step 94310: loss = 2.83554
Step 94315: loss = 2.67220
Step 94320: loss = 2.65412
Step 94325: loss = 2.98205
Step 94330: loss = 2.80926
Step 94335: loss = 2.88853
Step 94340: loss = 3.11910
Step 94345: loss = 2.85159
Step 94350: loss = 2.67813
Step 94355: loss = 2.70999
Step 94360: loss = 3.83661
Step 94365: loss = 2.72104
Step 94370: loss = 2.81435
Step 94375: loss = 2.89936
Step 94380: loss = 3.10211
Training Data Eval:
  Num examples: 49920, Num correct: 6614, Precision @ 1: 0.1325
('Testing Data Eval: EPOCH->', 243)
  Num examples: 9984, Num correct: 1376, Precision @ 1: 0.1378
Step 94385: loss = 2.69190
Step 94390: loss = 2.96350
Step 94395: loss = 2.87522
Step 94400: loss = 3.16122
Step 94405: loss = 2.77886
Step 94410: loss = 2.78292
Step 94415: loss = 2.98116
Step 94420: loss = 2.59023
Step 94425: loss = 2.98232
Step 94430: loss = 2.85465
Step 94435: loss = 2.91395
Step 94440: loss = 2.98461
Step 94445: loss = 2.98413
Step 94450: loss = 2.77642
Step 94455: loss = 2.90953
Step 94460: loss = 3.07269
Step 94465: loss = 3.08793
Step 94470: loss = 2.93725
Step 94475: loss = 2.90519
Step 94480: loss = 2.81460
Step 94485: loss = 2.92160
Step 94490: loss = 2.85037
Step 94495: loss = 2.98391
Step 94500: loss = 2.69910
Step 94505: loss = 2.70361
Step 94510: loss = 2.77760
Step 94515: loss = 2.95711
Step 94520: loss = 3.11113
Step 94525: loss = 2.97436
Step 94530: loss = 2.59597
Step 94535: loss = 2.88273
Step 94540: loss = 3.20592
Step 94545: loss = 3.11133
Step 94550: loss = 2.74393
Step 94555: loss = 3.08388
Step 94560: loss = 2.89264
Step 94565: loss = 3.11671
Step 94570: loss = 2.91841
Step 94575: loss = 2.85039
Step 94580: loss = 2.80109
Step 94585: loss = 2.82694
Step 94590: loss = 2.87034
Step 94595: loss = 3.04841
Step 94600: loss = 3.13815
Step 94605: loss = 3.06933
Step 94610: loss = 2.82234
Step 94615: loss = 2.90978
Step 94620: loss = 2.83856
Step 94625: loss = 2.86759
Step 94630: loss = 2.90581
Step 94635: loss = 2.93249
Step 94640: loss = 3.05443
Step 94645: loss = 2.65929
Step 94650: loss = 2.74717
Step 94655: loss = 2.98021
Step 94660: loss = 2.92866
Step 94665: loss = 2.88065
Step 94670: loss = 2.57870
Step 94675: loss = 2.73315
Step 94680: loss = 2.80720
Step 94685: loss = 2.76175
Step 94690: loss = 2.83305
Step 94695: loss = 3.07838
Step 94700: loss = 3.00814
Step 94705: loss = 2.69889
Step 94710: loss = 2.84722
Step 94715: loss = 2.97700
Step 94720: loss = 3.09380
Step 94725: loss = 3.05574
Step 94730: loss = 3.01795
Step 94735: loss = 2.80309
Step 94740: loss = 2.82697
Step 94745: loss = 2.91087
Step 94750: loss = 3.17851
Step 94755: loss = 3.05287
Step 94760: loss = 2.66877
Step 94765: loss = 3.07940
Step 94770: loss = 2.63450
Training Data Eval:
  Num examples: 49920, Num correct: 6621, Precision @ 1: 0.1326
('Testing Data Eval: EPOCH->', 244)
  Num examples: 9984, Num correct: 1373, Precision @ 1: 0.1375
Step 94775: loss = 2.90578
Step 94780: loss = 2.94050
Step 94785: loss = 2.68850
Step 94790: loss = 2.96811
Step 94795: loss = 2.98133
Step 94800: loss = 2.75147
Step 94805: loss = 2.73324
Step 94810: loss = 2.91804
Step 94815: loss = 2.98642
Step 94820: loss = 2.84951
Step 94825: loss = 2.82179
Step 94830: loss = 2.79644
Step 94835: loss = 2.96604
Step 94840: loss = 2.81268
Step 94845: loss = 3.12802
Step 94850: loss = 2.70612
Step 94855: loss = 2.89053
Step 94860: loss = 2.92168
Step 94865: loss = 2.91801
Step 94870: loss = 2.89320
Step 94875: loss = 2.88824
Step 94880: loss = 2.94302
Step 94885: loss = 2.91274
Step 94890: loss = 3.13118
Step 94895: loss = 2.83494
Step 94900: loss = 3.11452
Step 94905: loss = 2.88927
Step 94910: loss = 2.94804
Step 94915: loss = 2.92305
Step 94920: loss = 2.99733
Step 94925: loss = 2.94620
Step 94930: loss = 3.23278
Step 94935: loss = 3.08717
Step 94940: loss = 2.86080
Step 94945: loss = 2.93601
Step 94950: loss = 2.99599
Step 94955: loss = 3.04903
Step 94960: loss = 3.17429
Step 94965: loss = 3.15120
Step 94970: loss = 3.19922
Step 94975: loss = 2.80077
Step 94980: loss = 2.76118
Step 94985: loss = 2.94689
Step 94990: loss = 3.08187
Step 94995: loss = 2.95939
Step 95000: loss = 2.92610
Step 95005: loss = 2.94205
Step 95010: loss = 2.85862
Step 95015: loss = 2.82443
Step 95020: loss = 2.99262
Step 95025: loss = 2.75874
Step 95030: loss = 2.95188
Step 95035: loss = 2.96922
Step 95040: loss = 2.82630
Step 95045: loss = 2.94049
Step 95050: loss = 2.86368
Step 95055: loss = 2.89265
Step 95060: loss = 3.03142
Step 95065: loss = 2.97900
Step 95070: loss = 2.84062
Step 95075: loss = 2.76135
Step 95080: loss = 2.98963
Step 95085: loss = 2.90175
Step 95090: loss = 3.12187
Step 95095: loss = 2.77109
Step 95100: loss = 2.83818
Step 95105: loss = 2.82324
Step 95110: loss = 2.84770
Step 95115: loss = 2.77513
Step 95120: loss = 2.91203
Step 95125: loss = 2.70640
Step 95130: loss = 3.06918
Step 95135: loss = 3.06633
Step 95140: loss = 3.11357
Step 95145: loss = 2.96871
Step 95150: loss = 2.72568
Step 95155: loss = 3.25168
Step 95160: loss = 2.71394
Training Data Eval:
  Num examples: 49920, Num correct: 6935, Precision @ 1: 0.1389
('Testing Data Eval: EPOCH->', 245)
  Num examples: 9984, Num correct: 1409, Precision @ 1: 0.1411
Step 95165: loss = 2.79577
Step 95170: loss = 2.99109
Step 95175: loss = 3.24293
Step 95180: loss = 2.77019
Step 95185: loss = 2.93729
Step 95190: loss = 2.86268
Step 95195: loss = 2.71845
Step 95200: loss = 2.89523
Step 95205: loss = 2.94780
Step 95210: loss = 2.83795
Step 95215: loss = 2.95100
Step 95220: loss = 2.91074
Step 95225: loss = 2.97979
Step 95230: loss = 2.90222
Step 95235: loss = 3.02760
Step 95240: loss = 3.05203
Step 95245: loss = 2.96727
Step 95250: loss = 2.79032
Step 95255: loss = 2.95845
Step 95260: loss = 2.87522
Step 95265: loss = 2.64963
Step 95270: loss = 2.90697
Step 95275: loss = 2.85645
Step 95280: loss = 3.01121
Step 95285: loss = 2.95867
Step 95290: loss = 2.98532
Step 95295: loss = 2.98919
Step 95300: loss = 2.83822
Step 95305: loss = 2.65211
Step 95310: loss = 2.99273
Step 95315: loss = 2.82442
Step 95320: loss = 2.83991
Step 95325: loss = 3.00060
Step 95330: loss = 2.94446
Step 95335: loss = 2.84678
Step 95340: loss = 2.90035
Step 95345: loss = 2.83390
Step 95350: loss = 2.83861
Step 95355: loss = 2.97685
Step 95360: loss = 3.01853
Step 95365: loss = 2.99569
Step 95370: loss = 3.05405
Step 95375: loss = 2.94836
Step 95380: loss = 2.68744
Step 95385: loss = 2.84574
Step 95390: loss = 2.89469
Step 95395: loss = 2.86760
Step 95400: loss = 3.01997
Step 95405: loss = 3.19183
Step 95410: loss = 3.20153
Step 95415: loss = 2.93583
Step 95420: loss = 2.66755
Step 95425: loss = 2.74546
Step 95430: loss = 3.10129
Step 95435: loss = 3.01087
Step 95440: loss = 2.90832
Step 95445: loss = 2.79137
Step 95450: loss = 3.03071
Step 95455: loss = 2.75896
Step 95460: loss = 2.95564
Step 95465: loss = 3.01614
Step 95470: loss = 2.90691
Step 95475: loss = 2.91881
Step 95480: loss = 2.93367
Step 95485: loss = 2.84389
Step 95490: loss = 2.98149
Step 95495: loss = 2.89846
Step 95500: loss = 2.69353
Step 95505: loss = 3.02786
Step 95510: loss = 3.03226
Step 95515: loss = 2.96779
Step 95520: loss = 2.93674
Step 95525: loss = 2.79827
Step 95530: loss = 2.89534
Step 95535: loss = 2.85372
Step 95540: loss = 2.96388
Step 95545: loss = 2.92710
Step 95550: loss = 2.71517
Training Data Eval:
  Num examples: 49920, Num correct: 6935, Precision @ 1: 0.1389
('Testing Data Eval: EPOCH->', 246)
  Num examples: 9984, Num correct: 1424, Precision @ 1: 0.1426
Step 95555: loss = 2.87594
Step 95560: loss = 3.02281
Step 95565: loss = 2.73350
Step 95570: loss = 2.92065
Step 95575: loss = 2.80920
Step 95580: loss = 2.85710
Step 95585: loss = 2.92931
Step 95590: loss = 2.64028
Step 95595: loss = 2.85816
Step 95600: loss = 2.96814
Step 95605: loss = 2.72779
Step 95610: loss = 2.85423
Step 95615: loss = 2.89542
Step 95620: loss = 2.98348
Step 95625: loss = 3.13741
Step 95630: loss = 2.67633
Step 95635: loss = 2.95568
Step 95640: loss = 2.87206
Step 95645: loss = 2.81680
Step 95650: loss = 2.72507
Step 95655: loss = 2.94305
Step 95660: loss = 3.07032
Step 95665: loss = 2.95280
Step 95670: loss = 2.86814
Step 95675: loss = 3.01947
Step 95680: loss = 3.00836
Step 95685: loss = 2.88579
Step 95690: loss = 2.83282
Step 95695: loss = 2.77174
Step 95700: loss = 2.73389
Step 95705: loss = 2.86309
Step 95710: loss = 2.74225
Step 95715: loss = 2.86344
Step 95720: loss = 2.77457
Step 95725: loss = 2.75401
Step 95730: loss = 2.79339
Step 95735: loss = 3.02145
Step 95740: loss = 2.65608
Step 95745: loss = 2.66627
Step 95750: loss = 2.74350
Step 95755: loss = 2.94466
Step 95760: loss = 3.14018
Step 95765: loss = 2.89354
Step 95770: loss = 2.98268
Step 95775: loss = 2.83966
Step 95780: loss = 2.79633
Step 95785: loss = 3.12144
Step 95790: loss = 2.87781
Step 95795: loss = 2.85643
Step 95800: loss = 2.77124
Step 95805: loss = 2.85059
Step 95810: loss = 2.65175
Step 95815: loss = 2.78693
Step 95820: loss = 3.01846
Step 95825: loss = 2.88427
Step 95830: loss = 2.91859
Step 95835: loss = 3.03079
Step 95840: loss = 3.13801
Step 95845: loss = 2.90074
Step 95850: loss = 3.05750
Step 95855: loss = 3.26118
Step 95860: loss = 3.00461
Step 95865: loss = 3.02211
Step 95870: loss = 3.17870
Step 95875: loss = 2.67191
Step 95880: loss = 2.87377
Step 95885: loss = 2.97086
Step 95890: loss = 2.92527
Step 95895: loss = 3.19377
Step 95900: loss = 2.72604
Step 95905: loss = 2.67972
Step 95910: loss = 3.13895
Step 95915: loss = 2.95384
Step 95920: loss = 2.90635
Step 95925: loss = 2.98147
Step 95930: loss = 2.99814
Step 95935: loss = 2.97999
Step 95940: loss = 2.72413
Training Data Eval:
  Num examples: 49920, Num correct: 6889, Precision @ 1: 0.1380
('Testing Data Eval: EPOCH->', 247)
  Num examples: 9984, Num correct: 1416, Precision @ 1: 0.1418
Step 95945: loss = 2.85612
Step 95950: loss = 2.89630
Step 95955: loss = 2.76875
Step 95960: loss = 2.86756
Step 95965: loss = 2.88054
Step 95970: loss = 2.98716
Step 95975: loss = 2.78750
Step 95980: loss = 2.99965
Step 95985: loss = 2.80350
Step 95990: loss = 2.69775
Step 95995: loss = 3.12129
Step 96000: loss = 2.77765
Step 96005: loss = 2.73903
Step 96010: loss = 2.82845
Step 96015: loss = 2.97176
Step 96020: loss = 2.94951
Step 96025: loss = 2.94799
Step 96030: loss = 2.57814
Step 96035: loss = 2.83914
Step 96040: loss = 2.77598
Step 96045: loss = 2.99755
Step 96050: loss = 2.70116
Step 96055: loss = 2.89155
Step 96060: loss = 3.00128
Step 96065: loss = 3.39539
Step 96070: loss = 2.95625
Step 96075: loss = 2.71157
Step 96080: loss = 2.81331
Step 96085: loss = 2.75553
Step 96090: loss = 2.95881
Step 96095: loss = 2.93629
Step 96100: loss = 2.93126
Step 96105: loss = 3.22473
Step 96110: loss = 3.02593
Step 96115: loss = 2.95008
Step 96120: loss = 2.92714
Step 96125: loss = 2.83879
Step 96130: loss = 3.13947
Step 96135: loss = 2.94900
Step 96140: loss = 2.99730
Step 96145: loss = 2.92096
Step 96150: loss = 2.75442
Step 96155: loss = 2.85667
Step 96160: loss = 2.92862
Step 96165: loss = 3.07755
Step 96170: loss = 2.79078
Step 96175: loss = 2.61083
Step 96180: loss = 2.84523
Step 96185: loss = 3.18353
Step 96190: loss = 2.83434
Step 96195: loss = 2.81917
Step 96200: loss = 2.59489
Step 96205: loss = 2.91522
Step 96210: loss = 2.78068
Step 96215: loss = 3.18667
Step 96220: loss = 2.63842
Step 96225: loss = 2.99776
Step 96230: loss = 2.65375
Step 96235: loss = 3.01825
Step 96240: loss = 2.98453
Step 96245: loss = 2.80378
Step 96250: loss = 2.78501
Step 96255: loss = 2.86811
Step 96260: loss = 2.79326
Step 96265: loss = 3.10647
Step 96270: loss = 2.83977
Step 96275: loss = 3.02357
Step 96280: loss = 3.06441
Step 96285: loss = 2.98621
Step 96290: loss = 2.76544
Step 96295: loss = 3.07215
Step 96300: loss = 2.87294
Step 96305: loss = 2.91052
Step 96310: loss = 3.03007
Step 96315: loss = 2.97303
Step 96320: loss = 2.80975
Step 96325: loss = 2.87008
Step 96330: loss = 2.93745
Training Data Eval:
  Num examples: 49920, Num correct: 6577, Precision @ 1: 0.1318
('Testing Data Eval: EPOCH->', 248)
  Num examples: 9984, Num correct: 1372, Precision @ 1: 0.1374
Step 96335: loss = 2.90394
Step 96340: loss = 2.63013
Step 96345: loss = 2.97420
Step 96350: loss = 2.59483
Step 96355: loss = 2.85592
Step 96360: loss = 2.97856
Step 96365: loss = 2.90611
Step 96370: loss = 2.94019
Step 96375: loss = 3.04386
Step 96380: loss = 2.92202
Step 96385: loss = 2.90517
Step 96390: loss = 3.17670
Step 96395: loss = 2.97320
Step 96400: loss = 2.81046
Step 96405: loss = 2.69894
Step 96410: loss = 2.78549
Step 96415: loss = 2.76896
Step 96420: loss = 3.13926
Step 96425: loss = 3.14520
Step 96430: loss = 3.02652
Step 96435: loss = 2.93261
Step 96440: loss = 2.65083
Step 96445: loss = 2.71110
Step 96450: loss = 3.03118
Step 96455: loss = 2.87095
Step 96460: loss = 3.00652
Step 96465: loss = 2.94739
Step 96470: loss = 2.95664
Step 96475: loss = 3.06397
Step 96480: loss = 2.64932
Step 96485: loss = 2.77559
Step 96490: loss = 2.88150
Step 96495: loss = 2.96140
Step 96500: loss = 2.87331
Step 96505: loss = 2.75227
Step 96510: loss = 3.12367
Step 96515: loss = 3.08726
Step 96520: loss = 3.00789
Step 96525: loss = 3.01765
Step 96530: loss = 2.64310
Step 96535: loss = 2.82035
Step 96540: loss = 2.79856
Step 96545: loss = 2.69780
Step 96550: loss = 2.76397
Step 96555: loss = 2.89185
Step 96560: loss = 2.69375
Step 96565: loss = 3.00553
Step 96570: loss = 2.84287
Step 96575: loss = 3.13734
Step 96580: loss = 3.09285
Step 96585: loss = 2.62319
Step 96590: loss = 3.09339
Step 96595: loss = 2.89986
Step 96600: loss = 2.87619
Step 96605: loss = 2.68234
Step 96610: loss = 2.93339
Step 96615: loss = 2.81861
Step 96620: loss = 3.02262
Step 96625: loss = 2.95399
Step 96630: loss = 2.83393
Step 96635: loss = 3.19234
Step 96640: loss = 3.16659
Step 96645: loss = 3.06750
Step 96650: loss = 2.98477
Step 96655: loss = 3.03867
Step 96660: loss = 2.96336
Step 96665: loss = 2.92407
Step 96670: loss = 3.14646
Step 96675: loss = 2.70709
Step 96680: loss = 2.77795
Step 96685: loss = 2.88922
Step 96690: loss = 2.98132
Step 96695: loss = 2.73407
Step 96700: loss = 2.99261
Step 96705: loss = 2.87464
Step 96710: loss = 2.96752
Step 96715: loss = 2.77594
Step 96720: loss = 2.84422
Training Data Eval:
  Num examples: 49920, Num correct: 6832, Precision @ 1: 0.1369
('Testing Data Eval: EPOCH->', 249)
  Num examples: 9984, Num correct: 1376, Precision @ 1: 0.1378
Step 96725: loss = 2.73552
Step 96730: loss = 2.87168
Step 96735: loss = 2.90651
Step 96740: loss = 2.80305
Step 96745: loss = 2.72457
Step 96750: loss = 2.89991
Step 96755: loss = 3.02216
Step 96760: loss = 2.92335
Step 96765: loss = 2.81331
Step 96770: loss = 3.05060
Step 96775: loss = 2.91033
Step 96780: loss = 2.87575
Step 96785: loss = 3.13039
Step 96790: loss = 3.16143
Step 96795: loss = 2.87081
Step 96800: loss = 2.84742
Step 96805: loss = 3.06886
Step 96810: loss = 2.73883
Step 96815: loss = 2.69760
Step 96820: loss = 2.98310
Step 96825: loss = 2.87792
Step 96830: loss = 2.92107
Step 96835: loss = 2.74840
Step 96840: loss = 2.99427
Step 96845: loss = 2.96828
Step 96850: loss = 2.93307
Step 96855: loss = 2.98200
Step 96860: loss = 2.86288
Step 96865: loss = 3.05197
Step 96870: loss = 2.76589
Step 96875: loss = 2.93540
Step 96880: loss = 3.13314
Step 96885: loss = 3.00684
Step 96890: loss = 2.93436
Step 96895: loss = 2.67327
Step 96900: loss = 2.67749
Step 96905: loss = 2.85762
Step 96910: loss = 2.99353
Step 96915: loss = 2.75658
Step 96920: loss = 2.58167
Step 96925: loss = 2.77160
Step 96930: loss = 2.75687
Step 96935: loss = 3.14445
Step 96940: loss = 3.03539
Step 96945: loss = 2.97951
Step 96950: loss = 2.87267
Step 96955: loss = 3.02545
Step 96960: loss = 3.07843
Step 96965: loss = 2.99096
Step 96970: loss = 2.91641
Step 96975: loss = 2.91684
Step 96980: loss = 3.11585
Step 96985: loss = 2.97205
Step 96990: loss = 2.68875
Step 96995: loss = 2.99896
Step 97000: loss = 2.85098
Step 97005: loss = 2.85987
Step 97010: loss = 2.67559
Step 97015: loss = 3.03912
Step 97020: loss = 2.88612
Step 97025: loss = 3.10229
Step 97030: loss = 2.89074
Step 97035: loss = 2.90788
Step 97040: loss = 2.75153
Step 97045: loss = 2.94534
Step 97050: loss = 2.89305
Step 97055: loss = 2.89852
Step 97060: loss = 2.92258
Step 97065: loss = 3.06151
Step 97070: loss = 3.06739
Step 97075: loss = 2.92481
Step 97080: loss = 2.81671
Step 97085: loss = 2.78808
Step 97090: loss = 2.81709
Step 97095: loss = 3.01985
Step 97100: loss = 2.95371
Step 97105: loss = 2.60913
Step 97110: loss = 2.95825
Training Data Eval:
  Num examples: 49920, Num correct: 6908, Precision @ 1: 0.1384
('Testing Data Eval: EPOCH->', 250)
  Num examples: 9984, Num correct: 1499, Precision @ 1: 0.1501
Step 97115: loss = 2.98049
Step 97120: loss = 2.76770
Step 97125: loss = 2.70416
Step 97130: loss = 2.83702
Step 97135: loss = 2.82738
Step 97140: loss = 2.87699
Step 97145: loss = 2.75637
Step 97150: loss = 2.78181
Step 97155: loss = 2.86523
Step 97160: loss = 2.80136
Step 97165: loss = 2.98443
Step 97170: loss = 3.11058
Step 97175: loss = 2.77380
Step 97180: loss = 2.87241
Step 97185: loss = 2.99340
Step 97190: loss = 2.88036
Step 97195: loss = 3.00277
Step 97200: loss = 2.86261
Step 97205: loss = 2.95911
Step 97210: loss = 2.84259
Step 97215: loss = 3.06916
Step 97220: loss = 2.91589
Step 97225: loss = 2.91436
Step 97230: loss = 2.87940
Step 97235: loss = 2.92011
Step 97240: loss = 2.89898
Step 97245: loss = 2.84420
Step 97250: loss = 3.07343
Step 97255: loss = 2.95396
Step 97260: loss = 2.77263
Step 97265: loss = 2.75319
Step 97270: loss = 2.91226
Step 97275: loss = 2.86635
Step 97280: loss = 3.08185
Step 97285: loss = 3.02138
Step 97290: loss = 2.77623
Step 97295: loss = 3.13688
Step 97300: loss = 2.94271
Step 97305: loss = 2.94591
Step 97310: loss = 2.89124
Step 97315: loss = 2.90879
Step 97320: loss = 3.04393
Step 97325: loss = 2.94321
Step 97330: loss = 3.14500
Step 97335: loss = 3.10080
Step 97340: loss = 2.87724
Step 97345: loss = 2.83098
Step 97350: loss = 2.79804
Step 97355: loss = 3.00735
Step 97360: loss = 2.89294
Step 97365: loss = 2.80331
Step 97370: loss = 3.07825
Step 97375: loss = 2.92096
Step 97380: loss = 3.03052
Step 97385: loss = 2.97119
Step 97390: loss = 2.74211
Step 97395: loss = 2.75024
Step 97400: loss = 2.94411
Step 97405: loss = 2.92029
Step 97410: loss = 2.69209
Step 97415: loss = 2.73583
Step 97420: loss = 2.96923
Step 97425: loss = 3.05873
Step 97430: loss = 3.01441
Step 97435: loss = 3.10467
Step 97440: loss = 2.87495
Step 97445: loss = 2.99173
Step 97450: loss = 2.79116
Step 97455: loss = 2.83454
Step 97460: loss = 3.03452
Step 97465: loss = 2.75182
Step 97470: loss = 3.17703
Step 97475: loss = 2.78746
Step 97480: loss = 2.85536
Step 97485: loss = 2.78043
Step 97490: loss = 2.91664
Step 97495: loss = 3.08144
Step 97500: loss = 2.87915
Training Data Eval:
  Num examples: 49920, Num correct: 6678, Precision @ 1: 0.1338
('Testing Data Eval: EPOCH->', 251)
  Num examples: 9984, Num correct: 1384, Precision @ 1: 0.1386
Step 97505: loss = 2.88402
Step 97510: loss = 2.78367
Step 97515: loss = 2.79004
Step 97520: loss = 3.06932
Step 97525: loss = 2.75170
Step 97530: loss = 3.02472
Step 97535: loss = 2.78297
Step 97540: loss = 3.13899
Step 97545: loss = 2.78452
Step 97550: loss = 2.95719
Step 97555: loss = 2.75470
Step 97560: loss = 2.96885
Step 97565: loss = 2.76619
Step 97570: loss = 3.02264
Step 97575: loss = 3.27692
Step 97580: loss = 2.86963
Step 97585: loss = 2.93906
Step 97590: loss = 2.94199
Step 97595: loss = 2.75006
Step 97600: loss = 3.02278
Step 97605: loss = 2.90790
Step 97610: loss = 2.84581
Step 97615: loss = 2.74631
Step 97620: loss = 2.81437
Step 97625: loss = 2.86395
Step 97630: loss = 2.83830
Step 97635: loss = 3.01289
Step 97640: loss = 2.77732
Step 97645: loss = 2.95732
Step 97650: loss = 2.96001
Step 97655: loss = 2.95234
Step 97660: loss = 2.79212
Step 97665: loss = 3.13185
Step 97670: loss = 2.94411
Step 97675: loss = 2.82298
Step 97680: loss = 3.04970
Step 97685: loss = 2.81925
Step 97690: loss = 3.02437
Step 97695: loss = 2.95473
Step 97700: loss = 2.83356
Step 97705: loss = 3.12440
Step 97710: loss = 2.89420
Step 97715: loss = 3.20142
Step 97720: loss = 2.77297
Step 97725: loss = 2.77375
Step 97730: loss = 2.97206
Step 97735: loss = 2.98031
Step 97740: loss = 2.95375
Step 97745: loss = 3.05146
Step 97750: loss = 2.97848
Step 97755: loss = 2.76241
Step 97760: loss = 2.93184
Step 97765: loss = 3.00352
Step 97770: loss = 3.02882
Step 97775: loss = 2.86782
Step 97780: loss = 3.16104
Step 97785: loss = 2.76882
Step 97790: loss = 2.56958
Step 97795: loss = 2.89242
Step 97800: loss = 3.01430
Step 97805: loss = 3.01403
Step 97810: loss = 3.24018
Step 97815: loss = 3.19321
Step 97820: loss = 3.27334
Step 97825: loss = 3.29310
Step 97830: loss = 3.01855
Step 97835: loss = 2.92163
Step 97840: loss = 2.93022
Step 97845: loss = 2.94273
Step 97850: loss = 2.92736
Step 97855: loss = 2.80205
Step 97860: loss = 3.00991
Step 97865: loss = 2.91906
Step 97870: loss = 2.70403
Step 97875: loss = 2.77871
Step 97880: loss = 2.60849
Step 97885: loss = 2.98206
Step 97890: loss = 2.90941
Training Data Eval:
  Num examples: 49920, Num correct: 6938, Precision @ 1: 0.1390
('Testing Data Eval: EPOCH->', 252)
  Num examples: 9984, Num correct: 1398, Precision @ 1: 0.1400
Step 97895: loss = 3.08529
Step 97900: loss = 2.94481
Step 97905: loss = 3.07468
Step 97910: loss = 2.82834
Step 97915: loss = 2.91255
Step 97920: loss = 2.80424
Step 97925: loss = 2.74677
Step 97930: loss = 2.80708
Step 97935: loss = 3.02615
Step 97940: loss = 2.76770
Step 97945: loss = 2.76521
Step 97950: loss = 2.89342
Step 97955: loss = 3.28028
Step 97960: loss = 3.14332
Step 97965: loss = 2.83705
Step 97970: loss = 2.85000
Step 97975: loss = 2.82524
Step 97980: loss = 3.01219
Step 97985: loss = 3.03338
Step 97990: loss = 2.97366
Step 97995: loss = 3.33560
Step 98000: loss = 2.86734
Step 98005: loss = 2.97247
Step 98010: loss = 2.88021
Step 98015: loss = 2.66110
Step 98020: loss = 2.76551
Step 98025: loss = 2.87811
Step 98030: loss = 2.85117
Step 98035: loss = 2.99094
Step 98040: loss = 2.83015
Step 98045: loss = 3.07568
Step 98050: loss = 2.97085
Step 98055: loss = 3.08855
Step 98060: loss = 2.97873
Step 98065: loss = 2.84835
Step 98070: loss = 2.80251
Step 98075: loss = 2.84982
Step 98080: loss = 2.86196
Step 98085: loss = 2.90677
Step 98090: loss = 2.83660
Step 98095: loss = 2.86152
Step 98100: loss = 2.76227
Step 98105: loss = 2.85482
Step 98110: loss = 3.12427
Step 98115: loss = 2.96056
Step 98120: loss = 2.88205
Step 98125: loss = 2.91365
Step 98130: loss = 2.89424
Step 98135: loss = 3.23435
Step 98140: loss = 2.76164
Step 98145: loss = 2.80802
Step 98150: loss = 2.94653
Step 98155: loss = 3.13041
Step 98160: loss = 2.94084
Step 98165: loss = 2.77298
Step 98170: loss = 3.12538
Step 98175: loss = 2.89595
Step 98180: loss = 2.91003
Step 98185: loss = 2.86310
Step 98190: loss = 2.97091
Step 98195: loss = 2.98487
Step 98200: loss = 2.73046
Step 98205: loss = 3.13920
Step 98210: loss = 2.94522
Step 98215: loss = 3.04886
Step 98220: loss = 2.94108
Step 98225: loss = 2.96372
Step 98230: loss = 2.84362
Step 98235: loss = 2.82764
Step 98240: loss = 2.80480
Step 98245: loss = 2.96454
Step 98250: loss = 2.91916
Step 98255: loss = 2.85318
Step 98260: loss = 3.14166
Step 98265: loss = 3.04370
Step 98270: loss = 3.00612
Step 98275: loss = 3.29203
Step 98280: loss = 2.92616
Training Data Eval:
  Num examples: 49920, Num correct: 6610, Precision @ 1: 0.1324
('Testing Data Eval: EPOCH->', 253)
  Num examples: 9984, Num correct: 1327, Precision @ 1: 0.1329
Step 98285: loss = 2.90127
Step 98290: loss = 2.84014
Step 98295: loss = 3.08055
Step 98300: loss = 2.70914
Step 98305: loss = 2.93108
Step 98310: loss = 3.01789
Step 98315: loss = 2.95101
Step 98320: loss = 3.07397
Step 98325: loss = 2.85793
Step 98330: loss = 2.84979
Step 98335: loss = 2.86084
Step 98340: loss = 3.06016
Step 98345: loss = 2.94936
Step 98350: loss = 2.97994
Step 98355: loss = 2.87644
Step 98360: loss = 3.12936
Step 98365: loss = 3.10393
Step 98370: loss = 2.61249
Step 98375: loss = 2.84855
Step 98380: loss = 2.96101
Step 98385: loss = 2.98403
Step 98390: loss = 2.94340
Step 98395: loss = 2.92050
Step 98400: loss = 2.90728
Step 98405: loss = 2.94579
Step 98410: loss = 2.87316
Step 98415: loss = 2.88196
Step 98420: loss = 2.91259
Step 98425: loss = 3.10568
Step 98430: loss = 2.91567
Step 98435: loss = 3.02469
Step 98440: loss = 2.92817
Step 98445: loss = 2.79568
Step 98450: loss = 3.19924
Step 98455: loss = 2.77333
Step 98460: loss = 2.94965
Step 98465: loss = 3.04542
Step 98470: loss = 3.18578
Step 98475: loss = 3.15168
Step 98480: loss = 2.86702
Step 98485: loss = 2.73173
Step 98490: loss = 2.82826
Step 98495: loss = 2.82650
Step 98500: loss = 2.75019
Step 98505: loss = 3.11771
Step 98510: loss = 2.70781
Step 98515: loss = 2.79241
Step 98520: loss = 2.79684
Step 98525: loss = 2.98860
Step 98530: loss = 2.86568
Step 98535: loss = 2.97231
Step 98540: loss = 2.83101
Step 98545: loss = 2.89872
Step 98550: loss = 2.99079
Step 98555: loss = 2.88691
Step 98560: loss = 2.86142
Step 98565: loss = 2.88217
Step 98570: loss = 2.88799
Step 98575: loss = 2.86074
Step 98580: loss = 3.05395
Step 98585: loss = 3.09013
Step 98590: loss = 2.75435
Step 98595: loss = 2.88779
Step 98600: loss = 2.77538
Step 98605: loss = 2.92732
Step 98610: loss = 2.75865
Step 98615: loss = 2.98409
Step 98620: loss = 2.94777
Step 98625: loss = 2.81890
Step 98630: loss = 2.81976
Step 98635: loss = 2.82826
Step 98640: loss = 2.94048
Step 98645: loss = 2.84978
Step 98650: loss = 3.16344
Step 98655: loss = 2.92535
Step 98660: loss = 2.90286
Step 98665: loss = 2.83717
Step 98670: loss = 2.84086
Training Data Eval:
  Num examples: 49920, Num correct: 6665, Precision @ 1: 0.1335
('Testing Data Eval: EPOCH->', 254)
  Num examples: 9984, Num correct: 1356, Precision @ 1: 0.1358
Step 98675: loss = 3.08263
Step 98680: loss = 2.84434
Step 98685: loss = 2.97044
Step 98690: loss = 2.79338
Step 98695: loss = 3.39584
Step 98700: loss = 2.83917
Step 98705: loss = 4.12849
Step 98710: loss = 3.70658
Step 98715: loss = 2.88900
Step 98720: loss = 2.90851
Step 98725: loss = 3.06872
Step 98730: loss = 3.93388
Step 98735: loss = 4.31897
Step 98740: loss = 3.17998
Step 98745: loss = 2.76835
Step 98750: loss = 3.41632
Step 98755: loss = 3.48664
Step 98760: loss = 3.16963
Step 98765: loss = 2.74682
Step 98770: loss = 3.05655
Step 98775: loss = 3.12395
Step 98780: loss = 2.99884
Step 98785: loss = 4.19341
Step 98790: loss = 3.08175
Step 98795: loss = 2.93480
Step 98800: loss = 2.96381
Step 98805: loss = 3.20409
Step 98810: loss = 2.84169
Step 98815: loss = 2.84487
Step 98820: loss = 3.07800
Step 98825: loss = 3.13928
Step 98830: loss = 2.99262
Step 98835: loss = 3.10046
Step 98840: loss = 2.75681
Step 98845: loss = 2.93719
Step 98850: loss = 2.57450
Step 98855: loss = 3.18463
Step 98860: loss = 3.18908
Step 98865: loss = 2.95598
Step 98870: loss = 2.77233
Step 98875: loss = 2.99357
Step 98880: loss = 2.99226
Step 98885: loss = 2.94706
Step 98890: loss = 2.90092
Step 98895: loss = 2.85310
Step 98900: loss = 2.94886
Step 98905: loss = 3.03946
Step 98910: loss = 3.08449
Step 98915: loss = 2.77932
Step 98920: loss = 2.80517
Step 98925: loss = 2.94140
Step 98930: loss = 2.85006
Step 98935: loss = 2.70462
Step 98940: loss = 2.94167
Step 98945: loss = 2.87288
Step 98950: loss = 2.85892
Step 98955: loss = 2.88700
Step 98960: loss = 3.21082
Step 98965: loss = 2.71176
Step 98970: loss = 3.09107
Step 98975: loss = 3.01524
Step 98980: loss = 3.26209
Step 98985: loss = 3.02704
Step 98990: loss = 2.61193
Step 98995: loss = 2.92783
Step 99000: loss = 2.93330
Step 99005: loss = 2.88653
Step 99010: loss = 2.69290
Step 99015: loss = 2.98760
Step 99020: loss = 2.84277
Step 99025: loss = 2.84582
Step 99030: loss = 2.76286
Step 99035: loss = 2.97728
Step 99040: loss = 2.85978
Step 99045: loss = 3.02383
Step 99050: loss = 2.88649
Step 99055: loss = 2.94158
Step 99060: loss = 2.87715
Training Data Eval:
  Num examples: 49920, Num correct: 6911, Precision @ 1: 0.1384
('Testing Data Eval: EPOCH->', 255)
  Num examples: 9984, Num correct: 1464, Precision @ 1: 0.1466
Step 99065: loss = 2.82520
Step 99070: loss = 2.98098
Step 99075: loss = 2.65657
Step 99080: loss = 2.80138
Step 99085: loss = 2.65941
Step 99090: loss = 2.94593
Step 99095: loss = 2.57639
Step 99100: loss = 2.87723
Step 99105: loss = 2.66091
Step 99110: loss = 2.85900
Step 99115: loss = 2.89956
Step 99120: loss = 2.99731
Step 99125: loss = 3.05770
Step 99130: loss = 3.04517
Step 99135: loss = 2.82048
Step 99140: loss = 2.80284
Step 99145: loss = 2.74091
Step 99150: loss = 2.84684
Step 99155: loss = 2.97365
Step 99160: loss = 2.88500
Step 99165: loss = 2.72182
Step 99170: loss = 3.17191
Step 99175: loss = 3.23624
Step 99180: loss = 3.15245
Step 99185: loss = 2.87422
Step 99190: loss = 2.85103
Step 99195: loss = 2.84669
Step 99200: loss = 3.04243
Step 99205: loss = 2.96048
Step 99210: loss = 2.88646
Step 99215: loss = 3.11273
Step 99220: loss = 3.00678
Step 99225: loss = 3.06682
Step 99230: loss = 2.87314
Step 99235: loss = 3.04554
Step 99240: loss = 3.02404
Step 99245: loss = 2.83547
Step 99250: loss = 2.95479
Step 99255: loss = 2.83078
Step 99260: loss = 2.89632
Step 99265: loss = 3.16201
Step 99270: loss = 2.80298
Step 99275: loss = 2.90952
Step 99280: loss = 2.98218
Step 99285: loss = 2.61278
Step 99290: loss = 2.88794
Step 99295: loss = 2.86644
Step 99300: loss = 2.97637
Step 99305: loss = 2.87694
Step 99310: loss = 2.99547
Step 99315: loss = 3.15127
Step 99320: loss = 2.82047
Step 99325: loss = 2.77383
Step 99330: loss = 2.99885
Step 99335: loss = 2.82792
Step 99340: loss = 2.82493
Step 99345: loss = 3.24052
Step 99350: loss = 2.93700
Step 99355: loss = 3.21040
Step 99360: loss = 3.05373
Step 99365: loss = 2.92751
Step 99370: loss = 2.93166
Step 99375: loss = 2.82683
Step 99380: loss = 3.09853
Step 99385: loss = 3.01082
Step 99390: loss = 2.84458
Step 99395: loss = 2.97465
Step 99400: loss = 2.65544
Step 99405: loss = 2.73973
Step 99410: loss = 2.76437
Step 99415: loss = 2.87493
Step 99420: loss = 2.66932
Step 99425: loss = 2.78334
Step 99430: loss = 2.73013
Step 99435: loss = 2.94272
Step 99440: loss = 3.18142
Step 99445: loss = 2.61438
Step 99450: loss = 3.08333
Training Data Eval:
  Num examples: 49920, Num correct: 6626, Precision @ 1: 0.1327
('Testing Data Eval: EPOCH->', 256)
  Num examples: 9984, Num correct: 1357, Precision @ 1: 0.1359
Step 99455: loss = 3.15274
Step 99460: loss = 2.87523
Step 99465: loss = 3.30798
Step 99470: loss = 2.89295
Step 99475: loss = 3.00047
Step 99480: loss = 2.80207
Step 99485: loss = 2.67855
Step 99490: loss = 2.66283
Step 99495: loss = 2.72670
Step 99500: loss = 2.87186
Step 99505: loss = 2.68638
Step 99510: loss = 3.08083
Step 99515: loss = 2.77169
Step 99520: loss = 3.00165
Step 99525: loss = 2.85089
Step 99530: loss = 2.79072
Step 99535: loss = 3.05853
Step 99540: loss = 3.18320
Step 99545: loss = 3.04837
Step 99550: loss = 2.88247
Step 99555: loss = 2.95814
Step 99560: loss = 2.80582
Step 99565: loss = 2.93493
Step 99570: loss = 2.99201
Step 99575: loss = 2.64974
Step 99580: loss = 2.87680
Step 99585: loss = 2.85093
Step 99590: loss = 2.86954
Step 99595: loss = 2.74136
Step 99600: loss = 2.95871
Step 99605: loss = 2.91141
Step 99610: loss = 3.09497
Step 99615: loss = 2.95991
Step 99620: loss = 3.17022
Step 99625: loss = 3.00252
Step 99630: loss = 2.89611
Step 99635: loss = 2.68592
Step 99640: loss = 2.85941
Step 99645: loss = 2.78545
Step 99650: loss = 2.82510
Step 99655: loss = 2.95957
Step 99660: loss = 2.86981
Step 99665: loss = 2.68870
Step 99670: loss = 2.89676
Step 99675: loss = 2.81460
Step 99680: loss = 2.84290
Step 99685: loss = 2.86360
Step 99690: loss = 2.79899
Step 99695: loss = 2.96906
Step 99700: loss = 2.96006
Step 99705: loss = 3.11136
Step 99710: loss = 2.97330
Step 99715: loss = 2.98854
Step 99720: loss = 2.95470
Step 99725: loss = 2.97501
Step 99730: loss = 2.88650
Step 99735: loss = 2.85558
Step 99740: loss = 3.12830
Step 99745: loss = 3.22124
Step 99750: loss = 2.87473
Step 99755: loss = 2.80765
Step 99760: loss = 2.85376
Step 99765: loss = 2.80298
Step 99770: loss = 2.90231
Step 99775: loss = 2.77260
Step 99780: loss = 3.15172
Step 99785: loss = 2.79235
Step 99790: loss = 2.99238
Step 99795: loss = 3.17425
Step 99800: loss = 2.94127
Step 99805: loss = 2.94337
Step 99810: loss = 2.84516
Step 99815: loss = 3.07877
Step 99820: loss = 3.05346
Step 99825: loss = 2.97562
Step 99830: loss = 3.09589
Step 99835: loss = 2.91712
Step 99840: loss = 2.89229
Training Data Eval:
  Num examples: 49920, Num correct: 6743, Precision @ 1: 0.1351
('Testing Data Eval: EPOCH->', 257)
  Num examples: 9984, Num correct: 1396, Precision @ 1: 0.1398
Step 99845: loss = 3.01560
Step 99850: loss = 2.96398
Step 99855: loss = 3.04591
Step 99860: loss = 2.78339
Step 99865: loss = 2.88046
Step 99870: loss = 3.13267
Step 99875: loss = 2.89519
Step 99880: loss = 3.01324
Step 99885: loss = 2.98214
Step 99890: loss = 2.75507
Step 99895: loss = 2.75284
Step 99900: loss = 2.95427
Step 99905: loss = 2.82454
Step 99910: loss = 2.63001
Step 99915: loss = 3.03301
Step 99920: loss = 2.78640
Step 99925: loss = 3.34589
Step 99930: loss = 2.89966
Step 99935: loss = 2.76581
Step 99940: loss = 2.56009
Step 99945: loss = 2.98026
Step 99950: loss = 2.75690
Step 99955: loss = 2.97583
Step 99960: loss = 2.88072
Step 99965: loss = 2.96683
Step 99970: loss = 2.89239
Step 99975: loss = 2.66743
Step 99980: loss = 2.92015
Step 99985: loss = 2.82777
Step 99990: loss = 2.94001
Step 99995: loss = 3.06119
Step 100000: loss = 2.77440
Step 100005: loss = 2.99185
Step 100010: loss = 2.82077
Step 100015: loss = 2.99773
Step 100020: loss = 2.83324
Step 100025: loss = 2.83188
Step 100030: loss = 2.81918
Step 100035: loss = 2.99004
Step 100040: loss = 2.78574
Step 100045: loss = 2.92598
Step 100050: loss = 2.79773
Step 100055: loss = 2.80057
Step 100060: loss = 3.04369
Step 100065: loss = 2.89832
Step 100070: loss = 2.93529
Step 100075: loss = 2.83010
Step 100080: loss = 2.86019
Step 100085: loss = 2.94255
Step 100090: loss = 2.92743
Step 100095: loss = 3.01583
Step 100100: loss = 3.02931
Step 100105: loss = 2.76836
Step 100110: loss = 2.90364
Step 100115: loss = 3.22816
Step 100120: loss = 3.02928
Step 100125: loss = 2.99986
Step 100130: loss = 2.96835
Step 100135: loss = 3.04082
Step 100140: loss = 2.86095
Step 100145: loss = 2.92374
Step 100150: loss = 2.90113
Step 100155: loss = 2.81694
Step 100160: loss = 2.88967
Step 100165: loss = 3.01319
Step 100170: loss = 2.93023
Step 100175: loss = 2.88096
Step 100180: loss = 2.94887
Step 100185: loss = 2.89939
Step 100190: loss = 2.99859
Step 100195: loss = 2.82430
Step 100200: loss = 2.92395
Step 100205: loss = 2.92921
Step 100210: loss = 2.98468
Step 100215: loss = 2.87190
Step 100220: loss = 2.84572
Step 100225: loss = 2.89796
Step 100230: loss = 3.05308
Training Data Eval:
  Num examples: 49920, Num correct: 6822, Precision @ 1: 0.1367
('Testing Data Eval: EPOCH->', 258)
  Num examples: 9984, Num correct: 1427, Precision @ 1: 0.1429
Step 100235: loss = 2.98516
Step 100240: loss = 2.88961
Step 100245: loss = 2.77968
Step 100250: loss = 2.80197
Step 100255: loss = 2.69161
Step 100260: loss = 2.84095
Step 100265: loss = 3.00487
Step 100270: loss = 2.79280
Step 100275: loss = 2.95386
Step 100280: loss = 2.79482
Step 100285: loss = 2.83775
Step 100290: loss = 2.83125
Step 100295: loss = 3.02830
Step 100300: loss = 2.76082
Step 100305: loss = 2.76936
Step 100310: loss = 2.95019
Step 100315: loss = 3.03574
Step 100320: loss = 3.02819
Step 100325: loss = 3.37458
Step 100330: loss = 2.87465
Step 100335: loss = 2.93279
Step 100340: loss = 2.93854
Step 100345: loss = 2.85471
Step 100350: loss = 2.90508
Step 100355: loss = 2.87173
Step 100360: loss = 2.90794
Step 100365: loss = 3.04807
Step 100370: loss = 3.01331
Step 100375: loss = 2.75918
Step 100380: loss = 2.93042
Step 100385: loss = 2.96530
Step 100390: loss = 2.99035
Step 100395: loss = 2.76734
Step 100400: loss = 3.00198
Step 100405: loss = 2.77868
Step 100410: loss = 2.92526
Step 100415: loss = 2.88485
Step 100420: loss = 2.95692
Step 100425: loss = 3.02925
Step 100430: loss = 3.02368
Step 100435: loss = 2.91608
Step 100440: loss = 2.88943
Step 100445: loss = 3.02190
Step 100450: loss = 2.98168
Step 100455: loss = 2.91733
Step 100460: loss = 3.15735
Step 100465: loss = 2.83635
Step 100470: loss = 2.86811
Step 100475: loss = 3.03439
Step 100480: loss = 2.77987
Step 100485: loss = 2.71667
Step 100490: loss = 2.92903
Step 100495: loss = 2.83160
Step 100500: loss = 3.07981
Step 100505: loss = 2.89972
Step 100510: loss = 2.85264
Step 100515: loss = 2.84944
Step 100520: loss = 2.86219
Step 100525: loss = 2.94543
Step 100530: loss = 3.18838
Step 100535: loss = 2.93058
Step 100540: loss = 2.78880
Step 100545: loss = 2.92573
Step 100550: loss = 2.71692
Step 100555: loss = 3.01478
Step 100560: loss = 2.89231
Step 100565: loss = 2.92670
Step 100570: loss = 2.75235
Step 100575: loss = 2.86152
Step 100580: loss = 3.13070
Step 100585: loss = 2.93847
Step 100590: loss = 2.98193
Step 100595: loss = 3.04346
Step 100600: loss = 3.03390
Step 100605: loss = 3.00006
Step 100610: loss = 2.85600
Step 100615: loss = 2.78712
Step 100620: loss = 2.83612
Training Data Eval:
  Num examples: 49920, Num correct: 6686, Precision @ 1: 0.1339
('Testing Data Eval: EPOCH->', 259)
  Num examples: 9984, Num correct: 1391, Precision @ 1: 0.1393
Step 100625: loss = 3.11376
Step 100630: loss = 2.80088
Step 100635: loss = 2.87630
Step 100640: loss = 2.86700
Step 100645: loss = 2.87620
Step 100650: loss = 2.99883
Step 100655: loss = 3.13645
Step 100660: loss = 3.01186
Step 100665: loss = 2.89441
Step 100670: loss = 2.84262
Step 100675: loss = 2.99787
Step 100680: loss = 2.78235
Step 100685: loss = 3.10668
Step 100690: loss = 2.99452
Step 100695: loss = 3.17367
Step 100700: loss = 2.98423
Step 100705: loss = 2.72904
Step 100710: loss = 2.70288
Step 100715: loss = 2.95557
Step 100720: loss = 3.14563
Step 100725: loss = 2.95276
Step 100730: loss = 2.74892
Step 100735: loss = 3.04576
Step 100740: loss = 2.78191
Step 100745: loss = 2.76575
Step 100750: loss = 2.98936
Step 100755: loss = 2.79388
Step 100760: loss = 3.14920
Step 100765: loss = 2.87858
Step 100770: loss = 2.93956
Step 100775: loss = 2.96969
Step 100780: loss = 2.86327
Step 100785: loss = 3.23140
Step 100790: loss = 3.03469
Step 100795: loss = 2.85938
Step 100800: loss = 2.93794
Step 100805: loss = 3.04879
Step 100810: loss = 2.97925
Step 100815: loss = 2.85444
Step 100820: loss = 3.03590
Step 100825: loss = 2.72353
Step 100830: loss = 3.11190
Step 100835: loss = 2.72781
Step 100840: loss = 3.26700
Step 100845: loss = 3.02719
Step 100850: loss = 3.27654
Step 100855: loss = 2.87515
Step 100860: loss = 2.99021
Step 100865: loss = 3.01425
Step 100870: loss = 2.95959
Step 100875: loss = 2.84908
Step 100880: loss = 3.12206
Step 100885: loss = 3.23294
Step 100890: loss = 2.97564
Step 100895: loss = 2.73618
Step 100900: loss = 2.84742
Step 100905: loss = 2.74237
Step 100910: loss = 2.86449
Step 100915: loss = 2.85971
Step 100920: loss = 2.85722
Step 100925: loss = 2.68721
Step 100930: loss = 2.99297
Step 100935: loss = 2.75901
Step 100940: loss = 2.97056
Step 100945: loss = 2.97226
Step 100950: loss = 2.67910
Step 100955: loss = 2.97544
Step 100960: loss = 2.96093
Step 100965: loss = 3.05593
Step 100970: loss = 2.77245
Step 100975: loss = 2.95148
Step 100980: loss = 2.76339
Step 100985: loss = 2.90979
Step 100990: loss = 3.00671
Step 100995: loss = 3.10533
Step 101000: loss = 2.81776
Step 101005: loss = 2.91087
Step 101010: loss = 3.19335
Training Data Eval:
  Num examples: 49920, Num correct: 6810, Precision @ 1: 0.1364
('Testing Data Eval: EPOCH->', 260)
  Num examples: 9984, Num correct: 1445, Precision @ 1: 0.1447
Step 101015: loss = 3.26832
Step 101020: loss = 2.82413
Step 101025: loss = 3.21760
Step 101030: loss = 3.02055
Step 101035: loss = 2.83772
Step 101040: loss = 2.90206
Step 101045: loss = 2.79875
Step 101050: loss = 3.00357
Step 101055: loss = 2.63259
Step 101060: loss = 2.71779
Step 101065: loss = 2.79867
Step 101070: loss = 2.83133
Step 101075: loss = 2.90026
Step 101080: loss = 3.12608
Step 101085: loss = 3.08582
Step 101090: loss = 2.80617
Step 101095: loss = 2.67484
Step 101100: loss = 2.70015
Step 101105: loss = 2.89469
Step 101110: loss = 2.89884
Step 101115: loss = 2.84071
Step 101120: loss = 3.00510
Step 101125: loss = 2.83496
Step 101130: loss = 2.71023
Step 101135: loss = 2.80472
Step 101140: loss = 3.04081
Step 101145: loss = 2.89797
Step 101150: loss = 2.82787
Step 101155: loss = 2.92699
Step 101160: loss = 2.82312
Step 101165: loss = 2.89752
Step 101170: loss = 3.01566
Step 101175: loss = 2.95083
Step 101180: loss = 2.92378
Step 101185: loss = 2.95715
Step 101190: loss = 3.05192
Step 101195: loss = 3.13822
Step 101200: loss = 3.00499
Step 101205: loss = 3.04292
Step 101210: loss = 2.64371
Step 101215: loss = 2.90249
Step 101220: loss = 2.79142
Step 101225: loss = 2.83867
Step 101230: loss = 2.99284
Step 101235: loss = 2.82210
Step 101240: loss = 2.95874
Step 101245: loss = 3.07878
Step 101250: loss = 3.11062
Step 101255: loss = 2.84257
Step 101260: loss = 3.09198
Step 101265: loss = 2.91604
Step 101270: loss = 2.84518
Step 101275: loss = 3.01069
Step 101280: loss = 3.03171
Step 101285: loss = 2.87491
Step 101290: loss = 2.82270
Step 101295: loss = 2.98141
Step 101300: loss = 2.83313
Step 101305: loss = 2.83289
Step 101310: loss = 2.76113
Step 101315: loss = 2.71838
Step 101320: loss = 2.94299
Step 101325: loss = 2.86116
Step 101330: loss = 2.84421
Step 101335: loss = 2.89914
Step 101340: loss = 2.92837
Step 101345: loss = 2.82288
Step 101350: loss = 2.74610
Step 101355: loss = 3.09537
Step 101360: loss = 2.64555
Step 101365: loss = 3.04797
Step 101370: loss = 2.86119
Step 101375: loss = 3.08029
Step 101380: loss = 2.82479
Step 101385: loss = 3.11998
Step 101390: loss = 3.03162
Step 101395: loss = 2.75341
Step 101400: loss = 2.70555
Training Data Eval:
  Num examples: 49920, Num correct: 6693, Precision @ 1: 0.1341
('Testing Data Eval: EPOCH->', 261)
  Num examples: 9984, Num correct: 1349, Precision @ 1: 0.1351
Step 101405: loss = 3.07937
Step 101410: loss = 2.83651
Step 101415: loss = 3.01451
Step 101420: loss = 2.90864
Step 101425: loss = 2.85195
Step 101430: loss = 2.85271
Step 101435: loss = 2.86683
Step 101440: loss = 3.09888
Step 101445: loss = 2.76291
Step 101450: loss = 2.98900
Step 101455: loss = 2.84097
Step 101460: loss = 2.74242
Step 101465: loss = 2.69320
Step 101470: loss = 2.74119
Step 101475: loss = 2.83877
Step 101480: loss = 2.84361
Step 101485: loss = 3.03595
Step 101490: loss = 2.99718
Step 101495: loss = 3.08754
Step 101500: loss = 2.98749
Step 101505: loss = 2.84755
Step 101510: loss = 3.06037
Step 101515: loss = 2.83396
Step 101520: loss = 2.61815
Step 101525: loss = 3.02198
Step 101530: loss = 2.68480
Step 101535: loss = 2.74614
Step 101540: loss = 2.69928
Step 101545: loss = 2.85936
Step 101550: loss = 3.00307
Step 101555: loss = 2.92862
Step 101560: loss = 2.73549
Step 101565: loss = 2.94476
Step 101570: loss = 2.95692
Step 101575: loss = 2.96677
Step 101580: loss = 2.92920
Step 101585: loss = 2.87925
Step 101590: loss = 2.87207
Step 101595: loss = 2.79895
Step 101600: loss = 3.00340
Step 101605: loss = 3.01512
Step 101610: loss = 3.11765
Step 101615: loss = 2.90904
Step 101620: loss = 2.98335
Step 101625: loss = 2.81669
Step 101630: loss = 3.00756
Step 101635: loss = 2.81007
Step 101640: loss = 3.16013
Step 101645: loss = 2.95148
Step 101650: loss = 2.75356
Step 101655: loss = 2.67328
Step 101660: loss = 2.78224
Step 101665: loss = 2.92998
Step 101670: loss = 2.83647
Step 101675: loss = 3.05639
Step 101680: loss = 3.03454
Step 101685: loss = 3.01404
Step 101690: loss = 2.66833
Step 101695: loss = 2.98236
Step 101700: loss = 2.83263
Step 101705: loss = 2.87591
Step 101710: loss = 2.86609
Step 101715: loss = 2.94775
Step 101720: loss = 2.89595
Step 101725: loss = 2.85704
Step 101730: loss = 2.87525
Step 101735: loss = 3.03940
Step 101740: loss = 2.70528
Step 101745: loss = 2.94116
Step 101750: loss = 2.94815
Step 101755: loss = 2.74032
Step 101760: loss = 2.99611
Step 101765: loss = 2.74775
Step 101770: loss = 2.90510
Step 101775: loss = 2.70191
Step 101780: loss = 2.71881
Step 101785: loss = 3.01030
Step 101790: loss = 2.78618
Training Data Eval:
  Num examples: 49920, Num correct: 6791, Precision @ 1: 0.1360
('Testing Data Eval: EPOCH->', 262)
  Num examples: 9984, Num correct: 1428, Precision @ 1: 0.1430
Step 101795: loss = 2.71584
Step 101800: loss = 2.64319
Step 101805: loss = 3.38203
Step 101810: loss = 3.10339
Step 101815: loss = 3.09094
Step 101820: loss = 2.77983
Step 101825: loss = 2.89881
Step 101830: loss = 2.84757
Step 101835: loss = 2.90493
Step 101840: loss = 2.95280
Step 101845: loss = 2.86197
Step 101850: loss = 2.99537
Step 101855: loss = 2.94899
Step 101860: loss = 3.01056
Step 101865: loss = 2.77092
Step 101870: loss = 2.76164
Step 101875: loss = 2.68566
Step 101880: loss = 2.84585
Step 101885: loss = 2.77629
Step 101890: loss = 2.87049
Step 101895: loss = 2.75510
Step 101900: loss = 2.76991
Step 101905: loss = 2.87132
Step 101910: loss = 2.74054
Step 101915: loss = 2.75945
Step 101920: loss = 2.61337
Step 101925: loss = 3.07273
Step 101930: loss = 2.92921
Step 101935: loss = 2.89284
Step 101940: loss = 2.84608
Step 101945: loss = 3.04662
Step 101950: loss = 2.94108
Step 101955: loss = 2.94449
Step 101960: loss = 2.80041
Step 101965: loss = 2.81874
Step 101970: loss = 2.85072
Step 101975: loss = 3.13485
Step 101980: loss = 3.00587
Step 101985: loss = 2.76141
Step 101990: loss = 2.91182
Step 101995: loss = 3.06479
Step 102000: loss = 2.89445
Step 102005: loss = 3.04474
Step 102010: loss = 2.86041
Step 102015: loss = 2.85040
Step 102020: loss = 2.88967
Step 102025: loss = 2.70125
Step 102030: loss = 2.71814
Step 102035: loss = 2.86590
Step 102040: loss = 3.03502
Step 102045: loss = 2.97878
Step 102050: loss = 3.04107
Step 102055: loss = 2.99597
Step 102060: loss = 2.84307
Step 102065: loss = 3.01513
Step 102070: loss = 2.91774
Step 102075: loss = 2.96066
Step 102080: loss = 2.81570
Step 102085: loss = 3.09489
Step 102090: loss = 2.94744
Step 102095: loss = 2.76911
Step 102100: loss = 2.61098
Step 102105: loss = 2.80983
Step 102110: loss = 2.74226
Step 102115: loss = 2.87481
Step 102120: loss = 2.90472
Step 102125: loss = 3.08344
Step 102130: loss = 2.83388
Step 102135: loss = 2.91767
Step 102140: loss = 3.06980
Step 102145: loss = 2.76205
Step 102150: loss = 2.70988
Step 102155: loss = 2.91511
Step 102160: loss = 3.06382
Step 102165: loss = 2.87600
Step 102170: loss = 3.12176
Step 102175: loss = 2.76269
Step 102180: loss = 3.30770
Training Data Eval:
  Num examples: 49920, Num correct: 6652, Precision @ 1: 0.1333
('Testing Data Eval: EPOCH->', 263)
  Num examples: 9984, Num correct: 1321, Precision @ 1: 0.1323
Step 102185: loss = 2.91254
Step 102190: loss = 3.33254
Step 102195: loss = 2.95561
Step 102200: loss = 2.81296
Step 102205: loss = 3.13740
Step 102210: loss = 2.87666
Step 102215: loss = 2.97634
Step 102220: loss = 2.71329
Step 102225: loss = 2.88780
Step 102230: loss = 2.98798
Step 102235: loss = 2.96243
Step 102240: loss = 2.79272
Step 102245: loss = 3.06381
Step 102250: loss = 3.03177
Step 102255: loss = 3.04652
Step 102260: loss = 2.95519
Step 102265: loss = 2.66926
Step 102270: loss = 2.86308
Step 102275: loss = 3.12712
Step 102280: loss = 2.88687
Step 102285: loss = 2.80350
Step 102290: loss = 2.73421
Step 102295: loss = 2.79196
Step 102300: loss = 3.03584
Step 102305: loss = 2.85404
Step 102310: loss = 2.91688
Step 102315: loss = 2.89189
Step 102320: loss = 2.89446
Step 102325: loss = 3.04515
Step 102330: loss = 3.27389
Step 102335: loss = 3.07000
Step 102340: loss = 2.72374
Step 102345: loss = 3.00939
Step 102350: loss = 2.92441
Step 102355: loss = 2.87967
Step 102360: loss = 3.12890
Step 102365: loss = 2.98377
Step 102370: loss = 2.84260
Step 102375: loss = 3.33367
Step 102380: loss = 2.86380
Step 102385: loss = 2.87146
Step 102390: loss = 2.99029
Step 102395: loss = 2.94348
Step 102400: loss = 2.93087
Step 102405: loss = 2.96400
Step 102410: loss = 2.77043
Step 102415: loss = 2.84803
Step 102420: loss = 2.72272
Step 102425: loss = 3.15558
Step 102430: loss = 2.81863
Step 102435: loss = 2.96403
Step 102440: loss = 3.09373
Step 102445: loss = 2.89868
Step 102450: loss = 2.76271
Step 102455: loss = 3.06571
Step 102460: loss = 2.94999
Step 102465: loss = 3.32929
Step 102470: loss = 3.02743
Step 102475: loss = 3.06351
Step 102480: loss = 2.70941
Step 102485: loss = 2.53173
Step 102490: loss = 2.87645
Step 102495: loss = 3.01815
Step 102500: loss = 2.87210
Step 102505: loss = 2.84983
Step 102510: loss = 2.61767
Step 102515: loss = 2.80329
Step 102520: loss = 2.87263
Step 102525: loss = 2.99119
Step 102530: loss = 3.16863
Step 102535: loss = 2.98995
Step 102540: loss = 3.16052
Step 102545: loss = 2.96648
Step 102550: loss = 3.12225
Step 102555: loss = 2.95538
Step 102560: loss = 2.89237
Step 102565: loss = 3.00383
Step 102570: loss = 2.93225
Training Data Eval:
  Num examples: 49920, Num correct: 6780, Precision @ 1: 0.1358
('Testing Data Eval: EPOCH->', 264)
  Num examples: 9984, Num correct: 1309, Precision @ 1: 0.1311
Step 102575: loss = 3.13409
Step 102580: loss = 2.80736
Step 102585: loss = 2.96151
Step 102590: loss = 2.65781
Step 102595: loss = 3.21157
Step 102600: loss = 3.01220
Step 102605: loss = 2.79188
Step 102610: loss = 2.89862
Step 102615: loss = 2.79583
Step 102620: loss = 3.02677
Step 102625: loss = 2.79274
Step 102630: loss = 2.66075
Step 102635: loss = 3.03654
Step 102640: loss = 2.82583
Step 102645: loss = 2.87521
Step 102650: loss = 2.88766
Step 102655: loss = 2.96698
Step 102660: loss = 2.96029
Step 102665: loss = 2.75330
Step 102670: loss = 2.84202
Step 102675: loss = 2.92436
Step 102680: loss = 2.61269
Step 102685: loss = 2.71149
Step 102690: loss = 2.99223
Step 102695: loss = 2.92393
Step 102700: loss = 3.11718
Step 102705: loss = 3.10212
Step 102710: loss = 3.16276
Step 102715: loss = 2.90118
Step 102720: loss = 2.99396
Step 102725: loss = 2.90153
Step 102730: loss = 3.42084
Step 102735: loss = 3.13085
Step 102740: loss = 3.20597
Step 102745: loss = 3.13419
Step 102750: loss = 2.92523
Step 102755: loss = 3.13398
Step 102760: loss = 2.90543
Step 102765: loss = 3.02637
Step 102770: loss = 2.78814
Step 102775: loss = 2.76558
Step 102780: loss = 3.02395
Step 102785: loss = 2.84382
Step 102790: loss = 3.22609
Step 102795: loss = 3.08138
Step 102800: loss = 3.22567
Step 102805: loss = 3.22255
Step 102810: loss = 2.90097
Step 102815: loss = 2.90853
Step 102820: loss = 2.95373
Step 102825: loss = 2.82515
Step 102830: loss = 2.98643
Step 102835: loss = 3.03637
Step 102840: loss = 3.02655
Step 102845: loss = 3.15057
Step 102850: loss = 3.04950
Step 102855: loss = 2.95794
Step 102860: loss = 2.95074
Step 102865: loss = 2.85879
Step 102870: loss = 2.98448
Step 102875: loss = 3.04788
Step 102880: loss = 2.83733
Step 102885: loss = 2.90293
Step 102890: loss = 3.21542
Step 102895: loss = 3.21440
Step 102900: loss = 2.94717
Step 102905: loss = 2.86024
Step 102910: loss = 3.04550
Step 102915: loss = 2.81901
Step 102920: loss = 2.87364
Step 102925: loss = 2.82038
Step 102930: loss = 2.68765
Step 102935: loss = 2.86531
Step 102940: loss = 3.05540
Step 102945: loss = 3.30692
Step 102950: loss = 2.98885
Step 102955: loss = 3.06458
Step 102960: loss = 3.00578
Training Data Eval:
  Num examples: 49920, Num correct: 6633, Precision @ 1: 0.1329
('Testing Data Eval: EPOCH->', 265)
  Num examples: 9984, Num correct: 1371, Precision @ 1: 0.1373
Step 102965: loss = 3.28169
Step 102970: loss = 2.72132
Step 102975: loss = 3.20461
Step 102980: loss = 2.96274
Step 102985: loss = 2.76746
Step 102990: loss = 3.03344
Step 102995: loss = 2.87569
Step 103000: loss = 2.99475
Step 103005: loss = 3.18905
Step 103010: loss = 2.94731
Step 103015: loss = 2.79243
Step 103020: loss = 2.98257
Step 103025: loss = 2.93504
Step 103030: loss = 2.78287
Step 103035: loss = 2.90829
Step 103040: loss = 3.09845
Step 103045: loss = 3.11892
Step 103050: loss = 2.95908
Step 103055: loss = 3.00676
Step 103060: loss = 2.74428
Step 103065: loss = 3.02549
Step 103070: loss = 3.06701
Step 103075: loss = 2.97906
Step 103080: loss = 2.97679
Step 103085: loss = 2.75007
Step 103090: loss = 2.81507
Step 103095: loss = 2.94316
Step 103100: loss = 3.14712
Step 103105: loss = 2.85338
Step 103110: loss = 3.15607
Step 103115: loss = 3.02049
Step 103120: loss = 3.04186
Step 103125: loss = 2.70118
Step 103130: loss = 3.04035
Step 103135: loss = 3.10498
Step 103140: loss = 2.85107
Step 103145: loss = 2.73278
Step 103150: loss = 2.61348
Step 103155: loss = 2.89047
Step 103160: loss = 3.20902
Step 103165: loss = 3.22640
Step 103170: loss = 2.83848
Step 103175: loss = 2.90087
Step 103180: loss = 2.94579
Step 103185: loss = 2.83328
Step 103190: loss = 3.16127
Step 103195: loss = 3.05917
Step 103200: loss = 2.99649
Step 103205: loss = 2.98338
Step 103210: loss = 2.83520
Step 103215: loss = 2.72175
Step 103220: loss = 3.02179
Step 103225: loss = 2.96028
Step 103230: loss = 2.83802
Step 103235: loss = 2.85788
Step 103240: loss = 3.16043
Step 103245: loss = 2.89241
Step 103250: loss = 2.97412
Step 103255: loss = 3.06944
Step 103260: loss = 2.70970
Step 103265: loss = 2.88206
Step 103270: loss = 3.02163
Step 103275: loss = 2.97403
Step 103280: loss = 2.98848
Step 103285: loss = 2.78634
Step 103290: loss = 3.08570
Step 103295: loss = 2.71294
Step 103300: loss = 2.98811
Step 103305: loss = 3.01272
Step 103310: loss = 3.03039
Step 103315: loss = 3.03048
Step 103320: loss = 2.83793
Step 103325: loss = 3.08174
Step 103330: loss = 3.02457
Step 103335: loss = 3.00163
Step 103340: loss = 2.98683
Step 103345: loss = 2.67449
Step 103350: loss = 2.93794
Training Data Eval:
  Num examples: 49920, Num correct: 6619, Precision @ 1: 0.1326
('Testing Data Eval: EPOCH->', 266)
  Num examples: 9984, Num correct: 1395, Precision @ 1: 0.1397
Step 103355: loss = 3.14464
Step 103360: loss = 2.81578
Step 103365: loss = 2.98032
Step 103370: loss = 3.10807
Step 103375: loss = 3.03478
Step 103380: loss = 2.89175
Step 103385: loss = 2.96289
Step 103390: loss = 2.94145
Step 103395: loss = 3.17037
Step 103400: loss = 2.86981
Step 103405: loss = 3.17864
Step 103410: loss = 2.70754
Step 103415: loss = 2.88081
Step 103420: loss = 3.09608
Step 103425: loss = 2.87116
Step 103430: loss = 3.00288
Step 103435: loss = 2.91188
Step 103440: loss = 2.95500
Step 103445: loss = 2.82066
Step 103450: loss = 2.92279
Step 103455: loss = 2.91233
Step 103460: loss = 2.86578
Step 103465: loss = 2.93403
Step 103470: loss = 2.76864
Step 103475: loss = 2.98425
Step 103480: loss = 3.22405
Step 103485: loss = 2.80987
Step 103490: loss = 2.92300
Step 103495: loss = 2.97173
Step 103500: loss = 2.73744
Step 103505: loss = 3.02280
Step 103510: loss = 3.02425
Step 103515: loss = 3.00659
Step 103520: loss = 3.09539
Step 103525: loss = 2.89558
Step 103530: loss = 2.79191
Step 103535: loss = 2.74566
Step 103540: loss = 3.13179
Step 103545: loss = 3.06114
Step 103550: loss = 2.85742
Step 103555: loss = 3.13257
Step 103560: loss = 2.95112
Step 103565: loss = 2.82395
Step 103570: loss = 2.64298
Step 103575: loss = 2.81396
Step 103580: loss = 2.89468
Step 103585: loss = 2.80043
Step 103590: loss = 2.86316
Step 103595: loss = 2.81225
Step 103600: loss = 2.98430
Step 103605: loss = 2.83230
Step 103610: loss = 2.81215
Step 103615: loss = 2.87329
Step 103620: loss = 2.94463
Step 103625: loss = 2.89585
Step 103630: loss = 2.95375
Step 103635: loss = 2.66680
Step 103640: loss = 3.00425
Step 103645: loss = 2.69656
Step 103650: loss = 3.22314
Step 103655: loss = 2.85685
Step 103660: loss = 2.77886
Step 103665: loss = 2.98070
Step 103670: loss = 2.95701
Step 103675: loss = 3.13147
Step 103680: loss = 2.96011
Step 103685: loss = 3.05004
Step 103690: loss = 3.02190
Step 103695: loss = 2.89646
Step 103700: loss = 2.83083
Step 103705: loss = 3.15491
Step 103710: loss = 2.96945
Step 103715: loss = 3.04260
Step 103720: loss = 2.89727
Step 103725: loss = 2.84394
Step 103730: loss = 2.96679
Step 103735: loss = 3.13948
Step 103740: loss = 3.11519
Training Data Eval:
  Num examples: 49920, Num correct: 6660, Precision @ 1: 0.1334
('Testing Data Eval: EPOCH->', 267)
  Num examples: 9984, Num correct: 1414, Precision @ 1: 0.1416
Step 103745: loss = 2.89494
Step 103750: loss = 2.73332
Step 103755: loss = 3.19303
Step 103760: loss = 3.04536
Step 103765: loss = 2.91792
Step 103770: loss = 2.77690
Step 103775: loss = 3.13431
Step 103780: loss = 2.75711
Step 103785: loss = 2.71513
Step 103790: loss = 2.72103
Step 103795: loss = 2.76794
Step 103800: loss = 3.10360
Step 103805: loss = 2.90958
Step 103810: loss = 2.84733
Step 103815: loss = 2.89209
Step 103820: loss = 3.01038
Step 103825: loss = 2.94992
Step 103830: loss = 3.03588
Step 103835: loss = 2.73001
Step 103840: loss = 2.90749
Step 103845: loss = 2.92908
Step 103850: loss = 2.86450
Step 103855: loss = 2.67098
Step 103860: loss = 3.09541
Step 103865: loss = 3.05903
Step 103870: loss = 3.01008
Step 103875: loss = 2.83452
Step 103880: loss = 2.85517
Step 103885: loss = 2.98771
Step 103890: loss = 2.93603
Step 103895: loss = 3.02446
Step 103900: loss = 3.01948
Step 103905: loss = 2.77326
Step 103910: loss = 2.82487
Step 103915: loss = 2.96473
Step 103920: loss = 2.79003
Step 103925: loss = 2.78385
Step 103930: loss = 2.99566
Step 103935: loss = 2.95562
Step 103940: loss = 3.09539
Step 103945: loss = 2.95434
Step 103950: loss = 2.75520
Step 103955: loss = 3.08695
Step 103960: loss = 2.90675
Step 103965: loss = 3.21778
Step 103970: loss = 2.89914
Step 103975: loss = 2.96456
Step 103980: loss = 3.09877
Step 103985: loss = 2.98621
Step 103990: loss = 3.02786
Step 103995: loss = 2.89456
Step 104000: loss = 3.06711
Step 104005: loss = 2.95435
Step 104010: loss = 2.92386
Step 104015: loss = 3.03265
Step 104020: loss = 2.91045
Step 104025: loss = 2.88907
Step 104030: loss = 3.12466
Step 104035: loss = 3.02958
Step 104040: loss = 2.90853
Step 104045: loss = 2.98026
Step 104050: loss = 3.02291
Step 104055: loss = 3.21910
Step 104060: loss = 3.17542
Step 104065: loss = 2.97585
Step 104070: loss = 2.88003
Step 104075: loss = 2.98533
Step 104080: loss = 2.85333
Step 104085: loss = 2.79120
Step 104090: loss = 2.84826
Step 104095: loss = 3.00858
Step 104100: loss = 3.04882
Step 104105: loss = 3.06192
Step 104110: loss = 2.92726
Step 104115: loss = 2.83540
Step 104120: loss = 2.87078
Step 104125: loss = 2.76790
Step 104130: loss = 2.81502
Training Data Eval:
  Num examples: 49920, Num correct: 6545, Precision @ 1: 0.1311
('Testing Data Eval: EPOCH->', 268)
  Num examples: 9984, Num correct: 1362, Precision @ 1: 0.1364
Step 104135: loss = 2.90014
Step 104140: loss = 3.01164
Step 104145: loss = 2.87751
Step 104150: loss = 2.93134
Step 104155: loss = 2.98244
Step 104160: loss = 2.74620
Step 104165: loss = 3.30783
Step 104170: loss = 2.90909
Step 104175: loss = 2.79394
Step 104180: loss = 2.91391
Step 104185: loss = 3.01165
Step 104190: loss = 3.03232
Step 104195: loss = 2.86207
Step 104200: loss = 2.89136
Step 104205: loss = 2.93303
Step 104210: loss = 2.78816
Step 104215: loss = 2.83688
Step 104220: loss = 2.88794
Step 104225: loss = 3.14363
Step 104230: loss = 2.97344
Step 104235: loss = 3.03846
Step 104240: loss = 2.99364
Step 104245: loss = 2.81806
Step 104250: loss = 3.04665
Step 104255: loss = 3.17522
Step 104260: loss = 2.79147
Step 104265: loss = 3.26840
Step 104270: loss = 2.88715
Step 104275: loss = 2.76312
Step 104280: loss = 2.71510
Step 104285: loss = 2.82559
Step 104290: loss = 3.15636
Step 104295: loss = 2.85235
Step 104300: loss = 2.84804
Step 104305: loss = 2.86456
Step 104310: loss = 2.87151
Step 104315: loss = 3.16785
Step 104320: loss = 2.98422
Step 104325: loss = 2.96194
Step 104330: loss = 2.96593
Step 104335: loss = 2.85820
Step 104340: loss = 2.97332
Step 104345: loss = 3.09380
Step 104350: loss = 3.06620
Step 104355: loss = 2.99855
Step 104360: loss = 3.02877
Step 104365: loss = 3.23208
Step 104370: loss = 2.95901
Step 104375: loss = 2.88029
Step 104380: loss = 3.38105
Step 104385: loss = 3.09708
Step 104390: loss = 3.06308
Step 104395: loss = 3.03680
Step 104400: loss = 3.14477
Step 104405: loss = 3.17296
Step 104410: loss = 2.85085
Step 104415: loss = 3.04658
Step 104420: loss = 3.04288
Step 104425: loss = 2.94623
Step 104430: loss = 2.89429
Step 104435: loss = 2.80204
Step 104440: loss = 3.20027
Step 104445: loss = 3.03664
Step 104450: loss = 3.07461
Step 104455: loss = 2.71492
Step 104460: loss = 2.97186
Step 104465: loss = 2.88448
Step 104470: loss = 2.82172
Step 104475: loss = 2.75616
Step 104480: loss = 3.05403
Step 104485: loss = 3.09948
Step 104490: loss = 2.91998
Step 104495: loss = 2.93335
Step 104500: loss = 3.12989
Step 104505: loss = 2.91815
Step 104510: loss = 2.88975
Step 104515: loss = 2.98595
Step 104520: loss = 3.05734
Training Data Eval:
  Num examples: 49920, Num correct: 6537, Precision @ 1: 0.1309
('Testing Data Eval: EPOCH->', 269)
  Num examples: 9984, Num correct: 1379, Precision @ 1: 0.1381
Step 104525: loss = 2.81794
Step 104530: loss = 2.76952
Step 104535: loss = 2.68343
Step 104540: loss = 2.85377
Step 104545: loss = 3.04176
Step 104550: loss = 2.90067
Step 104555: loss = 3.15427
Step 104560: loss = 2.95971
Step 104565: loss = 2.96884
Step 104570: loss = 2.71183
Step 104575: loss = 2.82898
Step 104580: loss = 2.84424
Step 104585: loss = 2.98151
Step 104590: loss = 2.89459
Step 104595: loss = 2.76055
Step 104600: loss = 2.93121
Step 104605: loss = 2.88885
Step 104610: loss = 2.64747
Step 104615: loss = 2.68974
Step 104620: loss = 2.92440
Step 104625: loss = 2.96547
Step 104630: loss = 3.23227
Step 104635: loss = 2.86233
Step 104640: loss = 3.02949
Step 104645: loss = 2.77391
Step 104650: loss = 2.94527
Step 104655: loss = 2.93529
Step 104660: loss = 3.11547
Step 104665: loss = 2.98070
Step 104670: loss = 3.05179
Step 104675: loss = 3.04981
Step 104680: loss = 2.96608
Step 104685: loss = 3.00998
Step 104690: loss = 2.78717
Step 104695: loss = 2.75470
Step 104700: loss = 2.74464
Step 104705: loss = 3.16881
Step 104710: loss = 2.86584
Step 104715: loss = 2.97575
Step 104720: loss = 2.81210
Step 104725: loss = 2.74466
Step 104730: loss = 3.13519
Step 104735: loss = 2.98935
Step 104740: loss = 2.79138
Step 104745: loss = 3.16101
Step 104750: loss = 2.91208
Step 104755: loss = 2.84737
Step 104760: loss = 2.74211
Step 104765: loss = 2.79089
Step 104770: loss = 2.97290
Step 104775: loss = 3.03112
Step 104780: loss = 2.93029
Step 104785: loss = 3.05524
Step 104790: loss = 3.04911
Step 104795: loss = 2.88042
Step 104800: loss = 2.89586
Step 104805: loss = 2.96056
Step 104810: loss = 2.92574
Step 104815: loss = 2.86397
Step 104820: loss = 2.76800
Step 104825: loss = 2.90668
Step 104830: loss = 2.74721
Step 104835: loss = 2.89570
Step 104840: loss = 3.06182
Step 104845: loss = 2.86886
Step 104850: loss = 2.85665
Step 104855: loss = 3.08478
Step 104860: loss = 3.06275
Step 104865: loss = 3.07967
Step 104870: loss = 2.70583
Step 104875: loss = 2.88664
Step 104880: loss = 2.56974
Step 104885: loss = 2.95847
Step 104890: loss = 3.02695
Step 104895: loss = 2.73754
Step 104900: loss = 2.94323
Step 104905: loss = 3.14685
Step 104910: loss = 2.92453
Training Data Eval:
  Num examples: 49920, Num correct: 6889, Precision @ 1: 0.1380
('Testing Data Eval: EPOCH->', 270)
  Num examples: 9984, Num correct: 1379, Precision @ 1: 0.1381
Step 104915: loss = 2.79480
Step 104920: loss = 2.97296
Step 104925: loss = 2.95468
Step 104930: loss = 2.83383
Step 104935: loss = 2.92063
Step 104940: loss = 2.79038
Step 104945: loss = 2.95823
Step 104950: loss = 2.95112
Step 104955: loss = 2.69323
Step 104960: loss = 2.94393
Step 104965: loss = 2.95017
Step 104970: loss = 2.81728
Step 104975: loss = 2.81711
Step 104980: loss = 3.00813
Step 104985: loss = 2.95255
Step 104990: loss = 2.80197
Step 104995: loss = 2.82984
Step 105000: loss = 2.83363
Step 105005: loss = 2.88073
Step 105010: loss = 2.92565
Step 105015: loss = 2.61148
Step 105020: loss = 2.71412
Step 105025: loss = 3.04510
Step 105030: loss = 3.04731
Step 105035: loss = 2.98501
Step 105040: loss = 2.92913
Step 105045: loss = 3.06313
Step 105050: loss = 2.88056
Step 105055: loss = 2.91868
Step 105060: loss = 3.03667
Step 105065: loss = 3.04176
Step 105070: loss = 3.16572
Step 105075: loss = 2.91995
Step 105080: loss = 2.91146
Step 105085: loss = 2.81779
Step 105090: loss = 2.98543
Step 105095: loss = 3.00190
Step 105100: loss = 2.98574
Step 105105: loss = 3.25712
Step 105110: loss = 3.20740
Step 105115: loss = 2.85145
Step 105120: loss = 3.06713
Step 105125: loss = 2.93568
Step 105130: loss = 3.04576
Step 105135: loss = 3.12442
Step 105140: loss = 2.76452
Step 105145: loss = 2.95328
Step 105150: loss = 2.86127
Step 105155: loss = 2.86530
Step 105160: loss = 2.90186
Step 105165: loss = 2.85698
Step 105170: loss = 2.78462
Step 105175: loss = 2.79184
Step 105180: loss = 2.84922
Step 105185: loss = 3.27436
Step 105190: loss = 2.74877
Step 105195: loss = 2.74584
Step 105200: loss = 3.03896
Step 105205: loss = 3.07719
Step 105210: loss = 3.00222
Step 105215: loss = 3.18312
Step 105220: loss = 2.99731
Step 105225: loss = 2.93328
Step 105230: loss = 3.05240
Step 105235: loss = 2.82871
Step 105240: loss = 2.87966
Step 105245: loss = 3.13856
Step 105250: loss = 2.78866
Step 105255: loss = 2.88524
Step 105260: loss = 2.85869
Step 105265: loss = 2.68862
Step 105270: loss = 2.85501
Step 105275: loss = 2.83766
Step 105280: loss = 2.81007
Step 105285: loss = 2.76407
Step 105290: loss = 3.21751
Step 105295: loss = 3.14595
Step 105300: loss = 2.78705
Training Data Eval:
  Num examples: 49920, Num correct: 6790, Precision @ 1: 0.1360
('Testing Data Eval: EPOCH->', 271)
  Num examples: 9984, Num correct: 1452, Precision @ 1: 0.1454
Step 105305: loss = 2.90538
Step 105310: loss = 3.21863
Step 105315: loss = 2.90740
Step 105320: loss = 2.96937
Step 105325: loss = 2.99769
Step 105330: loss = 3.10180
Step 105335: loss = 2.98231
Step 105340: loss = 3.12809
Step 105345: loss = 3.07024
Step 105350: loss = 2.69399
Step 105355: loss = 2.79617
Step 105360: loss = 3.18640
Step 105365: loss = 2.71969
Step 105370: loss = 2.97115
Step 105375: loss = 3.03013
Step 105380: loss = 2.68695
Step 105385: loss = 2.67378
Step 105390: loss = 3.03010
Step 105395: loss = 2.86913
Step 105400: loss = 3.02559
Step 105405: loss = 2.94383
Step 105410: loss = 3.10475
Step 105415: loss = 3.00193
Step 105420: loss = 3.06532
Step 105425: loss = 2.96148
Step 105430: loss = 2.81673
Step 105435: loss = 2.79667
Step 105440: loss = 2.82847
Step 105445: loss = 3.24460
Step 105450: loss = 2.74147
Step 105455: loss = 2.96150
Step 105460: loss = 2.66392
Step 105465: loss = 2.92079
Step 105470: loss = 2.82674
Step 105475: loss = 2.83451
Step 105480: loss = 3.11828
Step 105485: loss = 3.05777
Step 105490: loss = 2.83628
Step 105495: loss = 2.88816
Step 105500: loss = 2.98782
Step 105505: loss = 2.99256
Step 105510: loss = 3.10780
Step 105515: loss = 2.75394
Step 105520: loss = 3.00941
Step 105525: loss = 2.81669
Step 105530: loss = 2.96627
Step 105535: loss = 3.09785
Step 105540: loss = 2.90809
Step 105545: loss = 2.76031
Step 105550: loss = 2.98732
Step 105555: loss = 2.92410
Step 105560: loss = 2.92691
Step 105565: loss = 2.68607
Step 105570: loss = 2.89920
Step 105575: loss = 2.75530
Step 105580: loss = 3.27784
Step 105585: loss = 2.96534
Step 105590: loss = 2.79110
Step 105595: loss = 2.92715
Step 105600: loss = 2.62751
Step 105605: loss = 3.00698
Step 105610: loss = 2.75211
Step 105615: loss = 2.73824
Step 105620: loss = 2.80904
Step 105625: loss = 2.95095
Step 105630: loss = 2.61012
Step 105635: loss = 2.92068
Step 105640: loss = 2.90886
Step 105645: loss = 2.88561
Step 105650: loss = 2.74659
Step 105655: loss = 2.97293
Step 105660: loss = 2.78856
Step 105665: loss = 2.76465
Step 105670: loss = 3.04835
Step 105675: loss = 2.91162
Step 105680: loss = 2.91385
Step 105685: loss = 2.87452
Step 105690: loss = 3.17340
Training Data Eval:
  Num examples: 49920, Num correct: 6651, Precision @ 1: 0.1332
('Testing Data Eval: EPOCH->', 272)
  Num examples: 9984, Num correct: 1322, Precision @ 1: 0.1324
Step 105695: loss = 2.73875
Step 105700: loss = 3.25527
Step 105705: loss = 2.83464
Step 105710: loss = 2.90581
Step 105715: loss = 3.05240
Step 105720: loss = 3.20376
Step 105725: loss = 2.91230
Step 105730: loss = 2.83674
Step 105735: loss = 2.94562
Step 105740: loss = 3.04385
Step 105745: loss = 3.02757
Step 105750: loss = 2.93886
Step 105755: loss = 2.81008
Step 105760: loss = 3.17671
Step 105765: loss = 2.88919
Step 105770: loss = 2.91217
Step 105775: loss = 3.03572
Step 105780: loss = 2.88142
Step 105785: loss = 3.09924
Step 105790: loss = 2.87292
Step 105795: loss = 2.96656
Step 105800: loss = 2.68630
Step 105805: loss = 3.05383
Step 105810: loss = 2.79488
Step 105815: loss = 3.02219
Step 105820: loss = 3.00651
Step 105825: loss = 2.91321
Step 105830: loss = 2.90489
Step 105835: loss = 3.04327
Step 105840: loss = 2.94902
Step 105845: loss = 3.32617
Step 105850: loss = 3.00405
Step 105855: loss = 2.90534
Step 105860: loss = 3.10128
Step 105865: loss = 2.72875
Step 105870: loss = 2.63620
Step 105875: loss = 2.93328
Step 105880: loss = 2.89745
Step 105885: loss = 2.74529
Step 105890: loss = 3.06732
Step 105895: loss = 3.09527
Step 105900: loss = 2.91604
Step 105905: loss = 2.73569
Step 105910: loss = 2.62963
Step 105915: loss = 3.00169
Step 105920: loss = 2.95224
Step 105925: loss = 2.83218
Step 105930: loss = 3.18135
Step 105935: loss = 2.91204
Step 105940: loss = 2.93842
Step 105945: loss = 2.87609
Step 105950: loss = 3.05368
Step 105955: loss = 2.88915
Step 105960: loss = 2.89952
Step 105965: loss = 2.83392
Step 105970: loss = 3.09463
Step 105975: loss = 2.90306
Step 105980: loss = 2.93323
Step 105985: loss = 3.06617
Step 105990: loss = 3.07014
Step 105995: loss = 2.85961
Step 106000: loss = 3.13826
Step 106005: loss = 2.85950
Step 106010: loss = 2.79870
Step 106015: loss = 3.22287
Step 106020: loss = 2.92385
Step 106025: loss = 2.97866
Step 106030: loss = 2.97789
Step 106035: loss = 2.80692
Step 106040: loss = 2.95457
Step 106045: loss = 3.09305
Step 106050: loss = 2.99652
Step 106055: loss = 2.86953
Step 106060: loss = 2.94436
Step 106065: loss = 2.90215
Step 106070: loss = 3.00542
Step 106075: loss = 2.89414
Step 106080: loss = 2.93497
Training Data Eval:
  Num examples: 49920, Num correct: 6906, Precision @ 1: 0.1383
('Testing Data Eval: EPOCH->', 273)
  Num examples: 9984, Num correct: 1373, Precision @ 1: 0.1375
Step 106085: loss = 2.68316
Step 106090: loss = 2.95711
Step 106095: loss = 2.67239
Step 106100: loss = 2.97483
Step 106105: loss = 2.91852
Step 106110: loss = 2.91214
Step 106115: loss = 2.81416
Step 106120: loss = 2.98370
Step 106125: loss = 3.05427
Step 106130: loss = 3.12475
Step 106135: loss = 3.10341
Step 106140: loss = 3.11850
Step 106145: loss = 3.11617
Step 106150: loss = 2.98657
Step 106155: loss = 2.87320
Step 106160: loss = 2.94712
Step 106165: loss = 2.86123
Step 106170: loss = 3.00376
Step 106175: loss = 2.77894
Step 106180: loss = 2.94789
Step 106185: loss = 3.03546
Step 106190: loss = 3.11693
Step 106195: loss = 2.88393
Step 106200: loss = 2.86997
Step 106205: loss = 2.93931
Step 106210: loss = 2.89213
Step 106215: loss = 3.04853
Step 106220: loss = 3.13326
Step 106225: loss = 2.72044
Step 106230: loss = 3.12093
Step 106235: loss = 2.92682
Step 106240: loss = 3.09799
Step 106245: loss = 2.94095
Step 106250: loss = 2.88517
Step 106255: loss = 3.10462
Step 106260: loss = 2.93643
Step 106265: loss = 3.02061
Step 106270: loss = 3.05455
Step 106275: loss = 2.84122
Step 106280: loss = 3.23253
Step 106285: loss = 3.15076
Step 106290: loss = 2.87784
Step 106295: loss = 3.05219
Step 106300: loss = 2.93276
Step 106305: loss = 2.78656
Step 106310: loss = 2.99943
Step 106315: loss = 2.91406
Step 106320: loss = 3.19530
Step 106325: loss = 3.15017
Step 106330: loss = 2.84510
Step 106335: loss = 2.74345
Step 106340: loss = 2.88872
Step 106345: loss = 2.84186
Step 106350: loss = 2.87103
Step 106355: loss = 2.90016
Step 106360: loss = 2.75910
Step 106365: loss = 2.92139
Step 106370: loss = 3.01925
Step 106375: loss = 2.88580
Step 106380: loss = 2.86162
Step 106385: loss = 2.80049
Step 106390: loss = 2.83038
Step 106395: loss = 2.94557
Step 106400: loss = 2.76245
Step 106405: loss = 3.02593
Step 106410: loss = 3.30195
Step 106415: loss = 2.92963
Step 106420: loss = 3.00401
Step 106425: loss = 2.74262
Step 106430: loss = 2.82110
Step 106435: loss = 2.95041
Step 106440: loss = 2.98386
Step 106445: loss = 2.82611
Step 106450: loss = 2.92061
Step 106455: loss = 2.92710
Step 106460: loss = 2.77497
Step 106465: loss = 2.86742
Step 106470: loss = 2.84262
Training Data Eval:
