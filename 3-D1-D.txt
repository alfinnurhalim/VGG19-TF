Student with Knowledge Distillation Approach
Step 0: loss = 51.20527
Training Data Eval:
  Num examples: 49920, Num correct: 5045, Precision @ 1: 0.1011
('Testing Data Eval: EPOCH->', 1)
  Num examples: 9984, Num correct: 1056, Precision @ 1: 0.1058
Step 5: loss = 31.94417
Step 10: loss = 27.70046
Step 15: loss = 17.56751
Step 20: loss = 16.26523
Step 25: loss = 14.00085
Step 30: loss = 9.25707
Step 35: loss = 8.00253
Step 40: loss = 6.41550
Step 45: loss = 5.11770
Step 50: loss = 5.67600
Step 55: loss = 4.18473
Step 60: loss = 7.74598
Step 65: loss = 4.60228
Step 70: loss = 5.47087
Step 75: loss = 5.95866
Step 80: loss = 5.21311
Step 85: loss = 5.28585
Step 90: loss = 5.16578
Step 95: loss = 7.76888
Step 100: loss = 6.61462
Step 105: loss = 6.93445
Step 110: loss = 4.24847
Step 115: loss = 3.66881
Step 120: loss = 5.58073
Step 125: loss = 3.05335
Step 130: loss = 3.13117
Step 135: loss = 3.87784
Step 140: loss = 3.54090
Step 145: loss = 3.48494
Step 150: loss = 3.20461
Step 155: loss = 2.55625
Step 160: loss = 2.72778
Step 165: loss = 4.07732
Step 170: loss = 5.31385
Step 175: loss = 2.72138
Step 180: loss = 3.87016
Step 185: loss = 2.96965
Step 190: loss = 2.37748
Step 195: loss = 3.77787
Step 200: loss = 2.71308
Step 205: loss = 5.28308
Step 210: loss = 3.74111
Step 215: loss = 2.72493
Step 220: loss = 6.67578
Step 225: loss = 2.92468
Step 230: loss = 3.54875
Step 235: loss = 3.16610
Step 240: loss = 3.82122
Step 245: loss = 3.90516
Step 250: loss = 2.40940
Step 255: loss = 3.98985
Step 260: loss = 2.68958
Step 265: loss = 3.87256
Step 270: loss = 2.86500
Step 275: loss = 3.08069
Step 280: loss = 2.49226
Step 285: loss = 2.34860
Step 290: loss = 4.51990
Step 295: loss = 3.76962
Step 300: loss = 3.53788
Step 305: loss = 3.63013
Step 310: loss = 2.94759
Step 315: loss = 2.66708
Step 320: loss = 2.61834
Step 325: loss = 2.70934
Step 330: loss = 2.70551
Step 335: loss = 3.03284
Step 340: loss = 3.46835
Step 345: loss = 5.01615
Step 350: loss = 3.12962
Step 355: loss = 2.90888
Step 360: loss = 2.99389
Step 365: loss = 3.09053
Step 370: loss = 3.11688
Step 375: loss = 3.11561
Step 380: loss = 2.66975
Step 385: loss = 2.33823
Step 390: loss = 4.51822
Training Data Eval:
  Num examples: 49920, Num correct: 11000, Precision @ 1: 0.2204
('Testing Data Eval: EPOCH->', 2)
  Num examples: 9984, Num correct: 2275, Precision @ 1: 0.2279
Step 395: loss = 3.31425
Step 400: loss = 4.02221
Step 405: loss = 2.50005
Step 410: loss = 2.95941
Step 415: loss = 2.73380
Step 420: loss = 3.21152
Step 425: loss = 2.59185
Step 430: loss = 2.30029
Step 435: loss = 2.66071
Step 440: loss = 1.99443
Step 445: loss = 2.19564
Step 450: loss = 3.11038
Step 455: loss = 3.64401
Step 460: loss = 3.16940
Step 465: loss = 2.45876
Step 470: loss = 2.33859
Step 475: loss = 3.33178
Step 480: loss = 2.28882
Step 485: loss = 2.44935
Step 490: loss = 2.05313
Step 495: loss = 1.99203
Step 500: loss = 2.68398
Step 505: loss = 1.91390
Step 510: loss = 1.84931
Step 515: loss = 2.01566
Step 520: loss = 2.45157
Step 525: loss = 1.81726
Step 530: loss = 1.81845
Step 535: loss = 1.80639
Step 540: loss = 2.42960
Step 545: loss = 3.02836
Step 550: loss = 2.42178
Step 555: loss = 2.83378
Step 560: loss = 2.24478
Step 565: loss = 1.84640
Step 570: loss = 2.59643
Step 575: loss = 2.23171
Step 580: loss = 2.22062
Step 585: loss = 2.22088
Step 590: loss = 2.01271
Step 595: loss = 2.79739
Step 600: loss = 2.04269
Step 605: loss = 2.04769
Step 610: loss = 1.85696
Step 615: loss = 1.94435
Step 620: loss = 1.94677
Step 625: loss = 1.90644
Step 630: loss = 4.19913
Step 635: loss = 2.34818
Step 640: loss = 2.00622
Step 645: loss = 1.98964
Step 650: loss = 1.77713
Step 655: loss = 1.91335
Step 660: loss = 2.25973
Step 665: loss = 1.80950
Step 670: loss = 1.77314
Step 675: loss = 3.03114
Step 680: loss = 2.29853
Step 685: loss = 2.54842
Step 690: loss = 1.79996
Step 695: loss = 3.20597
Step 700: loss = 1.82933
Step 705: loss = 2.99150
Step 710: loss = 2.30448
Step 715: loss = 2.41396
Step 720: loss = 1.80986
Step 725: loss = 3.21787
Step 730: loss = 1.82728
Step 735: loss = 2.06264
Step 740: loss = 2.40654
Step 745: loss = 1.75794
Step 750: loss = 1.94606
Step 755: loss = 2.04365
Step 760: loss = 2.13017
Step 765: loss = 2.40011
Step 770: loss = 1.81321
Step 775: loss = 1.76269
Step 780: loss = 2.50877
Training Data Eval:
  Num examples: 49920, Num correct: 16807, Precision @ 1: 0.3367
('Testing Data Eval: EPOCH->', 3)
  Num examples: 9984, Num correct: 3317, Precision @ 1: 0.3322
Step 785: loss = 1.65041
Step 790: loss = 2.68030
Step 795: loss = 2.14005
Step 800: loss = 2.19378
Step 805: loss = 1.81654
Step 810: loss = 2.44399
Step 815: loss = 1.73706
Step 820: loss = 1.88807
Step 825: loss = 1.84711
Step 830: loss = 1.76941
Step 835: loss = 2.14471
Step 840: loss = 2.20291
Step 845: loss = 1.77740
Step 850: loss = 1.97330
Step 855: loss = 1.84931
Step 860: loss = 2.52579
Step 865: loss = 1.71883
Step 870: loss = 2.77251
Step 875: loss = 2.60756
Step 880: loss = 2.01620
Step 885: loss = 2.55619
Step 890: loss = 2.02253
Step 895: loss = 2.23216
Step 900: loss = 2.23382
Step 905: loss = 1.77812
Step 910: loss = 2.29821
Step 915: loss = 1.76654
Step 920: loss = 1.75302
Step 925: loss = 1.82078
Step 930: loss = 2.11424
Step 935: loss = 1.62257
Step 940: loss = 1.81089
Step 945: loss = 1.96412
Step 950: loss = 1.90292
Step 955: loss = 1.63269
Step 960: loss = 1.95326
Step 965: loss = 1.73136
Step 970: loss = 1.96362
Step 975: loss = 1.89965
Step 980: loss = 1.75285
Step 985: loss = 2.26344
Step 990: loss = 1.64126
Step 995: loss = 1.69372
Step 1000: loss = 1.89965
Step 1005: loss = 1.91867
Step 1010: loss = 1.64644
Step 1015: loss = 1.71276
Step 1020: loss = 3.30911
Step 1025: loss = 1.72925
Step 1030: loss = 2.38326
Step 1035: loss = 1.57962
Step 1040: loss = 1.51060
Step 1045: loss = 1.89281
Step 1050: loss = 1.79860
Step 1055: loss = 2.53015
Step 1060: loss = 1.69836
Step 1065: loss = 1.79659
Step 1070: loss = 1.62857
Step 1075: loss = 1.77748
Step 1080: loss = 1.71505
Step 1085: loss = 1.74185
Step 1090: loss = 1.74434
Step 1095: loss = 1.66694
Step 1100: loss = 1.55457
Step 1105: loss = 1.70841
Step 1110: loss = 1.79293
Step 1115: loss = 1.64261
Step 1120: loss = 1.74289
Step 1125: loss = 2.39302
Step 1130: loss = 2.49476
Step 1135: loss = 1.70836
Step 1140: loss = 1.50876
Step 1145: loss = 1.50797
Step 1150: loss = 1.88292
Step 1155: loss = 1.72775
Step 1160: loss = 1.68225
Step 1165: loss = 1.96744
Step 1170: loss = 2.07473
Training Data Eval:
  Num examples: 49920, Num correct: 19592, Precision @ 1: 0.3925
('Testing Data Eval: EPOCH->', 4)
  Num examples: 9984, Num correct: 3710, Precision @ 1: 0.3716
Step 1175: loss = 1.99310
Step 1180: loss = 2.05168
Step 1185: loss = 1.71849
Step 1190: loss = 1.81992
Step 1195: loss = 1.74303
Step 1200: loss = 1.64738
Step 1205: loss = 1.65158
Step 1210: loss = 1.47020
Step 1215: loss = 2.39876
Step 1220: loss = 1.96252
Step 1225: loss = 2.25291
Step 1230: loss = 1.71635
Step 1235: loss = 2.22743
Step 1240: loss = 1.75959
Step 1245: loss = 1.51948
Step 1250: loss = 2.01240
Step 1255: loss = 1.44102
Step 1260: loss = 1.66101
Step 1265: loss = 1.44662
Step 1270: loss = 1.63179
Step 1275: loss = 1.61244
Step 1280: loss = 2.09212
Step 1285: loss = 2.14211
Step 1290: loss = 1.72236
Step 1295: loss = 1.63099
Step 1300: loss = 1.67176
Step 1305: loss = 2.21104
Step 1310: loss = 1.51177
Step 1315: loss = 1.56907
Step 1320: loss = 1.99292
Step 1325: loss = 1.45314
Step 1330: loss = 1.92605
Step 1335: loss = 1.85013
Step 1340: loss = 1.41102
Step 1345: loss = 2.04872
Step 1350: loss = 1.58036
Step 1355: loss = 1.59759
Step 1360: loss = 2.05957
Step 1365: loss = 2.15354
Step 1370: loss = 1.57306
Step 1375: loss = 1.48860
Step 1380: loss = 2.69435
Step 1385: loss = 1.66554
Step 1390: loss = 2.13400
Step 1395: loss = 1.74917
Step 1400: loss = 1.91344
Step 1405: loss = 1.91568
Step 1410: loss = 1.62059
Step 1415: loss = 1.57050
Step 1420: loss = 1.38137
Step 1425: loss = 1.65227
Step 1430: loss = 1.57064
Step 1435: loss = 1.43172
Step 1440: loss = 1.47825
Step 1445: loss = 1.65004
Step 1450: loss = 1.49018
Step 1455: loss = 1.39552
Step 1460: loss = 1.53496
Step 1465: loss = 1.62623
Step 1470: loss = 1.52007
Step 1475: loss = 1.66743
Step 1480: loss = 1.76187
Step 1485: loss = 2.39641
Step 1490: loss = 2.10081
Step 1495: loss = 1.56106
Step 1500: loss = 1.69741
Step 1505: loss = 1.44180
Step 1510: loss = 1.76538
Step 1515: loss = 1.58513
Step 1520: loss = 1.60585
Step 1525: loss = 1.47670
Step 1530: loss = 1.42625
Step 1535: loss = 1.80740
Step 1540: loss = 1.42801
Step 1545: loss = 1.43850
Step 1550: loss = 1.40527
Step 1555: loss = 1.51739
Step 1560: loss = 1.96475
Training Data Eval:
  Num examples: 49920, Num correct: 22215, Precision @ 1: 0.4450
('Testing Data Eval: EPOCH->', 5)
  Num examples: 9984, Num correct: 4256, Precision @ 1: 0.4263
Step 1565: loss = 1.59945
Step 1570: loss = 1.48481
Step 1575: loss = 1.43879
Step 1580: loss = 1.64762
Step 1585: loss = 1.38623
Step 1590: loss = 1.47826
Step 1595: loss = 1.97036
Step 1600: loss = 1.46498
Step 1605: loss = 1.57976
Step 1610: loss = 1.46663
Step 1615: loss = 1.73243
Step 1620: loss = 1.34358
Step 1625: loss = 1.89241
Step 1630: loss = 1.69319
Step 1635: loss = 1.62237
Step 1640: loss = 1.70348
Step 1645: loss = 1.60855
Step 1650: loss = 1.55483
Step 1655: loss = 1.50468
Step 1660: loss = 1.53201
Step 1665: loss = 1.48091
Step 1670: loss = 1.62188
Step 1675: loss = 1.63395
Step 1680: loss = 1.49908
Step 1685: loss = 1.47312
Step 1690: loss = 1.52032
Step 1695: loss = 1.57810
Step 1700: loss = 1.58599
Step 1705: loss = 1.41516
Step 1710: loss = 1.32462
Step 1715: loss = 1.32812
Step 1720: loss = 1.46942
Step 1725: loss = 1.35488
Step 1730: loss = 1.52508
Step 1735: loss = 1.41512
Step 1740: loss = 1.47084
Step 1745: loss = 1.31204
Step 1750: loss = 1.35619
Step 1755: loss = 1.46710
Step 1760: loss = 1.60855
Step 1765: loss = 1.47272
Step 1770: loss = 1.42731
Step 1775: loss = 1.54446
Step 1780: loss = 1.50208
Step 1785: loss = 1.52878
Step 1790: loss = 1.57096
Step 1795: loss = 1.54940
Step 1800: loss = 1.40275
Step 1805: loss = 1.48394
Step 1810: loss = 1.53387
Step 1815: loss = 1.31417
Step 1820: loss = 1.67245
Step 1825: loss = 1.48139
Step 1830: loss = 2.17802
Step 1835: loss = 1.50035
Step 1840: loss = 1.29593
Step 1845: loss = 1.57560
Step 1850: loss = 1.54804
Step 1855: loss = 1.43998
Step 1860: loss = 1.42717
Step 1865: loss = 1.93805
Step 1870: loss = 1.40774
Step 1875: loss = 1.26403
Step 1880: loss = 1.46660
Step 1885: loss = 1.53637
Step 1890: loss = 1.57891
Step 1895: loss = 1.39571
Step 1900: loss = 1.55054
Step 1905: loss = 1.46385
Step 1910: loss = 1.55418
Step 1915: loss = 1.48741
Step 1920: loss = 1.38264
Step 1925: loss = 1.41066
Step 1930: loss = 1.27142
Step 1935: loss = 1.36811
Step 1940: loss = 1.67379
Step 1945: loss = 1.55675
Step 1950: loss = 1.37723
Training Data Eval:
  Num examples: 49920, Num correct: 24478, Precision @ 1: 0.4903
('Testing Data Eval: EPOCH->', 6)
  Num examples: 9984, Num correct: 4505, Precision @ 1: 0.4512
Step 1955: loss = 1.50323
Step 1960: loss = 1.50989
Step 1965: loss = 1.49368
Step 1970: loss = 1.40605
Step 1975: loss = 1.37890
Step 1980: loss = 1.34810
Step 1985: loss = 1.35245
Step 1990: loss = 1.33589
Step 1995: loss = 1.31872
Step 2000: loss = 1.31731
Step 2005: loss = 1.62427
Step 2010: loss = 1.50327
Step 2015: loss = 1.24448
Step 2020: loss = 1.49511
Step 2025: loss = 1.39869
Step 2030: loss = 1.37712
Step 2035: loss = 1.20965
Step 2040: loss = 1.60279
Step 2045: loss = 1.50064
Step 2050: loss = 1.67643
Step 2055: loss = 1.34800
Step 2060: loss = 1.39105
Step 2065: loss = 1.30521
Step 2070: loss = 1.90195
Step 2075: loss = 1.32247
Step 2080: loss = 1.35482
Step 2085: loss = 1.64782
Step 2090: loss = 1.31644
Step 2095: loss = 1.40716
Step 2100: loss = 1.46281
Step 2105: loss = 1.49997
Step 2110: loss = 1.43600
Step 2115: loss = 1.37971
Step 2120: loss = 1.51443
Step 2125: loss = 1.30799
Step 2130: loss = 1.51946
Step 2135: loss = 1.36328
Step 2140: loss = 1.34868
Step 2145: loss = 1.34589
Step 2150: loss = 2.24524
Step 2155: loss = 1.51343
Step 2160: loss = 1.39190
Step 2165: loss = 1.34818
Step 2170: loss = 1.48057
Step 2175: loss = 1.26309
Step 2180: loss = 1.61016
Step 2185: loss = 1.22272
Step 2190: loss = 1.35003
Step 2195: loss = 1.43931
Step 2200: loss = 1.41687
Step 2205: loss = 1.53567
Step 2210: loss = 1.47426
Step 2215: loss = 1.55304
Step 2220: loss = 1.49493
Step 2225: loss = 1.31744
Step 2230: loss = 1.22027
Step 2235: loss = 1.45533
Step 2240: loss = 1.34623
Step 2245: loss = 1.79802
Step 2250: loss = 1.58645
Step 2255: loss = 1.34381
Step 2260: loss = 1.32474
Step 2265: loss = 1.56726
Step 2270: loss = 1.18323
Step 2275: loss = 1.16221
Step 2280: loss = 1.29410
Step 2285: loss = 1.30986
Step 2290: loss = 1.49351
Step 2295: loss = 1.26001
Step 2300: loss = 1.53488
Step 2305: loss = 1.61307
Step 2310: loss = 1.47981
Step 2315: loss = 1.38467
Step 2320: loss = 1.34867
Step 2325: loss = 1.56451
Step 2330: loss = 1.41766
Step 2335: loss = 1.55974
Step 2340: loss = 1.26803
Training Data Eval:
  Num examples: 49920, Num correct: 25884, Precision @ 1: 0.5185
('Testing Data Eval: EPOCH->', 7)
  Num examples: 9984, Num correct: 4766, Precision @ 1: 0.4774
Step 2345: loss = 1.64515
Step 2350: loss = 1.44023
Step 2355: loss = 1.46151
Step 2360: loss = 1.35160
Step 2365: loss = 1.32490
Step 2370: loss = 1.39610
Step 2375: loss = 1.55210
Step 2380: loss = 1.41519
Step 2385: loss = 1.24083
Step 2390: loss = 1.39746
Step 2395: loss = 1.37084
Step 2400: loss = 1.62799
Step 2405: loss = 1.34075
Step 2410: loss = 1.38233
Step 2415: loss = 1.81532
Step 2420: loss = 1.19758
Step 2425: loss = 1.35680
Step 2430: loss = 1.18979
Step 2435: loss = 1.53290
Step 2440: loss = 1.31938
Step 2445: loss = 1.34197
Step 2450: loss = 1.51938
Step 2455: loss = 1.34776
Step 2460: loss = 1.33223
Step 2465: loss = 1.25917
Step 2470: loss = 1.25243
Step 2475: loss = 1.50243
Step 2480: loss = 1.54701
Step 2485: loss = 1.24807
Step 2490: loss = 1.41496
Step 2495: loss = 1.25895
Step 2500: loss = 1.30628
Step 2505: loss = 1.34538
